diff --git a/src/raylight/distributed_worker/ray_worker.py b/src/raylight/distributed_worker/ray_worker.py
index 4414fd0..bc6b3b8 100644
--- a/src/raylight/distributed_worker/ray_worker.py
+++ b/src/raylight/distributed_worker/ray_worker.py
@@ -34,6 +34,13 @@ from ray.exceptions import RayActorError
 # If called from ray actor within. dist.barrier() become the sync.
 
 # Comfy cli args, does not get pass through into ray actor
+try:
+    import intel_extension_for_pytorch as ipex
+except:
+    pass
+
+# import oneccl_bindings_for_pytorch
+
 class RayWorker:
     def __init__(self, local_rank, device_id, parallel_dict):
         self.model = None
@@ -47,9 +54,13 @@ class RayWorker:
 
         self.device_id = device_id
         self.parallel_dict = parallel_dict
-        self.device = torch.device(f"cuda:{self.device_id}")
+        # self.device = torch.device(f"cuda:{self.device_id}")
         self.device_mesh = None
-        self.compute_capability = int("{}{}".format(*torch.cuda.get_device_capability()))
+        # self.compute_capability = int("{}{}".format(*torch.cuda.get_device_capability()))
+        self.compute_capability = 86  # For Intel GPU, just set to 8.6 for now
+
+        self.parallel_dict["is_fsdp_wrapped"] = False
+        self.device = torch.device(f"xpu:{self.device_id}")
 
         self.is_model_loaded = False
         self.is_cpu_offload = self.parallel_dict.get("fsdp_cpu_offload", False)
@@ -59,8 +70,16 @@ class RayWorker:
         os.environ["CUDA_VISIBLE_DEVICES"] = str(self.device_id)
 
         if sys.platform.startswith("linux"):
+            # dist.init_process_group(
+            #     "nccl",
+            #     rank=local_rank,
+            #     world_size=self.global_world_size,
+            #     timeout=timedelta(minutes=1),
+            #     # device_id=self.device
+            # )
+            torch.xpu.set_device(local_rank)
             dist.init_process_group(
-                "nccl",
+                "xccl",
                 rank=local_rank,
                 world_size=self.global_world_size,
                 timeout=timedelta(minutes=1),
@@ -78,7 +97,7 @@ class RayWorker:
 
         # (TODO-Komikndr) Should be modified so it can do support DP on top of FSDP
         if self.parallel_dict["is_xdit"] or self.parallel_dict["is_fsdp"]:
-            self.device_mesh = dist.device_mesh.init_device_mesh("cuda", mesh_shape=(self.global_world_size,))
+            self.device_mesh = dist.device_mesh.init_device_mesh("xpu", mesh_shape=(self.global_world_size,))
         else:
             print(f"Running Ray in normal seperate sampler with: {self.global_world_size} number of workers")
 
@@ -385,10 +404,10 @@ class RayWorker:
             out = latent.copy()
             out["samples"] = samples
 
-        if ray.get_runtime_context().get_accelerator_ids()["GPU"][0] and self.parallel_dict["is_fsdp"] == "0":
-            self.model.detach()
-        else:
-            self.model.detach()
+        # if ray.get_runtime_context().get_accelerator_ids()["GPU"][0] and self.parallel_dict["is_fsdp"] == "0":
+        #     self.model.detach()
+        # else:
+        self.model.detach()
         comfy.model_management.soft_empty_cache()
         gc.collect()
         return out
@@ -460,12 +479,12 @@ class RayWorker:
             out = latent.copy()
             out["samples"] = samples
 
-        if ray.get_runtime_context().get_accelerator_ids()["GPU"][0] and self.parallel_dict["is_fsdp"] == "0":
-            self.model.detach()
+        # if ray.get_runtime_context().get_accelerator_ids()["GPU"][0] and self.parallel_dict["is_fsdp"] == "0":
+        # self.model.detach()
 
         # I haven't implemented for non FSDP detached, so all rank model will be move into RAM
-        else:
-            self.model.detach()
+        # else:
+        self.model.detach()
         comfy.model_management.soft_empty_cache()
         gc.collect()
         return (out,)
@@ -473,29 +492,25 @@ class RayWorker:
 
 class RayCOMMTester:
     def __init__(self, local_rank, world_size, device_id):
-        device = torch.device(f"cuda:{device_id}")
-        os.environ["CUDA_VISIBLE_DEVICES"] = str(device_id)
+        local_rank = local_rank
+        world_size = world_size
+        device_id = device_id
+        # device = torch.device(f"cuda:{device_id}")
+        # os.environ["CUDA_VISIBLE_DEVICES"] = str(device_id)
+        device = torch.device(f"xpu:{device_id}")
+        os.environ["ZE_AFFINITY_MASK"] = str(device_id)
+
+        dist.init_process_group(
+            "xccl",
+            rank=local_rank,
+            world_size=world_size,
+            timeout=timedelta(minutes=1),
+            device_id=device
+        )
+        # pg = dist.group.WORLD
+        # cp.set_cp_group(pg, list(range(world_size)), local_rank)
 
-        if sys.platform.startswith("linux"):
-            dist.init_process_group(
-                "nccl",
-                rank=local_rank,
-                world_size=world_size,
-                timeout=timedelta(minutes=1),
-                # device_id=self.device
-            )
-        elif sys.platform.startswith("win"):
-            os.environ["USE_LIBUV"] = "0"
-            if local_rank == 0:
-                print("Windows detected, falling back to GLOO backend, consider using WSL, GLOO is slower than NCCL")
-            dist.init_process_group(
-                "gloo",
-                rank=local_rank,
-                world_size=world_size,
-                timeout=timedelta(minutes=1),
-                # device_id=self.device
-            )
-        print("Running COMM pre-run")
+        print("Running NCCL COMM pre-run")
 
         # Each rank contributes rank+1
         x = torch.ones(1, device=device) * (local_rank + 1)
@@ -525,7 +540,7 @@ def ray_nccl_tester(world_size):
 
     for local_rank in range(world_size):
         gpu_actors.append(
-            gpu_actor.options(num_gpus=1, name=f"RayTest:{local_rank}").remote(
+            gpu_actor.options(name=f"RayTest:{local_rank}").remote(
                 local_rank=local_rank,
                 world_size=world_size,
                 device_id=0,
@@ -552,7 +567,7 @@ def make_ray_actor_fn(
 
         for local_rank in range(world_size):
             gpu_actors.append(
-                gpu_actor.options(num_gpus=1, name=f"RayWorker:{local_rank}").remote(
+                gpu_actor.options(name=f"RayWorker:{local_rank}").remote(
                     local_rank=local_rank,
                     device_id=0,
                     parallel_dict=parallel_dict,
diff --git a/src/raylight/nodes.py b/src/raylight/nodes.py
index 27b39ef..483668d 100644
--- a/src/raylight/nodes.py
+++ b/src/raylight/nodes.py
@@ -174,9 +174,9 @@ class RayInitializer:
         _monkey()
 
         world_size = GPU
-        max_world_size = torch.cuda.device_count()
-        if world_size > max_world_size:
-            raise ValueError("Too many gpus")
+        #max_world_size = torch.xpu.device_count()
+        #if world_size > max_world_size:
+        #    raise ValueError("To many gpus")
         if world_size == 0:
             raise ValueError("Num of cuda/cudalike device is 0")
         if world_size < ulysses_degree * ring_degree * cfg_degree:
diff --git a/src/raylight/nodes_debug.py b/src/raylight/nodes_debug.py
index 0e0f2b8..ca56612 100644
--- a/src/raylight/nodes_debug.py
+++ b/src/raylight/nodes_debug.py
@@ -76,7 +76,8 @@ class RayInitializerDebug:
         self.parallel_dict = dict()
 
         world_size = GPU
-        max_world_size = torch.cuda.device_count()
+        # max_world_size = torch.cuda.device_count()
+        max_world_size = 8  # For Intel GPU, just set max to 8 for now
         if world_size > max_world_size:
             raise ValueError("Too many gpus")
         if world_size == 0:
