diff --git a/hymotion/pipeline/motion_diffusion.py b/hymotion/pipeline/motion_diffusion.py
index 0360a82..e75552e 100644
--- a/hymotion/pipeline/motion_diffusion.py
+++ b/hymotion/pipeline/motion_diffusion.py
@@ -76,7 +76,7 @@ def randn_tensor(
                     f" Tensors will be created on 'cpu' and then moved to {device}. Note that one can probably"
                     f" slighly speed up this function by passing a generator that was created on the {device} device."
                 )
-        elif gen_device_type != device.type and gen_device_type == "cuda":
+        elif gen_device_type != device.type and gen_device_type == "xpu":
             raise ValueError(f"Cannot generate a {device} tensor from a generator of type {gen_device_type}.")
 
     # make sure generator list of length 1 is treated like a non-list
@@ -621,7 +621,7 @@ if __name__ == "__main__":
 
     import torch
 
-    device = "cuda:0"
+    device = "xpu:0"
     bsz, input_dim = 64, 272
     seq_lens = [90, 180, 360]
     ctxt_seq_lens = 64
diff --git a/hymotion/utils/t2m_runtime.py b/hymotion/utils/t2m_runtime.py
index 97e05d2..fa64741 100644
--- a/hymotion/utils/t2m_runtime.py
+++ b/hymotion/utils/t2m_runtime.py
@@ -64,8 +64,8 @@ class T2MRuntime:
         if force_cpu:
             print(">>> [INFO] CPU mode enabled via HY_MOTION_DEVICE=cpu environment variable")
             self.device_ids = []
-        elif torch.cuda.is_available():
-            all_ids = list(range(torch.cuda.device_count()))
+        elif torch.xpu.is_available():
+            all_ids = list(range(torch.xpu.device_count()))
             self.device_ids = all_ids if device_ids is None else [i for i in device_ids if i in all_ids]
         else:
             self.device_ids = []
@@ -149,7 +149,7 @@ class T2MRuntime:
                     build_text_encoder=not self.skip_text,
                     allow_empty_ckpt=allow_empty_ckpt,
                 )
-                p.to(torch.device(f"cuda:{gid}"))
+                p.to(torch.device(f"xpu:{gid}"))
                 self.pipelines.append(p)
             self._gpu_load = [0] * len(self.pipelines)
 
@@ -310,8 +310,8 @@ class T2MRuntime:
                 )
         finally:
             self._release_pipeline(pi)
-            if torch.cuda.is_available():
-                torch.cuda.empty_cache()
+            if torch.xpu.is_available():
+                torch.xpu.empty_cache()
 
         ts = _now()
         save_data, base_filename = save_visualization_data(
diff --git a/hymotion/utils/type_converter.py b/hymotion/utils/type_converter.py
index 9f1d02a..ada3aab 100644
--- a/hymotion/utils/type_converter.py
+++ b/hymotion/utils/type_converter.py
@@ -16,7 +16,7 @@ def get_module_device(module: nn.Module) -> torch.device:
     except StopIteration:
         raise ValueError("The input module should contain parameters.")
 
-    if next(module.parameters()).is_cuda:
+    if next(module.parameters()).is_xpu:
         return torch.device(next(module.parameters()).get_device())
 
     return torch.device("cpu")
