diff --git a/cosyvoice/cli/frontend.py b/cosyvoice/cli/frontend.py
index 3d752ef..aeba457 100644
--- a/cosyvoice/cli/frontend.py
+++ b/cosyvoice/cli/frontend.py
@@ -70,7 +70,7 @@ class SpeechTokenizer:
                 else:
                     audio, sample_rate = torchaudio.load(utt)
 
-                audio = audio.cuda()
+                audio = audio.xpu()
 
                 # Resample to 16k if needed
                 if sample_rate != 16000:
@@ -78,7 +78,7 @@ class SpeechTokenizer:
                         _resample_buffer[sample_rate] = torchaudio.transforms.Resample(
                             orig_freq=sample_rate,
                             new_freq=16000
-                        ).to('cuda')
+                        ).to('xpu')
                     audio = _resample_buffer[sample_rate](audio)
 
                 audio = audio[0]  # Take first channel
@@ -99,9 +99,9 @@ class SpeechTokenizer:
 
             for start in range(0, len(audios), batch_size):
                 features = feature_extractor(audios[start: start + batch_size], sampling_rate=16000,
-                                             return_attention_mask=True, return_tensors="pt", device='cuda',
+                                             return_attention_mask=True, return_tensors="pt", device='xpu',
                                              padding="longest", pad_to_multiple_of=stride)
-                features = features.to(device="cuda")
+                features = features.to(device="xpu")
                 outputs = model(**features)
                 speech_tokens = outputs.quantized_token_ids
 
@@ -589,7 +589,7 @@ class TTSFrontEnd:
                  device=None):
 
         if device is None:
-            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
+            self.device = torch.device('xpu' if torch.xpu.is_available() else 'cpu')
         else:
             self.device = device
 
@@ -603,8 +603,8 @@ class TTSFrontEnd:
         
         # Determine providers based on availability
         providers = ['CPUExecutionProvider']
-        if torch.cuda.is_available():
-             providers.insert(0, ('CUDAExecutionProvider', {
+        if torch.xpu.is_available():
+             providers.insert(0, ('XPUExecutionProvider', {
                  'device_id': 0,
                  'arena_extend_strategy': 'kSameAsRequested',
                  'cudnn_conv_algo_search': 'DEFAULT',
diff --git a/cosyvoice/utils/file_utils.py b/cosyvoice/utils/file_utils.py
index 0fe5943..968430d 100644
--- a/cosyvoice/utils/file_utils.py
+++ b/cosyvoice/utils/file_utils.py
@@ -35,14 +35,14 @@ def read_json_lists(list_file):
 
 def load_wav(wav, target_sr):
     speech, sample_rate = torchaudio.load(wav)
-    speech = speech.to("cuda")
+    speech = speech.to("xpu")
     speech = speech.mean(dim=0, keepdim=True)
     if sample_rate != target_sr:
         resampler = torchaudio.transforms.Resample(
                     orig_freq=sample_rate,
                     new_freq=target_sr
-                ).to('cuda')
-        speech = resampler(speech).to("cuda")
+                ).to('xpu')
+        speech = resampler(speech).to("xpu")
     return speech
 
 def speed_change(waveform, sample_rate, speed_factor: str):
diff --git a/examples/example_zh.jsonl b/examples/example_zh.jsonl
index 2331be1..8ac6d55 100644
--- a/examples/example_zh.jsonl
+++ b/examples/example_zh.jsonl
@@ -1,10 +1,3 @@
 {"uttid": "0", "prompt_text": "他当时还跟线下其他的站姐吵架，然后，打架进局子了。", "prompt_speech": "examples/prompt/jiayan_zh.wav", "syn_text": "我看了《疯狂元素城》，里边水火元素谈恋爱的设定很妙！"}
-{"uttid": "1", "prompt_text": "他当时还跟线下其他的站姐吵架，然后，打架进局子了。", "prompt_speech": "examples/prompt/jiayan_zh.wav", "syn_text": "每次熬煮小米粥时，奶奶习惯加入一小把西洋参片，淡淡的药香融入粥里，口感温润，特别适合熬夜后调理身体。"}
-{"uttid": "2", "prompt_text": "他当时还跟线下其他的站姐吵架，然后，打架进局子了。", "prompt_speech": "examples/prompt/jiayan_zh.wav", "syn_text": "老街上的房屋参差错落，黛瓦与砖墙相映，时光仿佛在此刻放慢了脚步。"}
-{"uttid": "3", "prompt_text": "他当时还跟线下其他的站姐吵架，然后，打架进局子了。", "prompt_speech": "examples/prompt/jiayan_zh.wav", "syn_text": "她提前一周准备演讲稿，只为在参加校园辩论赛时，能清晰表达观点。"}
-{"uttid": "4", "prompt_text": "他当时还跟线下其他的站姐吵架，然后，打架进局子了。", "prompt_speech": "examples/prompt/jiayan_zh.wav", "syn_text": "弟弟蹦蹦跳跳地跑向操场，迫不及待要和小伙伴们踢球。"}
-{"uttid": "5", "prompt_text": "他当时还跟线下其他的站姐吵架，然后，打架进局子了。", "prompt_speech": "examples/prompt/jiayan_zh.wav", "syn_text": "他单独负责这项任务，从策划到执行亲力亲为，最终圆满完成，赢得了大家的一致认可。真是可喜可贺"}
-{"uttid": "6", "prompt_text": "他当时还跟线下其他的站姐吵架，然后，打架进局子了。", "prompt_speech": "examples/prompt/jiayan_zh.wav", "syn_text": "挺好挺好，最近天气不算太热，吹吹晚风应该很舒服。"}
-{"uttid": "7", "prompt_text": "他当时还跟线下其他的站姐吵架，然后，打架进局子了。", "prompt_speech": "examples/prompt/jiayan_zh.wav", "syn_text": "孩子们在操场上跑完步，纷纷跑到树荫下喝着凉开水，脸上洋溢着畅快的笑容。"}
-{"uttid": "8", "prompt_text": "他当时还跟线下其他的站姐吵架，然后，打架进局子了。", "prompt_speech": "examples/prompt/jiayan_zh.wav", "syn_text": "她专注地看着窗外，思绪不知飘向了何方，直到手机铃声响起才回过神来。"}
-{"uttid": "9", "prompt_text": "他当时还跟线下其他的站姐吵架，然后，打架进局子了。", "prompt_speech": "examples/prompt/jiayan_zh.wav", "syn_text": "我捡到一只超可爱的流浪猫。我给它取了一个名字，叫丁满。"}
\ No newline at end of file
+{"uttid": "1", "prompt_text": "他当时还跟线下其他的站姐吵架，然后，打架进局子了。", "prompt_speech": "examples/prompt/jiayan_zh.wav", "syn_text": "第一题：在作业前步骤1中，将窑头皮带控制柜面板小车旋钮开关切换至“就地”的目的是什么？"}
+{"uttid": "2", "prompt_text": "他当时还跟线下其他的站姐吵架，然后，打架进局子了。", "prompt_speech": "examples/prompt/jiayan_zh.wav", "syn_text": "第二题：将窑头皮带控制柜面板小车旋钮开关打到“______”位置，以实现手动操作。"}
\ No newline at end of file
diff --git a/glmtts_inference.py b/glmtts_inference.py
index 80c1902..1d4633a 100644
--- a/glmtts_inference.py
+++ b/glmtts_inference.py
@@ -29,7 +29,7 @@ from utils.audio import mel_spectrogram
 from functools import partial
 # --- Global Constants ---
 CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
-DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
+DEVICE = torch.device("xpu" if torch.xpu.is_available() else "cpu")
 MAX_LLM_SEQ_INP_LEN = 750
 TOKEN_RATE = 25
 EOS_TOKEN_ID_AFTER_MINUS_BOS = None
diff --git a/requirements.txt b/requirements.txt
index a0c9293..a2562f6 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -3,20 +3,16 @@ scipy==1.15.3
 sentencepiece==0.1.99
 soundfile==0.12.1
 tiktoken==0.6.0
-torch==2.3.1
-torchaudio==2.3.1
-torchvision==0.18.1
 tqdm==4.66.4
 train==0.0.5
 transformers==4.57.3
 wandb==0.16.6
-WeTextProcessing==1.0.3
 x_transformers==1.42.26
 zhconv==1.4.3
 zhon==2.1.1
 
 inflect==7.3.1
-onnxruntime_gpu==1.19.0
+onnxruntime==1.20.0
 openai_whisper==20231117
 contractions==0.1.73
 debugpy==1.8.0
@@ -40,4 +36,10 @@ pypinyin==0.44.0
 PyYAML==6.0.3
 regex==2023.12.25
 Requests==2.32.5
-safetensors==0.7.0
\ No newline at end of file
+safetensors==0.7.0
+soxr
+ruamel.yaml==0.17.40
+torchcodec
+protobuf==4.25.0
+grpcio<1.63.0
+grpcio-tools<1.63.0
\ No newline at end of file
diff --git a/utils/file_utils.py b/utils/file_utils.py
index a586dcb..f250d38 100644
--- a/utils/file_utils.py
+++ b/utils/file_utils.py
@@ -17,14 +17,14 @@ import os
 
 def load_wav(wav, target_sr):
     speech, sample_rate = torchaudio.load(wav)
-    speech = speech.to("cuda")
+    speech = speech.to("xpu")
     speech = speech.mean(dim=0, keepdim=True)
     if sample_rate != target_sr:
         resampler = torchaudio.transforms.Resample(
                     orig_freq=sample_rate,
                     new_freq=target_sr
-                ).to('cuda')
-        speech = resampler(speech).to("cuda")
+                ).to('xpu')
+        speech = resampler(speech).to("xpu")
     return speech
 
 def get_jsonl(jsonl_file_path=None):
diff --git a/utils/hift_util.py b/utils/hift_util.py
index 66a0235..e83f21b 100644
--- a/utils/hift_util.py
+++ b/utils/hift_util.py
@@ -22,7 +22,7 @@ from cosyvoice.hifigan_cosy2.generator import HiFTGenerator
 class HiFTInference:
     def __init__(self, 
                  ckpt_path: Union[str, pathlib.Path], 
-                 device: Union[str, torch.device] = 'cuda',
+                 device: Union[str, torch.device] = 'xpu',
                  load_only_nsf: bool = False):
         """
         Wrapper class for HiFTGenerator inference.
@@ -78,8 +78,8 @@ class HiFTInference:
             raise FileNotFoundError(f"Checkpoint not found at {ckpt_path}")
         
         # Clear CUDA cache to free up memory before loading
-        if torch.cuda.is_available():
-            torch.cuda.empty_cache()
+        if torch.xpu.is_available():
+            torch.xpu.empty_cache()
 
         state_dict = torch.load(ckpt_path, map_location=self.device)
 
@@ -126,7 +126,7 @@ class HiFTInference:
             center=False
         )
 
-def load_hift(device: str = "cuda", load_only_nsf: bool = False) -> HiFTInference:
+def load_hift(device: str = "xpu", load_only_nsf: bool = False) -> HiFTInference:
     """Factory function to load HiFT model."""
     # Update this path to your actual relative path for the open source release
     ckpt_path = 'ckpt/hift/hift.pt'
diff --git a/utils/seed_util.py b/utils/seed_util.py
index c5d9583..acf5594 100755
--- a/utils/seed_util.py
+++ b/utils/seed_util.py
@@ -21,7 +21,7 @@ def set_seed(seed):
     numpy.random.seed(seed)
 
     torch.manual_seed(seed)
-    torch.cuda.manual_seed(seed)
+    torch.xpu.manual_seed(seed)
 
     torch.backends.cudnn.deterministic = True
     torch.backends.cudnn.benchmark = False
diff --git a/utils/tts_model_util.py b/utils/tts_model_util.py
index 002ce1b..56ac53a 100644
--- a/utils/tts_model_util.py
+++ b/utils/tts_model_util.py
@@ -18,7 +18,7 @@ from utils.vocos_util import load_vocos_jit
 from utils.hift_util import load_hift
 
 class Token2Wav:
-    def __init__(self, flow, sample_rate: int = 24000, device: str = "cuda"):
+    def __init__(self, flow, sample_rate: int = 24000, device: str = "xpu"):
         self.device = device
         self.flow = flow
         self.input_frame_rate = flow.input_frame_rate
diff --git a/utils/vocos_util.py b/utils/vocos_util.py
index 5a9d616..b3ca1b0 100644
--- a/utils/vocos_util.py
+++ b/utils/vocos_util.py
@@ -53,7 +53,7 @@ class Vocos2DInference:
         xs_mel_tr: torch.Tensor = xs_mel.transpose(-1, -2) - MEL_LOGDIFF
         return xs_mel_tr
 
-def load_vocos_jit(device: str = "cuda") -> Vocos2DInference:
+def load_vocos_jit(device: str = "xpu") -> Vocos2DInference:
     """Factory function to load Vocos model"""
     ckpt_path = 'ckpt/vocos2d/generator_jit.ckpt'
     print(f"Loading Vocos JIT model from {ckpt_path} on {device}...")
diff --git a/utils/whisper_models/modeling_whisper.py b/utils/whisper_models/modeling_whisper.py
index b2d8eff..34fc79e 100644
--- a/utils/whisper_models/modeling_whisper.py
+++ b/utils/whisper_models/modeling_whisper.py
@@ -1302,7 +1302,7 @@ class WhisperVQEncoder(WhisperPreTrainedModel):
                         self.num_active_codes = n.nonzero().shape[0]
                         if self.config.quantize_ema_decay:
                             hidden_flat = hidden_states.detach().float().reshape(-1, hidden_states.shape[-1])
-                            with torch.autocast(device_type='cuda', dtype=torch.float32):
+                            with torch.autocast(device_type='xpu', dtype=torch.float32):
                                 dw = torch.matmul(encodings.t(), hidden_flat)
                             torch.distributed.all_reduce(dw, op=torch.distributed.ReduceOp.SUM)
                             self.ema_count = self.ema_count * self.config.quantize_ema_decay + (
@@ -1729,7 +1729,7 @@ class WhisperVQDecoder(WhisperPreTrainedModel):
         if (
                 self.config._attn_implementation == "sdpa"
                 and attention_mask is not None
-                and attention_mask.device.type == "cuda"
+                and attention_mask.device.type == "xpu"
                 and not output_attentions
         ):
             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when
diff --git a/utils/yaml_util.py b/utils/yaml_util.py
index 4cbb2d2..fe3087a 100644
--- a/utils/yaml_util.py
+++ b/utils/yaml_util.py
@@ -56,7 +56,7 @@ def load_quantize_encoder(model_path):
                     state_dict[new_key] = f.get_tensor(key)
     model.load_state_dict(state_dict)
     model.eval()
-    model.cuda()
+    model.xpu()
     return model
 
 def load_speech_tokenizer(model_path):
