From 8bfd3ac94570692bef46a63fd0ec2afc7bdff898 Mon Sep 17 00:00:00 2001
From: Kunshang Ji <kunshang.ji@intel.com>
Date: Mon, 24 Mar 2025 18:03:11 +0800
Subject: [PATCH 01/36] fix lm_eval default params

Signed-off-by: Kunshang Ji <kunshang.ji@intel.com>
---
 .buildkite/lm-eval-harness/run-lm-eval-gsm-vllm-baseline.sh | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/.buildkite/lm-eval-harness/run-lm-eval-gsm-vllm-baseline.sh b/.buildkite/lm-eval-harness/run-lm-eval-gsm-vllm-baseline.sh
index 1b617ff17..091f60b51 100644
--- a/.buildkite/lm-eval-harness/run-lm-eval-gsm-vllm-baseline.sh
+++ b/.buildkite/lm-eval-harness/run-lm-eval-gsm-vllm-baseline.sh
@@ -46,6 +46,6 @@ while getopts "m:b:l:f:t:" OPT; do
 done
 
 lm_eval --model vllm \
-  --model_args "pretrained=$MODEL,tensor_parallel_size=$TP_SIZE,add_bos_token=true,trust_remote_code=true,max_model_len=4096" \
+  --model_args "pretrained=$MODEL,tensor_parallel_size=$TP_SIZE,add_bos_token=true,distributed_executor_backend=mp,trust_remote_code=true,max_model_len=4096,enforce_eager=true,max_num_batched_tokens=4096" \
   --tasks gsm8k --num_fewshot "$FEWSHOT" --limit "$LIMIT" \
   --batch_size "$BATCH_SIZE"
-- 
2.43.0


From 53a080a990a83c7e61531e48f9ad05a384d9fb8e Mon Sep 17 00:00:00 2001
From: yan ma <yan.ma@intel.com>
Date: Thu, 20 Feb 2025 00:17:57 +0800
Subject: [PATCH 02/36] update gptq example

Signed-off-by: yan ma <yan.ma@intel.com>

Signed-off-by: Yan Ma <yan.ma@intel.com>
---
 tests/quantization/test_ipex_quant.py | 4 ++--
 1 file changed, 2 insertions(+), 2 deletions(-)

diff --git a/tests/quantization/test_ipex_quant.py b/tests/quantization/test_ipex_quant.py
index 4f3c52df6..65febae59 100644
--- a/tests/quantization/test_ipex_quant.py
+++ b/tests/quantization/test_ipex_quant.py
@@ -26,7 +26,7 @@ DTYPE = ["bfloat16"]
 @pytest.mark.parametrize("model", MODELS)
 @pytest.mark.parametrize("dtype", DTYPE)
 def test_ipex_quant(vllm_runner, model, dtype):
-    with vllm_runner(model, dtype=dtype, enforce_eager=True) as llm:
-        output = llm.generate_greedy(["The capital of France is"], max_tokens=4)
+    with vllm_runner(model, dtype=dtype, enforce_eager=True, block_size=64) as llm:
+        output = llm.generate_greedy(["The capital of France is"], max_tokens=32)
     assert output
     print(output)
-- 
2.43.0


From c0dce603c064fefdaffb7d3b05875dffb208c94d Mon Sep 17 00:00:00 2001
From: yan <yan.ma@intel.com>
Date: Thu, 24 Jul 2025 02:33:45 +0000
Subject: [PATCH 03/36] update sliding window UT

Signed-off-by: yan <yan.ma@intel.com>

Signed-off-by: Yan Ma <yan.ma@intel.com>
---
 tests/v1/e2e/test_correctness_sliding_window.py | 9 ++++-----
 1 file changed, 4 insertions(+), 5 deletions(-)

diff --git a/tests/v1/e2e/test_correctness_sliding_window.py b/tests/v1/e2e/test_correctness_sliding_window.py
index b6a78eaa0..2e215acb6 100644
--- a/tests/v1/e2e/test_correctness_sliding_window.py
+++ b/tests/v1/e2e/test_correctness_sliding_window.py
@@ -5,7 +5,6 @@ from dataclasses import dataclass
 import pytest
 
 from vllm import LLM, SamplingParams
-from vllm.platforms import current_platform
 
 from ...utils import check_answers, prep_prompts
 
@@ -18,7 +17,7 @@ class TestConfig:
 
 model_config = {
     "bigcode/starcoder2-3b": TestConfig(4096, (800, 1100)),
-    "google/gemma-3-1b-it": TestConfig(4096, (400, 800)),
+    # "google/gemma-3-1b-it": TestConfig(4096, (400, 800)),
 }
 
 
@@ -26,7 +25,7 @@ model_config = {
     "model",
     [
         "bigcode/starcoder2-3b",  # sliding window only
-        "google/gemma-3-1b-it",  # sliding window + full attention
+        # "google/gemma-3-1b-it",  # sliding window + full attention
     ],
 )
 @pytest.mark.parametrize("batch_size", [5])
@@ -44,14 +43,14 @@ def test_sliding_window_retrieval(
     # NOTE: For ROCm, we have to enforce eager mode to use custom kernel
     # implementation of GELU with tanh approximation, as PyTorch's native
     # implementation is currently unstable with torch.compile and produces garbage.
-    enforce_eager = current_platform.is_rocm()
 
     test_config = model_config[model]
 
     llm = LLM(
         model=model,
         disable_hybrid_kv_cache_manager=disable_hybrid_kv_cache_manager,
-        enforce_eager=enforce_eager,
+        enforce_eager=True,
+        block_size=64,
     )
     sampling_params = SamplingParams(temperature=0.0, max_tokens=100)
 
-- 
2.43.0


From 8ad9b0747167fe8f66ba96a250923347432c69e5 Mon Sep 17 00:00:00 2001
From: Yan Ma <yan.ma@intel.com>
Date: Wed, 10 Sep 2025 06:13:27 +0000
Subject: [PATCH 04/36] set test_output_len=10 to avoid long prompt test run

Signed-off-by: Yan Ma <yan.ma@intel.com>
---
 vllm/benchmarks/serve.py | 3 ++-
 1 file changed, 2 insertions(+), 1 deletion(-)

diff --git a/vllm/benchmarks/serve.py b/vllm/benchmarks/serve.py
index 9d6298927..00546dd5c 100644
--- a/vllm/benchmarks/serve.py
+++ b/vllm/benchmarks/serve.py
@@ -643,7 +643,8 @@ async def benchmark(
     test_prompt, test_prompt_len, test_output_len, test_mm_content = (
         input_requests[0].prompt,
         input_requests[0].prompt_len,
-        input_requests[0].expected_output_len,
+        #input_requests[0].expected_output_len,
+        10,
         input_requests[0].multi_modal_data,
     )
 
-- 
2.43.0


From 496629b30d6403191244ee4113245a2758d71874 Mon Sep 17 00:00:00 2001
From: yan <yanma1@habana.ai>
Date: Tue, 20 May 2025 05:27:11 +0000
Subject: [PATCH 05/36] add VLLM_XPU_FP8_DTYPE env for fp8 online quantization

Signed-off-by: Yan Ma <yan.ma@intel.com>
---
 vllm/envs.py          | 3 +++
 vllm/platforms/xpu.py | 6 +++++-
 2 files changed, 8 insertions(+), 1 deletion(-)

diff --git a/vllm/envs.py b/vllm/envs.py
index d77c1e9d9..6bc8cbf52 100755
--- a/vllm/envs.py
+++ b/vllm/envs.py
@@ -252,6 +252,7 @@ if TYPE_CHECKING:
     VLLM_USE_V2_MODEL_RUNNER: bool = False
     VLLM_LOG_MODEL_INSPECTION: bool = False
     VLLM_DEBUG_MFU_METRICS: bool = False
+    VLLM_XPU_FP8_DTYPE: str = "e5m2"
 
 
 def get_default_cache_root():
@@ -1611,6 +1612,8 @@ environment_variables: dict[str, Callable[[], Any]] = {
     "VLLM_DEBUG_MFU_METRICS": lambda: bool(
         int(os.getenv("VLLM_DEBUG_MFU_METRICS", "0"))
     ),
+    # fp8 dtype for XPU platform
+    "VLLM_XPU_FP8_DTYPE": lambda: os.environ.get("VLLM_XPU_FP8_DTYPE", "e5m2"),
 }
 
 # --8<-- [end:env-vars-definition]
diff --git a/vllm/platforms/xpu.py b/vllm/platforms/xpu.py
index b2d7bf38d..3f3c26dfe 100644
--- a/vllm/platforms/xpu.py
+++ b/vllm/platforms/xpu.py
@@ -7,6 +7,7 @@ from typing import TYPE_CHECKING, Optional
 
 import torch
 
+import vllm.envs as envs
 from vllm.logger import init_logger
 from vllm.v1.attention.backends.registry import AttentionBackendEnum
 
@@ -206,7 +207,10 @@ class XPUPlatform(Platform):
 
     @classmethod
     def fp8_dtype(cls) -> torch.dtype:
-        return torch.float8_e5m2
+        if envs.VLLM_XPU_FP8_DTYPE == "e4m3":
+            return torch.float8_e4m3fn
+        else:
+            return torch.float8_e5m2
 
     @classmethod
     def is_data_center_gpu(cls) -> bool:
-- 
2.43.0


From 671299c02cf3e681c1da5989ca43175c5207f660 Mon Sep 17 00:00:00 2001
From: yan <yan.ma@intel.com>
Date: Thu, 24 Jul 2025 03:29:57 +0000
Subject: [PATCH 06/36] skip dummy sampler run when use spec decode

Signed-off-by: yan <yan.ma@intel.com>

Signed-off-by: Yan Ma <yan.ma@intel.com>
---
 vllm/v1/worker/gpu_model_runner.py | 5 ++++-
 vllm/v1/worker/gpu_worker.py       | 2 +-
 2 files changed, 5 insertions(+), 2 deletions(-)

diff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py
index 525ad5db4..3d73b777e 100644
--- a/vllm/v1/worker/gpu_model_runner.py
+++ b/vllm/v1/worker/gpu_model_runner.py
@@ -431,6 +431,9 @@ class GPUModelRunner(
         # NOTE(Jiayi): currently we put the entire draft model on
         # the last PP rank. This is not ideal if there are many
         # layers in the draft model.
+        self.use_spec_decode = False
+        if self.speculative_config:
+            self.use_spec_decode = True
         if self.speculative_config and get_pp_group().is_last_rank:
             self.drafter: (
                 NgramProposer | SuffixDecodingProposer | EagleProposer | MedusaProposer
@@ -4743,7 +4746,7 @@ class GPUModelRunner(
         hidden_states, last_hidden_states = self._dummy_run(
             self.max_num_tokens, is_profile=True
         )
-        if get_pp_group().is_last_rank:
+        if get_pp_group().is_last_rank and not self.use_spec_decode:
             if self.is_pooling_model:
                 output = self._dummy_pooler_run(hidden_states)
             else:
diff --git a/vllm/v1/worker/gpu_worker.py b/vllm/v1/worker/gpu_worker.py
index 562664524..3cc998c45 100644
--- a/vllm/v1/worker/gpu_worker.py
+++ b/vllm/v1/worker/gpu_worker.py
@@ -529,7 +529,7 @@ class Worker(WorkerBase):
             )
             if self.model_runner.is_pooling_model:
                 self.model_runner._dummy_pooler_run(hidden_states)
-            else:
+            elif not self.model_runner.use_spec_decode:
                 self.model_runner._dummy_sampler_run(hidden_states=last_hidden_states)
 
         # Reset the seed to ensure that the random state is not affected by
-- 
2.43.0


From a0c84aa72d5e7129362c666343b0311282dbc381 Mon Sep 17 00:00:00 2001
From: Yan Ma <yanma1@habana.ai>
Date: Mon, 23 Jun 2025 18:46:19 +0800
Subject: [PATCH 07/36] increase AIOHTTP_TIMEOUT for long context test (#226)

---
 benchmarks/backend_request_func.py | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/benchmarks/backend_request_func.py b/benchmarks/backend_request_func.py
index 831b76b66..6bebc7aee 100644
--- a/benchmarks/backend_request_func.py
+++ b/benchmarks/backend_request_func.py
@@ -17,7 +17,7 @@ from transformers import AutoTokenizer, PreTrainedTokenizer, PreTrainedTokenizer
 # NOTE(simon): do not import vLLM here so the benchmark script
 # can run without vLLM installed.
 
-AIOHTTP_TIMEOUT = aiohttp.ClientTimeout(total=6 * 60 * 60)
+AIOHTTP_TIMEOUT = aiohttp.ClientTimeout(total=2 * 6 * 60 * 60)
 
 
 @dataclass
-- 
2.43.0


From 767d9929b8249906216ff6690500167596cad837 Mon Sep 17 00:00:00 2001
From: Yan Ma <yan.ma@intel.com>
Date: Sun, 31 Aug 2025 12:50:14 +0000
Subject: [PATCH 08/36] enable microsoft/Phi-4-multimodal-instruct

Signed-off-by: Yan Ma <yan.ma@intel.com>
---
 vllm/model_executor/models/phi4mm_audio.py | 12 ++++++------
 1 file changed, 6 insertions(+), 6 deletions(-)

diff --git a/vllm/model_executor/models/phi4mm_audio.py b/vllm/model_executor/models/phi4mm_audio.py
index 493fdb465..bedd6812e 100644
--- a/vllm/model_executor/models/phi4mm_audio.py
+++ b/vllm/model_executor/models/phi4mm_audio.py
@@ -587,10 +587,10 @@ class TransformerEncoderBase(abc.ABC, nn.Module):
         enc_streaming_mask = self._streaming_mask(
             seq_len, batch_size, self.chunk_size, self.left_chunk
         )
-
-        if xs_pad.is_cuda:
-            enc_streaming_mask = enc_streaming_mask.cuda()
-            xs_pad = xs_pad.cuda()
+        device = xs_pad.device
+        if device.type != "cpu":
+            enc_streaming_mask = enc_streaming_mask.to(device)
+            xs_pad = xs_pad.to(device)
 
         input_tensor = xs_pad
         input_tensor, masks = self._forward_embeddings_core(input_tensor, masks)
@@ -607,8 +607,8 @@ class TransformerEncoderBase(abc.ABC, nn.Module):
             enc_streaming_mask_nc = self._streaming_mask(
                 seq_len, batch_size, chunk_size_nc, left_chunk_nc
             )
-            if xs_pad.is_cuda:
-                enc_streaming_mask_nc = enc_streaming_mask_nc.cuda()
+            if device.type != "cpu":
+                enc_streaming_mask_nc = enc_streaming_mask_nc.to(device)
             if masks is not None:
                 hs_mask_nc = masks & enc_streaming_mask_nc
             else:
-- 
2.43.0


From 08ca56be7517eb831c2db38f9f7a1a67d2269347 Mon Sep 17 00:00:00 2001
From: wenjun liu <wenjun.liu@intel.com>
Date: Wed, 30 Jul 2025 13:43:56 +0800
Subject: [PATCH 09/36] Enable 2508 release CI (#269)

* add ci workflow files

* 1

* 1

* add docker prune for building

* 1

* 1

* 1

* 1

* 1

* 1

* 1

* 1

* add cache clean

* add cache clean

* add cache clean

* add cache clean

* only for release branch enable

* fix results upload

* 1

* fix proxy

* 1

* change proxy for bmg
---
 .github/workflows/ci.yaml     | 185 ++++++++++++++++++++++++++++++++++
 .github/workflows/ci_pvc.yaml | 181 +++++++++++++++++++++++++++++++++
 2 files changed, 366 insertions(+)
 create mode 100644 .github/workflows/ci.yaml
 create mode 100644 .github/workflows/ci_pvc.yaml

diff --git a/.github/workflows/ci.yaml b/.github/workflows/ci.yaml
new file mode 100644
index 000000000..aef250abe
--- /dev/null
+++ b/.github/workflows/ci.yaml
@@ -0,0 +1,185 @@
+name: Run Intel XPU BMG CI
+
+on:
+  pull_request:
+    branches:
+      - '**release**'  
+    types: [opened, synchronize, reopened]  # 
+
+jobs:
+  run-xpu-BMG-CI:
+    if: |
+        github.event_name == 'pull_request' ||
+        (github.event_name == 'issue_comment' && 
+        github.event.issue.pull_request &&
+        contains(github.event.comment.body, '/BMG_CI'))
+    runs-on: BMG 
+
+    steps:
+    - name: Fix workspace permissions
+      run: |
+        sudo chown -R $(whoami):$(whoami) "$GITHUB_WORKSPACE"
+        sudo chmod -R 755 "$GITHUB_WORKSPACE"
+        sudo rm -f "$GITHUB_WORKSPACE/.git/index.lock" || true
+
+    - name: Checkout QA_ci (test code)  #
+      uses: actions/checkout@v4
+      with:
+        ref: QA_ci
+        path: qa_ci_code
+
+    - name: Checkout PR + Release Branch (DUT code)
+      uses: actions/checkout@v4
+      with:
+        ref: ${{ github.event.pull_request.head.ref }}
+        path: target_code
+        fetch-depth: 0  # 
+
+    - name: Merge PR into Release Branch
+      run: |
+        cd target_code
+        git fetch origin ${{ github.base_ref }}
+        git merge origin/${{ github.base_ref }} --no-commit
+      shell: bash
+
+    - name: Build docker image
+      run: |
+        echo "start to build image"
+        cd target_code
+        if [ -n "${{ github.event.pull_request.number }}" ]; then
+          image_name="vllm_xpu_ci_${{ github.event.pull_request.number }}"
+        else
+          image_name="vllm_xpu_ci_$(echo $GITHUB_REF | awk -F '/' '{print $3}')"
+        fi
+        image_name=$(echo "$image_name" | tr '[:upper:]' '[:lower:]')
+        #!/bin/bash
+
+        # Configuration
+        MAX_RETRIES=6                      # Maximum number of retry attempts
+        TIMEOUT=1800                       # 30-minute timeout per attempt (in seconds)
+        LOG_FILE="docker_build.log"         # Log file path
+
+        # Proxy configurations - add more if needed
+        PROXIES=(
+          "http://child-prc.intel.com:913"    # First fallback
+          "http://proxy.ims.intel.com:911"    # Primary proxy
+          "http://child-prc.intel.com:913"    # First fallback
+        )
+
+        # No-proxy configuration
+        NO_PROXY=".intel.com,intel.com,localhost,127.0.0.1"
+        #docker builder prune -f  #clean cache     
+        docker builder prune --all --force
+
+        #Loop through proxy configurations
+        for (( attempt=1; attempt<=$MAX_RETRIES; attempt++ )); do
+          proxy_index=$(( (attempt-1) % ${#PROXIES[@]} ))
+          proxy=${PROXIES[$proxy_index]}
+          echo "=== Attempt $attempt/$MAX_RETRIES (Proxy: $proxy) ===" | tee -a "$LOG_FILE"
+          
+          if [ $attempt -eq 1 ]; then
+            # First attempt without no_proxy
+            timeout $TIMEOUT docker build \
+              --build-arg http_proxy=$proxy \
+              --build-arg https_proxy=$proxy \
+              -f docker/Dockerfile.xpu \
+              -t "$image_name" \
+              --shm-size=4g . 2>&1 | tee -a "$LOG_FILE"
+          else
+            # Subsequent attempts with no_proxy
+            timeout $TIMEOUT docker build \
+              --build-arg http_proxy=$proxy \
+              --build-arg https_proxy=$proxy \
+              --build-arg no_proxy="$NO_PROXY" \
+              -f docker/Dockerfile.xpu \
+              -t "$image_name" \
+              --shm-size=4g . 2>&1 | tee -a "$LOG_FILE"
+          fi
+
+          # Check if build succeeded
+          if [ ${PIPESTATUS[0]} -eq 0 ]; then
+            echo "=== Build succeeded on attempt $attempt ===" | tee -a "$LOG_FILE"
+            exit 0
+          fi
+        done
+
+        echo "=== ERROR: All $MAX_RETRIES attempts failed. Check $LOG_FILE for details. ===" | tee -a "$LOG_FILE"
+        exit 1
+
+    - name: Prepare environment (clean up old processes and containers)
+      run: |
+        echo "Killing any process on port 8000..."
+        lsof -t -i:8000 | xargs -r kill -9 || true
+
+        echo "Killing old vllm server processes..."
+        pkill -f "python3 -m vllm.entrypoints.openai.api_server" || true
+
+        echo "Removing old container if exists..."
+        docker rm -f vllm_internal_ci || true
+    
+    - name: Run benchmark inside local Docker image
+      run: |
+        # Reuse the image_name from previous step
+        if [ -n "${{ github.event.pull_request.number }}" ]; then
+          image_name="vllm_xpu_ci_${{ github.event.pull_request.number }}"
+        else
+          image_name="vllm_xpu_ci_$(echo $GITHUB_REF | awk -F '/' '{print $3}')"
+        fi
+        image_name=$(echo "$image_name" | tr '[:upper:]' '[:lower:]')
+
+        echo "Running benchmark using image: $image_name"
+        docker run -t --rm --name vllm_internal_ci --shm-size 10g \
+          --net=host \
+          --ipc=host \
+          --privileged \
+          -v ${HOME}/actions-runner/_work/vllm-xpu/vllm-xpu/qa_ci_code:/WORKSPACE \
+          -v /dev/dri/by-path:/dev/dri/by-path \
+          -v ${HOME}/.cache:/root/.cache/ \
+          -e http_proxy=${http_proxy:-"http://proxy-dmz.intel.com:912"} \
+          -e https_proxy=${http_proxy:-"http://proxy-dmz.intel.com:912"} \
+          -e no_proxy=${no_proxy:-"127.0.0.1,localhost"} \
+          --device /dev/dri:/dev/dri \
+          -w /workspace \
+          --entrypoint='' \
+          --mount type=bind,source="$HOME/.secrets/my_token",target=/run/secrets/my_token,readonly \
+          $image_name \
+          bash -c "bash /WORKSPACE/.buildkite/nightly-benchmarks/scripts/CI_run_server_benchmarks.sh BMG || true; chown -R \$(id -u):\$(id -g) /WORKSPACE"
+
+    - name: Validate server benchmark results
+      run: |
+            python3 ${HOME}/actions-runner/_work/vllm-xpu/vllm-xpu/qa_ci_code/.buildkite/nightly-benchmarks/scripts/analyze_benchmark_results_final.py --test-selector BMG
+            cat ${HOME}/actions-runner/_work/vllm-xpu/vllm-xpu/qa_ci_code/benchmarks/results/benchmark_analysis_final.json
+
+    - name: Fix permissions
+      run: sudo chmod -R 755 ${{ runner.workspace }}/vllm-xpu/qa_ci_code/benchmarks/results/
+    
+    - name: Debug path
+      run: ls -la ${{ runner.workspace }}/vllm-xpu/qa_ci_code/benchmarks/results/
+
+    - name: Upload benchmark results
+      if: always()
+      uses: actions/upload-artifact@v4
+      with:
+        name: benchmark-results
+        path: ${{ runner.workspace }}/vllm-xpu/qa_ci_code/benchmarks/results/
+
+    - name: Analyze and validate benchmark results
+      if: always()
+      run: |
+        RESULTS_FILE="$HOME/actions-runner/_work/vllm-xpu/vllm-xpu/qa_ci_code/benchmarks/results/benchmark_analysis_final.json"
+        if [ ! -f "$RESULTS_FILE" ]; then
+          echo "‚ùå Benchmark analysis file not found!"
+          exit 1
+        fi
+        
+        echo "üìä Benchmark Results:"
+        cat "$RESULTS_FILE"
+        FAILURES=$(jq -r '.[] | select(.function != "pass") | .case_name' "$RESULTS_FILE")
+        
+        if [ -n "$FAILURES" ]; then
+          echo "‚ùå Failed cases detected:"
+          echo "$FAILURES"
+          exit 1
+        else
+          echo "‚úÖ All benchmarks passed"
+        fi
diff --git a/.github/workflows/ci_pvc.yaml b/.github/workflows/ci_pvc.yaml
new file mode 100644
index 000000000..eaa2f332a
--- /dev/null
+++ b/.github/workflows/ci_pvc.yaml
@@ -0,0 +1,181 @@
+name: Run Intel XPU PVC CI
+
+on:
+  pull_request:
+    branches:
+      - '**release**' 
+    types: [opened, synchronize, reopened]  # 
+
+jobs:
+  run-xpu-PVC-CI:
+    runs-on: PVC
+
+    steps:
+    - name: Fix workspace permissions
+      run: |
+        sudo chown -R $(whoami):$(whoami) "$GITHUB_WORKSPACE"
+        sudo chmod -R 755 "$GITHUB_WORKSPACE"
+        sudo rm -f "$GITHUB_WORKSPACE/.git/index.lock" || true
+
+    - name: Checkout QA_ci (test code)  # 
+      uses: actions/checkout@v4
+      with:
+        ref: QA_ci
+        path: qa_ci_code
+
+    - name: Checkout PR + Release Branch (DUT code)
+      uses: actions/checkout@v4
+      with:
+        ref: ${{ github.event.pull_request.head.ref }}
+        path: target_code
+        fetch-depth: 0  # 
+
+    # 
+    - name: Merge PR into Release Branch
+      run: |
+        cd target_code
+        git fetch origin ${{ github.base_ref }}
+        git merge origin/${{ github.base_ref }} --no-commit
+      shell: bash
+
+    - name: Build docker image
+      run: |
+        echo "start to build image"
+        cd target_code
+        if [ -n "${{ github.event.pull_request.number }}" ]; then
+          image_name="vllm_xpu_ci_${{ github.event.pull_request.number }}"
+        else
+          image_name="vllm_xpu_ci_$(echo $GITHUB_REF | awk -F '/' '{print $3}')"
+        fi
+        image_name=$(echo "$image_name" | tr '[:upper:]' '[:lower:]')
+        #!/bin/bash
+
+        # Configuration
+        MAX_RETRIES=6                      # Maximum number of retry attempts
+        TIMEOUT=1800                       # 30-minute timeout per attempt (in seconds)
+        LOG_FILE="docker_build.log"         # Log file path
+
+        # Proxy configurations - add more if needed
+        PROXIES=(
+          "http://proxy.ims.intel.com:911"    # Primary proxy
+          "http://child-prc.intel.com:913"    # First fallback
+        )
+
+        # No-proxy configuration
+        NO_PROXY=".intel.com,intel.com,localhost,127.0.0.1"
+        #docker builder prune -f  #clean cache  
+        docker builder prune --all --force
+  
+        # Loop through proxy configurations
+        for (( attempt=1; attempt<=$MAX_RETRIES; attempt++ )); do
+                  proxy_index=$(( (attempt-1) % ${#PROXIES[@]} ))
+                  proxy=${PROXIES[$proxy_index]}
+                  echo "=== Attempt $attempt/$MAX_RETRIES (Proxy: $proxy) ===" | tee -a "$LOG_FILE"
+                  
+                  if [ $attempt -eq 1 ]; then
+                    # First attempt without no_proxy
+                    timeout $TIMEOUT docker build \
+                      --build-arg http_proxy=$proxy \
+                      --build-arg https_proxy=$proxy \
+                      -f docker/Dockerfile.xpu \
+                      -t "$image_name" \
+                      --shm-size=4g . 2>&1 | tee -a "$LOG_FILE"
+                  else
+                    # Subsequent attempts with no_proxy
+                    timeout $TIMEOUT docker build \
+                      --build-arg http_proxy=$proxy \
+                      --build-arg https_proxy=$proxy \
+                      --build-arg no_proxy="$NO_PROXY" \
+                      -f docker/Dockerfile.xpu \
+                      -t "$image_name" \
+                      --shm-size=4g . 2>&1 | tee -a "$LOG_FILE"
+                  fi
+
+                  # Check if build succeeded
+                  if [ ${PIPESTATUS[0]} -eq 0 ]; then
+                    echo "=== Build succeeded on attempt $attempt ===" | tee -a "$LOG_FILE"
+                    exit 0
+                  fi
+                done
+
+        echo "=== ERROR: All $MAX_RETRIES attempts failed. Check $LOG_FILE for details. ===" | tee -a "$LOG_FILE"
+        exit 1
+
+    - name: Prepare environment (clean up old processes and containers)
+      run: |
+        echo "Killing any process on port 8000..."
+        lsof -t -i:8000 | xargs -r kill -9 || true
+
+        echo "Killing old vllm server processes..."
+        pkill -f "python3 -m vllm.entrypoints.openai.api_server" || true
+
+        echo "Removing old container if exists..."
+        docker rm -f vllm_internal_ci || true
+    
+    - name: Run benchmark inside local Docker image
+      run: |
+        # Reuse the image_name from previous step
+        if [ -n "${{ github.event.pull_request.number }}" ]; then
+          image_name="vllm_xpu_ci_${{ github.event.pull_request.number }}"
+        else
+          image_name="vllm_xpu_ci_$(echo $GITHUB_REF | awk -F '/' '{print $3}')"
+        fi
+        image_name=$(echo "$image_name" | tr '[:upper:]' '[:lower:]')
+
+        echo "Running benchmark using image: $image_name"
+        docker run -t --rm --name vllm_internal_ci --shm-size 10g \
+          --net=host \
+          --ipc=host \
+          --privileged \
+          -v $HOME/actions-runner/_work/vllm-xpu/vllm-xpu/qa_ci_code:/WORKSPACE \
+          -v /dev/dri/by-path:/dev/dri/by-path \
+          -v /mnt/data3:/root/.cache/ \
+          -e http_proxy=${http_proxy:-"http://proxy-dmz.intel.com:912"} \
+          -e https_proxy=${http_proxy:-"http://proxy-dmz.intel.com:912"} \
+          -e no_proxy=${no_proxy:-"127.0.0.1,localhost"} \
+          --device /dev/dri:/dev/dri \
+          -w /workspace \
+          --entrypoint='' \
+          --mount type=bind,source="$HOME/.secrets/my_token",target=/run/secrets/my_token,readonly \
+          $image_name \
+          bash -c "IPEX_FMHA_V3=0 bash /WORKSPACE/.buildkite/nightly-benchmarks/scripts/CI_run_server_benchmarks.sh "PVC" || true; chown -R \$(id -u):\$(id -g) /WORKSPACE"
+
+    - name: Validate server benchmark results
+      run: |
+        python3 $HOME/actions-runner/_work/vllm-xpu/vllm-xpu/qa_ci_code/.buildkite/nightly-benchmarks/scripts/analyze_benchmark_results_final.py --test-selector "PVC"
+        cat $HOME/actions-runner/_work/vllm-xpu/vllm-xpu/qa_ci_code/benchmarks/results/benchmark_analysis_final.json
+
+    - name: Fix permissions
+      run: sudo chmod -R 755 ${{ runner.workspace }}/vllm-xpu/qa_ci_code/benchmarks/results/
+    
+    - name: Debug path
+      run: ls -la ${{ runner.workspace }}/vllm-xpu/qa_ci_code/benchmarks/results/
+
+    - name: Upload benchmark results
+      if: always()
+      uses: actions/upload-artifact@v4
+      with:
+        name: benchmark-results
+        path: ${{ runner.workspace }}/vllm-xpu/qa_ci_code/benchmarks/results/
+
+    - name: Analyze and validate benchmark results
+      if: always()
+      run: |
+        RESULTS_FILE="$HOME/actions-runner/_work/vllm-xpu/vllm-xpu/qa_ci_code/benchmarks/results/benchmark_analysis_final.json"
+        if [ ! -f "$RESULTS_FILE" ]; then
+          echo "‚ùå Benchmark analysis file not found!"
+          exit 1
+        fi
+        
+        echo "üìä Benchmark Results:"
+        cat "$RESULTS_FILE"
+        
+        FAILURES=$(jq -r '.[] | select(.function != "pass") | .case_name' "$RESULTS_FILE")
+        
+        if [ -n "$FAILURES" ]; then
+          echo "‚ùå Failed cases detected:"
+          echo "$FAILURES"
+          exit 1
+        else
+          echo "‚úÖ All benchmarks passed"
+        fi
-- 
2.43.0


From 3bb9b313f96eefb160651bc3e299bbd9d4b22474 Mon Sep 17 00:00:00 2001
From: Yan Ma <yan.ma@intel.com>
Date: Sun, 31 Aug 2025 13:32:44 +0000
Subject: [PATCH 10/36] add example scripts for validation

---
 examples/bmg/reasoning.py                     | 27 ++++++++++++++
 examples/bmg/tooling.py                       | 37 +++++++++++++++++++
 .../structured_outputs/structured_outputs.py  |  4 +-
 3 files changed, 67 insertions(+), 1 deletion(-)
 create mode 100644 examples/bmg/reasoning.py
 create mode 100644 examples/bmg/tooling.py

diff --git a/examples/bmg/reasoning.py b/examples/bmg/reasoning.py
new file mode 100644
index 000000000..04f91786e
--- /dev/null
+++ b/examples/bmg/reasoning.py
@@ -0,0 +1,27 @@
+from openai import OpenAI
+
+# Modify OpenAI's API key and API base to use vLLM's API server.
+openai_api_key = "EMPTY"
+openai_api_base = "http://0.0.0.0:8000/v1"
+
+client = OpenAI(
+    api_key=openai_api_key,
+    base_url=openai_api_base,
+)
+
+models = client.models.list()
+model = models.data[0].id
+
+# Round 1
+messages = [{"role": "user", "content": "9.11 and 9.8, which is greater?"}]
+# For granite, add: `extra_body={"chat_template_kwargs": {"thinking": True}}`
+# For Qwen3 series, if you want to disable thinking in reasoning mode, add:
+# extra_body={"chat_template_kwargs": {"enable_thinking": False}}
+response = client.chat.completions.create(model=model, messages=messages)
+
+reasoning_content = response.choices[0].message.reasoning_content
+content = response.choices[0].message.content
+
+print("reasoning_content:", reasoning_content)
+print("content:", content)
+
diff --git a/examples/bmg/tooling.py b/examples/bmg/tooling.py
new file mode 100644
index 000000000..bf8375831
--- /dev/null
+++ b/examples/bmg/tooling.py
@@ -0,0 +1,37 @@
+import json
+
+client = OpenAI(base_url="http://0.0.0.0:8000/v1", api_key="dummy")
+
+def get_weather(location: str, unit: str):
+    return f"Getting the weather for {location} in {unit}..."
+tool_functions = {"get_weather": get_weather}
+
+tools = [{
+    "type": "function",
+    "function": {
+        "name": "get_weather",
+        "description": "Get the current weather in a given location",
+        "parameters": {
+            "type": "object",
+            "properties": {
+                "location": {"type": "string", "description": "City and state, e.g., 'San Francisco, CA'"},
+                "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]}
+            },
+            "required": ["location", "unit"]
+        }
+    }
+}]
+
+response = client.chat.completions.create(
+    model=client.models.list().data[0].id,
+    messages=[{"role": "user", "content": "What's the weather like in San Francisco?"}],
+    tools=tools,
+    temperature=0,
+    tool_choice="auto"
+)
+
+tool_call = response.choices[0].message.tool_calls[0].function
+print(f"Function called: {tool_call.name}")
+print(f"Arguments: {tool_call.arguments}")
+print(f"Result: {tool_functions[tool_call.name](**json.loads(tool_call.arguments))}")                                                                               30,22         Bot
+
diff --git a/examples/online_serving/structured_outputs/structured_outputs.py b/examples/online_serving/structured_outputs/structured_outputs.py
index 2599c951e..a95a74475 100644
--- a/examples/online_serving/structured_outputs/structured_outputs.py
+++ b/examples/online_serving/structured_outputs/structured_outputs.py
@@ -221,7 +221,7 @@ async def cli():
     )
     args = parser.parse_args()
 
-    base_url = os.getenv("OPENAI_BASE_URL", "http://localhost:8000/v1")
+    base_url = os.getenv("OPENAI_BASE_URL", "http://0.0.0.0:8000/v1")
     client = openai.AsyncOpenAI(base_url=base_url, api_key="EMPTY")
     constraints = list(PARAMS) if "*" in args.constraint else list(set(args.constraint))
     model = (await client.models.list()).data[0].id
@@ -232,6 +232,7 @@ async def cli():
                 client.chat.completions.create(
                     model=model,
                     max_tokens=1024,
+                    temperature=0,
                     stream=True,
                     **PARAMS[name],
                 )
@@ -246,6 +247,7 @@ async def cli():
                 client.chat.completions.create(
                     model=model,
                     max_tokens=1024,
+                    temperature=0,
                     stream=False,
                     **PARAMS[name],
                 )
-- 
2.43.0


From 9a5857e38a0633a831290c1fd72f370feabf64eb Mon Sep 17 00:00:00 2001
From: liuzhenwei <zhenweiliu@habana.ai>
Date: Wed, 24 Sep 2025 19:16:10 +0800
Subject: [PATCH 11/36] [P/D] add 1p1d acc test script (#343)

* [P/D] add 1p1d acc test script

Signed-off-by: zhenwei-intel <zhenwei.liu@intel.com>

* Update run_xpu_disagg_accuracy_test.sh

---------

Signed-off-by: zhenwei-intel <zhenwei.liu@intel.com>
---
 .../run_xpu_disagg_accuracy_test.sh           | 156 ++++++++++++++++++
 1 file changed, 156 insertions(+)
 create mode 100644 tests/v1/kv_connector/nixl_integration/run_xpu_disagg_accuracy_test.sh

diff --git a/tests/v1/kv_connector/nixl_integration/run_xpu_disagg_accuracy_test.sh b/tests/v1/kv_connector/nixl_integration/run_xpu_disagg_accuracy_test.sh
new file mode 100644
index 000000000..ae4909b29
--- /dev/null
+++ b/tests/v1/kv_connector/nixl_integration/run_xpu_disagg_accuracy_test.sh
@@ -0,0 +1,156 @@
+#!/bin/bash
+set -e
+
+# Hosts / ports
+PREFILL_HOST=${PREFILL_HOST:-"localhost"}
+PREFILL_PORT=${PREFILL_PORT:-8100}
+PREFILL_NIXL_SIDE_PORT=${PREFILL_NIXL_SIDE_PORT:-5577}
+DECODE_HOST=${DECODE_HOST:-"localhost"}
+DECODE_PORT=${DECODE_PORT:-8200}
+PROXY_HOST=${PROXY_HOST:-"localhost"}
+PROXY_PORT=${PROXY_PORT:-8192}
+BASELINE_HOST=${BASELINE_HOST:-"localhost"}
+BASELINE_PORT=${BASELINE_PORT:-9290}
+
+
+# Model to run.
+MODEL_NAME=${MODEL_NAME:-"Qwen/Qwen3-0.6B"}
+MAX_MODEL_LEN=${MAX_MODEL_LEN:-1024}
+BLOCK_SIZE=${BLOCK_SIZE:-16}
+
+
+# execution env
+GIT_ROOT=$(git rev-parse --show-toplevel)
+EXP_ROOT="${GIT_ROOT}/tests/v1/kv_connector/nixl_integration"
+
+OUTPUT_FILE=${OUTPUT_FILE:-"${EXP_ROOT}/.xpu_accuracy_test_outputs.txt"}
+
+# Trap the SIGINT signal (triggered by Ctrl+C)
+trap 'kill $(jobs -pr)' SIGINT SIGTERM EXIT
+
+cleanup() {
+  echo "Cleaning up any running vLLM instances..."
+  pkill -f "vllm serve" || true
+  sleep 2
+}
+
+wait_for_server() {
+  local host=$1
+  local port=$2
+  timeout 1200 bash -c "
+    until curl -s ${host}:${port}/v1/completions > /dev/null; do
+      sleep 1
+    done" && return 0 || return 1
+}
+
+launch_baseline() {
+  BASELINE_BASE_CMD="
+  ONEAPI_DEVICE_SELECTOR=level_zero:0 \
+  VLLM_USE_V1=1 \
+  VLLM_WORKER_MULTIPROC_METHOD=spawn \
+  VLLM_ENABLE_V1_MULTIPROCESSING=1 vllm serve $MODEL_NAME \
+      --host ${BASELINE_HOST} \
+      --port ${BASELINE_PORT} \
+      --max-model-len ${MAX_MODEL_LEN}\
+      --seed 42 \
+      -tp 1 \
+      --block-size ${BLOCK_SIZE} \
+      --gpu-memory-utilization 0.8 \
+      --disable-log-requests \
+      --dtype float16 \
+      --enforce-eager"
+  echo ${BASELINE_BASE_CMD}      
+  bash -c "${BASELINE_BASE_CMD}" &
+  sleep 10
+  wait_for_server ${BASELINE_HOST} ${BASELINE_PORT}
+}
+
+launch_pd() {
+  PREFILL_BASE_CMD="
+  ONEAPI_DEVICE_SELECTOR=level_zero:0 \
+  VLLM_MULTIPROC_EXECUTE_MODEL_TIMEOUT_S=200 \
+  VLLM_USE_V1=1 \
+  VLLM_NIXL_SIDE_CHANNEL_HOST=${PREFILL_HOST} \
+  VLLM_NIXL_SIDE_CHANNEL_PORT=${PREFILL_NIXL_SIDE_PORT} \
+  VLLM_WORKER_MULTIPROC_METHOD=spawn \
+  VLLM_ENABLE_V1_MULTIPROCESSING=1 vllm serve $MODEL_NAME \
+      --host ${PREFILL_HOST} \
+      --port ${PREFILL_PORT} \
+      --max-model-len ${MAX_MODEL_LEN}\
+      --seed 42 \
+      --block-size ${BLOCK_SIZE} \
+      --enforce-eager \
+      --dtype float16 \
+      -tp 1 \
+      --gpu-memory-utilization 0.8 \
+      --disable-log-requests \
+      --kv-transfer-config '{\"kv_connector\":\"NixlConnector\",\"kv_role\":\"kv_both\",\"kv_buffer_device\":\"cpu\"}'"
+
+
+  DECODE_BASE_CMD="
+  ONEAPI_DEVICE_SELECTOR=level_zero:1 \
+  VLLM_MULTIPROC_EXECUTE_MODEL_TIMEOUT_S=200 \
+  VLLM_USE_V1=1 \
+  VLLM_WORKER_MULTIPROC_METHOD=spawn \
+  VLLM_ENABLE_V1_MULTIPROCESSING=1 vllm serve $MODEL_NAME \
+      --host ${DECODE_HOST} \
+      --port ${DECODE_PORT} \
+      --max-model-len ${MAX_MODEL_LEN}\
+      --seed 42 \
+      --block-size ${BLOCK_SIZE} \
+      --enforce-eager \
+      -tp 1 \
+      --dtype float16 \
+      --gpu-memory-utilization 0.8 \
+      --disable-log-requests \
+      --kv-transfer-config '{\"kv_connector\":\"NixlConnector\",\"kv_role\":\"kv_both\",\"kv_buffer_device\":\"cpu\"}'"
+
+  echo ${PREFILL_BASE_CMD}
+  echo ${DECODE_BASE_CMD}
+  sleep 2
+
+  # execute on hosts
+  bash -c "${PREFILL_BASE_CMD}" &
+  bash -c "${DECODE_BASE_CMD}" &
+  sleep 1
+  wait_for_server ${PREFILL_HOST} ${PREFILL_PORT}
+  sleep 1
+  wait_for_server ${DECODE_HOST} ${DECODE_PORT}
+  sleep 1
+}
+
+launch_pd_proxy(){
+  PROXY_BASE_CMD="
+  python3 ${EXP_ROOT}/toy_proxy_server.py \
+  --prefiller-host ${PREFILL_HOST} --prefiller-port ${PREFILL_PORT} \
+  --decoder-host ${DECODE_HOST} --decoder-port ${DECODE_PORT} \
+  --host=${PROXY_HOST} --port ${PROXY_PORT}"
+  echo ${PROXY_BASE_CMD} 
+  bash -c "${PROXY_BASE_CMD}" &
+  sleep 2
+}
+
+run_tests(){
+  local service_url=$1
+  local mode=$2
+  python3 ${EXP_ROOT}/test_disagg_accuracy.py --service_url=${service_url} --model_name=${MODEL_NAME} --mode=${mode} --file_name=${OUTPUT_FILE}
+}
+
+
+# run non-disagg. baseline & save outputs
+launch_baseline
+run_tests "http://${BASELINE_HOST}:${BASELINE_PORT}" "baseline"
+cleanup
+sleep 10
+
+
+# run disagg. & do exact-match with the outputs from baseline
+launch_pd
+launch_pd_proxy
+run_tests "http://${PROXY_HOST}:${PROXY_PORT}" "disagg"
+echo "-----P/D success----"
+
+rm ${OUTPUT_FILE}
+cleanup
+
+exit 0
-- 
2.43.0


From 994ce6edc0f6599f81838f814e936d02ce610039 Mon Sep 17 00:00:00 2001
From: Qiming Zhang <qiming1.zhang@intel.com>
Date: Tue, 4 Nov 2025 16:55:59 -0800
Subject: [PATCH 12/36] support qwen next (#393)

Signed-off-by: mayuyuace <qiming1.zhang@intel.com>
---
 vllm/model_executor/layers/fla/ops/chunk.py           | 4 +++-
 vllm/model_executor/layers/fla/ops/layernorm_guard.py | 9 +++++++--
 vllm/model_executor/layers/fla/ops/utils.py           | 5 ++++-
 vllm/model_executor/models/config.py                  | 3 +++
 4 files changed, 17 insertions(+), 4 deletions(-)

diff --git a/vllm/model_executor/layers/fla/ops/chunk.py b/vllm/model_executor/layers/fla/ops/chunk.py
index 4c8bf9f43..3df4d1aca 100644
--- a/vllm/model_executor/layers/fla/ops/chunk.py
+++ b/vllm/model_executor/layers/fla/ops/chunk.py
@@ -12,6 +12,8 @@ import warnings
 import torch
 from einops import rearrange
 
+from vllm.platforms import current_platform
+
 from .chunk_delta_h import chunk_gated_delta_rule_fwd_h
 from .chunk_o import chunk_fwd_o
 from .chunk_scaled_dot_kkt import chunk_scaled_dot_kkt_fwd
@@ -74,7 +76,7 @@ def chunk_gated_delta_rule_fwd(
 class ChunkGatedDeltaRuleFunction(torch.autograd.Function):
     @staticmethod
     @input_guard
-    @torch.amp.custom_fwd(device_type="cuda")
+    @torch.amp.custom_fwd(device_type="xpu" if current_platform.is_xpu() else "cuda")
     def forward(
         ctx,
         q: torch.Tensor,
diff --git a/vllm/model_executor/layers/fla/ops/layernorm_guard.py b/vllm/model_executor/layers/fla/ops/layernorm_guard.py
index 89352d12b..0cf18c4e0 100644
--- a/vllm/model_executor/layers/fla/ops/layernorm_guard.py
+++ b/vllm/model_executor/layers/fla/ops/layernorm_guard.py
@@ -20,6 +20,7 @@ import torch.nn as nn
 import torch.nn.functional as F
 from einops import rearrange
 
+from vllm.platforms import current_platform
 from vllm.triton_utils import tl, triton
 from vllm.utils.math_utils import cdiv, next_power_of_2
 
@@ -165,8 +166,12 @@ def layer_norm_fwd_kernel(
 @lru_cache
 def _get_sm_count(device: torch.device) -> int:
     """Get and cache the SM count for a given device."""
-    props = torch.cuda.get_device_properties(device)
-    return props.multi_processor_count
+    if current_platform.is_xpu():
+        props = torch.xpu.get_device_properties(device)
+        return props.max_compute_units
+    else:
+        props = torch.cuda.get_device_properties(device)
+        return props.multi_processor_count
 
 
 def calc_rows_per_block(M: int, device: torch.device) -> int:
diff --git a/vllm/model_executor/layers/fla/ops/utils.py b/vllm/model_executor/layers/fla/ops/utils.py
index 18e17a511..65af42589 100644
--- a/vllm/model_executor/layers/fla/ops/utils.py
+++ b/vllm/model_executor/layers/fla/ops/utils.py
@@ -105,7 +105,10 @@ def input_guard(fn: Callable[..., torch.Tensor]) -> Callable[..., torch.Tensor]:
                     break
 
         if tensor is not None:
-            ctx = torch.cuda.device(tensor.device.index)
+            if current_platform.is_xpu():
+                ctx = torch.xpu.device(tensor.device.index)
+            else:
+                ctx = torch.cuda.device(tensor.device.index)
         else:
             ctx = contextlib.nullcontext()
 
diff --git a/vllm/model_executor/models/config.py b/vllm/model_executor/models/config.py
index e51a110ce..58ab0dca5 100644
--- a/vllm/model_executor/models/config.py
+++ b/vllm/model_executor/models/config.py
@@ -410,6 +410,9 @@ class HybridAttentionMambaModelConfig(VerifyAndUpdateConfig):
                 # https://github.com/flashinfer-ai/flashinfer/issues/1993 reports that`
                 # head size 256 and block size 16 is not supported on blackwell.
                 kernel_block_alignment_size = 32
+            # Xpu only supports block_size that is divisible by 64.
+            if current_platform.is_xpu():
+                kernel_block_alignment_size = 64
             attn_page_size_1_token = FullAttentionSpec(
                 block_size=1,
                 num_kv_heads=model_config.get_num_kv_heads(parallel_config),
-- 
2.43.0


From d7a996c0049c18dd8e95820962cba7b9df073c03 Mon Sep 17 00:00:00 2001
From: yan <yanma1@habana.ai>
Date: Fri, 18 Apr 2025 02:10:40 +0000
Subject: [PATCH 13/36] revert determine_available_memory logic

Signed-off-by: yan <yanma1@intel.com>

Signed-off-by: Yan Ma <yan.ma@intel.com>
---
 vllm/v1/worker/xpu_worker.py | 42 +++++++++++-------------------------
 1 file changed, 12 insertions(+), 30 deletions(-)

diff --git a/vllm/v1/worker/xpu_worker.py b/vllm/v1/worker/xpu_worker.py
index fe0850771..916df1e1b 100644
--- a/vllm/v1/worker/xpu_worker.py
+++ b/vllm/v1/worker/xpu_worker.py
@@ -67,9 +67,11 @@ class XPUWorker(Worker):
     def determine_available_memory(self) -> int:
         """Profiles the peak memory usage of the model to determine how many
         KV blocks may be allocated without OOMs.
+
         The engine will first conduct a profiling of the existing memory usage.
         Then, it calculates the maximum possible number of GPU and CPU blocks
         that can be allocated with the remaining free memory.
+
         .. tip::
             You may limit the usage of GPU memory
             by adjusting the `gpu_memory_utilization` parameter.
@@ -77,54 +79,34 @@ class XPUWorker(Worker):
         # Profile the memory usage of the model and get the maximum number of
         # cache blocks that can be allocated with the remaining free memory.
         torch.xpu.empty_cache()
-        torch.xpu.reset_peak_memory_stats()
-
-        free_gpu_memory, total_gpu_memory = torch.xpu.mem_get_info()
-        current_allocated_bytes = torch.xpu.memory_allocated()
-        msg = (
-            "Before memory profiling run, "
-            f"total GPU memory: {total_gpu_memory / 1024**2:.2f} MB, "
-            f"model load takes {current_allocated_bytes / 1024**2:.2f} MB, "
-            f"free gpu memory is {free_gpu_memory / 1024**2:.2f} MB."
-        )
-        logger.info(msg)
+
         # Execute a forward pass with dummy inputs to profile the memory usage
         # of the model.
         self.model_runner.profile_run()
 
-        free_gpu_memory, _ = self.xpu_get_mem_info()
+        # Calculate the number of blocks that can be allocated with the
+        # profiled peak memory.
+        torch.xpu.synchronize()
+        used_memory = torch.xpu.memory_allocated()
+        total_gpu_memory = torch.xpu.get_device_properties(self.local_rank).total_memory
+        free_gpu_memory = total_gpu_memory - used_memory
+
         # NOTE(woosuk): Here we assume that the other processes using the same
         # GPU did not change their memory usage during the profiling.
-        assert self.init_gpu_memory > free_gpu_memory, (
+        peak_memory = self.init_gpu_memory - free_gpu_memory
+        assert peak_memory > 0, (
             "Error in memory profiling. "
             f"Initial free memory {self.init_gpu_memory}, current free memory"
             f" {free_gpu_memory}. This happens when the GPU memory was "
             "not properly cleaned up before initializing the vLLM instance."
         )
 
-        # Get the peak memory allocation recorded by torch
-        peak_memory = torch.xpu.memory_stats()["allocated_bytes.all.peak"]
-
         torch.xpu.empty_cache()
-        torch_allocated_bytes = torch.xpu.memory_stats()["allocated_bytes.all.current"]
-        total_allocated_bytes = self.xpu_get_mem_info()[1] - self.xpu_get_mem_info()[0]
 
-        non_torch_allocations = total_allocated_bytes - torch_allocated_bytes
-        if non_torch_allocations > 0:
-            peak_memory += non_torch_allocations
         available_kv_cache_memory = (
             total_gpu_memory * self.cache_config.gpu_memory_utilization - peak_memory
         )
 
-        msg = (
-            "After memory profiling run, "
-            f"peak memory usage is {peak_memory / 1024**2:.2f} MB,"
-            f"torch mem is {torch_allocated_bytes / 1024**2:.2f} MB, "
-            f"non-torch mem is {non_torch_allocations / 1024**2:.2f} MB, "
-            f"free gpu memory is {free_gpu_memory / 1024**2:.2f} MB."
-        )
-        logger.info(msg)
-
         return int(available_kv_cache_memory)
 
     def init_device(self):
-- 
2.43.0


From fea21d9162d879b69dd2923c33e21902aa0fc50c Mon Sep 17 00:00:00 2001
From: Yan Ma <yan.ma@intel.com>
Date: Tue, 30 Dec 2025 02:49:56 +0000
Subject: [PATCH 14/36] Update CI repo name

Signed-off-by: wenjun liu <wenjun.liu@intel.com>
---
 .github/workflows/ci.yaml     | 14 +++++++-------
 .github/workflows/ci_pvc.yaml | 16 ++++++++--------
 2 files changed, 15 insertions(+), 15 deletions(-)

diff --git a/.github/workflows/ci.yaml b/.github/workflows/ci.yaml
index aef250abe..1649fadad 100644
--- a/.github/workflows/ci.yaml
+++ b/.github/workflows/ci.yaml
@@ -132,7 +132,7 @@ jobs:
           --net=host \
           --ipc=host \
           --privileged \
-          -v ${HOME}/actions-runner/_work/vllm-xpu/vllm-xpu/qa_ci_code:/WORKSPACE \
+          -v ${HOME}/actions-runner/_work/applications.ai.gpu.vllm-xpu/applications.ai.gpu.vllm-xpu/qa_ci_code:/WORKSPACE \
           -v /dev/dri/by-path:/dev/dri/by-path \
           -v ${HOME}/.cache:/root/.cache/ \
           -e http_proxy=${http_proxy:-"http://proxy-dmz.intel.com:912"} \
@@ -147,26 +147,26 @@ jobs:
 
     - name: Validate server benchmark results
       run: |
-            python3 ${HOME}/actions-runner/_work/vllm-xpu/vllm-xpu/qa_ci_code/.buildkite/nightly-benchmarks/scripts/analyze_benchmark_results_final.py --test-selector BMG
-            cat ${HOME}/actions-runner/_work/vllm-xpu/vllm-xpu/qa_ci_code/benchmarks/results/benchmark_analysis_final.json
+            python3 ${HOME}/actions-runner/_work/applications.ai.gpu.vllm-xpu/applications.ai.gpu.vllm-xpu/qa_ci_code/.buildkite/nightly-benchmarks/scripts/analyze_benchmark_results_final.py --test-selector BMG
+            cat ${HOME}/actions-runner/_work/applications.ai.gpu.vllm-xpu/applications.ai.gpu.vllm-xpu/qa_ci_code/benchmarks/results/benchmark_analysis_final.json
 
     - name: Fix permissions
-      run: sudo chmod -R 755 ${{ runner.workspace }}/vllm-xpu/qa_ci_code/benchmarks/results/
+      run: sudo chmod -R 755 ${{ runner.workspace }}/applications.ai.gpu.vllm-xpu/qa_ci_code/benchmarks/results/
     
     - name: Debug path
-      run: ls -la ${{ runner.workspace }}/vllm-xpu/qa_ci_code/benchmarks/results/
+      run: ls -la ${{ runner.workspace }}/applications.ai.gpu.vllm-xpu/qa_ci_code/benchmarks/results/
 
     - name: Upload benchmark results
       if: always()
       uses: actions/upload-artifact@v4
       with:
         name: benchmark-results
-        path: ${{ runner.workspace }}/vllm-xpu/qa_ci_code/benchmarks/results/
+        path: ${{ runner.workspace }}/applications.ai.gpu.vllm-xpu/qa_ci_code/benchmarks/results/
 
     - name: Analyze and validate benchmark results
       if: always()
       run: |
-        RESULTS_FILE="$HOME/actions-runner/_work/vllm-xpu/vllm-xpu/qa_ci_code/benchmarks/results/benchmark_analysis_final.json"
+        RESULTS_FILE="$HOME/actions-runner/_work/applications.ai.gpu.vllm-xpu/applications.ai.gpu.vllm-xpu/qa_ci_code/benchmarks/results/benchmark_analysis_final.json"
         if [ ! -f "$RESULTS_FILE" ]; then
           echo "‚ùå Benchmark analysis file not found!"
           exit 1
diff --git a/.github/workflows/ci_pvc.yaml b/.github/workflows/ci_pvc.yaml
index eaa2f332a..4a061f324 100644
--- a/.github/workflows/ci_pvc.yaml
+++ b/.github/workflows/ci_pvc.yaml
@@ -127,7 +127,7 @@ jobs:
           --net=host \
           --ipc=host \
           --privileged \
-          -v $HOME/actions-runner/_work/vllm-xpu/vllm-xpu/qa_ci_code:/WORKSPACE \
+          -v $HOME/actions-runner/_work/applications.ai.gpu.vllm-xpu/applications.ai.gpu.vllm-xpu/qa_ci_code:/WORKSPACE \
           -v /dev/dri/by-path:/dev/dri/by-path \
           -v /mnt/data3:/root/.cache/ \
           -e http_proxy=${http_proxy:-"http://proxy-dmz.intel.com:912"} \
@@ -138,30 +138,30 @@ jobs:
           --entrypoint='' \
           --mount type=bind,source="$HOME/.secrets/my_token",target=/run/secrets/my_token,readonly \
           $image_name \
-          bash -c "IPEX_FMHA_V3=0 bash /WORKSPACE/.buildkite/nightly-benchmarks/scripts/CI_run_server_benchmarks.sh "PVC" || true; chown -R \$(id -u):\$(id -g) /WORKSPACE"
+          bash -c "bash /WORKSPACE/.buildkite/nightly-benchmarks/scripts/CI_run_server_benchmarks.sh "PVC" || true; chown -R \$(id -u):\$(id -g) /WORKSPACE"
 
     - name: Validate server benchmark results
       run: |
-        python3 $HOME/actions-runner/_work/vllm-xpu/vllm-xpu/qa_ci_code/.buildkite/nightly-benchmarks/scripts/analyze_benchmark_results_final.py --test-selector "PVC"
-        cat $HOME/actions-runner/_work/vllm-xpu/vllm-xpu/qa_ci_code/benchmarks/results/benchmark_analysis_final.json
+        python3 $HOME/actions-runner/_work/applications.ai.gpu.vllm-xpu/applications.ai.gpu.vllm-xpu/qa_ci_code/.buildkite/nightly-benchmarks/scripts/analyze_benchmark_results_final.py --test-selector "PVC"
+        cat $HOME/actions-runner/_work/applications.ai.gpu.vllm-xpu/applications.ai.gpu.vllm-xpu/qa_ci_code/benchmarks/results/benchmark_analysis_final.json
 
     - name: Fix permissions
-      run: sudo chmod -R 755 ${{ runner.workspace }}/vllm-xpu/qa_ci_code/benchmarks/results/
+      run: sudo chmod -R 755 ${{ runner.workspace }}/applications.ai.gpu.vllm-xpu/qa_ci_code/benchmarks/results/
     
     - name: Debug path
-      run: ls -la ${{ runner.workspace }}/vllm-xpu/qa_ci_code/benchmarks/results/
+      run: ls -la ${{ runner.workspace }}/applications.ai.gpu.vllm-xpu/qa_ci_code/benchmarks/results/
 
     - name: Upload benchmark results
       if: always()
       uses: actions/upload-artifact@v4
       with:
         name: benchmark-results
-        path: ${{ runner.workspace }}/vllm-xpu/qa_ci_code/benchmarks/results/
+        path: ${{ runner.workspace }}/applications.ai.gpu.vllm-xpu/qa_ci_code/benchmarks/results/
 
     - name: Analyze and validate benchmark results
       if: always()
       run: |
-        RESULTS_FILE="$HOME/actions-runner/_work/vllm-xpu/vllm-xpu/qa_ci_code/benchmarks/results/benchmark_analysis_final.json"
+        RESULTS_FILE="$HOME/actions-runner/_work/applications.ai.gpu.vllm-xpu/applications.ai.gpu.vllm-xpu/qa_ci_code/benchmarks/results/benchmark_analysis_final.json"
         if [ ! -f "$RESULTS_FILE" ]; then
           echo "‚ùå Benchmark analysis file not found!"
           exit 1
-- 
2.43.0


From c51d4a272f3a806776e1548771a8182a42ba8211 Mon Sep 17 00:00:00 2001
From: Qiming Zhang <qiming1.zhang@intel.com>
Date: Thu, 27 Nov 2025 23:46:21 -0800
Subject: [PATCH 15/36] fix qwen next (#44)

Signed-off-by: mayuyuace <qiming1.zhang@intel.com>
---
 vllm/model_executor/layers/layernorm.py | 14 ++++++++++++++
 1 file changed, 14 insertions(+)

diff --git a/vllm/model_executor/layers/layernorm.py b/vllm/model_executor/layers/layernorm.py
index d962ab8bb..7fd73a7b3 100644
--- a/vllm/model_executor/layers/layernorm.py
+++ b/vllm/model_executor/layers/layernorm.py
@@ -432,6 +432,20 @@ class RMSNormGated(CustomOp):
             norm_before_gate=self.norm_before_gate,
         )
 
+    def forward_xpu(
+        self, x: torch.Tensor, z: torch.Tensor | None = None
+    ) -> torch.Tensor:
+        from vllm.model_executor.layers.fla.ops.layernorm_guard import rmsnorm_fn
+
+        return rmsnorm_fn(
+            x,
+            self.weight,
+            self.bias,
+            z=z,
+            eps=self.eps,
+            group_size=self.group_size,
+            norm_before_gate=self.norm_before_gate,
+        )
 
 class LayerNorm(nn.Module):
     """
-- 
2.43.0


From b8dffa7725cf8616ccb46607978266d994fc5761 Mon Sep 17 00:00:00 2001
From: liuzhenwei <zhenwei.liu@intel.com>
Date: Tue, 2 Dec 2025 14:31:28 +0800
Subject: [PATCH 16/36] [PD] revert change of kv_buffer_device (#52)

Signed-off-by: zhenwei-intel <zhenwei.liu@intel.com>
---
 .../nixl_integration/run_xpu_disagg_accuracy_test.sh      | 8 ++++----
 1 file changed, 4 insertions(+), 4 deletions(-)

diff --git a/tests/v1/kv_connector/nixl_integration/run_xpu_disagg_accuracy_test.sh b/tests/v1/kv_connector/nixl_integration/run_xpu_disagg_accuracy_test.sh
index ae4909b29..3e9e2cb73 100644
--- a/tests/v1/kv_connector/nixl_integration/run_xpu_disagg_accuracy_test.sh
+++ b/tests/v1/kv_connector/nixl_integration/run_xpu_disagg_accuracy_test.sh
@@ -16,7 +16,7 @@ BASELINE_PORT=${BASELINE_PORT:-9290}
 # Model to run.
 MODEL_NAME=${MODEL_NAME:-"Qwen/Qwen3-0.6B"}
 MAX_MODEL_LEN=${MAX_MODEL_LEN:-1024}
-BLOCK_SIZE=${BLOCK_SIZE:-16}
+BLOCK_SIZE=${BLOCK_SIZE:-64}
 
 
 # execution env
@@ -45,7 +45,7 @@ wait_for_server() {
 
 launch_baseline() {
   BASELINE_BASE_CMD="
-  ONEAPI_DEVICE_SELECTOR=level_zero:0 \
+  ZE_AFFINITY_MASK=0 \
   VLLM_USE_V1=1 \
   VLLM_WORKER_MULTIPROC_METHOD=spawn \
   VLLM_ENABLE_V1_MULTIPROCESSING=1 vllm serve $MODEL_NAME \
@@ -67,7 +67,7 @@ launch_baseline() {
 
 launch_pd() {
   PREFILL_BASE_CMD="
-  ONEAPI_DEVICE_SELECTOR=level_zero:0 \
+  ZE_AFFINITY_MASK=0 \
   VLLM_MULTIPROC_EXECUTE_MODEL_TIMEOUT_S=200 \
   VLLM_USE_V1=1 \
   VLLM_NIXL_SIDE_CHANNEL_HOST=${PREFILL_HOST} \
@@ -88,7 +88,7 @@ launch_pd() {
 
 
   DECODE_BASE_CMD="
-  ONEAPI_DEVICE_SELECTOR=level_zero:1 \
+  ZE_AFFINITY_MASK=1 \
   VLLM_MULTIPROC_EXECUTE_MODEL_TIMEOUT_S=200 \
   VLLM_USE_V1=1 \
   VLLM_WORKER_MULTIPROC_METHOD=spawn \
-- 
2.43.0


From 366eb73fe12e2501f31ed4c365a35a07b48d3b8d Mon Sep 17 00:00:00 2001
From: Kunshang Ji <kunshang.ji@intel.com>
Date: Mon, 8 Dec 2025 15:45:56 +0800
Subject: [PATCH 17/36] xpu support fp8 kv (#57)

* xpu support fp8 kv

Signed-off-by: Kunshang Ji <kunshang.ji@intel.com>

* add fa_utils

Signed-off-by: Kunshang Ji <kunshang.ji@intel.com>

---------

Signed-off-by: Kunshang Ji <kunshang.ji@intel.com>

Signed-off-by: Yan Ma <yan.ma@intel.com>
---
 vllm/v1/attention/backends/fa_utils.py   | 6 ++++++
 vllm/v1/attention/backends/flash_attn.py | 3 ++-
 2 files changed, 8 insertions(+), 1 deletion(-)

diff --git a/vllm/v1/attention/backends/fa_utils.py b/vllm/v1/attention/backends/fa_utils.py
index 130d85efb..2f5eb177c 100644
--- a/vllm/v1/attention/backends/fa_utils.py
+++ b/vllm/v1/attention/backends/fa_utils.py
@@ -92,12 +92,18 @@ def get_flash_attn_version(requires_alibi: bool = False) -> int | None:
 
 
 def flash_attn_supports_fp8() -> bool:
+    if current_platform.is_xpu():
+        return True
     return (
         get_flash_attn_version() == 3
         and current_platform.is_device_capability_family(90)
     )
 
 
+def flash_attn_supports_quant_query_input() -> bool:
+    return not current_platform.is_xpu()
+
+
 def flash_attn_supports_sinks() -> bool:
     if current_platform.is_xpu():
         return True
diff --git a/vllm/v1/attention/backends/flash_attn.py b/vllm/v1/attention/backends/flash_attn.py
index 6fec5001b..4d2eb5585 100755
--- a/vllm/v1/attention/backends/flash_attn.py
+++ b/vllm/v1/attention/backends/flash_attn.py
@@ -19,6 +19,7 @@ from vllm.v1.attention.backend import (
 )
 from vllm.v1.attention.backends.fa_utils import (
     flash_attn_supports_fp8,
+    flash_attn_supports_quant_query_input,
     get_flash_attn_version,
     is_flash_attn_varlen_func_available,
 )
@@ -575,7 +576,7 @@ class FlashAttentionImpl(AttentionImpl):
                 "heads in the layer"
             )
 
-        self.supports_quant_query_input = True
+        self.supports_quant_query_input = flash_attn_supports_quant_query_input()
 
     def forward(
         self,
-- 
2.43.0


From b8eafd6c5334e7bf5d6d13f0889d40e20efe57c7 Mon Sep 17 00:00:00 2001
From: Yan Ma <yan.ma@intel.com>
Date: Wed, 31 Dec 2025 01:17:21 +0000
Subject: [PATCH 18/36] update spec-decode UT

Signed-off-by: Yan Ma <yan.ma@intel.com>
---
 tests/v1/e2e/test_spec_decode.py | 260 +++++++------------------------
 1 file changed, 56 insertions(+), 204 deletions(-)

diff --git a/tests/v1/e2e/test_spec_decode.py b/tests/v1/e2e/test_spec_decode.py
index a25114a4d..43bf3c962 100644
--- a/tests/v1/e2e/test_spec_decode.py
+++ b/tests/v1/e2e/test_spec_decode.py
@@ -16,16 +16,6 @@ from vllm.platforms import current_platform
 MTP_SIMILARITY_RATE = 0.8
 
 
-def _skip_if_insufficient_gpus_for_tp(tp_size: int):
-    """Skip test if available GPUs < tp_size on ROCm."""
-    if current_platform.is_rocm():
-        available_gpus = torch.cuda.device_count()
-        if available_gpus < tp_size:
-            pytest.skip(
-                f"Test requires {tp_size} GPUs, but only {available_gpus} available"
-            )
-
-
 def get_test_prompts(mm_enabled: bool):
     prompt_types = ["repeat", "sentence"]
     if mm_enabled:
@@ -120,16 +110,23 @@ def test_ngram_and_suffix_correctness(
     """
     test_prompts = get_test_prompts(mm_enabled=False)
 
-    ref_llm = LLM(model=model_name, max_model_len=1024)
+    ref_llm = LLM(
+        model=model_name,
+        max_model_len=1024,
+        enforce_eager=True,
+        max_num_batched_tokens=2048,
+    )
     ref_outputs = ref_llm.chat(test_prompts, sampling_config)
     del ref_llm
-    torch.cuda.empty_cache()
+    torch.xpu.empty_cache()
     cleanup_dist_env_and_memory()
 
     spec_llm = LLM(
         model=model_name,
         speculative_config=speculative_config,
         max_model_len=1024,
+        enforce_eager=True,
+        max_num_batched_tokens=2048,
     )
     spec_outputs = spec_llm.chat(test_prompts, sampling_config)
     matches = 0
@@ -146,7 +143,7 @@ def test_ngram_and_suffix_correctness(
     # Upon failure, inspect the outputs to check for inaccuracy.
     assert matches >= int(0.66 * len(ref_outputs))
     del spec_llm
-    torch.cuda.empty_cache()
+    torch.xpu.empty_cache()
     cleanup_dist_env_and_memory()
 
 
@@ -170,6 +167,8 @@ def test_suffix_decoding_acceptance(
         },
         max_model_len=1024,
         disable_log_stats=False,
+        max_num_batched_tokens=2048,
+        enforce_eager=True,
     )
 
     # Run several times and check that the accepted tokens increase.
@@ -201,14 +200,16 @@ def test_suffix_decoding_acceptance(
     # Expect the acceptance rate to improve.
     assert first_accept_rate < last_accept_rate
 
-    # Heuristic: expect at least 80.0% acceptance rate at the end.
-    assert last_accept_rate > 0.80
+    # Heuristic: expect at least 85% acceptance rate at the end.
+    assert last_accept_rate > 0.85
 
     del spec_llm
-    torch.cuda.empty_cache()
+    torch.xpu.empty_cache()
     cleanup_dist_env_and_memory()
 
 
+'''
+
 @pytest.mark.parametrize(
     "model_path",
     [
@@ -242,7 +243,10 @@ def test_speculators_model_integration(
     test_prompts = get_test_prompts(mm_enabled=False)
 
     # First run: Direct speculator model (simplified integration)
-    spec_llm = LLM(model=model_path, max_model_len=1024)
+    spec_llm = LLM(model=model_path,
+            max_model_len=1024,
+            enforce_eager=True,
+            max_num_batched_tokens=2048)
     spec_outputs = spec_llm.chat(test_prompts, sampling_config)
 
     # Verify speculative config was auto-detected
@@ -265,14 +269,17 @@ def test_speculators_model_integration(
     verifier_model = spec_llm.llm_engine.vllm_config.model_config.model
 
     del spec_llm
-    torch.cuda.empty_cache()
+    torch.xpu.empty_cache()
     cleanup_dist_env_and_memory()
 
     # Second run: Reference without speculative decoding
-    ref_llm = LLM(model=verifier_model, max_model_len=1024)
+    ref_llm = LLM(model=verifier_model,
+            max_model_len=1024,
+            enforce_eager=True,
+            max_num_batched_tokens=2048)
     ref_outputs = ref_llm.chat(test_prompts, sampling_config)
     del ref_llm
-    torch.cuda.empty_cache()
+    torch.xpu.empty_cache()
     cleanup_dist_env_and_memory()
 
     # Compare outputs
@@ -287,37 +294,13 @@ def test_speculators_model_integration(
         f"Only {matches}/{len(ref_outputs)} outputs matched. "
         f"Expected at least {int(0.66 * len(ref_outputs))} matches."
     )
+'''
 
 
 @pytest.mark.parametrize(
-    ["model_setup", "mm_enabled", "enable_chunked_prefill", "model_impl"],
+    ["model_setup", "mm_enabled", "enable_chunked_prefill"],
     [
-        (
-            ("eagle3", "Qwen/Qwen3-8B", "AngelSlim/Qwen3-8B_eagle3", 1),
-            False,
-            False,
-            "auto",
-        ),
-        (
-            ("eagle3", "Qwen/Qwen3-8B", "AngelSlim/Qwen3-8B_eagle3", 1),
-            False,
-            False,
-            "transformers",
-        ),
-        pytest.param(
-            (
-                "eagle3",
-                "Qwen/Qwen3-VL-8B-Instruct",
-                "taobao-mnn/Qwen3-VL-8B-Instruct-Eagle3",
-                1,
-            ),
-            False,
-            False,
-            "auto",
-            marks=pytest.mark.skip(
-                reason="architecture of its eagle3 is LlamaForCausalLMEagle3"
-            ),
-        ),
+        (("eagle3", "Qwen/Qwen3-8B", "AngelSlim/Qwen3-8B_eagle3", 1), False, False),
         pytest.param(
             (
                 "eagle3",
@@ -327,7 +310,6 @@ def test_speculators_model_integration(
             ),
             False,
             False,
-            "auto",
             marks=pytest.mark.skip(
                 reason="Skipping due to its head_dim not being a a multiple of 32"
             ),
@@ -341,7 +323,6 @@ def test_speculators_model_integration(
             ),
             False,
             True,
-            "auto",
             marks=large_gpu_mark(min_gb=40),
         ),  # works on 4x H100
         (
@@ -353,54 +334,13 @@ def test_speculators_model_integration(
             ),
             False,
             False,
-            "auto",
-        ),
-        pytest.param(
-            (
-                "eagle",
-                "meta-llama/Llama-4-Scout-17B-16E-Instruct",
-                "morgendave/EAGLE-Llama-4-Scout-17B-16E-Instruct",
-                4,
-            ),
-            False,
-            False,
-            "auto",
-            marks=large_gpu_mark(min_gb=80),
-        ),  # works on 4x H100
-        pytest.param(
-            (
-                "eagle",
-                "meta-llama/Llama-4-Scout-17B-16E-Instruct",
-                "morgendave/EAGLE-Llama-4-Scout-17B-16E-Instruct",
-                4,
-            ),
-            True,
-            True,
-            "auto",
-            marks=large_gpu_mark(min_gb=80),
-        ),  # works on 4x H100
-        (
-            (
-                "eagle",
-                "eagle618/deepseek-v3-random",
-                "eagle618/eagle-deepseek-v3-random",
-                1,
-            ),
-            False,
-            False,
-            "auto",
         ),
     ],
     ids=[
         "qwen3_eagle3",
-        "qwen3_eagle3-transformers",
-        "qwen3_vl_eagle3",
         "qwen2_5_vl_eagle3",
         "llama3_eagle",
         "llama3_eagle3",
-        "llama4_eagle",
-        "llama4_eagle_mm",
-        "deepseek_eagle",
     ],
 )
 @pytest.mark.parametrize("attn_backend", get_attn_backend_list_based_on_platform())
@@ -410,7 +350,6 @@ def test_eagle_correctness(
     model_setup: tuple[str, str, str, int],
     mm_enabled: bool,
     enable_chunked_prefill: bool,
-    model_impl: str,
     attn_backend: str,
 ):
     if attn_backend == "TREE_ATTN":
@@ -419,17 +358,6 @@ def test_eagle_correctness(
             "TREE_ATTN is flaky in the test disable for now until it can be "
             "resolved (see https://github.com/vllm-project/vllm/issues/22922)"
         )
-    if model_impl == "transformers":
-        import transformers
-        from packaging.version import Version
-
-        installed = Version(transformers.__version__)
-        required = Version("5.0.0.dev")
-        if installed < required:
-            pytest.skip(
-                "Eagle3 with the Transformers modeling backend requires "
-                f"transformers>={required}, but got {installed}"
-            )
 
     # Generate test prompts inside the function instead of using fixture
     test_prompts = get_test_prompts(mm_enabled)
@@ -438,48 +366,41 @@ def test_eagle_correctness(
     should be the same when using eagle speculative decoding.
     model_setup: (method, model_name, eagle_model_name, tp_size)
     """
-    # Determine attention config
-    # Scout requires default backend selection because vision encoder has
-    # head_dim 88 being incompatible with FLASH_ATTN and needs to fall back
-    # to Flex Attn
-    if "Llama-4-Scout" in model_setup[1] and attn_backend == "FLASH_ATTN":
-        if current_platform.is_rocm():
-            # TODO: Enable Flex Attn for spec_decode on ROCm
-            pytest.skip("Flex Attn for spec_decode not supported on ROCm currently")
-        attention_config = None  # Let it fall back to default
-    else:
-        attention_config = {"backend": attn_backend}
-
-    if attn_backend == "TRITON_ATTN" and not current_platform.is_rocm():
-        pytest.skip(
-            "TRITON_ATTN does not support "
-            "multi-token eagle spec decode on current platform"
-        )
-
     with monkeypatch.context() as m:
-        m.setenv("VLLM_MLA_DISABLE", "1")
+        if "Llama-4-Scout" in model_setup[1] and attn_backend == "FLASH_ATTN":
+            # Scout requires default backend selection
+            # because vision encoder has head_dim 88 being incompatible
+            #  with FLASH_ATTN and needs to fall back to Flex Attn
+            pass
+        else:
+            m.setenv("VLLM_MLA_DISABLE", "1")
+            m.setenv("VLLM_ATTENTION_BACKEND", attn_backend)
 
-        if attn_backend == "ROCM_AITER_FA" and current_platform.is_rocm():
-            if "deepseek" in model_setup[1].lower():
-                pytest.skip("ROCM_AITER_FA for deepseek not supported on ROCm platform")
-            else:
-                m.setenv("VLLM_ROCM_USE_AITER", "1")
+        if attn_backend == "TRITON_ATTN" and not current_platform.is_rocm():
+            pytest.skip(
+                "TRITON_ATTN does not support "
+                "multi-token eagle spec decode on current platform"
+            )
 
-        method, model_name, spec_model_name, tp_size = model_setup
-        _skip_if_insufficient_gpus_for_tp(tp_size)
+        if attn_backend == "FLASH_ATTN" and current_platform.is_rocm():
+            m.setenv("VLLM_ROCM_USE_AITER", "1")
 
+        method, model_name, spec_model_name, tp_size = model_setup
         max_model_len = 2048
-        max_num_batched_tokens = 128 if enable_chunked_prefill else max_model_len
+        # max_num_batched_tokens = max_model_len
+        # if chunked_prefill_enabled:
+        #    max_num_batched_tokens = 128
 
         ref_llm = LLM(
             model=model_name,
             max_model_len=max_model_len,
             tensor_parallel_size=tp_size,
-            attention_config=attention_config,
+            enforce_eager=True,
+            max_num_batched_tokens=2048,
         )
         ref_outputs = ref_llm.chat(test_prompts, sampling_config)
         del ref_llm
-        torch.cuda.empty_cache()
+        torch.xpu.empty_cache()
         cleanup_dist_env_and_memory()
 
         spec_llm = LLM(
@@ -493,10 +414,9 @@ def test_eagle_correctness(
                 "max_model_len": max_model_len,
             },
             max_model_len=max_model_len,
-            max_num_batched_tokens=max_num_batched_tokens,
-            enable_chunked_prefill=enable_chunked_prefill,
-            model_impl=model_impl,
-            attention_config=attention_config,
+            enforce_eager=True,
+            max_num_batched_tokens=2048,
+            enable_chunked_prefill=False,
         )
         spec_outputs = spec_llm.chat(test_prompts, sampling_config)
         matches = 0
@@ -513,73 +433,5 @@ def test_eagle_correctness(
         # Upon failure, inspect the outputs to check for inaccuracy.
         assert matches > int(0.6 * len(ref_outputs))
         del spec_llm
-        torch.cuda.empty_cache()
-        cleanup_dist_env_and_memory()
-
-
-@pytest.mark.parametrize(
-    ["model_setup", "mm_enabled"],
-    [
-        (("mtp", "XiaomiMiMo/MiMo-7B-Base", 1), False),
-        (("mtp", "ZixiQi/DeepSeek-V3-4layers-MTP-FP8", 1), False),
-    ],
-    ids=["mimo", "deepseek"],
-)
-def test_mtp_correctness(
-    monkeypatch: pytest.MonkeyPatch,
-    sampling_config: SamplingParams,
-    model_setup: tuple[str, str, int],
-    mm_enabled: bool,
-):
-    # Generate test prompts inside the function instead of using fixture
-    test_prompts = get_test_prompts(mm_enabled)
-    """
-    Compare the outputs of a original LLM and a speculative LLM
-    should be the same when using MTP speculative decoding.
-    model_setup: (method, model_name, tp_size)
-    """
-    with monkeypatch.context() as m:
-        m.setenv("VLLM_MLA_DISABLE", "1")
-
-        method, model_name, tp_size = model_setup
-        _skip_if_insufficient_gpus_for_tp(tp_size)
-
-        ref_llm = LLM(
-            model=model_name,
-            max_model_len=2048,
-            tensor_parallel_size=tp_size,
-            trust_remote_code=True,
-        )
-        ref_outputs = ref_llm.chat(test_prompts, sampling_config)
-        del ref_llm
-        torch.cuda.empty_cache()
-        cleanup_dist_env_and_memory()
-
-        spec_llm = LLM(
-            model=model_name,
-            trust_remote_code=True,
-            tensor_parallel_size=tp_size,
-            speculative_config={
-                "method": method,
-                "num_speculative_tokens": 1,
-                "max_model_len": 2048,
-            },
-            max_model_len=2048,
-        )
-        spec_outputs = spec_llm.chat(test_prompts, sampling_config)
-        matches = 0
-        misses = 0
-        for ref_output, spec_output in zip(ref_outputs, spec_outputs):
-            if ref_output.outputs[0].text == spec_output.outputs[0].text:
-                matches += 1
-            else:
-                misses += 1
-                print(f"ref_output: {ref_output.outputs[0].text}")
-                print(f"spec_output: {spec_output.outputs[0].text}")
-
-        # Heuristic: expect at least 80% of the prompts to match exactly
-        # Upon failure, inspect the outputs to check for inaccuracy.
-        assert matches > int(MTP_SIMILARITY_RATE * len(ref_outputs))
-        del spec_llm
-        torch.cuda.empty_cache()
+        torch.xpu.empty_cache()
         cleanup_dist_env_and_memory()
-- 
2.43.0


From fc5a0a6f07bd2217d90da3d5374c2ed39b9d9660 Mon Sep 17 00:00:00 2001
From: Yan Ma <yan.ma@intel.com>
Date: Thu, 26 Jun 2025 14:14:48 +0800
Subject: [PATCH 19/36] offload weights to cpu before fp8 online quant (#225)

Signed-off-by: Yan Ma <yan.ma@intel.com>

Signed-off-by: Fanli Lin <fanli.lin@intel.com>
---
 docs/features/quantization/fp8.md             |  2 +-
 tests/quantization/test_cpu_offload.py        | 15 +++++
 vllm/envs.py                                  |  6 ++
 .../model_executor/layers/quantization/fp8.py |  6 ++
 .../layers/quantization/ipex_quant.py         | 63 ++++++++++++++++++-
 vllm/model_executor/model_loader/utils.py     | 44 +++++++------
 6 files changed, 114 insertions(+), 22 deletions(-)

diff --git a/docs/features/quantization/fp8.md b/docs/features/quantization/fp8.md
index f17ef89a5..b3ceab13f 100644
--- a/docs/features/quantization/fp8.md
+++ b/docs/features/quantization/fp8.md
@@ -139,4 +139,4 @@ print(result[0].outputs[0].text)
 ```
 
 !!! warning
-    Currently, we load the model at original precision before quantizing down to 8-bits, so you need enough memory to load the whole model.
+    Currently, by default we load the model at original precision before quantizing down to 8-bits, so you need enough memory to load the whole model. To avoid this, adding `VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT=1` can allow offloading weights to cpu before quantization and quantized weights will be kept in device.
diff --git a/tests/quantization/test_cpu_offload.py b/tests/quantization/test_cpu_offload.py
index 3b58614e5..a02504f5f 100644
--- a/tests/quantization/test_cpu_offload.py
+++ b/tests/quantization/test_cpu_offload.py
@@ -25,6 +25,21 @@ def test_cpu_offload_fp8():
     )
 
 
+@pytest.mark.skipif(
+    not is_quant_method_supported("fp8"),
+    reason="fp8 is not supported on this GPU type.",
+)
+def test_offload_weights_before_quant_fp8():
+    # Test quantization of an unquantized checkpoint
+    compare_two_settings(
+        "meta-llama/Llama-3.2-1B-Instruct",
+        ["--quantization", "fp8"],
+        ["--quantization", "fp8"],
+        {"VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT": "1"},
+        max_wait_seconds=480,
+    )
+
+
 @pytest.mark.skipif(
     not is_quant_method_supported("gptq_marlin"),
     reason="gptq_marlin is not supported on this GPU type.",
diff --git a/vllm/envs.py b/vllm/envs.py
index 6bc8cbf52..d7df1f48e 100755
--- a/vllm/envs.py
+++ b/vllm/envs.py
@@ -253,6 +253,7 @@ if TYPE_CHECKING:
     VLLM_LOG_MODEL_INSPECTION: bool = False
     VLLM_DEBUG_MFU_METRICS: bool = False
     VLLM_XPU_FP8_DTYPE: str = "e5m2"
+    VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT: bool = False
 
 
 def get_default_cache_root():
@@ -1614,6 +1615,11 @@ environment_variables: dict[str, Callable[[], Any]] = {
     ),
     # fp8 dtype for XPU platform
     "VLLM_XPU_FP8_DTYPE": lambda: os.environ.get("VLLM_XPU_FP8_DTYPE", "e5m2"),
+    # Offload model weights to cpu before online fp8 quantization
+    "VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT": lambda: os.environ.get(
+        "VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT", "0"
+    )
+    == "1",
 }
 
 # --8<-- [end:env-vars-definition]
diff --git a/vllm/model_executor/layers/quantization/fp8.py b/vllm/model_executor/layers/quantization/fp8.py
index 1c0c35bf6..8f4536baf 100644
--- a/vllm/model_executor/layers/quantization/fp8.py
+++ b/vllm/model_executor/layers/quantization/fp8.py
@@ -417,11 +417,15 @@ class Fp8LinearMethod(LinearMethodBase):
                 return res
 
             # For non-serialized checkpoints, use original dtype
+            # Force offloading weights to cpu if VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT
+            # enabled, otherwise use original device config which can be gpu or cpu
+            # (may happen when cpu_offload_gb > 0)
             weight = ModelWeightParameter(
                 data=torch.empty(
                     output_size_per_partition,
                     input_size_per_partition,
                     dtype=params_dtype,
+                    device="cpu" if envs.VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT else None,
                 ),
                 input_dim=1,
                 output_dim=0,
@@ -715,6 +719,7 @@ class Fp8MoEMethod(FusedMoEMethodBase):
                 2 * intermediate_size_per_partition,
                 hidden_size,
                 dtype=params_dtype,
+                device="cpu" if envs.VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT else None,
             ),
             requires_grad=False,
         )
@@ -727,6 +732,7 @@ class Fp8MoEMethod(FusedMoEMethodBase):
                 hidden_size,
                 intermediate_size_per_partition,
                 dtype=params_dtype,
+                device="cpu" if envs.VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT else None,
             ),
             requires_grad=False,
         )
diff --git a/vllm/model_executor/layers/quantization/ipex_quant.py b/vllm/model_executor/layers/quantization/ipex_quant.py
index 475bd8536..15397a7c9 100644
--- a/vllm/model_executor/layers/quantization/ipex_quant.py
+++ b/vllm/model_executor/layers/quantization/ipex_quant.py
@@ -7,11 +7,13 @@ import torch
 from packaging import version
 from torch.nn import Module
 
+import vllm.envs as envs
 from vllm._ipex_ops import ipex_ops as ops
 from vllm.model_executor.layers.fused_moe.config import FusedMoEQuantConfig
 from vllm.model_executor.layers.fused_moe.fused_moe_router import (
     FusedMoERouter,
 )
+from vllm.model_executor.layers.fused_moe.layer import FusedMoE
 from vllm.model_executor.layers.linear import (
     LinearBase,
     LinearMethodBase,
@@ -29,7 +31,7 @@ from vllm.model_executor.layers.quantization.fp8 import (
 )
 from vllm.model_executor.layers.quantization.gptq import GPTQLinearMethod
 from vllm.model_executor.layers.quantization.utils.quant_utils import is_layer_skipped
-from vllm.model_executor.utils import replace_parameter
+from vllm.model_executor.utils import replace_parameter, set_weight_attrs
 from vllm.platforms import current_platform
 
 MIN_IPEX_VERSION = "2.6.0"
@@ -337,6 +339,65 @@ class XPUFp8MoEMethod(Fp8OnlineMoEMethod):
         super().__init__(quant_config, layer)
         self.quant_config = quant_config
 
+    def create_weights(
+        self,
+        layer: Module,
+        num_experts: int,
+        hidden_size: int,
+        intermediate_size_per_partition: int,
+        params_dtype: torch.dtype,
+        **extra_weight_attrs,
+    ):
+        layer.intermediate_size_per_partition = intermediate_size_per_partition
+        layer.hidden_size = hidden_size
+        layer.num_experts = num_experts
+        layer.orig_dtype = params_dtype
+        layer.weight_block_size = None
+        # WEIGHTS
+        w13_weight = torch.nn.Parameter(
+            torch.empty(
+                num_experts,
+                2 * intermediate_size_per_partition,
+                hidden_size,
+                dtype=params_dtype,
+                device="cpu" if envs.VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT else None,
+            ),
+            requires_grad=False,
+        )
+        layer.register_parameter("w13_weight", w13_weight)
+        set_weight_attrs(w13_weight, extra_weight_attrs)
+
+        w2_weight = torch.nn.Parameter(
+            torch.empty(
+                num_experts,
+                hidden_size,
+                intermediate_size_per_partition,
+                dtype=params_dtype,
+                device="cpu" if envs.VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT else None,
+            ),
+            requires_grad=False,
+        )
+        layer.register_parameter("w2_weight", w2_weight)
+        set_weight_attrs(w2_weight, extra_weight_attrs)
+
+        # Allocate 2 scales for w1 and w3 respectively.
+        # They will be combined to a single scale after weight loading.
+        w13_weight_scale = torch.nn.Parameter(
+            torch.ones(num_experts, 2, dtype=torch.float32), requires_grad=False
+        )
+        w2_weight_scale = torch.nn.Parameter(
+            torch.ones(num_experts, dtype=torch.float32), requires_grad=False
+        )
+        layer.register_parameter("w13_weight_scale", w13_weight_scale)
+        layer.register_parameter("w2_weight_scale", w2_weight_scale)
+
+        extra_weight_attrs.update(
+            {"quant_method": FusedMoeWeightScaleSupported.TENSOR.value}
+        )
+        # INPUT_SCALES
+        layer.w13_input_scale = None
+        layer.w2_input_scale = None
+
     def process_weights_after_loading(self, layer: Module) -> None:
         if getattr(layer, "_already_called_process_weights_after_loading", False):
             return
diff --git a/vllm/model_executor/model_loader/utils.py b/vllm/model_executor/model_loader/utils.py
index 08d7a851a..24b66eb91 100644
--- a/vllm/model_executor/model_loader/utils.py
+++ b/vllm/model_executor/model_loader/utils.py
@@ -13,6 +13,7 @@ from typing_extensions import assert_never
 
 from vllm.attention.layer import Attention, MLAAttention
 from vllm.config import ModelConfig, VllmConfig, set_current_vllm_config
+from vllm.envs import VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT
 from vllm.logger import init_logger
 from vllm.model_executor.layers.quantization.base_config import (
     QuantizationConfig,
@@ -138,26 +139,29 @@ def device_loading_context(module: torch.nn.Module, target_device: torch.device)
         yield module
 
     finally:
-        # Restore parameters to their original devices, ignoring new parameters
-        pin_memory = is_pin_memory_available()
-        for name, p in module.named_parameters():
-            if name in original_device_states:
-                original_device: torch.device = original_device_states[name]
-                if original_device.type == "cpu":
-                    # `torch.empty_like` does not support `pin_memory` argument
-                    cpu_data = torch.empty_strided(
-                        size=p.data.size(),
-                        stride=p.data.stride(),
-                        dtype=p.data.dtype,
-                        layout=p.data.layout,
-                        device="cpu",
-                        pin_memory=pin_memory,
-                    )
-                    cpu_data.copy_(p.data)
-                    p.data = cpu_data
-                else:
-                    p.data = p.data.to(original_device)
-        # New parameters or parameters already on target device are untouched
+        # If weights were loaded onto the CPU for FP8 online quantization, there
+        # is no need to move them back to the original device.
+        if not VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT:
+            # Restore parameters to their original devices, ignoring new parameters # noqa: E501
+            pin_memory = is_pin_memory_available()
+            for name, p in module.named_parameters():
+                if name in original_device_states:
+                    original_device: torch.device = original_device_states[name]
+                    if original_device.type == "cpu":
+                        # `torch.empty_like` does not support `pin_memory` argument # noqa: E501
+                        cpu_data = torch.empty_strided(
+                            size=p.data.size(),
+                            stride=p.data.stride(),
+                            dtype=p.data.dtype,
+                            layout=p.data.layout,
+                            device="cpu",
+                            pin_memory=pin_memory,
+                        )
+                        cpu_data.copy_(p.data)
+                        p.data = cpu_data
+                    else:
+                        p.data = p.data.to(original_device)
+            # New parameters or parameters already on target device are untouched # noqa: E501
 
 
 _MODEL_ARCH_BY_HASH = dict[int, tuple[type[nn.Module], str]]()
-- 
2.43.0


From 4ae4d2286955fbbda963052b15fb0d96ced2f8de Mon Sep 17 00:00:00 2001
From: mayuyuace <qiming1.zhang@intel.com>
Date: Fri, 26 Dec 2025 00:10:34 -0800
Subject: [PATCH 20/36] new rope

Signed-off-by: mayuyuace <qiming1.zhang@intel.com>
---
 vllm/_ipex_ops.py                             |  5 ++---
 .../layers/rotary_embedding/base.py           | 22 +++++++------------
 2 files changed, 10 insertions(+), 17 deletions(-)

diff --git a/vllm/_ipex_ops.py b/vllm/_ipex_ops.py
index 239f5376e..5b04d8d90 100644
--- a/vllm/_ipex_ops.py
+++ b/vllm/_ipex_ops.py
@@ -142,9 +142,8 @@ class ipex_ops:
         cos_sin_cache: torch.Tensor,  # [cos_sin_dim, rot_dim]
         is_neox: bool,
     ) -> None:
-        rot_dim = cos_sin_cache.size(1)
-        ipex.llm.functional.rotary_embedding_batched(
-            positions, query, key, head_size, cos_sin_cache, is_neox, rot_dim
+        torch.ops.torch_ipex.rotary_embedding(
+            positions, query, key, head_size, cos_sin_cache, is_neox
         )
 
     @staticmethod
diff --git a/vllm/model_executor/layers/rotary_embedding/base.py b/vllm/model_executor/layers/rotary_embedding/base.py
index d63367af5..3bf648b87 100644
--- a/vllm/model_executor/layers/rotary_embedding/base.py
+++ b/vllm/model_executor/layers/rotary_embedding/base.py
@@ -237,20 +237,14 @@ class RotaryEmbedding(RotaryEmbeddingBase):
         self._match_cos_sin_cache_dtype(query)
         # ops.rotary_embedding() is an in-place operation
         # that updates the query and key tensors.
-        if key is None:
-            # XPU kernel doesn't support key=None so fall back to native impl
-            # TODO(sarckk): add support for optional key in
-            # ipex.llm.functional.rotary_embedding_batched
-            return self.forward_native(positions, query, key)
-        else:
-            ops.rotary_embedding(
-                positions,
-                query,
-                key,
-                self.head_size,
-                self.cos_sin_cache,
-                self.is_neox_style,
-            )
+        ops.rotary_embedding(
+            positions,
+            query,
+            key,
+            self.head_size,
+            self.cos_sin_cache,
+            self.is_neox_style,
+        )
         return query, key
 
     def forward_cpu(
-- 
2.43.0


From 5ca319f4304a4b3de55c90f2c06c8d355876f4ff Mon Sep 17 00:00:00 2001
From: Yan Ma <yan.ma@intel.com>
Date: Tue, 30 Dec 2025 02:08:31 +0000
Subject: [PATCH 21/36] update deps for new release

Signed-off-by: Yan Ma <yan.ma@intel.com>
---
 docker/Dockerfile.xpu | 20 ++++++++------------
 requirements/xpu.txt  |  9 +++++----
 2 files changed, 13 insertions(+), 16 deletions(-)

diff --git a/docker/Dockerfile.xpu b/docker/Dockerfile.xpu
index f63ce2c50..583d3e3b4 100644
--- a/docker/Dockerfile.xpu
+++ b/docker/Dockerfile.xpu
@@ -1,8 +1,8 @@
-FROM intel/deep-learning-essentials:2025.2.2-0-devel-ubuntu24.04 AS vllm-base
+FROM intel/deep-learning-essentials:2025.3.0-0-devel-ubuntu24.04 AS vllm-base
 
 RUN wget -O- https://apt.repos.intel.com/intel-gpg-keys/GPG-PUB-KEY-INTEL-SW-PRODUCTS.PUB | gpg --dearmor | tee /usr/share/keyrings/oneapi-archive-keyring.gpg > /dev/null && \
     echo "deb [signed-by=/usr/share/keyrings/oneapi-archive-keyring.gpg] https://apt.repos.intel.com/oneapi all main" | tee /etc/apt/sources.list.d/oneAPI.list && \
-    add-apt-repository -y ppa:kobuk-team/intel-graphics-staging
+    add-apt-repository -y ppa:kobuk-team/intel-graphics
 
 RUN apt clean && apt-get update -y && \
     apt-get install -y --no-install-recommends --fix-missing \
@@ -27,16 +27,10 @@ RUN update-alternatives --install /usr/bin/python python /usr/bin/python3.12 1
 
 RUN apt install -y libze1 libze-dev libze-intel-gpu1 intel-opencl-icd libze-intel-gpu-raytracing intel-ocloc
 
-# This oneccl contains the BMG support which is not the case for default version of oneapi 2025.2.
-ARG ONECCL_INSTALLER="intel-oneccl-2021.15.7.6_offline.sh"
-RUN wget "https://github.com/uxlfoundation/oneCCL/releases/download/2021.15.7/${ONECCL_INSTALLER}" && \
-    bash "${ONECCL_INSTALLER}" -a --silent --eula accept && \
-    rm "${ONECCL_INSTALLER}" && \
-    echo "source /opt/intel/oneapi/setvars.sh --force" >> /root/.bashrc && \
-    echo "source /opt/intel/oneapi/ccl/2021.15/env/vars.sh --force" >> /root/.bashrc
+RUN wget https://github.com/uxlfoundation/oneCCL/releases/download/2021.15.7/intel-oneccl-2021.15.7.8_offline.sh
+RUN bash intel-oneccl-2021.15.7.8_offline.sh -a --silent --eula accept && echo "source /opt/intel/oneapi/setvars.sh --force" >> /root/.bashrc
 RUN rm -f /opt/intel/oneapi/ccl/latest && \
     ln -s /opt/intel/oneapi/ccl/2021.15 /opt/intel/oneapi/ccl/latest
-
 SHELL ["bash", "-c"]
 CMD ["bash", "-c", "source /root/.bashrc && exec bash"]
 
@@ -52,7 +46,6 @@ RUN --mount=type=cache,target=/root/.cache/pip \
     -r requirements/xpu.txt
 
 # arctic-inference is built from source which needs torch-xpu properly installed
-# used for suffix method speculative decoding
 RUN --mount=type=cache,target=/root/.cache/pip \
     pip install --no-cache-dir arctic-inference==0.1.1
 
@@ -76,7 +69,7 @@ FROM vllm-base AS vllm-openai
 
 # install additional dependencies for openai api server
 RUN --mount=type=cache,target=/root/.cache/pip \
-    pip install accelerate hf_transfer pytest pytest_asyncio lm_eval[api] modelscope
+    pip install accelerate hf_transfer pytest pytest_asyncio lm_eval[api] 'modelscope!=1.15.0'
 
 # install development dependencies (for testing)
 RUN python3 -m pip install -e tests/vllm_test_utils
@@ -85,6 +78,9 @@ RUN python3 -m pip install -e tests/vllm_test_utils
 ENV NIXL_VERSION=0.7.0
 RUN python3 /workspace/vllm/tools/install_nixl_from_source_ubuntu.py
 
+# FIX triton
+RUN --mount=type=cache,target=/root/.cache/pip pip uninstall triton triton-xpu -y && pip install triton-xpu==3.6.0 --extra-index-url=https://download.pytorch.org/whl/test/xpu
+
 # PyJWT-2.7.0 will influence some wheel behaviors, remove its dist-info to avoid conflicts
 RUN rm /usr/lib/python3/dist-packages/PyJWT-2.7.0.dist-info/ -rf
 
diff --git a/requirements/xpu.txt b/requirements/xpu.txt
index c1dc4195b..3fd7e240c 100644
--- a/requirements/xpu.txt
+++ b/requirements/xpu.txt
@@ -9,10 +9,11 @@ setuptools>=77.0.3,<81.0.0
 wheel
 jinja2>=3.1.6
 datasets # for benchmark scripts
-numba == 0.61.2 # Required for N-gram speculative decoding
---extra-index-url=https://download.pytorch.org/whl/xpu
-torch==2.9.0+xpu
+numba == 0.60.0 # v0.61 doesn't support Python 3.9. Required for N-gram speculative decoding
+tblib # for test
+--extra-index-url=https://download.pytorch.org/whl/test/xpu
+torch==2.10.0+xpu
 torchaudio
 torchvision
 
-intel-extension-for-pytorch @ https://intel-extension-for-pytorch.s3.us-east-1.amazonaws.com/ipex_dev/xpu/intel_extension_for_pytorch-2.9.10.post0%2Bxpu-cp312-cp312-linux_x86_64.whl
+intel-extension-for-pytorch @ https://ubit-artifactory-ba.intel.com/artifactory/aipc_releases-ba-local/gpu/new/validation/IPEX/nightly/PVC/UBUNTU/VLLM_nightly/20260114/1c16033db/intel_extension_for_pytorch-2.10.10.post1+xpu-cp312-cp312-linux_x86_64.whl
-- 
2.43.0


From 5192c1ca01c8fce6200c7123d010dab21f21efaf Mon Sep 17 00:00:00 2001
From: mayuyuace <qiming1.zhang@intel.com>
Date: Sun, 16 Nov 2025 18:54:28 -0800
Subject: [PATCH 22/36] [XPU][QWEN] support qwen3-30b-a3b gptq int4 and tp=1,2

Signed-off-by: mayuyuace <qiming1.zhang@intel.com>

Signed-off-by: Yan Ma <yan.ma@intel.com>
---
 vllm/model_executor/layers/fused_moe/layer.py |   1 +
 .../layers/quantization/auto_round.py         |  14 +-
 .../layers/quantization/ipex_quant.py         | 322 +++++++++++++++++-
 .../layers/quantization/utils/gptq_utils.py   |   8 +-
 4 files changed, 325 insertions(+), 20 deletions(-)

diff --git a/vllm/model_executor/layers/fused_moe/layer.py b/vllm/model_executor/layers/fused_moe/layer.py
index 3b3a789f6..712bdf85c 100644
--- a/vllm/model_executor/layers/fused_moe/layer.py
+++ b/vllm/model_executor/layers/fused_moe/layer.py
@@ -667,6 +667,7 @@ class FusedMoE(CustomOp):
         # need full intermediate size pre-sharding for WNA16 act order
         if self.quant_method.__class__.__name__ in (
             "GPTQMarlinMoEMethod",
+            "XPUGPTQMarlinMoEMethod",
             "CompressedTensorsWNA16MarlinMoEMethod",
             "CompressedTensorsWNA16MoEMethod",
         ):
diff --git a/vllm/model_executor/layers/quantization/auto_round.py b/vllm/model_executor/layers/quantization/auto_round.py
index 5d77d1e3c..0f0fd58c6 100644
--- a/vllm/model_executor/layers/quantization/auto_round.py
+++ b/vllm/model_executor/layers/quantization/auto_round.py
@@ -419,12 +419,22 @@ class AutoRoundConfig(QuantizationConfig):
         if isinstance(layer, (LinearBase, ParallelLMHead)):
             if "awq" in self.packing_format:
                 config = IPEXConfig(
-                    method="awq", weight_bits=weight_bits, group_size=group_size
+                    method="awq",
+                    weight_bits=weight_bits,
+                    group_size=group_size,
+                    is_qweight_sym=sym,
+                    dynamic={},
+                    full_config={},
                 )
                 return IPEXAWQLinearMethod(config)
             elif "gptq" in self.packing_format:
                 config = IPEXConfig(
-                    method="gptq", weight_bits=weight_bits, group_size=group_size
+                    method="gptq",
+                    weight_bits=weight_bits,
+                    group_size=group_size,
+                    is_qweight_sym=sym,
+                    dynamic={},
+                    full_config={},
                 )
                 return IPEXGPTQLinearMethod(config)
             else:
diff --git a/vllm/model_executor/layers/quantization/ipex_quant.py b/vllm/model_executor/layers/quantization/ipex_quant.py
index 15397a7c9..d3b7c0814 100644
--- a/vllm/model_executor/layers/quantization/ipex_quant.py
+++ b/vllm/model_executor/layers/quantization/ipex_quant.py
@@ -1,22 +1,28 @@
 # SPDX-License-Identifier: Apache-2.0
 # SPDX-FileCopyrightText: Copyright contributors to the vLLM project
 
-from typing import Any, Optional
+from typing import Any
 
 import torch
 from packaging import version
+from safetensors.torch import _TYPES as _SAFETENSORS_TO_TORCH_DTYPE
 from torch.nn import Module
 
 import vllm.envs as envs
 from vllm._ipex_ops import ipex_ops as ops
+from vllm.model_executor.layers.fused_moe import (
+    FusedMoEConfig,
+    FusedMoEMethodBase,
+    FusedMoeWeightScaleSupported,
+)
 from vllm.model_executor.layers.fused_moe.config import FusedMoEQuantConfig
+from vllm.model_executor.layers.fused_moe.layer import FusedMoE
 from vllm.model_executor.layers.fused_moe.fused_moe_router import (
     FusedMoERouter,
 )
 from vllm.model_executor.layers.fused_moe.layer import FusedMoE
 from vllm.model_executor.layers.linear import (
     LinearBase,
-    LinearMethodBase,
     UnquantizedLinearMethod,
 )
 from vllm.model_executor.layers.quantization import (
@@ -30,9 +36,16 @@ from vllm.model_executor.layers.quantization.fp8 import (
     Fp8OnlineMoEMethod,
 )
 from vllm.model_executor.layers.quantization.gptq import GPTQLinearMethod
+from vllm.model_executor.layers.quantization.utils.gptq_utils import (
+    get_linear_quant_method,
+)
 from vllm.model_executor.layers.quantization.utils.quant_utils import is_layer_skipped
 from vllm.model_executor.utils import replace_parameter, set_weight_attrs
 from vllm.platforms import current_platform
+from vllm.scalar_type import scalar_types
+from vllm.transformers_utils.config import get_safetensors_params_metadata
+from vllm.utils.collection_utils import is_list_of
+from vllm.utils.math_utils import round_up
 
 MIN_IPEX_VERSION = "2.6.0"
 
@@ -52,20 +65,28 @@ class IPEXConfig(QuantizationConfig):
         method: str,
         weight_bits: int,
         group_size: int,
+        is_qweight_sym: bool,
+        full_config: dict[str, Any],
+        dynamic: dict[str, dict[str, int | bool]],
         modules_to_not_convert: list[str] | None = None,
         desc_act: bool | None = None,
         lm_head_quantized: bool | None = None,
-        is_sym: bool | None = None,
+        modules_in_block_to_quantize: list[str] | None = None,
+        checkpoint_format: str = "",
     ) -> None:
         super().__init__()
+        self.dynamic = dynamic
         self.method = method
+        self.linear_quant_method = method
         self.weight_bits = weight_bits
         self.group_size = group_size
         self.modules_to_not_convert = modules_to_not_convert or []
         self.desc_act = desc_act
         self.lm_head_quantized = lm_head_quantized
-        self.is_sym = is_sym
+        self.modules_in_block_to_quantize = modules_in_block_to_quantize or []
+        self.full_config = full_config
         self.pack_factor = 32 // self.weight_bits
+        self.bit8_pack_factor = 8 // self.weight_bits
 
         if self.weight_bits not in [4]:
             raise ValueError(
@@ -77,12 +98,24 @@ class IPEXConfig(QuantizationConfig):
             raise ValueError(
                 f"IPEX quantization supports [awq, gptq], but got {self.method}."
             )
+        self.is_qweight_sym = is_qweight_sym
+        # used to identify GPTQ model quantized by autoround
+        self.autoround_version = (
+            full_config.get("autoround_version", "") if full_config is not None else ""
+        )
+        # GPTQ v1 and v2 format deals with zero points differently.
+        # Currently GPTQModel stores v1 format checkpoints by default,
+        # but provides the option to set `format="gptq_v2"` in `QuantizeConfig`.
+        self.checkpoint_format = checkpoint_format
 
     def __repr__(self) -> str:
         return (
             f"IPEXConfig(method={self.method},"
             f"weight_bits={self.weight_bits}, "
-            f"group_size={self.group_size})"
+            f"group_size={self.group_size}),"
+            f"dynamic={self.dynamic}, "
+            f"modules_in_block_to_quantize={self.modules_in_block_to_quantize}),"
+            f"checkpoint_format={self.checkpoint_format})"
         )
 
     @classmethod
@@ -106,6 +139,8 @@ class IPEXConfig(QuantizationConfig):
 
     @classmethod
     def from_config(cls, config: dict[str, Any]) -> "IPEXConfig":
+        dynamic = cls.get_from_keys_or(config, ["dynamic"], default={})
+        dynamic = {} if dynamic is None else dynamic
         method = cls.get_from_keys(config, ["quant_method"]).lower()
         if method == "awq":
             weight_bits = cls.get_from_keys(config, ["w_bit", "bits"])
@@ -113,24 +148,44 @@ class IPEXConfig(QuantizationConfig):
             modules_to_not_convert = cls.get_from_keys_or(
                 config, ["modules_to_not_convert"], None
             )
-            is_sym = not cls.get_from_keys_or(config, ["zero_point"], default=False)
+            is_qweight_sym = not cls.get_from_keys_or(
+                config, ["zero_point"], default=False
+            )
             return cls(
                 method,
                 weight_bits,
                 group_size,
+                is_qweight_sym,
+                config,
+                dynamic,
                 modules_to_not_convert,
                 False,
                 False,
-                is_sym,
             )
         # otherwise for gptq
         weight_bits = cls.get_from_keys(config, ["bits"])
         group_size = cls.get_from_keys(config, ["group_size"])
         lm_head_quantized = cls.get_from_keys_or(config, ["lm_head"], default=False)
         desc_act = cls.get_from_keys_or(config, ["desc_act"], default=False)
-        is_sym = cls.get_from_keys_or(config, ["sym"], default=True)
+        is_qweight_sym = cls.get_from_keys_or(config, ["sym"], default=True)
+        modules_in_block_to_quantize = cls.get_from_keys_or(
+            config, ["modules_in_block_to_quantize"], default=None
+        )
+        checkpoint_format = cls.get_from_keys_or(
+            config, ["checkpoint_format"], default=""
+        )
         return cls(
-            method, weight_bits, group_size, [], desc_act, lm_head_quantized, is_sym
+            method,
+            weight_bits,
+            group_size,
+            is_qweight_sym,
+            config,
+            dynamic,
+            [],
+            desc_act,
+            lm_head_quantized,
+            modules_in_block_to_quantize,
+            checkpoint_format,
         )
 
     @classmethod
@@ -147,9 +202,7 @@ class IPEXConfig(QuantizationConfig):
 
         return None
 
-    def get_quant_method(
-        self, layer: torch.nn.Module, prefix: str
-    ) -> Optional["LinearMethodBase"]:
+    def get_quant_method(self, layer: torch.nn.Module, prefix: str):
         if isinstance(layer, LinearBase):
             if self.method == "awq":
                 if is_layer_skipped(
@@ -161,9 +214,41 @@ class IPEXConfig(QuantizationConfig):
                     return UnquantizedLinearMethod()
                 return IPEXAWQLinearMethod(self)
             if self.method == "gptq":
-                return IPEXGPTQLinearMethod(self)
+                return get_linear_quant_method(
+                    self, layer, prefix, IPEXGPTQLinearMethod
+                )
+        if isinstance(layer, FusedMoE) and self.method == "gptq":
+            return XPUGPTQMarlinMoEMethod(self, layer.moe_config)
         return None
 
+    def apply_vllm_mapper(self, hf_to_vllm_mapper):
+        if self.modules_in_block_to_quantize is not None:
+            self.modules_in_block_to_quantize = hf_to_vllm_mapper.apply_list(
+                self.modules_in_block_to_quantize
+            )
+
+    def maybe_update_config(self, model_name: str, revision: str | None = None):
+        if self.modules_in_block_to_quantize:
+            if is_list_of(self.modules_in_block_to_quantize, list):
+                # original modules_in_block_to_quantize: list[list[str]]
+                # flatten original modules_in_block_to_quantize
+                self.modules_in_block_to_quantize = [
+                    item
+                    for sublist in self.modules_in_block_to_quantize
+                    for item in sublist
+                ]
+            return
+
+        unquant_dtypes = [torch.float16, torch.bfloat16, torch.float32]
+        metadata = get_safetensors_params_metadata(model_name, revision=revision)
+        quant_layers: set[str] = {
+            param_name.rsplit(".", 1)[0]
+            for param_name, info in metadata.items()
+            if (dtype := info.get("dtype", None))
+            and _SAFETENSORS_TO_TORCH_DTYPE[dtype] not in unquant_dtypes
+        }
+        self.modules_in_block_to_quantize = list(quant_layers)
+
 
 class IPEXGPTQLinearMethod(GPTQLinearMethod):
     """GPTQ linear method using IPEX for the CPU/XPU backend."""
@@ -219,7 +304,7 @@ class IPEXGPTQLinearMethod(GPTQLinearMethod):
                 bias=bias,
                 group_size=self.quant_config.group_size,
                 quant_method=IPEXConfig.IPEX_QUANT_METHOD_MAP["gptq"],
-                weight_qscheme="sym" if self.quant_config.is_sym else "asym",
+                weight_qscheme="sym" if self.quant_config.is_qweight_sym else "asym",
             )
         )
 
@@ -290,7 +375,7 @@ class IPEXAWQLinearMethod(AWQLinearMethod):
                 bias=bias,
                 group_size=self.quant_config.group_size,
                 quant_method=IPEXConfig.IPEX_QUANT_METHOD_MAP["awq"],  # type: ignore
-                weight_qscheme="sym" if self.quant_config.is_sym else "asym",
+                weight_qscheme="sym" if self.quant_config.is_qweight_sym else "asym",
             )
         )
 
@@ -462,3 +547,210 @@ class XPUFp8MoEMethod(Fp8OnlineMoEMethod):
             layer.num_expert_group,
             custom_routing_function=layer.custom_routing_function,
         )
+
+
+class XPUGPTQMarlinMoEMethod(FusedMoEMethodBase):
+    TYPE_MAP = {
+        (4, True): scalar_types.uint4b8,
+        (8, True): scalar_types.uint8b128,
+    }
+
+    def __init__(
+        self,
+        quant_config: IPEXConfig,
+        moe: FusedMoEConfig,
+    ) -> None:
+        super().__init__(moe)
+        self.quant_config = quant_config
+
+        weight_bits = quant_config.weight_bits
+        is_qweight_sym = quant_config.is_qweight_sym
+        self.quant_type = self.TYPE_MAP[(weight_bits, is_qweight_sym)]
+
+        if self.quant_type.size_bits != 4:
+            raise ValueError("XPUGPTQMarlinMoEMethod only supports int4 now.")
+
+    def get_fused_moe_quant_config(
+        self, layer: torch.nn.Module
+    ) -> FusedMoEQuantConfig | None:
+        return None
+
+    def create_weights(
+        self,
+        layer: torch.nn.Module,
+        num_experts: int,
+        hidden_size: int,
+        intermediate_size_per_partition: int,
+        params_dtype: torch.dtype,
+        **extra_weight_attrs,
+    ):
+        intermediate_size_full = extra_weight_attrs.pop("intermediate_size_full")
+
+        self.is_k_full = (not self.quant_config.desc_act) or (
+            intermediate_size_per_partition == intermediate_size_full
+        )
+        intermediate_size_per_partition = round_up(
+            intermediate_size_per_partition, self.quant_config.group_size
+        )
+        if self.quant_config.group_size != -1:
+            scales_size13 = hidden_size // self.quant_config.group_size
+            w2_scales_size = (
+                intermediate_size_full
+                if self.quant_config.desc_act
+                else intermediate_size_per_partition
+            )
+            scales_size2 = w2_scales_size // self.quant_config.group_size
+            strategy = FusedMoeWeightScaleSupported.GROUP.value
+        else:
+            scales_size13 = 1
+            scales_size2 = 1
+            strategy = FusedMoeWeightScaleSupported.CHANNEL.value
+
+        extra_weight_attrs.update({"quant_method": strategy, "is_transposed": True})
+        # Fused gate_up_proj (column parallel)
+        w13_qweight = torch.nn.Parameter(
+            torch.empty(
+                num_experts,
+                hidden_size // self.quant_config.pack_factor,
+                2 * intermediate_size_per_partition,
+                dtype=torch.int32,
+            ),
+            requires_grad=False,
+        )
+        layer.register_parameter("w13_qweight", w13_qweight)
+        set_weight_attrs(w13_qweight, extra_weight_attrs)
+        # down_proj (row parallel)
+        w2_qweight = torch.nn.Parameter(
+            torch.empty(
+                num_experts,
+                intermediate_size_per_partition // self.quant_config.pack_factor,
+                hidden_size,
+                dtype=torch.int32,
+            ),
+            requires_grad=False,
+        )
+        layer.register_parameter("w2_qweight", w2_qweight)
+        set_weight_attrs(w2_qweight, extra_weight_attrs)
+        # up_proj scales
+        w13_scales = torch.nn.Parameter(
+            torch.empty(
+                num_experts,
+                scales_size13,
+                2 * intermediate_size_per_partition,
+                dtype=params_dtype,
+            ),
+            requires_grad=False,
+        )
+        layer.register_parameter("w13_scales", w13_scales)
+        set_weight_attrs(w13_scales, extra_weight_attrs)
+        # down_proj scales
+        w2_scales = torch.nn.Parameter(
+            torch.empty(num_experts, scales_size2, hidden_size, dtype=params_dtype),
+            requires_grad=False,
+        )
+        layer.register_parameter("w2_scales", w2_scales)
+        set_weight_attrs(w2_scales, extra_weight_attrs)
+        # don't shard the w2 scales when running act order
+        set_weight_attrs(w2_scales, {"load_full_w2": self.quant_config.desc_act})
+        # up_proj scales
+        w13_qzeros = torch.nn.Parameter(
+            torch.empty(
+                num_experts,
+                scales_size13,
+                2 * intermediate_size_per_partition // self.quant_config.pack_factor,
+                dtype=params_dtype,
+            ),
+            requires_grad=False,
+        )
+        layer.register_parameter("w13_qzeros", w13_qzeros)
+        set_weight_attrs(w13_qzeros, extra_weight_attrs)
+        # down_proj scales
+        w2_qzeros = torch.nn.Parameter(
+            torch.empty(
+                num_experts,
+                scales_size2,
+                hidden_size // self.quant_config.pack_factor,
+                dtype=params_dtype,
+            ),
+            requires_grad=False,
+        )
+        layer.register_parameter("w2_qzeros", w2_qzeros)
+        set_weight_attrs(w2_qzeros, extra_weight_attrs)
+        # don't shard the w2 scales when running act order
+        set_weight_attrs(w2_qzeros, {"load_full_w2": self.quant_config.desc_act})
+        w13_g_idx = torch.nn.Parameter(
+            torch.empty(
+                num_experts,
+                hidden_size,
+                dtype=torch.int32,
+            ),
+            requires_grad=False,
+        )
+        layer.register_parameter("w13_g_idx", w13_g_idx)
+        set_weight_attrs(w13_g_idx, extra_weight_attrs)
+        w2_g_idx = torch.nn.Parameter(
+            torch.empty(
+                num_experts,
+                intermediate_size_per_partition,
+                dtype=torch.int32,
+            ),
+            requires_grad=False,
+        )
+        layer.register_parameter("w2_g_idx", w2_g_idx)
+        set_weight_attrs(w2_g_idx, extra_weight_attrs)
+        w13_g_idx_sort_indices = torch.nn.Parameter(
+            torch.empty(
+                num_experts,
+                hidden_size,
+                dtype=torch.int32,
+            ),
+            requires_grad=False,
+        )
+        layer.register_parameter("w13_g_idx_sort_indices", w13_g_idx_sort_indices)
+        set_weight_attrs(w13_g_idx_sort_indices, extra_weight_attrs)
+        w2_g_idx_sort_indices = torch.nn.Parameter(
+            torch.empty(
+                num_experts,
+                intermediate_size_per_partition,
+                dtype=torch.int32,
+            ),
+            requires_grad=False,
+        )
+        layer.register_parameter("w2_g_idx_sort_indices", w2_g_idx_sort_indices)
+        set_weight_attrs(w2_g_idx_sort_indices, extra_weight_attrs)
+
+    def process_weights_after_loading(self, layer: torch.nn.Module) -> None:
+        import intel_extension_for_pytorch as ipex
+
+        if self.quant_config.linear_quant_method == "gptq":
+            layer.ipex_fusion = ipex.llm.modules.GatedMLPMOE(
+                layer.w13_qweight.permute(0, 2, 1),
+                layer.w2_qweight.permute(0, 2, 1),
+                w1_scale_inv=layer.w13_scales.permute(0, 2, 1),
+                w2_scale_inv=layer.w2_scales.permute(0, 2, 1),
+                is_int4=True,
+            )
+        else:
+            raise NotImplementedError(
+                f"Unsupported quant method {self.quant_config.linear_quant_method} "
+                "for XPU MOE."
+            )
+
+    def apply(
+        self,
+        layer: FusedMoE,
+        x: torch.Tensor,
+        router_logits: torch.Tensor,
+    ) -> torch.Tensor | tuple[torch.Tensor, torch.Tensor]:
+        res = layer.ipex_fusion(
+            x,
+            layer.use_grouped_topk,
+            layer.top_k,
+            router_logits,
+            layer.renormalize,
+            topk_group=layer.topk_group,
+            num_expert_group=layer.num_expert_group,
+            custom_routing_function=layer.custom_routing_function,
+            scoring_func=layer.scoring_func,
+        )
+        return res
diff --git a/vllm/model_executor/layers/quantization/utils/gptq_utils.py b/vllm/model_executor/layers/quantization/utils/gptq_utils.py
index dfebeca93..8c55f5f51 100644
--- a/vllm/model_executor/layers/quantization/utils/gptq_utils.py
+++ b/vllm/model_executor/layers/quantization/utils/gptq_utils.py
@@ -18,14 +18,16 @@ from vllm.model_executor.layers.vocab_parallel_embedding import (
 if TYPE_CHECKING:
     from ..gptq import GPTQConfig
     from ..gptq_marlin import GPTQMarlinConfig
+    from ..ipex_quant import IPEXConfig
 else:
     GPTQConfig = object
     GPTQMarlinConfig = object
+    IPEXConfig = object
 
 
 # Match dynamic rules with module name (prefix) and override quantize
 # config if module (prefix) matches a rule
-def override_config(config: GPTQConfig | GPTQMarlinConfig, prefix: str):
+def override_config(config: GPTQConfig | GPTQMarlinConfig | IPEXConfig, prefix: str):
     weight_bits = get_dynamic_override(config, prefix, "bits", config.weight_bits)
     if isinstance(weight_bits, int):
         config.weight_bits = weight_bits
@@ -60,7 +62,7 @@ def override_config(config: GPTQConfig | GPTQMarlinConfig, prefix: str):
 
 
 def get_dynamic_override(
-    config: GPTQConfig | GPTQMarlinConfig,
+    config: GPTQConfig | GPTQMarlinConfig | IPEXConfig,
     layer_name: str,
     key: str | None = None,
     default_value: int | bool | None = None,
@@ -126,7 +128,7 @@ def is_layer_gptq_quantized(
 
 
 def get_linear_quant_method(
-    config: GPTQConfig | GPTQMarlinConfig,
+    config: GPTQConfig | GPTQMarlinConfig | IPEXConfig,
     layer: torch.nn.Module,
     prefix: str,
     linear_method_cls: type,
-- 
2.43.0


From 464c06b7e9a08fd6de4ac4414ebdbf4622474e62 Mon Sep 17 00:00:00 2001
From: Chaojun Zhang <chaojun.zhang@intel.com>
Date: Tue, 6 Jan 2026 09:49:37 +0800
Subject: [PATCH 23/36] Support cpu kv-cache offloading on XPU (#78)

Signed-off-by: chaojun-zhang <chaojun.zhang@intel.com>
---
 tests/v1/kv_offload/test_cpu_gpu.py        | 12 +++++--
 tests/v1/kv_offload/test_cpu_offloading.py |  6 ++--
 vllm/v1/kv_offload/cpu.py                  | 29 +++++++++++-----
 vllm/v1/kv_offload/worker/cpu_gpu.py       |  7 +++-
 vllm/v1/kv_offload/worker/cpu_xpu.py       | 39 ++++++++++++++++++++++
 5 files changed, 77 insertions(+), 16 deletions(-)
 create mode 100644 vllm/v1/kv_offload/worker/cpu_xpu.py

diff --git a/tests/v1/kv_offload/test_cpu_gpu.py b/tests/v1/kv_offload/test_cpu_gpu.py
index b3696f1cc..1f6b9d90c 100644
--- a/tests/v1/kv_offload/test_cpu_gpu.py
+++ b/tests/v1/kv_offload/test_cpu_gpu.py
@@ -10,11 +10,17 @@ from vllm.platforms import current_platform
 from vllm.utils.torch_utils import set_random_seed
 from vllm.v1.attention.backends.flash_attn import FlashAttentionBackend
 from vllm.v1.kv_offload.mediums import CPULoadStoreSpec, GPULoadStoreSpec
-from vllm.v1.kv_offload.worker.cpu_gpu import CpuGpuOffloadingHandlers
+
+if current_platform.is_xpu():
+    from vllm.v1.kv_offload.worker.cpu_xpu import (
+        CpuXpuOffloadingHandlers as CpuGpuOffloadingHandlers,
+    )
+else:
+    from vllm.v1.kv_offload.worker.cpu_gpu import CpuGpuOffloadingHandlers
 
 BACKENDS_TO_TEST = [FlashAttentionBackend]
 
-if not current_platform.is_rocm():
+if not current_platform.is_rocm() and not current_platform.is_xpu():
     from vllm.v1.attention.backends.flashinfer import FlashInferBackend
 
     BACKENDS_TO_TEST.append(FlashInferBackend)
@@ -32,7 +38,7 @@ NUM_HEADS = [8]
 NUM_LAYERS = [4]
 DTYPES = [torch.bfloat16]
 SEEDS = [0]
-CUDA_DEVICES = ["cuda:0"]
+CUDA_DEVICES = [f"{current_platform.device_type}:0"]
 NUM_MAPPINGS = [3]
 
 
diff --git a/tests/v1/kv_offload/test_cpu_offloading.py b/tests/v1/kv_offload/test_cpu_offloading.py
index 103675608..497a9fe11 100644
--- a/tests/v1/kv_offload/test_cpu_offloading.py
+++ b/tests/v1/kv_offload/test_cpu_offloading.py
@@ -14,14 +14,14 @@ from vllm.config import KVEventsConfig, KVTransferConfig
 from vllm.distributed.kv_events import BlockStored, KVEventBatch
 from vllm.platforms import current_platform
 
-CPU_BLOCK_SIZES = [48]
-ATTN_BACKENDS = []
+CPU_BLOCK_SIZES = [64] if current_platform.is_xpu() else [48]
 
 if current_platform.is_cuda():
     ATTN_BACKENDS = ["FLASH_ATTN", "FLASHINFER", "TRITON_ATTN"]
 elif current_platform.is_rocm():
     ATTN_BACKENDS = ["TRITON_ATTN"]
-
+elif current_platform.is_xpu():
+    ATTN_BACKENDS = ["FLASH_ATTN"]
 
 class MockSubscriber:
     """Helper class to receive and verify published events"""
diff --git a/vllm/v1/kv_offload/cpu.py b/vllm/v1/kv_offload/cpu.py
index d07ef8ad0..51e84e59d 100644
--- a/vllm/v1/kv_offload/cpu.py
+++ b/vllm/v1/kv_offload/cpu.py
@@ -91,18 +91,29 @@ class CPUOffloadingSpec(OffloadingSpec):
         attn_backends: dict[str, type[AttentionBackend]],
     ) -> Iterator[tuple[type[LoadStoreSpec], type[LoadStoreSpec], OffloadingHandler]]:
         if not self._handlers:
-            if not current_platform.is_cuda_alike():
+            if not current_platform.is_cuda_alike() and not current_platform.is_xpu():
                 raise Exception(
                     "CPU Offloading is currently only supported on CUDA-alike GPUs"
+                    " and Intel XPUs"
+                )
+            if current_platform.is_xpu():
+                from vllm.v1.kv_offload.worker.cpu_xpu import CpuXpuOffloadingHandlers
+
+                self._handlers = CpuXpuOffloadingHandlers(
+                    attn_backends=attn_backends,
+                    gpu_block_size=self.gpu_block_size,
+                    cpu_block_size=self.offloaded_block_size,
+                    num_cpu_blocks=self.num_blocks,
+                    gpu_caches=kv_caches,
+                )
+            else:
+                self._handlers = CpuGpuOffloadingHandlers(
+                    attn_backends=attn_backends,
+                    gpu_block_size=self.gpu_block_size,
+                    cpu_block_size=self.offloaded_block_size,
+                    num_cpu_blocks=self.num_blocks,
+                    gpu_caches=kv_caches,
                 )
-
-            self._handlers = CpuGpuOffloadingHandlers(
-                attn_backends=attn_backends,
-                gpu_block_size=self.gpu_block_size,
-                cpu_block_size=self.offloaded_block_size,
-                num_cpu_blocks=self.num_blocks,
-                gpu_caches=kv_caches,
-            )
 
         assert self._handlers is not None
         yield GPULoadStoreSpec, CPULoadStoreSpec, self._handlers.gpu_to_cpu_handler
diff --git a/vllm/v1/kv_offload/worker/cpu_gpu.py b/vllm/v1/kv_offload/worker/cpu_gpu.py
index c18c4a411..24d7de2dd 100644
--- a/vllm/v1/kv_offload/worker/cpu_gpu.py
+++ b/vllm/v1/kv_offload/worker/cpu_gpu.py
@@ -5,7 +5,12 @@ from collections import deque
 import numpy as np
 import torch
 
-from vllm import _custom_ops as ops
+from vllm.platforms import current_platform
+
+if current_platform.is_cuda_alike():
+    from vllm import _custom_ops as ops
+elif current_platform.is_xpu():
+    from vllm._ipex_ops import ipex_ops as ops
 from vllm.logger import init_logger
 from vllm.utils.platform_utils import is_pin_memory_available
 from vllm.v1.attention.backend import AttentionBackend
diff --git a/vllm/v1/kv_offload/worker/cpu_xpu.py b/vllm/v1/kv_offload/worker/cpu_xpu.py
new file mode 100644
index 000000000..c88d325e0
--- /dev/null
+++ b/vllm/v1/kv_offload/worker/cpu_xpu.py
@@ -0,0 +1,39 @@
+# SPDX-License-Identifier: Apache-2.0
+# SPDX-FileCopyrightText: Copyright contributors to the vLLM project
+from contextlib import contextmanager
+
+import torch
+
+from vllm.attention.backends.abstract import AttentionBackend
+from vllm.v1.kv_offload.worker.cpu_gpu import CpuGpuOffloadingHandlers
+
+
+class CpuXpuOffloadingHandlers(CpuGpuOffloadingHandlers):
+    def __init__(
+        self,
+        gpu_block_size: int,
+        cpu_block_size: int,
+        num_cpu_blocks: int,
+        gpu_caches: dict[str, torch.Tensor],
+        attn_backends: dict[str, type[AttentionBackend]],
+    ):
+        with _torch_cuda_wrapper():
+            super().__init__(
+                gpu_block_size,
+                cpu_block_size,
+                num_cpu_blocks,
+                gpu_caches,
+                attn_backends,
+            )
+
+
+@contextmanager
+def _torch_cuda_wrapper():
+    try:
+        # replace cuda APIs with xpu APIs, this should work by default
+        torch.cuda.Event = torch.xpu.Event
+        torch.cuda.Stream = torch.xpu.Stream
+        torch.cuda.stream = torch.xpu.stream
+        yield
+    finally:
+        pass
-- 
2.43.0


From 5e2ecfb7b7a0390db89997911ed8ad54c752379f Mon Sep 17 00:00:00 2001
From: YiSheng5 <yi.sheng@intel.com>
Date: Thu, 22 Jan 2026 15:00:11 +0800
Subject: [PATCH 24/36] [XPU]Support AgRsAll2AllManager on XPU device (#32654)
 (#105)

Signed-off-by: yisheng <yi.sheng@intel.com>
---
 .../device_communicators/xpu_communicator.py  | 137 +++++++++++++++++-
 1 file changed, 130 insertions(+), 7 deletions(-)

diff --git a/vllm/distributed/device_communicators/xpu_communicator.py b/vllm/distributed/device_communicators/xpu_communicator.py
index f3d9262d2..6bc26b6f3 100644
--- a/vllm/distributed/device_communicators/xpu_communicator.py
+++ b/vllm/distributed/device_communicators/xpu_communicator.py
@@ -23,23 +23,146 @@ class XpuCommunicator(DeviceCommunicatorBase):
     ):
         super().__init__(cpu_group, device, device_group, unique_name)
         if self.use_all2all:
-            if self.all2all_backend != "naive":  # type: ignore[has-type]
-                logger.warning(
-                    "`%s` all2all manager is not supported on XPU. "
-                    "Falling back to `naive` all2all manager for XPU.",
-                    self.all2all_backend,  # type: ignore[has-type]
-                )
-                self.all2all_backend = "naive"
             if self.all2all_backend == "naive":
                 from .all2all import NaiveAll2AllManager
 
                 self.all2all_manager = NaiveAll2AllManager(self.cpu_group)
                 logger.info("Using naive all2all manager.")
 
+            elif self.all2all_backend == "allgather_reducescatter":
+                from .all2all import AgRsAll2AllManager
+
+                self.all2all_manager = AgRsAll2AllManager(self.cpu_group)
+                logger.info("Using AgRs manager on XPU device.")
+
+            else:  # type: ignore[has-type]
+                logger.warning(
+                    "`%s` all2all manager is not supported on XPU. "
+                    "Falling back to AgRs manager for XPU, "
+                    "which is the Default backend",
+                    self.all2all_backend,  # type: ignore[has-type]
+                )
+                from .all2all import AgRsAll2AllManager
+
+                self.all2all_manager = AgRsAll2AllManager(self.cpu_group)
+                logger.info("Using AgRs manager on XPU device.")
+
     def all_reduce(self, input_) -> torch.Tensor:
         dist.all_reduce(input_, group=self.device_group)
         return input_
 
+    def reduce_scatter(self, input_: torch.Tensor, dim: int = -1):
+        world_size = self.world_size
+
+        if dim < 0:
+            # Convert negative dim to positive.
+            dim += input_.dim()
+
+        # Note: This will produce an incorrect answer if we don't make
+        # the input_tensor contiguous. Possible bug in reduce_scatter_tensor?
+        input_tensor = input_.movedim(0, dim).contiguous()
+
+        assert input_tensor.shape[0] % world_size == 0
+        chunk_size = input_tensor.shape[0] // world_size
+        output_shape = (chunk_size,) + input_tensor.shape[1:]
+
+        output = torch.empty(
+            output_shape, dtype=input_tensor.dtype, device=input_tensor.device
+        )
+
+        dist.reduce_scatter_tensor(output, input_tensor)
+
+        # Reshape before returning
+        return output.movedim(0, dim).contiguous()
+
+    def reduce_scatterv(
+        self, input_: torch.Tensor, dim: int = -1, sizes: list[int] | None = None
+    ):
+        world_size = self.world_size
+
+        if dim < 0:
+            # Convert negative dim to positive.
+            dim += input_.dim()
+
+        # Note: This will produce an incorrect answer if we don't make
+        # the input_tensor contiguous. Possible bug in reduce_scatter_tensor?
+        input_tensor = input_.movedim(0, dim).contiguous()
+
+        if sizes is not None:
+            assert len(sizes) == world_size
+            assert input_tensor.shape[0] == sum(sizes)
+            chunk_size = sizes[self.rank_in_group]
+        else:
+            assert input_tensor.shape[0] % world_size == 0
+            chunk_size = input_tensor.shape[0] // world_size
+        output_shape = (chunk_size,) + input_tensor.shape[1:]
+
+        output = torch.empty(
+            output_shape, dtype=input_tensor.dtype, device=input_tensor.device
+        )
+        if sizes is not None and sizes.count(sizes[0]) != len(sizes):
+            # if inputs shape in different ranks is not the same using reduce_scatter
+            input_splits = list(input_tensor.split(sizes, dim=0))
+            dist.reduce_scatter(output, input_splits)
+        else:
+            dist.reduce_scatter_tensor(output, input_tensor)
+        # Reshape before returning
+        return output.movedim(0, dim).contiguous()
+
+    def all_gatherv(
+        self,
+        input_: torch.Tensor | list[torch.Tensor],
+        dim: int = 0,
+        sizes: list[int] | None = None,
+    ):
+        if dim != 0:
+            raise NotImplementedError("only dim 0 all-gatherv is supported")
+        world_size = self.world_size
+
+        # 'sizes' is not needed if all inputs in the same group have the same
+        # shape
+        if sizes is not None and all(s == sizes[0] for s in sizes):
+            sizes = None
+
+        def _all_gather_single(input_: torch.Tensor, sizes: list[int] | None = None):
+            input_size = input_.size()
+            if sizes is not None:
+                assert len(sizes) == world_size
+                assert input_.shape[dim] == sizes[self.rank_in_group], (
+                    f"{input_.shape[dim]} != {sizes[self.rank_in_group]}"
+                )
+                output_size = (sum(sizes),) + input_size[1:]
+            else:
+                output_size = (input_size[0] * world_size,) + input_size[1:]
+            # Allocate output tensor.
+            output_tensor = torch.empty(
+                output_size, dtype=input_.dtype, device=input_.device
+            )
+
+            if sizes is not None:
+                all_gather_list = []
+                for size in sizes:
+                    all_gather_list.append(
+                        torch.empty(
+                            (size,) + input_.shape[1:],
+                            dtype=input_.dtype,
+                            device=input_.device,
+                        )
+                    )
+                dist.all_gather(all_gather_list, input_)
+                output_tensor = torch.cat(all_gather_list, dim=0)
+            else:
+                dist.all_gather([output_tensor], input_)
+            return output_tensor
+
+        if isinstance(input_, torch.Tensor):
+            return _all_gather_single(input_, sizes)
+
+        output_list = []
+        for inp in input_:
+            output_list.append(_all_gather_single(inp, sizes=sizes))
+        return output_list
+
     def gather(
         self, input_: torch.Tensor, dst: int = 0, dim: int = -1
     ) -> torch.Tensor | None:
-- 
2.43.0


From 4d839d4edcfab2d0103d18eb6a56fe435941ce99 Mon Sep 17 00:00:00 2001
From: Chaojun Zhang <chaojun.zhang@intel.com>
Date: Fri, 23 Jan 2026 14:05:42 +0800
Subject: [PATCH 25/36] Support cpu kv-cache offloading on XPU (#103)

Signed-off-by: chaojun-zhang <chaojun.zhang@intel.com>
---
 vllm/v1/attention/backends/flash_attn.py | 13 +++++++++++--
 vllm/v1/kv_offload/worker/cpu_gpu.py     |  4 +++-
 vllm/v1/kv_offload/worker/cpu_xpu.py     |  3 ++-
 3 files changed, 16 insertions(+), 4 deletions(-)

diff --git a/vllm/v1/attention/backends/flash_attn.py b/vllm/v1/attention/backends/flash_attn.py
index 4d2eb5585..6942491f1 100755
--- a/vllm/v1/attention/backends/flash_attn.py
+++ b/vllm/v1/attention/backends/flash_attn.py
@@ -40,6 +40,7 @@ from vllm.logger import init_logger
 from vllm.model_executor.layers.batch_invariant import (
     vllm_is_batch_invariant,
 )
+from vllm.platforms import current_platform
 from vllm.platforms.interface import DeviceCapability
 from vllm.utils.math_utils import cdiv
 from vllm.v1.attention.backend import (
@@ -121,12 +122,20 @@ class FlashAttentionBackend(AttentionBackend):
         # `stride_order` indicates the permutation that gets
         # us from `get_kv_cache_shape` to the actual memory layout we want.
         cache_layout = get_kv_cache_layout()
-        if cache_layout == "NHD" and include_num_layers_dimension:
+        if (
+            cache_layout == "NHD"
+            and include_num_layers_dimension
+            and not current_platform.is_xpu()
+        ):
             # (num_blocks, num_layers, 2, block_size, num_kv_heads, head_size)
             return (2, 0, 1, 3, 4, 5)
         elif cache_layout == "NHD":
             stride_order = (0, 1, 2, 3, 4)
-        elif cache_layout == "HND" and include_num_layers_dimension:
+        elif (
+            cache_layout == "HND"
+            and include_num_layers_dimension
+            and not current_platform.is_xpu()
+        ):
             # (num_blocks, num_kv_heads, num_layers, 2, block_size, head_size)
             return (2, 4, 0, 1, 3, 5)
         elif cache_layout == "HND":
diff --git a/vllm/v1/kv_offload/worker/cpu_gpu.py b/vllm/v1/kv_offload/worker/cpu_gpu.py
index 24d7de2dd..675f76f24 100644
--- a/vllm/v1/kv_offload/worker/cpu_gpu.py
+++ b/vllm/v1/kv_offload/worker/cpu_gpu.py
@@ -99,7 +99,9 @@ class SingleDirectionOffloadingHandler(OffloadingHandler):
         self.dst_block_size_factor: int = dst_block_size_factor
 
         assert len(src_tensors) > 0
-        self.gpu_to_cpu: bool = self.src_tensors[0].is_cuda
+        self.gpu_to_cpu: bool = (
+            self.src_tensors[0].is_cuda or self.src_tensors[0].is_xpu
+        )
 
         # job_id -> event
         self._transfer_events: dict[int, torch.Event] = {}
diff --git a/vllm/v1/kv_offload/worker/cpu_xpu.py b/vllm/v1/kv_offload/worker/cpu_xpu.py
index c88d325e0..88a46f416 100644
--- a/vllm/v1/kv_offload/worker/cpu_xpu.py
+++ b/vllm/v1/kv_offload/worker/cpu_xpu.py
@@ -4,7 +4,7 @@ from contextlib import contextmanager
 
 import torch
 
-from vllm.attention.backends.abstract import AttentionBackend
+from vllm.v1.attention.backend import AttentionBackend
 from vllm.v1.kv_offload.worker.cpu_gpu import CpuGpuOffloadingHandlers
 
 
@@ -34,6 +34,7 @@ def _torch_cuda_wrapper():
         torch.cuda.Event = torch.xpu.Event
         torch.cuda.Stream = torch.xpu.Stream
         torch.cuda.stream = torch.xpu.stream
+        torch.cuda.current_stream = torch.xpu.current_stream
         yield
     finally:
         pass
-- 
2.43.0


From c86fdddfb646e1d27908c3e33381d8188101cf8d Mon Sep 17 00:00:00 2001
From: Yan Ma <yan.ma@intel.com>
Date: Tue, 27 Jan 2026 10:01:24 +0800
Subject: [PATCH 26/36] fix fp8 for some pooler models (#104)

Signed-off-by: Yan Ma <yan.ma@intel.com>
---
 vllm/model_executor/layers/quantization/ipex_quant.py | 5 ++++-
 1 file changed, 4 insertions(+), 1 deletion(-)

diff --git a/vllm/model_executor/layers/quantization/ipex_quant.py b/vllm/model_executor/layers/quantization/ipex_quant.py
index d3b7c0814..66ca98809 100644
--- a/vllm/model_executor/layers/quantization/ipex_quant.py
+++ b/vllm/model_executor/layers/quantization/ipex_quant.py
@@ -16,7 +16,6 @@ from vllm.model_executor.layers.fused_moe import (
     FusedMoeWeightScaleSupported,
 )
 from vllm.model_executor.layers.fused_moe.config import FusedMoEQuantConfig
-from vllm.model_executor.layers.fused_moe.layer import FusedMoE
 from vllm.model_executor.layers.fused_moe.fused_moe_router import (
     FusedMoERouter,
 )
@@ -413,6 +412,10 @@ class XPUFp8LinearMethod(Fp8LinearMethod):
     ) -> torch.Tensor:
         weight = layer.weight.data
         weight_scale = layer.weight_scale.data
+        # In case of pooler models, pooled_data may change to head_dtype like float
+        # which fp8_gemm_w8a16 doesn't support
+        if x.dtype == torch.float:
+            x = x.to(torch.bfloat16)
         output = torch.ops.torch_ipex.fp8_gemm_w8a16(
             x, weight, True, weight_scale, bias
         )
-- 
2.43.0


From dd0c986140c640d9bc8af622695d0c3d3118e5ec Mon Sep 17 00:00:00 2001
From: Roger Feng <roger.feng@intel.com>
Date: Tue, 27 Jan 2026 14:38:06 +0800
Subject: [PATCH 27/36] rebase to 0.14.1 (#111)
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

* Replace urllib's `urlparse` with urllib3's `parse_url` (#32746)

Signed-off-by: Isotr0py <mozf@mail2.sysu.edu.cn>
(cherry picked from commit 8ebf271bb6d1e7e9b1a55be73d755ef1a57dbbe5)

* Bump opencv-python dependecy version to 4.13 (#32668)

Signed-off-by: Isotr0py <mozf@mail2.sysu.edu.cn>
(cherry picked from commit 444e2e7e1f79f3c8fdf57c362272127df55e4847)

* Fix Whisper/encoder-decoder GPU memory leak (#32789)

Signed-off-by: NickLucche <nlucches@redhat.com>
(cherry picked from commit ea6102b85da808b23055912391977f43fbe3f227)

---------

Signed-off-by: Isotr0py <mozf@mail2.sysu.edu.cn>
Signed-off-by: NickLucche <nlucches@redhat.com>
Co-authored-by: Isotr0py <mozf@mail2.sysu.edu.cn>
Co-authored-by: Nicol√≤ Lucchesi <nlucches@redhat.com>
---
 requirements/common.txt                       |  2 +-
 requirements/nightly_torch_test.txt           |  6 +--
 requirements/test.in                          |  6 +--
 requirements/test.txt                         | 33 ++++++++++----
 .../openai/test_translation_validation.py     |  6 ++-
 .../multimodal/generation/test_whisper.py     | 43 +++++++++++++++++++
 vllm/connections.py                           |  4 +-
 vllm/envs.py                                  |  4 +-
 vllm/multimodal/utils.py                      | 25 ++++++-----
 vllm/utils/network_utils.py                   |  6 ++-
 vllm/v1/core/encoder_cache_manager.py         | 16 ++++---
 11 files changed, 114 insertions(+), 37 deletions(-)

diff --git a/requirements/common.txt b/requirements/common.txt
index 26d53f80a..285e758aa 100644
--- a/requirements/common.txt
+++ b/requirements/common.txt
@@ -32,7 +32,7 @@ pyzmq >= 25.0.0
 msgspec
 gguf >= 0.17.0
 mistral_common[image] >= 1.8.8
-opencv-python-headless >= 4.11.0    # required for video IO
+opencv-python-headless >= 4.13.0    # required for video IO
 pyyaml
 six>=1.16.0; python_version > '3.11' # transitive dependency of pandas that needs to be the latest version for python 3.12
 setuptools>=77.0.3,<81.0.0; python_version > '3.11' # Setuptools is used by triton, we need to ensure a modern version is installed for 3.12+ so that it does not try to import distutils, which was removed in 3.12
diff --git a/requirements/nightly_torch_test.txt b/requirements/nightly_torch_test.txt
index 72fa13692..cdb5aef4b 100644
--- a/requirements/nightly_torch_test.txt
+++ b/requirements/nightly_torch_test.txt
@@ -25,7 +25,7 @@ transformers_stream_generator # required for qwen-vl test
 matplotlib # required for qwen-vl test
 mistral_common[image,audio] >= 1.8.8 # required for voxtral test
 num2words # required for smolvlm test
-opencv-python-headless >= 4.11.0 # required for video test
+opencv-python-headless >= 4.13.0 # required for video test
 datamodel_code_generator # required for minicpm3 test
 lm-eval[api]>=0.4.9.2 # required for model evaluation test
 mteb>=1.38.11, <2 # required for mteb test
@@ -37,8 +37,8 @@ bitsandbytes>=0.46.1
 buildkite-test-collector==0.1.9
 
 
-genai_perf==0.0.8
-tritonclient==2.51.0
+genai_perf>=0.0.8
+tritonclient>=2.51.0
 
 numba == 0.61.2 # Required for N-gram speculative decoding
 numpy
diff --git a/requirements/test.in b/requirements/test.in
index 5fc405a63..f045918c4 100644
--- a/requirements/test.in
+++ b/requirements/test.in
@@ -33,7 +33,7 @@ matplotlib # required for qwen-vl test
 mistral_common[image,audio] >= 1.8.8 # required for voxtral test
 num2words # required for smolvlm test
 open_clip_torch==2.32.0 # Required for nemotron_vl test, Nemotron Parse in test_common.py
-opencv-python-headless >= 4.11.0 # required for video test
+opencv-python-headless >= 4.13.0 # required for video test
 datamodel_code_generator # required for minicpm3 test
 lm-eval[api]>=0.4.9.2 # required for model evaluation test
 mteb[bm25s]>=2, <3 # required for mteb test
@@ -45,8 +45,8 @@ bitsandbytes==0.46.1
 buildkite-test-collector==0.1.9
 
 
-genai_perf==0.0.8
-tritonclient==2.51.0
+genai_perf>=0.0.8
+tritonclient>=2.51.0
 
 arctic-inference == 0.1.1 # Required for suffix decoding test
 numba == 0.61.2 # Required for N-gram speculative decoding
diff --git a/requirements/test.txt b/requirements/test.txt
index e78431ab3..d946a7c33 100644
--- a/requirements/test.txt
+++ b/requirements/test.txt
@@ -31,7 +31,11 @@ albumentations==1.4.6
     #   -r requirements/test.in
     #   terratorch
 alembic==1.16.4
-    # via mlflow
+    # via
+    #   mlflow
+    #   optuna
+annotated-doc==0.0.4
+    # via fastapi
 annotated-types==0.7.0
     # via pydantic
 antlr4-python3-runtime==4.9.3
@@ -143,6 +147,8 @@ colorama==0.4.6
     #   tqdm-multiprocess
 colorful==0.5.6
     # via ray
+colorlog==6.10.1
+    # via optuna
 contourpy==1.3.0
     # via matplotlib
 coverage==7.10.6
@@ -250,7 +256,7 @@ fsspec==2024.9.0
     #   torch
 ftfy==6.3.1
     # via open-clip-torch
-genai-perf==0.0.8
+genai-perf==0.0.16
     # via -r requirements/test.in
 genson==1.3.0
     # via datamodel-code-generator
@@ -387,6 +393,7 @@ jinja2==3.1.6
     # via
     #   datamodel-code-generator
     #   flask
+    #   genai-perf
     #   mlflow
     #   torch
 jiwer==3.0.5
@@ -526,7 +533,7 @@ numba==0.61.2
     #   librosa
 numexpr==2.10.1
     # via lm-eval
-numpy==1.26.4
+numpy==2.2.6
     # via
     #   -r requirements/test.in
     #   accelerate
@@ -556,6 +563,7 @@ numpy==1.26.4
     #   numba
     #   numexpr
     #   opencv-python-headless
+    #   optuna
     #   pandas
     #   patsy
     #   peft
@@ -635,7 +643,7 @@ opencensus==0.11.4
     # via ray
 opencensus-context==0.1.3
     # via opencensus
-opencv-python-headless==4.11.0.86
+opencv-python-headless==4.13.0.90
     # via
     #   -r requirements/test.in
     #   albucore
@@ -658,6 +666,10 @@ opentelemetry-sdk==1.35.0
     #   ray
 opentelemetry-semantic-conventions==0.56b0
     # via opentelemetry-sdk
+optuna==3.6.1
+    # via genai-perf
+orjson==3.11.5
+    # via genai-perf
 packaging==24.2
     # via
     #   accelerate
@@ -676,6 +688,7 @@ packaging==24.2
     #   lightning-utilities
     #   matplotlib
     #   mlflow-skinny
+    #   optuna
     #   peft
     #   plotly
     #   pooch
@@ -715,6 +728,8 @@ peft==0.16.0
     #   lm-eval
 perceptron==0.1.4
     # via -r requirements/test.in
+perf-analyzer==0.1.0
+    # via genai-perf
 pillow==10.4.0
     # via
     #   genai-perf
@@ -901,6 +916,7 @@ pyyaml==6.0.2
     #   lightning
     #   mlflow-skinny
     #   omegaconf
+    #   optuna
     #   peft
     #   pytorch-lightning
     #   ray
@@ -1063,6 +1079,7 @@ sortedcontainers==2.4.0
 soundfile==0.12.1
     # via
     #   -r requirements/test.in
+    #   genai-perf
     #   librosa
     #   mistral-common
 soxr==0.5.0.post1
@@ -1073,6 +1090,7 @@ sqlalchemy==2.0.41
     # via
     #   alembic
     #   mlflow
+    #   optuna
 sqlitedict==2.1.0
     # via lm-eval
 sqlparse==0.5.3
@@ -1202,6 +1220,7 @@ tqdm==4.66.6
     #   mteb
     #   nltk
     #   open-clip-torch
+    #   optuna
     #   peft
     #   pqdm
     #   pretrainedmodels
@@ -1224,10 +1243,8 @@ transformers-stream-generator==0.0.5
     # via -r requirements/test.in
 triton==3.5.1
     # via torch
-tritonclient==2.51.0
-    # via
-    #   -r requirements/test.in
-    #   genai-perf
+tritonclient==2.64.0
+    # via -r requirements/test.in
 typepy==1.3.2
     # via
     #   dataproperty
diff --git a/tests/entrypoints/openai/test_translation_validation.py b/tests/entrypoints/openai/test_translation_validation.py
index cae45872e..23502ea1b 100644
--- a/tests/entrypoints/openai/test_translation_validation.py
+++ b/tests/entrypoints/openai/test_translation_validation.py
@@ -267,12 +267,16 @@ async def test_audio_with_max_tokens(mary_had_lamb, client_and_model):
     out_tokens = tok(out_text, add_special_tokens=False)["input_ids"]
     assert len(out_tokens) == 1
     # max_completion_tokens > max_model_len
+    # max_model_len=32768 for Gemma-3n-E2B-it
     transcription = await client.audio.transcriptions.create(
         model=model_name,
         file=mary_had_lamb,
         response_format="text",
         temperature=0.0,
-        extra_body={"max_completion_tokens": int(1e6)},
+        extra_body={
+            "max_completion_tokens": int(1e6),
+            "repetition_penalty": 1.3,
+        },
     )
     out = json.loads(transcription)
     out_text = out["text"]
diff --git a/tests/models/multimodal/generation/test_whisper.py b/tests/models/multimodal/generation/test_whisper.py
index 23459963f..2031a8d66 100644
--- a/tests/models/multimodal/generation/test_whisper.py
+++ b/tests/models/multimodal/generation/test_whisper.py
@@ -176,3 +176,46 @@ def test_models_distributed(
         distributed_executor_backend=distributed_executor_backend,
         enforce_eager=False,
     )
+
+
+@pytest.mark.core_model
+@pytest.mark.parametrize("model", ["openai/whisper-large-v3-turbo"])
+def test_encoder_cache_cleanup(
+    vllm_runner,
+    model: str,
+    input_audios,
+    monkeypatch,
+) -> None:
+    """Test that encoder cache is properly cleaned up after requests complete.
+
+    This is a regression test for a bug where encoder cache entries were freed
+    in the same scheduling step they were allocated, before the model could use
+    them.
+    """
+    # Set single-process mode to access the model runner's encoder cache directly
+    monkeypatch.setenv("VLLM_ENABLE_V1_MULTIPROCESSING", "0")
+    check_model_available(model)
+
+    with vllm_runner(
+        model,
+        dtype="half",
+        max_model_len=448,
+        tensor_parallel_size=1,
+        limit_mm_per_prompt={"audio": 2},
+        enforce_eager=True,
+    ) as vllm_model:
+        engine_core = vllm_model.llm.llm_engine.engine_core.engine_core
+        model_runner = engine_core.model_executor.driver_worker.worker.model_runner
+        encoder_cache = model_runner.encoder_cache
+
+        # Run multiple sequential requests to ensure cache is properly managed
+        for vllm_prompts, _, audios in input_audios:
+            vllm_model.generate_greedy(vllm_prompts, max_tokens=50, audios=audios)
+
+        # After all requests complete, encoder cache should be empty
+        cache_size = len(encoder_cache)
+        assert cache_size == 0, (
+            f"Encoder cache should be empty after all requests complete, "
+            f"but has {cache_size} entries. This indicates encoder cache "
+            f"entries are not being properly freed."
+        )
diff --git a/vllm/connections.py b/vllm/connections.py
index 31b0d5e9c..f79d681ce 100644
--- a/vllm/connections.py
+++ b/vllm/connections.py
@@ -3,10 +3,10 @@
 
 from collections.abc import Mapping, MutableMapping
 from pathlib import Path
-from urllib.parse import urlparse
 
 import aiohttp
 import requests
+from urllib3.util import parse_url
 
 from vllm.version import __version__ as VLLM_VERSION
 
@@ -37,7 +37,7 @@ class HTTPConnection:
         return self._async_client
 
     def _validate_http_url(self, url: str):
-        parsed_url = urlparse(url)
+        parsed_url = parse_url(url)
 
         if parsed_url.scheme not in ("http", "https"):
             raise ValueError(
diff --git a/vllm/envs.py b/vllm/envs.py
index d7df1f48e..7a2061b55 100755
--- a/vllm/envs.py
+++ b/vllm/envs.py
@@ -444,9 +444,9 @@ def get_vllm_port() -> int | None:
     try:
         return int(port)
     except ValueError as err:
-        from urllib.parse import urlparse
+        from urllib3.util import parse_url
 
-        parsed = urlparse(port)
+        parsed = parse_url(port)
         if parsed.scheme:
             raise ValueError(
                 f"VLLM_PORT '{port}' appears to be a URI. "
diff --git a/vllm/multimodal/utils.py b/vllm/multimodal/utils.py
index 07165430b..c53815e3a 100644
--- a/vllm/multimodal/utils.py
+++ b/vllm/multimodal/utils.py
@@ -9,13 +9,13 @@ from concurrent.futures import ThreadPoolExecutor
 from itertools import groupby
 from pathlib import Path
 from typing import TYPE_CHECKING, Any, TypeVar
-from urllib.parse import ParseResult, urlparse
 from urllib.request import url2pathname
 
 import numpy as np
 import numpy.typing as npt
 import torch
 from PIL import Image, UnidentifiedImageError
+from urllib3.util import Url, parse_url
 
 import vllm.envs as envs
 from vllm.connections import HTTPConnection, global_http_connection
@@ -101,11 +101,14 @@ class MediaConnector:
 
     def _load_data_url(
         self,
-        url_spec: ParseResult,
+        url_spec: Url,
         media_io: MediaIO[_M],
     ) -> _M:  # type: ignore[type-var]
-        data_spec, data = url_spec.path.split(",", 1)
+        url_spec_path = url_spec.path or ""
+        data_spec, data = url_spec_path.split(",", 1)
         media_type, data_type = data_spec.split(";", 1)
+        # media_type starts with a leading "/" (e.g., "/video/jpeg")
+        media_type = media_type.lstrip("/")
 
         if data_type != "base64":
             msg = "Only base64 data URLs are supported for now."
@@ -115,7 +118,7 @@ class MediaConnector:
 
     def _load_file_url(
         self,
-        url_spec: ParseResult,
+        url_spec: Url,
         media_io: MediaIO[_M],
     ) -> _M:  # type: ignore[type-var]
         allowed_local_media_path = self.allowed_local_media_path
@@ -124,7 +127,9 @@ class MediaConnector:
                 "Cannot load local files without `--allowed-local-media-path`."
             )
 
-        filepath = Path(url2pathname(url_spec.netloc + url_spec.path))
+        url_spec_path = url_spec.path or ""
+        url_spec_netloc = url_spec.netloc or ""
+        filepath = Path(url2pathname(url_spec_netloc + url_spec_path))
         if allowed_local_media_path not in filepath.resolve().parents:
             raise ValueError(
                 f"The file path {filepath} must be a subpath "
@@ -133,7 +138,7 @@ class MediaConnector:
 
         return media_io.load_file(filepath)
 
-    def _assert_url_in_allowed_media_domains(self, url_spec: ParseResult) -> None:
+    def _assert_url_in_allowed_media_domains(self, url_spec: Url) -> None:
         if (
             self.allowed_media_domains
             and url_spec.hostname not in self.allowed_media_domains
@@ -151,9 +156,9 @@ class MediaConnector:
         *,
         fetch_timeout: int | None = None,
     ) -> _M:  # type: ignore[type-var]
-        url_spec = urlparse(url)
+        url_spec = parse_url(url)
 
-        if url_spec.scheme.startswith("http"):
+        if url_spec.scheme and url_spec.scheme.startswith("http"):
             self._assert_url_in_allowed_media_domains(url_spec)
 
             connection = self.connection
@@ -181,10 +186,10 @@ class MediaConnector:
         *,
         fetch_timeout: int | None = None,
     ) -> _M:
-        url_spec = urlparse(url)
+        url_spec = parse_url(url)
         loop = asyncio.get_running_loop()
 
-        if url_spec.scheme.startswith("http"):
+        if url_spec.scheme and url_spec.scheme.startswith("http"):
             self._assert_url_in_allowed_media_domains(url_spec)
 
             connection = self.connection
diff --git a/vllm/utils/network_utils.py b/vllm/utils/network_utils.py
index 80ff0df28..7d01533cb 100644
--- a/vllm/utils/network_utils.py
+++ b/vllm/utils/network_utils.py
@@ -11,12 +11,12 @@ from collections.abc import (
     Sequence,
 )
 from typing import Any
-from urllib.parse import urlparse
 from uuid import uuid4
 
 import psutil
 import zmq
 import zmq.asyncio
+from urllib3.util import parse_url
 
 import vllm.envs as envs
 from vllm.logger import init_logger
@@ -217,13 +217,15 @@ def find_process_using_port(port: int) -> psutil.Process | None:
 
 def split_zmq_path(path: str) -> tuple[str, str, str]:
     """Split a zmq path into its parts."""
-    parsed = urlparse(path)
+    parsed = parse_url(path)
     if not parsed.scheme:
         raise ValueError(f"Invalid zmq path: {path}")
 
     scheme = parsed.scheme
     host = parsed.hostname or ""
     port = str(parsed.port or "")
+    if host.startswith("[") and host.endswith("]"):
+        host = host[1:-1]  # Remove brackets for IPv6 address
 
     if scheme == "tcp" and not all((host, port)):
         # The host and port fields are required for tcp
diff --git a/vllm/v1/core/encoder_cache_manager.py b/vllm/v1/core/encoder_cache_manager.py
index d73c05d2c..43b44fdaf 100644
--- a/vllm/v1/core/encoder_cache_manager.py
+++ b/vllm/v1/core/encoder_cache_manager.py
@@ -357,7 +357,8 @@ class EncoderDecoderCacheManager(EncoderCacheManager):
     def __init__(self, cache_size: int):
         self.cache_size = cache_size
         self.num_free_slots = cache_size
-        self.freed: list[str] = []
+        self.allocated: list[str] = []
+        self.to_free: list[str] = []
 
     def check_and_update_cache(self, request: Request, input_id: int) -> bool:
         return False
@@ -383,7 +384,7 @@ class EncoderDecoderCacheManager(EncoderCacheManager):
         self.num_free_slots -= num_encoder_embeds
 
         mm_hash = request.mm_features[input_id].identifier
-        self.freed.append(mm_hash)
+        self.allocated.append(mm_hash)
 
     def free(self, request: Request) -> None:
         for input_id in range(len(request.mm_features)):
@@ -393,9 +394,14 @@ class EncoderDecoderCacheManager(EncoderCacheManager):
         return set(range(len(request.mm_features)))
 
     def get_freed_mm_hashes(self) -> list[str]:
-        freed = self.freed
-        self.freed = []
-        return freed
+        # As encoder cache is not used for enc-dec models, we can free the entries here
+        # The actual free happens in the runner, *before* the model is executed.
+        # Therefore, `freeable` acts as a buffer to free the entries only after the
+        # model is executed, mimicking the state transition of `EncoderCacheManager`.
+        to_free = self.to_free
+        self.to_free = self.allocated
+        self.allocated = []
+        return to_free
 
     def free_encoder_input(self, request: Request, input_id: int) -> None:
         num_encoder_embeds = request.get_num_encoder_embeds(input_id)
-- 
2.43.0


From 9bbb6d2676598d61975807ca5091ce612b8ff0e8 Mon Sep 17 00:00:00 2001
From: Yan Ma <yan.ma@intel.com>
Date: Wed, 28 Jan 2026 10:10:58 +0800
Subject: [PATCH 28/36] fix qwen3-next (#117)

Signed-off-by: Yan Ma <yan.ma@intel.com>
---
 vllm/v1/attention/backends/gdn_attn.py | 13 +++++++++----
 1 file changed, 9 insertions(+), 4 deletions(-)

diff --git a/vllm/v1/attention/backends/gdn_attn.py b/vllm/v1/attention/backends/gdn_attn.py
index 426c17689..50845d0af 100644
--- a/vllm/v1/attention/backends/gdn_attn.py
+++ b/vllm/v1/attention/backends/gdn_attn.py
@@ -93,11 +93,16 @@ class GDNAttentionMetadataBuilder(AttentionMetadataBuilder[GDNAttentionMetadata]
         self.decode_cudagraph_max_bs = (
             self.vllm_config.scheduler_config.max_num_seqs * (self.num_spec + 1)
         )
-        if self.compilation_config.max_cudagraph_capture_size is not None:
-            self.decode_cudagraph_max_bs = min(
-                self.decode_cudagraph_max_bs,
-                self.compilation_config.max_cudagraph_capture_size,
+        if self.compilation_config.max_cudagraph_capture_size is None:
+            max_cudagraph_capture_size = 0
+        else:
+            max_cudagraph_capture_size = (
+                self.compilation_config.max_cudagraph_capture_size
             )
+        self.decode_cudagraph_max_bs = min(
+            self.decode_cudagraph_max_bs,
+            max_cudagraph_capture_size,
+        )
 
         self.spec_state_indices_tensor = torch.empty(
             (self.decode_cudagraph_max_bs, self.num_spec + 1),
-- 
2.43.0


From c70698756d0e953c43b15ec974f52bac02b68fb5 Mon Sep 17 00:00:00 2001
From: Yan Ma <yan.ma@intel.com>
Date: Wed, 28 Jan 2026 11:25:53 +0800
Subject: [PATCH 29/36] Update fp8 quantization with streaming quantization
 (#113)

* Revert "offload weights to cpu before fp8 online quant (#225)"

This reverts commit fc5a0a6f07bd2217d90da3d5374c2ed39b9d9660.

* fp8 online quant: split out Fp8OnlineLinearMethod (#32189)

Signed-off-by: Yan Ma <yan.ma@intel.com>

* cherry-pick:fix memory for online fp8 quantization with streaming weight load

Signed-off-by: Yan Ma <yan.ma@intel.com>

* fix fp8

Signed-off-by: Yan Ma <yan.ma@intel.com>

---------

Signed-off-by: Yan Ma <yan.ma@intel.com>
Co-authored-by: Vasiliy Kuznetsov <vkuzo@users.noreply.github.com>
---
 docs/features/quantization/fp8.md             |   2 -
 tests/quantization/test_cpu_offload.py        |  15 -
 tests/quantization/test_fp8.py                |  93 ++++-
 vllm/envs.py                                  |   6 -
 .../model_executor/layers/quantization/fp8.py | 366 ++++++++++++------
 .../layers/quantization/ipex_quant.py         | 132 +++----
 .../model_loader/base_loader.py               |  13 +
 .../model_loader/dummy_loader.py              |   2 +-
 vllm/model_executor/model_loader/utils.py     |  44 +--
 .../model_loader/weight_utils.py              |  85 ++--
 10 files changed, 475 insertions(+), 283 deletions(-)

diff --git a/docs/features/quantization/fp8.md b/docs/features/quantization/fp8.md
index b3ceab13f..f7efdb5f4 100644
--- a/docs/features/quantization/fp8.md
+++ b/docs/features/quantization/fp8.md
@@ -138,5 +138,3 @@ result = llm.generate("Hello, my name is")
 print(result[0].outputs[0].text)
 ```
 
-!!! warning
-    Currently, by default we load the model at original precision before quantizing down to 8-bits, so you need enough memory to load the whole model. To avoid this, adding `VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT=1` can allow offloading weights to cpu before quantization and quantized weights will be kept in device.
diff --git a/tests/quantization/test_cpu_offload.py b/tests/quantization/test_cpu_offload.py
index a02504f5f..3b58614e5 100644
--- a/tests/quantization/test_cpu_offload.py
+++ b/tests/quantization/test_cpu_offload.py
@@ -25,21 +25,6 @@ def test_cpu_offload_fp8():
     )
 
 
-@pytest.mark.skipif(
-    not is_quant_method_supported("fp8"),
-    reason="fp8 is not supported on this GPU type.",
-)
-def test_offload_weights_before_quant_fp8():
-    # Test quantization of an unquantized checkpoint
-    compare_two_settings(
-        "meta-llama/Llama-3.2-1B-Instruct",
-        ["--quantization", "fp8"],
-        ["--quantization", "fp8"],
-        {"VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT": "1"},
-        max_wait_seconds=480,
-    )
-
-
 @pytest.mark.skipif(
     not is_quant_method_supported("gptq_marlin"),
     reason="gptq_marlin is not supported on this GPU type.",
diff --git a/tests/quantization/test_fp8.py b/tests/quantization/test_fp8.py
index e8abe0d41..ddb63901f 100644
--- a/tests/quantization/test_fp8.py
+++ b/tests/quantization/test_fp8.py
@@ -5,7 +5,10 @@
 Run `pytest tests/quantization/test_fp8.py --forked`.
 """
 
+import logging
+
 import pytest
+import regex as re
 import torch
 
 from tests.quantization.utils import is_quant_method_supported
@@ -133,7 +136,7 @@ def test_kv_cache_model_load_and_run(
 @pytest.mark.parametrize(
     "use_rocm_aiter", [True, False] if current_platform.is_rocm() else [False]
 )
-def test_load_fp16_model(
+def test_online_quantization(
     vllm_runner,
     kv_cache_dtype: str,
     force_marlin: bool,
@@ -191,6 +194,94 @@ def test_load_fp16_model(
 
         llm.apply_model(check_model)
 
+        outputs = llm.generate_greedy(["Hello my name is"], max_tokens=4)
+        print(outputs[0][1])
+
+
+@pytest.mark.skipif(
+    not is_quant_method_supported("fp8"),
+    reason="FP8 is not supported on this GPU type.",
+)
+def test_online_quant_peak_mem(
+    vllm_runner,
+    monkeypatch,
+    caplog,
+) -> None:
+    # Disable multiprocessing so logs are captured in the main process
+    monkeypatch.setenv("VLLM_ENABLE_V1_MULTIPROCESSING", "0")
+
+    # Enable propagation so caplog can capture vllm logs
+    vllm_logger = logging.getLogger("vllm")
+    original_propagate = vllm_logger.propagate
+    vllm_logger.propagate = True
+
+    try:
+        with vllm_runner(
+            "Qwen/Qwen1.5-MoE-A2.7B",
+            quantization="fp8",
+            enforce_eager=True,
+        ) as llm:
+            outputs = llm.generate_greedy(["Hello my name is"], max_tokens=4)
+            print(outputs[0][1])
+    finally:
+        vllm_logger.propagate = original_propagate
+
+    # Parse memory usage from captured logs
+    model_memory_gib = None
+    peak_memory_gib = None
+    for record in caplog.records:
+        if model_memory_gib is None:
+            match = re.search(r"Model loading took ([\d.]+) GiB memory", record.message)
+            if match:
+                model_memory_gib = float(match.group(1))
+        if peak_memory_gib is None:
+            match = re.search(
+                r"Peak GPU memory after loading weights: ([\d.]+) GiB", record.message
+            )
+            if match:
+                peak_memory_gib = float(match.group(1))
+
+    assert model_memory_gib is not None, "Could not find model loading memory log"
+    assert peak_memory_gib is not None, "Could not find peak memory log"
+    print(f"GPU memory used after loading weights: {model_memory_gib} GiB")
+    print(f"Peak GPU memory usage while loading weights: {peak_memory_gib} GiB")
+
+    # model specific, Qwen/Qwen1.5-MoE-A2.7B fp8 online quant uses ~13.94
+    # GiB for weight loading (bf16 checkpoint is ~28 GiB)
+    expected_model_memory_gib = 14.0
+
+    # for Qwen/Qwen1.5-MoE-A2.7B the number we see today is 14.98 GiB, which
+    # is 1.07x above model_memory_gib. A slightly higher number is expected as
+    # when we load and quantize weights in a streaming fashion we need to
+    # have individual weights in bf16 + fp8 alive at the same time.
+    expected_peak_memory_gib = expected_model_memory_gib * 1.12
+
+    assert model_memory_gib < expected_model_memory_gib, (
+        f"{model_memory_gib=} higher than {expected_model_memory_gib}"
+    )
+    assert peak_memory_gib < expected_peak_memory_gib, (
+        f"{peak_memory_gib=} higher than {expected_peak_memory_gib}"
+    )
+
+
+@pytest.mark.skipif(
+    not is_quant_method_supported("fp8"),
+    reason="FP8 is not supported on this GPU type.",
+)
+def test_online_quant_load_format_dummy(
+    vllm_runner,
+    monkeypatch,
+    caplog,
+) -> None:
+    with vllm_runner(
+        "Qwen/Qwen1.5-MoE-A2.7B",
+        quantization="fp8",
+        enforce_eager=True,
+        load_format="dummy",
+    ) as llm:
+        outputs = llm.generate_greedy(["Hello my name is"], max_tokens=4)
+        print(outputs[0][1])
+
 
 @pytest.mark.skipif(
     not is_quant_method_supported("fp8"),
diff --git a/vllm/envs.py b/vllm/envs.py
index 7a2061b55..3825610f7 100755
--- a/vllm/envs.py
+++ b/vllm/envs.py
@@ -253,7 +253,6 @@ if TYPE_CHECKING:
     VLLM_LOG_MODEL_INSPECTION: bool = False
     VLLM_DEBUG_MFU_METRICS: bool = False
     VLLM_XPU_FP8_DTYPE: str = "e5m2"
-    VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT: bool = False
 
 
 def get_default_cache_root():
@@ -1615,11 +1614,6 @@ environment_variables: dict[str, Callable[[], Any]] = {
     ),
     # fp8 dtype for XPU platform
     "VLLM_XPU_FP8_DTYPE": lambda: os.environ.get("VLLM_XPU_FP8_DTYPE", "e5m2"),
-    # Offload model weights to cpu before online fp8 quantization
-    "VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT": lambda: os.environ.get(
-        "VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT", "0"
-    )
-    == "1",
 }
 
 # --8<-- [end:env-vars-definition]
diff --git a/vllm/model_executor/layers/quantization/fp8.py b/vllm/model_executor/layers/quantization/fp8.py
index 8f4536baf..320cc9e2a 100644
--- a/vllm/model_executor/layers/quantization/fp8.py
+++ b/vllm/model_executor/layers/quantization/fp8.py
@@ -84,6 +84,7 @@ from vllm.model_executor.layers.quantization.utils.w8a8_utils import (
     maybe_create_device_identity,
     normalize_e4m3fn_to_e4m3fnuz,
 )
+from vllm.model_executor.model_loader.weight_utils import initialize_single_dummy_weight
 from vllm.model_executor.parameter import (
     BlockQuantScaleParameter,
     ModelWeightParameter,
@@ -226,9 +227,14 @@ class Fp8Config(QuantizationConfig):
                 fused_mapping=self.packed_modules_mapping,
             ):
                 return UnquantizedLinearMethod()
-            quant_method = Fp8LinearMethod(self)
-            quant_method.marlin_input_dtype = get_marlin_input_dtype(prefix)
-            return quant_method
+            if not self.is_checkpoint_fp8_serialized:
+                online_method = Fp8OnlineLinearMethod(self)
+                online_method.marlin_input_dtype = get_marlin_input_dtype(prefix)
+                return online_method
+            else:
+                offline_method = Fp8LinearMethod(self)
+                offline_method.marlin_input_dtype = get_marlin_input_dtype(prefix)
+                return offline_method
         elif isinstance(layer, FusedMoE):
             if is_layer_skipped(
                 prefix=prefix,
@@ -286,18 +292,23 @@ class CopyNumelCounter(TorchDispatchMode):
         return out
 
 
+def _copy_missing_attrs(old: torch.Tensor, new: torch.Tensor) -> None:
+    """Copies any attrs present in `old` but not in `new` to `new`"""
+    new_attrs = set(dir(new))
+    attrs_to_set = {}
+    for attr in dir(old):
+        if attr not in new_attrs:
+            attrs_to_set[attr] = getattr(old, attr)
+    set_weight_attrs(new, attrs_to_set)
+
+
 class Fp8LinearMethod(LinearMethodBase):
     """Linear method for FP8.
     Supports loading FP8 checkpoints with static weight scale and
     dynamic/static activation scale.
 
-    Also supports loading quantized FP16/BF16 model checkpoints with dynamic
-    activation scaling. The weight scaling factor will be initialized after
-    the model weights are loaded.
-
     Limitations:
-    1. Only support per-tensor quantization due to torch._scaled_mm support.
-    2. Only support float8_e4m3fn data type due to the limitation of
+    1. Only support float8_e4m3fn data type due to the limitation of
        torch._scaled_mm (https://github.com/pytorch/pytorch/blob/2e48b39603411a41c5025efbe52f89560b827825/aten/src/ATen/native/cuda/Blas.cpp#L854-L856)
 
     Args:
@@ -384,95 +395,43 @@ class Fp8LinearMethod(LinearMethodBase):
                 self.weight_block_size,
             )
 
-        # WEIGHT
-        if self.quant_config.is_checkpoint_fp8_serialized:
-            weight = create_fp8_weight_parameter(
-                output_size_per_partition, input_size_per_partition, weight_loader
+        weight = create_fp8_weight_parameter(
+            output_size_per_partition, input_size_per_partition, weight_loader
+        )
+        layer.register_parameter("weight", weight)
+
+        # WEIGHT SCALE
+        if not self.block_quant:
+            scale = create_fp8_scale_parameter(
+                PerTensorScaleParameter,
+                output_partition_sizes,
+                input_size_per_partition,
+                None,
+                weight_loader,
             )
+            set_weight_attrs(scale, {"scale_type": "weight_scale"})
+            layer.register_parameter("weight_scale", scale)
         else:
-
-            def patched_weight_loader(param, loaded_weight, *args, **kwargs):
-                # track how many elements we have updated
-                if not hasattr(layer, "_loaded_numel"):
-                    layer._loaded_numel = 0
-
-                # load the current weight chunk
-                copy_numel_counter = CopyNumelCounter()
-                with copy_numel_counter:
-                    res = weight_loader(param, loaded_weight, *args, **kwargs)  # type: ignore[misc]
-                layer._loaded_numel += copy_numel_counter.copied_numel
-
-                # if we have loaded all of the elements, call
-                # process_weights_after_loading
-                target_loaded_numel = layer.weight.numel()
-                if layer._loaded_numel == target_loaded_numel:
-                    self.process_weights_after_loading(layer)
-
-                    # Delete the bookkeeping
-                    del layer._loaded_numel
-                    # Prevent the usual `process_weights_after_loading` call from doing
-                    # anything
-                    layer._already_called_process_weights_after_loading = True
-
-                return res
-
-            # For non-serialized checkpoints, use original dtype
-            # Force offloading weights to cpu if VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT
-            # enabled, otherwise use original device config which can be gpu or cpu
-            # (may happen when cpu_offload_gb > 0)
-            weight = ModelWeightParameter(
-                data=torch.empty(
-                    output_size_per_partition,
-                    input_size_per_partition,
-                    dtype=params_dtype,
-                    device="cpu" if envs.VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT else None,
-                ),
-                input_dim=1,
-                output_dim=0,
-                weight_loader=patched_weight_loader,
+            assert not self.act_q_static
+            assert self.weight_block_size is not None
+            scale = create_fp8_scale_parameter(
+                BlockQuantScaleParameter,
+                output_partition_sizes,
+                input_size_per_partition,
+                self.weight_block_size,
+                weight_loader,
             )
-        layer.register_parameter("weight", weight)
+            set_weight_attrs(scale, {"scale_type": "weight_scale"})
+            # The weight_scale_inv name is intentional for deepseekv3
+            layer.register_parameter("weight_scale_inv", scale)
 
-        # If checkpoint is serialized fp8, load them.
-        # Otherwise, wait until process_weights_after_loading.
-        if self.quant_config.is_checkpoint_fp8_serialized:
-            # WEIGHT SCALE
-            if not self.block_quant:
-                scale = create_fp8_scale_parameter(
-                    PerTensorScaleParameter,
-                    output_partition_sizes,
-                    input_size_per_partition,
-                    None,
-                    weight_loader,
-                )
-                set_weight_attrs(scale, {"scale_type": "weight_scale"})
-                layer.register_parameter("weight_scale", scale)
-            else:
-                assert not self.act_q_static
-                assert self.weight_block_size is not None
-                scale = create_fp8_scale_parameter(
-                    BlockQuantScaleParameter,
-                    output_partition_sizes,
-                    input_size_per_partition,
-                    self.weight_block_size,
-                    weight_loader,
-                )
-                set_weight_attrs(scale, {"scale_type": "weight_scale"})
-                # The weight_scale_inv name is intentional for deepseekv3
-                layer.register_parameter("weight_scale_inv", scale)
-
-            # INPUT ACTIVATION SCALE
-            if self.act_q_static:
-                scale = create_fp8_input_scale(output_partition_sizes, weight_loader)
-                set_weight_attrs(scale, {"scale_type": "input_scale"})
-                layer.register_parameter("input_scale", scale)
-            else:
-                layer.register_parameter("input_scale", None)
+        # INPUT ACTIVATION SCALE
+        if self.act_q_static:
+            scale = create_fp8_input_scale(output_partition_sizes, weight_loader)
+            set_weight_attrs(scale, {"scale_type": "input_scale"})
+            layer.register_parameter("input_scale", scale)
 
     def process_weights_after_loading(self, layer: Module) -> None:
-        if getattr(layer, "_already_called_process_weights_after_loading", False):
-            return
-
         size_k_first = True
         input_scale = None
         # TODO(rob): refactor block quant into separate class.
@@ -490,31 +449,24 @@ class Fp8LinearMethod(LinearMethodBase):
 
         # If checkpoint not serialized fp8, quantize the weights.
         else:
-            if not self.quant_config.is_checkpoint_fp8_serialized:
-                qweight, weight_scale = ops.scaled_fp8_quant(layer.weight, scale=None)
-                weight = qweight.t()
-
             # If checkpoint is fp8 per-tensor, handle that there are N scales for N
             # shards in a fused module
-            else:
-                weight = layer.weight
-                weight_scale = layer.weight_scale
-
-                # If using w8a8, torch._scaled_mm needs per tensor, so
-                # requantize the logical shards as a single weight.
-                if not self.use_marlin:
-                    weight, weight_scale, input_scale = (
-                        process_fp8_weight_tensor_strategy(
-                            weight,
-                            weight_scale,
-                            layer.logical_widths,
-                            getattr(layer, "input_scale", None),
-                        )
-                    )
-                    if self.act_q_static:
-                        assert input_scale is not None
-                        input_scale = input_scale.max()
-                weight = weight.t()
+            weight = layer.weight
+            weight_scale = layer.weight_scale
+
+            # If using w8a8, torch._scaled_mm needs per tensor, so
+            # requantize the logical shards as a single weight.
+            if not self.use_marlin:
+                weight, weight_scale, input_scale = process_fp8_weight_tensor_strategy(
+                    weight,
+                    weight_scale,
+                    layer.logical_widths,
+                    getattr(layer, "input_scale", None),
+                )
+                if self.act_q_static:
+                    assert input_scale is not None
+                    input_scale = input_scale.max()
+            weight = weight.t()
 
             # Update layer with new values.
             replace_parameter(layer, "weight", weight.data)
@@ -616,6 +568,122 @@ class Fp8LinearMethod(LinearMethodBase):
         )
 
 
+class Fp8OnlineLinearMethod(Fp8LinearMethod):
+    """Online version of Fp8LinearMethod, loads the fp16/bf16 checkpoint
+    and quantized the weights during loading."""
+
+    def create_weights(
+        self,
+        layer: torch.nn.Module,
+        input_size_per_partition: int,
+        output_partition_sizes: list[int],
+        input_size: int,
+        output_size: int,
+        params_dtype: torch.dtype,
+        **extra_weight_attrs,
+    ):
+        output_size_per_partition = sum(output_partition_sizes)
+        weight_loader = extra_weight_attrs.get("weight_loader")
+        layer.logical_widths = output_partition_sizes
+        layer.input_size_per_partition = input_size_per_partition
+        layer.output_size_per_partition = output_size_per_partition
+        layer.orig_dtype = params_dtype
+        layer.weight_block_size = None
+
+        # WEIGHT
+        def patched_weight_loader(param, loaded_weight, *args, **kwargs):
+            # track how many elements we have updated
+            if not hasattr(layer, "_loaded_numel"):
+                layer._loaded_numel = 0
+
+                # when the first `loaded_weight` is about to be
+                # loaded to `param`, materialize `param` just-in-time
+                weight = ModelWeightParameter(
+                    data=torch.empty_like(layer.weight, device=layer._load_device),
+                    input_dim=1,
+                    output_dim=0,
+                    weight_loader=patched_weight_loader,
+                )
+                _copy_missing_attrs(layer.weight, weight)
+                layer.register_parameter("weight", weight)
+                del layer._load_device
+
+            # refresh the reference to `param` to reflect just-in-time
+            # materialization
+            param = layer.weight
+
+            # load the current weight chunk
+            copy_numel_counter = CopyNumelCounter()
+            with copy_numel_counter:
+                res = weight_loader(param, loaded_weight, *args, **kwargs)  # type: ignore[misc]
+            layer._loaded_numel += copy_numel_counter.copied_numel
+
+            # if we have loaded all of the elements, call
+            # process_weights_after_loading
+            target_loaded_numel = layer.weight.numel()
+            if layer._loaded_numel == target_loaded_numel:
+                self.process_weights_after_loading(layer)
+
+                # Delete the bookkeeping
+                del layer._loaded_numel
+                # Prevent the usual `process_weights_after_loading` call from doing
+                # anything
+                layer._already_called_process_weights_after_loading = True
+
+            return res
+
+        weight = ModelWeightParameter(
+            data=torch.empty(
+                output_size_per_partition,
+                input_size_per_partition,
+                # materialized just-in-time in `patched_weight_loader`
+                device="meta",
+                dtype=params_dtype,
+            ),
+            input_dim=1,
+            output_dim=0,
+            weight_loader=patched_weight_loader,
+        )
+        # stash the correct device for `patched_weight_loader`
+        layer._load_device = torch.get_default_device()
+        layer.register_parameter("weight", weight)
+
+    def process_weights_after_loading(self, layer: Module) -> None:
+        if getattr(layer, "_already_called_process_weights_after_loading", False):
+            return
+
+        # deferred initialization of randomly initialized weights for the
+        # `--load_format dummy` feature
+        if layer.weight.device == torch.device("meta"):
+            weight = ModelWeightParameter(
+                data=torch.empty_like(layer.weight, device=layer._load_device),
+                input_dim=1,
+                output_dim=0,
+                weight_loader=layer.weight.weight_loader,
+            )
+            _copy_missing_attrs(layer.weight, weight)
+            layer.register_parameter("weight", weight)
+            initialize_single_dummy_weight(layer.weight)
+
+        # TODO(future): support block_quant in online quant path
+        assert not self.block_quant
+
+        layer.input_scale = None
+        qweight, weight_scale = ops.scaled_fp8_quant(layer.weight, scale=None)
+        weight = qweight.t()
+
+        # Update layer with new values.
+        replace_parameter(layer, "weight", weight.data)
+        replace_parameter(layer, "weight_scale", weight_scale.data)
+
+        if self.use_marlin:
+            size_k_first = True
+            prepare_fp8_layer_for_marlin(
+                layer, size_k_first, input_dtype=self.marlin_input_dtype
+            )
+            # Activations not quantized for marlin.
+
+
 class Fp8MoEMethod(FusedMoEMethodBase):
     """MoE method for FP8.
     Supports loading FP8 checkpoints with static weight scale and
@@ -719,7 +787,6 @@ class Fp8MoEMethod(FusedMoEMethodBase):
                 2 * intermediate_size_per_partition,
                 hidden_size,
                 dtype=params_dtype,
-                device="cpu" if envs.VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT else None,
             ),
             requires_grad=False,
         )
@@ -732,7 +799,6 @@ class Fp8MoEMethod(FusedMoEMethodBase):
                 hidden_size,
                 intermediate_size_per_partition,
                 dtype=params_dtype,
-                device="cpu" if envs.VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT else None,
             ),
             requires_grad=False,
         )
@@ -1125,6 +1191,39 @@ class Fp8OnlineMoEMethod(Fp8MoEMethod):
             if not hasattr(layer, "_loaded_numel"):
                 layer._loaded_numel = 0
 
+                # save the ids of original w13 and w2 so that we can
+                # distinguish which one `param` should map to further
+                # down in this file
+                layer._w13_weight_orig_id = id(layer.w13_weight)
+                layer._w2_weight_orig_id = id(layer.w2_weight)
+
+                # when the first `loaded_weight` is about to be
+                # loaded to `param`, materialize `param` just-in-time
+
+                w13_weight = torch.nn.Parameter(
+                    torch.empty_like(layer.w13_weight, device=layer._load_device),
+                    requires_grad=False,
+                )
+                set_weight_attrs(w13_weight, extra_weight_attrs)
+                _copy_missing_attrs(layer.w13_weight, w13_weight)
+                layer.register_parameter("w13_weight", w13_weight)
+
+                w2_weight = torch.nn.Parameter(
+                    torch.empty_like(layer.w2_weight, device=layer._load_device),
+                    requires_grad=False,
+                )
+                set_weight_attrs(w2_weight, extra_weight_attrs)
+                _copy_missing_attrs(layer.w2_weight, w2_weight)
+                layer.register_parameter("w2_weight", w2_weight)
+                del layer._load_device
+
+            # refresh the reference to `param` to reflect just-in-time
+            # materialization
+            if id(param) == layer._w13_weight_orig_id:
+                param = layer.w13_weight
+            elif id(param) == layer._w2_weight_orig_id:
+                param = layer.w2_weight
+
             # load the current weight chunk
             copy_numel_counter = CopyNumelCounter()
             with copy_numel_counter:
@@ -1139,6 +1238,8 @@ class Fp8OnlineMoEMethod(Fp8MoEMethod):
 
                 # Delete the bookkeeping
                 del layer._loaded_numel
+                del layer._w13_weight_orig_id
+                del layer._w2_weight_orig_id
                 # Prevent the usual `process_weights_after_loading` call
                 # from doing anything
                 layer._already_called_process_weights_after_loading = True
@@ -1154,6 +1255,8 @@ class Fp8OnlineMoEMethod(Fp8MoEMethod):
                 num_experts,
                 2 * intermediate_size_per_partition,
                 hidden_size,
+                # materialized just-in-time in `patched_weight_loader`
+                device="meta",
                 dtype=params_dtype,
             ),
             requires_grad=False,
@@ -1166,12 +1269,16 @@ class Fp8OnlineMoEMethod(Fp8MoEMethod):
                 num_experts,
                 hidden_size,
                 intermediate_size_per_partition,
+                # materialized just-in-time in `patched_weight_loader`
+                device="meta",
                 dtype=params_dtype,
             ),
             requires_grad=False,
         )
         layer.register_parameter("w2_weight", w2_weight)
         set_weight_attrs(w2_weight, extra_weight_attrs)
+        # stash the correct device for `patched_weight_loader`
+        layer._load_device = torch.get_default_device()
 
         # WEIGHT_SCALES
         # Allocate 2 scales for w1 and w3 respectively.
@@ -1194,6 +1301,31 @@ class Fp8OnlineMoEMethod(Fp8MoEMethod):
         if getattr(layer, "_already_called_process_weights_after_loading", False):
             return
 
+        # deferred initialization of randomly initialized weights for the
+        # `--load_format dummy` feature
+        if layer.w13_weight.device == torch.device("meta"):
+            w13_weight = torch.nn.Parameter(
+                torch.empty_like(layer.w13_weight, device=layer._load_device),
+                requires_grad=False,
+            )
+            set_weight_attrs(
+                w13_weight, {"weight_loader": layer.w13_weight.weight_loader}
+            )
+            _copy_missing_attrs(layer.w13_weight, w13_weight)
+            layer.register_parameter("w13_weight", w13_weight)
+            initialize_single_dummy_weight(layer.w13_weight)
+        if layer.w2_weight.device == torch.device("meta"):
+            w2_weight = torch.nn.Parameter(
+                torch.empty_like(layer.w2_weight, device=layer._load_device),
+                requires_grad=False,
+            )
+            set_weight_attrs(
+                w2_weight, {"weight_loader": layer.w2_weight.weight_loader}
+            )
+            _copy_missing_attrs(layer.w2_weight, w2_weight)
+            layer.register_parameter("w2_weight", w2_weight)
+            initialize_single_dummy_weight(layer.w2_weight)
+
         # If checkpoint is fp16, quantize in place.
         fp8_dtype = current_platform.fp8_dtype()
         w13 = torch.empty_like(layer.w13_weight, dtype=fp8_dtype)
diff --git a/vllm/model_executor/layers/quantization/ipex_quant.py b/vllm/model_executor/layers/quantization/ipex_quant.py
index 66ca98809..587bb5ff7 100644
--- a/vllm/model_executor/layers/quantization/ipex_quant.py
+++ b/vllm/model_executor/layers/quantization/ipex_quant.py
@@ -8,9 +8,9 @@ from packaging import version
 from safetensors.torch import _TYPES as _SAFETENSORS_TO_TORCH_DTYPE
 from torch.nn import Module
 
-import vllm.envs as envs
 from vllm._ipex_ops import ipex_ops as ops
 from vllm.model_executor.layers.fused_moe import (
+    FusedMoE,
     FusedMoEConfig,
     FusedMoEMethodBase,
     FusedMoeWeightScaleSupported,
@@ -19,7 +19,6 @@ from vllm.model_executor.layers.fused_moe.config import FusedMoEQuantConfig
 from vllm.model_executor.layers.fused_moe.fused_moe_router import (
     FusedMoERouter,
 )
-from vllm.model_executor.layers.fused_moe.layer import FusedMoE
 from vllm.model_executor.layers.linear import (
     LinearBase,
     UnquantizedLinearMethod,
@@ -31,14 +30,16 @@ from vllm.model_executor.layers.quantization import (
 from vllm.model_executor.layers.quantization.awq import AWQLinearMethod
 from vllm.model_executor.layers.quantization.fp8 import (
     Fp8Config,
-    Fp8LinearMethod,
+    Fp8OnlineLinearMethod,
     Fp8OnlineMoEMethod,
+    _copy_missing_attrs,
 )
 from vllm.model_executor.layers.quantization.gptq import GPTQLinearMethod
 from vllm.model_executor.layers.quantization.utils.gptq_utils import (
     get_linear_quant_method,
 )
 from vllm.model_executor.layers.quantization.utils.quant_utils import is_layer_skipped
+from vllm.model_executor.model_loader.weight_utils import initialize_single_dummy_weight
 from vllm.model_executor.utils import replace_parameter, set_weight_attrs
 from vllm.platforms import current_platform
 from vllm.scalar_type import scalar_types
@@ -389,7 +390,7 @@ class IPEXAWQLinearMethod(AWQLinearMethod):
         return out.reshape(x.shape[:-1] + (layer.ipex_output_size,))
 
 
-class XPUFp8LinearMethod(Fp8LinearMethod):
+class XPUFp8LinearMethod(Fp8OnlineLinearMethod):
     def __init__(self, quant_config: Fp8Config):
         super().__init__(quant_config)
 
@@ -427,92 +428,53 @@ class XPUFp8MoEMethod(Fp8OnlineMoEMethod):
         super().__init__(quant_config, layer)
         self.quant_config = quant_config
 
-    def create_weights(
-        self,
-        layer: Module,
-        num_experts: int,
-        hidden_size: int,
-        intermediate_size_per_partition: int,
-        params_dtype: torch.dtype,
-        **extra_weight_attrs,
-    ):
-        layer.intermediate_size_per_partition = intermediate_size_per_partition
-        layer.hidden_size = hidden_size
-        layer.num_experts = num_experts
-        layer.orig_dtype = params_dtype
-        layer.weight_block_size = None
-        # WEIGHTS
-        w13_weight = torch.nn.Parameter(
-            torch.empty(
-                num_experts,
-                2 * intermediate_size_per_partition,
-                hidden_size,
-                dtype=params_dtype,
-                device="cpu" if envs.VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT else None,
-            ),
-            requires_grad=False,
-        )
-        layer.register_parameter("w13_weight", w13_weight)
-        set_weight_attrs(w13_weight, extra_weight_attrs)
-
-        w2_weight = torch.nn.Parameter(
-            torch.empty(
-                num_experts,
-                hidden_size,
-                intermediate_size_per_partition,
-                dtype=params_dtype,
-                device="cpu" if envs.VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT else None,
-            ),
-            requires_grad=False,
-        )
-        layer.register_parameter("w2_weight", w2_weight)
-        set_weight_attrs(w2_weight, extra_weight_attrs)
-
-        # Allocate 2 scales for w1 and w3 respectively.
-        # They will be combined to a single scale after weight loading.
-        w13_weight_scale = torch.nn.Parameter(
-            torch.ones(num_experts, 2, dtype=torch.float32), requires_grad=False
-        )
-        w2_weight_scale = torch.nn.Parameter(
-            torch.ones(num_experts, dtype=torch.float32), requires_grad=False
-        )
-        layer.register_parameter("w13_weight_scale", w13_weight_scale)
-        layer.register_parameter("w2_weight_scale", w2_weight_scale)
-
-        extra_weight_attrs.update(
-            {"quant_method": FusedMoeWeightScaleSupported.TENSOR.value}
-        )
-        # INPUT_SCALES
-        layer.w13_input_scale = None
-        layer.w2_input_scale = None
-
     def process_weights_after_loading(self, layer: Module) -> None:
         if getattr(layer, "_already_called_process_weights_after_loading", False):
             return
-        if not self.quant_config.is_checkpoint_fp8_serialized:
-            fp8_dtype = current_platform.fp8_dtype()
-            w13_weight = torch.empty_like(layer.w13_weight.data, dtype=fp8_dtype)
-            w2_weight = torch.empty_like(layer.w2_weight.data, dtype=fp8_dtype)
-
-            # Re-initialize w13_scale because we directly quantize
-            # merged w13 weights and generate a single scaling factor.
-            layer.w13_weight_scale = torch.nn.Parameter(
-                torch.ones(
-                    layer.local_num_experts,
-                    dtype=torch.float32,
-                    device=w13_weight.device,
-                ),
+        # deferred initialization of randomly initialized weights for the
+        # `--load_format dummy` feature
+        if layer.w13_weight.device == torch.device("meta"):
+            w13_weight = torch.nn.Parameter(
+                torch.empty_like(layer.w13_weight, device=layer._load_device),
                 requires_grad=False,
             )
-            for expert in range(layer.local_num_experts):
-                w13_weight[expert, :, :], layer.w13_weight_scale[expert] = (
-                    ops.scaled_fp8_quant(layer.w13_weight.data[expert, :, :])
-                )
-                w2_weight[expert, :, :], layer.w2_weight_scale[expert] = (
-                    ops.scaled_fp8_quant(layer.w2_weight.data[expert, :, :])
-                )
-            replace_parameter(layer, "w13_weight", w13_weight)
-            replace_parameter(layer, "w2_weight", w2_weight)
+            set_weight_attrs(
+                w13_weight, {"weight_loader": layer.w13_weight.weight_loader}
+            )
+            _copy_missing_attrs(layer.w13_weight, w13_weight)
+            layer.register_parameter("w13_weight", w13_weight)
+            initialize_single_dummy_weight(layer.w13_weight)
+        if layer.w2_weight.device == torch.device("meta"):
+            w2_weight = torch.nn.Parameter(
+                torch.empty_like(layer.w2_weight, device=layer._load_device),
+                requires_grad=False,
+            )
+            set_weight_attrs(
+                w2_weight, {"weight_loader": layer.w2_weight.weight_loader}
+            )
+            _copy_missing_attrs(layer.w2_weight, w2_weight)
+            layer.register_parameter("w2_weight", w2_weight)
+            initialize_single_dummy_weight(layer.w2_weight)
+
+        # If checkpoint is fp16, quantize in place.
+        fp8_dtype = current_platform.fp8_dtype()
+        w13 = torch.empty_like(layer.w13_weight, dtype=fp8_dtype)
+        w2 = torch.empty_like(layer.w2_weight, dtype=fp8_dtype)
+        w13_scale = layer.w13_weight_scale
+        w2_scale = layer.w2_weight_scale
+
+        for expert in range(layer.local_num_experts):
+            w13[expert, :, :], w13_scale[expert] = ops.scaled_fp8_quant(
+                layer.w13_weight[expert, :, :]
+            )
+            w2[expert, :, :], w2_scale[expert] = ops.scaled_fp8_quant(
+                layer.w2_weight[expert, :, :]
+            )
+
+        replace_parameter(layer, "w13_weight", w13)
+        replace_parameter(layer, "w2_weight", w2)
+        replace_parameter(layer, f"w13_{self.weight_scale_name}", w13_scale)
+        replace_parameter(layer, f"w2_{self.weight_scale_name}", w2_scale)
 
         import intel_extension_for_pytorch as ipex
 
diff --git a/vllm/model_executor/model_loader/base_loader.py b/vllm/model_executor/model_loader/base_loader.py
index 2238b0cfe..cf9305bb8 100644
--- a/vllm/model_executor/model_loader/base_loader.py
+++ b/vllm/model_executor/model_loader/base_loader.py
@@ -13,6 +13,8 @@ from vllm.model_executor.model_loader.utils import (
     initialize_model,
     process_weights_after_loading,
 )
+from vllm.platforms import current_platform
+from vllm.utils.mem_utils import format_gib
 from vllm.utils.torch_utils import set_default_torch_dtype
 
 logger = init_logger(__name__)
@@ -56,6 +58,17 @@ class BaseModelLoader(ABC):
             logger.debug("Loading weights on %s ...", load_device)
             # Quantization does not happen in `load_weights` but after it
             self.load_weights(model, model_config)
+
+            # Log peak GPU memory after loading weights. This is needed
+            # to have test coverage on peak memory for online quantization.
+            if current_platform.is_cuda():
+                peak_memory = torch.cuda.max_memory_allocated()
+                logger.info_once(
+                    "Peak GPU memory after loading weights: %s GiB",
+                    format_gib(peak_memory),
+                    scope="local",
+                )
+
             process_weights_after_loading(model, model_config, target_device)
 
         return model.eval()
diff --git a/vllm/model_executor/model_loader/dummy_loader.py b/vllm/model_executor/model_loader/dummy_loader.py
index b2a934ce5..156071f1d 100644
--- a/vllm/model_executor/model_loader/dummy_loader.py
+++ b/vllm/model_executor/model_loader/dummy_loader.py
@@ -25,4 +25,4 @@ class DummyModelLoader(BaseModelLoader):
     def load_weights(self, model: nn.Module, model_config: ModelConfig) -> None:
         # NOTE(woosuk): For accurate performance evaluation, we assign
         # random values to the weights.
-        initialize_dummy_weights(model)
+        initialize_dummy_weights(model, model_config)
diff --git a/vllm/model_executor/model_loader/utils.py b/vllm/model_executor/model_loader/utils.py
index 24b66eb91..08d7a851a 100644
--- a/vllm/model_executor/model_loader/utils.py
+++ b/vllm/model_executor/model_loader/utils.py
@@ -13,7 +13,6 @@ from typing_extensions import assert_never
 
 from vllm.attention.layer import Attention, MLAAttention
 from vllm.config import ModelConfig, VllmConfig, set_current_vllm_config
-from vllm.envs import VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT
 from vllm.logger import init_logger
 from vllm.model_executor.layers.quantization.base_config import (
     QuantizationConfig,
@@ -139,29 +138,26 @@ def device_loading_context(module: torch.nn.Module, target_device: torch.device)
         yield module
 
     finally:
-        # If weights were loaded onto the CPU for FP8 online quantization, there
-        # is no need to move them back to the original device.
-        if not VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT:
-            # Restore parameters to their original devices, ignoring new parameters # noqa: E501
-            pin_memory = is_pin_memory_available()
-            for name, p in module.named_parameters():
-                if name in original_device_states:
-                    original_device: torch.device = original_device_states[name]
-                    if original_device.type == "cpu":
-                        # `torch.empty_like` does not support `pin_memory` argument # noqa: E501
-                        cpu_data = torch.empty_strided(
-                            size=p.data.size(),
-                            stride=p.data.stride(),
-                            dtype=p.data.dtype,
-                            layout=p.data.layout,
-                            device="cpu",
-                            pin_memory=pin_memory,
-                        )
-                        cpu_data.copy_(p.data)
-                        p.data = cpu_data
-                    else:
-                        p.data = p.data.to(original_device)
-            # New parameters or parameters already on target device are untouched # noqa: E501
+        # Restore parameters to their original devices, ignoring new parameters
+        pin_memory = is_pin_memory_available()
+        for name, p in module.named_parameters():
+            if name in original_device_states:
+                original_device: torch.device = original_device_states[name]
+                if original_device.type == "cpu":
+                    # `torch.empty_like` does not support `pin_memory` argument
+                    cpu_data = torch.empty_strided(
+                        size=p.data.size(),
+                        stride=p.data.stride(),
+                        dtype=p.data.dtype,
+                        layout=p.data.layout,
+                        device="cpu",
+                        pin_memory=pin_memory,
+                    )
+                    cpu_data.copy_(p.data)
+                    p.data = cpu_data
+                else:
+                    p.data = p.data.to(original_device)
+        # New parameters or parameters already on target device are untouched
 
 
 _MODEL_ARCH_BY_HASH = dict[int, tuple[type[nn.Module], str]]()
diff --git a/vllm/model_executor/model_loader/weight_utils.py b/vllm/model_executor/model_loader/weight_utils.py
index 02f10eb2a..6a45c3305 100644
--- a/vllm/model_executor/model_loader/weight_utils.py
+++ b/vllm/model_executor/model_loader/weight_utils.py
@@ -1042,6 +1042,7 @@ def composed_weight_loader(
 
 def initialize_dummy_weights(
     model: torch.nn.Module,
+    model_config: ModelConfig,
     low: float = -1e-3,
     high: float = 1e-3,
     seed: int = 1234,
@@ -1058,41 +1059,61 @@ def initialize_dummy_weights(
     is fixed, the random values generated by this function only depends on
     the parameter's number of elements and its data type.
     """
+    # TODO(future PR): make the check below more generic as more online
+    # quant backends are added
+    is_fp8_py_quant = model_config.quantization == "fp8"
+
     for param in model.state_dict().values():
-        if torch.is_floating_point(param):
-            if current_platform.is_tpu():
-                generator = torch.Generator(device="cpu")
-                generator.manual_seed(seed)
-                # Note: The param.uniform_ function cannot be used in this
-                # context because it demands more TPU HBM than directly copying
-                # from a CPU tensor.
-                # Note: We avoid using torch.rank_like as it doesn't currently
-                # support the generator argument.
-                param.copy_(
-                    (high - low)
-                    * torch.rand(
-                        param.shape,
-                        generator=generator,
-                        dtype=param.dtype,
-                        layout=param.layout,
-                        requires_grad=param.requires_grad,
-                        device="cpu",
-                    )
-                    + low
-                )
-                torch._sync(param)
-                continue
+        if is_fp8_py_quant and param.device == torch.device("meta"):
+            # for fp8.py's online quantization, dummy weight init will happen
+            # in `process_weights_after_loading`.
+            # TODO(future PR): consider refactoring dummy model init to compose
+            # better with online quantization
+            continue
 
-            generator = torch.Generator(device=param.data.device)
+        initialize_single_dummy_weight(param, low, high, seed)
+
+
+def initialize_single_dummy_weight(
+    param: torch.Tensor,
+    low: float = -1e-3,
+    high: float = 1e-3,
+    seed: int = 1234,
+) -> None:
+    if torch.is_floating_point(param):
+        if current_platform.is_tpu():
+            generator = torch.Generator(device="cpu")
             generator.manual_seed(seed)
-            if torch.finfo(param.data.dtype).bits < 16:
-                # uniform_ doesn't support < 16-bit datatypes (FP8)
-                dtype = param.data.dtype
-                tmp_param = param.data.to(torch.float16)
-                tmp_param = tmp_param.uniform_(low, high, generator=generator).to(dtype)
-                param.data.copy_(tmp_param)
-            else:
-                param.uniform_(low, high, generator=generator)
+            # Note: The param.uniform_ function cannot be used in this
+            # context because it demands more TPU HBM than directly copying
+            # from a CPU tensor.
+            # Note: We avoid using torch.rank_like as it doesn't currently
+            # support the generator argument.
+            param.copy_(
+                (high - low)
+                * torch.rand(
+                    param.shape,
+                    generator=generator,
+                    dtype=param.dtype,
+                    layout=param.layout,
+                    requires_grad=param.requires_grad,
+                    device="cpu",
+                )
+                + low
+            )
+            torch._sync(param)
+            return
+
+        generator = torch.Generator(device=param.data.device)
+        generator.manual_seed(seed)
+        if torch.finfo(param.data.dtype).bits < 16:
+            # uniform_ doesn't support < 16-bit datatypes (FP8)
+            dtype = param.data.dtype
+            tmp_param = param.data.to(torch.float16)
+            tmp_param = tmp_param.uniform_(low, high, generator=generator).to(dtype)
+            param.data.copy_(tmp_param)
+        else:
+            param.uniform_(low, high, generator=generator)
 
 
 def maybe_remap_kv_scale_name(name: str, params_dict: dict) -> str | None:
-- 
2.43.0


From e6d58addf96e9de802aff572b481d3690a846c97 Mon Sep 17 00:00:00 2001
From: YiSheng5 <yi.sheng@intel.com>
Date: Wed, 28 Jan 2026 11:26:53 +0800
Subject: [PATCH 30/36] when running with TP+DP, using naive backend (#114)

---
 .../distributed/device_communicators/xpu_communicator.py | 9 +++++++++
 1 file changed, 9 insertions(+)

diff --git a/vllm/distributed/device_communicators/xpu_communicator.py b/vllm/distributed/device_communicators/xpu_communicator.py
index 6bc26b6f3..b4b5b0378 100644
--- a/vllm/distributed/device_communicators/xpu_communicator.py
+++ b/vllm/distributed/device_communicators/xpu_communicator.py
@@ -22,6 +22,15 @@ class XpuCommunicator(DeviceCommunicatorBase):
         unique_name: str = "",
     ):
         super().__init__(cpu_group, device, device_group, unique_name)
+ 
+        from vllm.config import get_current_vllm_config_or_none
+        config = get_current_vllm_config_or_none()
+
+        if config.parallel_config.enable_expert_parallel == False or \
+        config.parallel_config.tensor_parallel_size == 1:
+            #AgRs only support TP+DP+EP for this release
+            self.all2all_backend = "naive"
+
         if self.use_all2all:
             if self.all2all_backend == "naive":
                 from .all2all import NaiveAll2AllManager
-- 
2.43.0


From 7ba909cc91c70a4ce4befbf48ec32cd1304adf51 Mon Sep 17 00:00:00 2001
From: wenjun liu <wenjun.liu@intel.com>
Date: Wed, 28 Jan 2026 15:46:45 +0800
Subject: [PATCH 31/36] update base image, torch and compiler (#109)

* update base image, torch and compiler

* update os pacakges

* update

* upgrade os version

* Update ipex to public link

---------

Co-authored-by: Du, Jun <jun.du@intel.com>
---
 docker/Dockerfile.xpu | 9 +++++----
 requirements/xpu.txt  | 4 ++--
 2 files changed, 7 insertions(+), 6 deletions(-)

diff --git a/docker/Dockerfile.xpu b/docker/Dockerfile.xpu
index 583d3e3b4..da2db6485 100644
--- a/docker/Dockerfile.xpu
+++ b/docker/Dockerfile.xpu
@@ -1,4 +1,4 @@
-FROM intel/deep-learning-essentials:2025.3.0-0-devel-ubuntu24.04 AS vllm-base
+FROM intel/deep-learning-essentials:2025.3.2-0-devel-ubuntu24.04 AS vllm-base
 
 RUN wget -O- https://apt.repos.intel.com/intel-gpg-keys/GPG-PUB-KEY-INTEL-SW-PRODUCTS.PUB | gpg --dearmor | tee /usr/share/keyrings/oneapi-archive-keyring.gpg > /dev/null && \
     echo "deb [signed-by=/usr/share/keyrings/oneapi-archive-keyring.gpg] https://apt.repos.intel.com/oneapi all main" | tee /etc/apt/sources.list.d/oneAPI.list && \
@@ -24,9 +24,10 @@ RUN apt clean && apt-get update -y && \
 
 RUN update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.12 1
 RUN update-alternatives --install /usr/bin/python python /usr/bin/python3.12 1
-
-RUN apt install -y libze1 libze-dev libze-intel-gpu1 intel-opencl-icd libze-intel-gpu-raytracing intel-ocloc
-
+RUN apt update && apt upgrade -y && \
+    apt install -y \
+    libze1 libze-dev libze-intel-gpu1 intel-opencl-icd libze-intel-gpu-raytracing intel-ocloc \
+    intel-oneapi-compiler-dpcpp-cpp-2025.3
 RUN wget https://github.com/uxlfoundation/oneCCL/releases/download/2021.15.7/intel-oneccl-2021.15.7.8_offline.sh
 RUN bash intel-oneccl-2021.15.7.8_offline.sh -a --silent --eula accept && echo "source /opt/intel/oneapi/setvars.sh --force" >> /root/.bashrc
 RUN rm -f /opt/intel/oneapi/ccl/latest && \
diff --git a/requirements/xpu.txt b/requirements/xpu.txt
index 3fd7e240c..dd4dfa467 100644
--- a/requirements/xpu.txt
+++ b/requirements/xpu.txt
@@ -11,9 +11,9 @@ jinja2>=3.1.6
 datasets # for benchmark scripts
 numba == 0.60.0 # v0.61 doesn't support Python 3.9. Required for N-gram speculative decoding
 tblib # for test
---extra-index-url=https://download.pytorch.org/whl/test/xpu
+--extra-index-url=https://download.pytorch.org/whl/xpu
 torch==2.10.0+xpu
 torchaudio
 torchvision
 
-intel-extension-for-pytorch @ https://ubit-artifactory-ba.intel.com/artifactory/aipc_releases-ba-local/gpu/new/validation/IPEX/nightly/PVC/UBUNTU/VLLM_nightly/20260114/1c16033db/intel_extension_for_pytorch-2.10.10.post1+xpu-cp312-cp312-linux_x86_64.whl
+intel-extension-for-pytorch @ https://intel-extension-for-pytorch.s3.us-east-1.amazonaws.com/ipex_dev/xpu/intel_extension_for_pytorch-2.10.10.post1%2Bxpu-cp312-cp312-linux_x86_64.whl
-- 
2.43.0


From 29eaffeb36eaa5be7bffdd2108758a0665fb97a7 Mon Sep 17 00:00:00 2001
From: Yan Ma <yan.ma@intel.com>
Date: Thu, 29 Jan 2026 16:11:03 +0800
Subject: [PATCH 32/36] fix online fp8 for MiniCPM models (#119)

Signed-off-by: Yan Ma <yan.ma@intel.com>
---
 vllm/model_executor/models/minicpmv.py | 32 +++++++++++++++++---------
 1 file changed, 21 insertions(+), 11 deletions(-)

diff --git a/vllm/model_executor/models/minicpmv.py b/vllm/model_executor/models/minicpmv.py
index 930ff737b..5cbcc0300 100644
--- a/vllm/model_executor/models/minicpmv.py
+++ b/vllm/model_executor/models/minicpmv.py
@@ -1047,9 +1047,17 @@ class MiniCPMVBaseModel(nn.Module, SupportsMultiModal, SupportsPP):
             quant_config=quant_config,
             prefix=maybe_prefix(prefix, "resampler"),
         )
+        self._resampler_moved = False
 
         self.make_empty_intermediate_tensors = self.llm.make_empty_intermediate_tensors
 
+    def _ensure_resampler_device(self) -> None:
+        if self._resampler_moved:
+            return
+        # Only move device, DO NOT touch dtype (fp8 quant needs its own dtype)
+        self.resampler.to(current_platform.device_type)
+        self._resampler_moved = True
+
     def _parse_and_validate_vision_input(
         self,
         modality: str,
@@ -1171,7 +1179,9 @@ class MiniCPMVBaseModel(nn.Module, SupportsMultiModal, SupportsPP):
 
     def load_weights(self, weights: Iterable[tuple[str, torch.Tensor]]) -> set[str]:
         loader = AutoWeightsLoader(self)
-        return loader.load_weights(weights)
+        loaded = loader.load_weights(weights)
+        self._ensure_resampler_device()
+        return loaded
 
     def get_mm_mapping(self) -> MultiModelKeys:
         """
@@ -1391,7 +1401,6 @@ class MiniCPMV2_5(MiniCPMVBaseModel, SupportsLoRA):
             patch_attention_mask=patch_attn_mask.unsqueeze(1),
             tgt_sizes=None,
         )
-
         return self.resampler(vision_embedding, tgt_sizes)
 
 
@@ -1485,12 +1494,13 @@ class MiniCPMV2_6(MiniCPMVBaseModel, SupportsLoRA):
             patch_attention_mask=patch_attn_mask.unsqueeze(1),
             tgt_sizes=tgt_sizes,
         )
-
         return self.resampler(vision_embedding, tgt_sizes)
 
     def load_weights(self, weights: Iterable[tuple[str, torch.Tensor]]) -> set[str]:
         loader = AutoWeightsLoader(self, skip_prefixes=["apm.", "audio", "tts"])
-        return loader.load_weights(weights)
+        loaded = loader.load_weights(weights)
+        self._ensure_resampler_device()
+        return loaded
 
 
 class MiniCPMV4_0(MiniCPMVBaseModel, SupportsLoRA):
@@ -1550,10 +1560,7 @@ class MiniCPMV4_0(MiniCPMVBaseModel, SupportsLoRA):
                 quant_config=quant_config,
                 prefix=prefix,
             )
-
-        return resampler.to(
-            device=current_platform.device_type, dtype=torch.get_default_dtype()
-        )
+        return resampler.to(dtype=torch.get_default_dtype())
 
     def get_vision_hidden_states(self, data: MiniCPMVImagePixelInputs) -> torch.Tensor:
         pixel_values = data["pixel_values"]
@@ -1588,7 +1595,9 @@ class MiniCPMV4_0(MiniCPMVBaseModel, SupportsLoRA):
 
     def load_weights(self, weights: Iterable[tuple[str, torch.Tensor]]) -> set[str]:
         loader = AutoWeightsLoader(self, skip_prefixes=["apm.", "audio", "tts"])
-        return loader.load_weights(weights)
+        loaded = loader.load_weights(weights)
+        self._ensure_resampler_device()
+        return loaded
 
 
 class MiniCPMV4_5(MiniCPMVBaseModel, SupportsLoRA):
@@ -1685,12 +1694,13 @@ class MiniCPMV4_5(MiniCPMVBaseModel, SupportsLoRA):
             patch_attention_mask=patch_attn_mask.unsqueeze(1),
             tgt_sizes=tgt_sizes,
         )
-
         return self.resampler(vision_embedding, tgt_sizes, all_temporal_ids)
 
     def load_weights(self, weights: Iterable[tuple[str, torch.Tensor]]) -> set[str]:
         loader = AutoWeightsLoader(self, skip_prefixes=["apm.", "audio", "tts"])
-        return loader.load_weights(weights)
+        loaded = loader.load_weights(weights)
+        self._ensure_resampler_device()
+        return loaded
 
 
 _SUPPORT_VERSION = {
-- 
2.43.0


From a7d33776ea2664e0e24cb82591f228b8bab0ff62 Mon Sep 17 00:00:00 2001
From: Yan Ma <yan.ma@intel.com>
Date: Fri, 30 Jan 2026 15:06:53 +0800
Subject: [PATCH 33/36] disable async scheduling unless user enable it
 explicitly (#121)

Signed-off-by: Yan Ma <yan.ma@intel.com>
---
 vllm/config/vllm.py | 10 +++++++---
 1 file changed, 7 insertions(+), 3 deletions(-)

diff --git a/vllm/config/vllm.py b/vllm/config/vllm.py
index ec699b629..72da0a11f 100644
--- a/vllm/config/vllm.py
+++ b/vllm/config/vllm.py
@@ -559,6 +559,8 @@ class VllmConfig:
             "external_launcher",
         )
 
+        from vllm.platforms import current_platform
+
         if self.scheduler_config.async_scheduling:
             # Async scheduling explicitly enabled, hard fail any incompatibilities.
             if self.parallel_config.pipeline_parallel_size > 1:
@@ -587,7 +589,11 @@ class VllmConfig:
                 )
         elif self.scheduler_config.async_scheduling is None:
             # Enable async scheduling unless there is an incompatible option.
-            if self.parallel_config.pipeline_parallel_size > 1:
+            if current_platform.is_xpu():
+                # We disable async scheduling for xpu if use doesn't explicitly
+                # enable it.
+                self.scheduler_config.async_scheduling = False
+            elif self.parallel_config.pipeline_parallel_size > 1:
                 logger.warning_once(
                     "Async scheduling is not yet supported with "
                     "pipeline_parallel_size > 1 and will be disabled.",
@@ -643,8 +649,6 @@ class VllmConfig:
             else:
                 self.parallel_config.disable_nccl_for_dp_synchronization = False
 
-        from vllm.platforms import current_platform
-
         if (
             self.model_config is not None
             and self.scheduler_config.enable_chunked_prefill
-- 
2.43.0


From d8a6f6b70f2b88258f55572c077f7fb579584f16 Mon Sep 17 00:00:00 2001
From: Yan Ma <yan.ma@intel.com>
Date: Fri, 30 Jan 2026 15:19:23 +0800
Subject: [PATCH 34/36] fix fp8 in DP+EP path (#122)

Signed-off-by: Yan Ma <yan.ma@intel.com>
---
 vllm/model_executor/layers/quantization/fp8.py | 2 --
 1 file changed, 2 deletions(-)

diff --git a/vllm/model_executor/layers/quantization/fp8.py b/vllm/model_executor/layers/quantization/fp8.py
index 320cc9e2a..23aafd228 100644
--- a/vllm/model_executor/layers/quantization/fp8.py
+++ b/vllm/model_executor/layers/quantization/fp8.py
@@ -606,7 +606,6 @@ class Fp8OnlineLinearMethod(Fp8LinearMethod):
                 )
                 _copy_missing_attrs(layer.weight, weight)
                 layer.register_parameter("weight", weight)
-                del layer._load_device
 
             # refresh the reference to `param` to reflect just-in-time
             # materialization
@@ -1215,7 +1214,6 @@ class Fp8OnlineMoEMethod(Fp8MoEMethod):
                 set_weight_attrs(w2_weight, extra_weight_attrs)
                 _copy_missing_attrs(layer.w2_weight, w2_weight)
                 layer.register_parameter("w2_weight", w2_weight)
-                del layer._load_device
 
             # refresh the reference to `param` to reflect just-in-time
             # materialization
-- 
2.43.0


From fa99ccebd403eb5ed97fda82530b797aee05d96a Mon Sep 17 00:00:00 2001
From: Yan Ma <yan.ma@intel.com>
Date: Fri, 6 Feb 2026 14:26:50 +0800
Subject: [PATCH 35/36] Fix VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT for fp8 online
 quantization (#133)

* Revert "fix fp8 in DP+EP path (#122)"

This reverts commit d8a6f6b70f2b88258f55572c077f7fb579584f16.

* Revert "Update fp8 quantization with streaming quantization (#113)"

This reverts commit c70698756d0e953c43b15ec974f52bac02b68fb5.

* fix VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT

Signed-off-by: Yan Ma <yan.ma@intel.com>

---------

Signed-off-by: Yan Ma <yan.ma@intel.com>
---
 docs/features/quantization/fp8.md             |   2 +
 tests/quantization/test_cpu_offload.py        |  15 +
 tests/quantization/test_fp8.py                |  93 +----
 vllm/envs.py                                  |   6 +
 .../model_executor/layers/quantization/fp8.py | 369 ++++++------------
 .../layers/quantization/ipex_quant.py         | 134 ++++---
 .../model_loader/base_loader.py               |  13 -
 .../model_loader/dummy_loader.py              |   2 +-
 vllm/model_executor/model_loader/utils.py     |  44 ++-
 .../model_loader/weight_utils.py              |  85 ++--
 10 files changed, 288 insertions(+), 475 deletions(-)

diff --git a/docs/features/quantization/fp8.md b/docs/features/quantization/fp8.md
index f7efdb5f4..b3ceab13f 100644
--- a/docs/features/quantization/fp8.md
+++ b/docs/features/quantization/fp8.md
@@ -138,3 +138,5 @@ result = llm.generate("Hello, my name is")
 print(result[0].outputs[0].text)
 ```
 
+!!! warning
+    Currently, by default we load the model at original precision before quantizing down to 8-bits, so you need enough memory to load the whole model. To avoid this, adding `VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT=1` can allow offloading weights to cpu before quantization and quantized weights will be kept in device.
diff --git a/tests/quantization/test_cpu_offload.py b/tests/quantization/test_cpu_offload.py
index 3b58614e5..a02504f5f 100644
--- a/tests/quantization/test_cpu_offload.py
+++ b/tests/quantization/test_cpu_offload.py
@@ -25,6 +25,21 @@ def test_cpu_offload_fp8():
     )
 
 
+@pytest.mark.skipif(
+    not is_quant_method_supported("fp8"),
+    reason="fp8 is not supported on this GPU type.",
+)
+def test_offload_weights_before_quant_fp8():
+    # Test quantization of an unquantized checkpoint
+    compare_two_settings(
+        "meta-llama/Llama-3.2-1B-Instruct",
+        ["--quantization", "fp8"],
+        ["--quantization", "fp8"],
+        {"VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT": "1"},
+        max_wait_seconds=480,
+    )
+
+
 @pytest.mark.skipif(
     not is_quant_method_supported("gptq_marlin"),
     reason="gptq_marlin is not supported on this GPU type.",
diff --git a/tests/quantization/test_fp8.py b/tests/quantization/test_fp8.py
index ddb63901f..e8abe0d41 100644
--- a/tests/quantization/test_fp8.py
+++ b/tests/quantization/test_fp8.py
@@ -5,10 +5,7 @@
 Run `pytest tests/quantization/test_fp8.py --forked`.
 """
 
-import logging
-
 import pytest
-import regex as re
 import torch
 
 from tests.quantization.utils import is_quant_method_supported
@@ -136,7 +133,7 @@ def test_kv_cache_model_load_and_run(
 @pytest.mark.parametrize(
     "use_rocm_aiter", [True, False] if current_platform.is_rocm() else [False]
 )
-def test_online_quantization(
+def test_load_fp16_model(
     vllm_runner,
     kv_cache_dtype: str,
     force_marlin: bool,
@@ -194,94 +191,6 @@ def test_online_quantization(
 
         llm.apply_model(check_model)
 
-        outputs = llm.generate_greedy(["Hello my name is"], max_tokens=4)
-        print(outputs[0][1])
-
-
-@pytest.mark.skipif(
-    not is_quant_method_supported("fp8"),
-    reason="FP8 is not supported on this GPU type.",
-)
-def test_online_quant_peak_mem(
-    vllm_runner,
-    monkeypatch,
-    caplog,
-) -> None:
-    # Disable multiprocessing so logs are captured in the main process
-    monkeypatch.setenv("VLLM_ENABLE_V1_MULTIPROCESSING", "0")
-
-    # Enable propagation so caplog can capture vllm logs
-    vllm_logger = logging.getLogger("vllm")
-    original_propagate = vllm_logger.propagate
-    vllm_logger.propagate = True
-
-    try:
-        with vllm_runner(
-            "Qwen/Qwen1.5-MoE-A2.7B",
-            quantization="fp8",
-            enforce_eager=True,
-        ) as llm:
-            outputs = llm.generate_greedy(["Hello my name is"], max_tokens=4)
-            print(outputs[0][1])
-    finally:
-        vllm_logger.propagate = original_propagate
-
-    # Parse memory usage from captured logs
-    model_memory_gib = None
-    peak_memory_gib = None
-    for record in caplog.records:
-        if model_memory_gib is None:
-            match = re.search(r"Model loading took ([\d.]+) GiB memory", record.message)
-            if match:
-                model_memory_gib = float(match.group(1))
-        if peak_memory_gib is None:
-            match = re.search(
-                r"Peak GPU memory after loading weights: ([\d.]+) GiB", record.message
-            )
-            if match:
-                peak_memory_gib = float(match.group(1))
-
-    assert model_memory_gib is not None, "Could not find model loading memory log"
-    assert peak_memory_gib is not None, "Could not find peak memory log"
-    print(f"GPU memory used after loading weights: {model_memory_gib} GiB")
-    print(f"Peak GPU memory usage while loading weights: {peak_memory_gib} GiB")
-
-    # model specific, Qwen/Qwen1.5-MoE-A2.7B fp8 online quant uses ~13.94
-    # GiB for weight loading (bf16 checkpoint is ~28 GiB)
-    expected_model_memory_gib = 14.0
-
-    # for Qwen/Qwen1.5-MoE-A2.7B the number we see today is 14.98 GiB, which
-    # is 1.07x above model_memory_gib. A slightly higher number is expected as
-    # when we load and quantize weights in a streaming fashion we need to
-    # have individual weights in bf16 + fp8 alive at the same time.
-    expected_peak_memory_gib = expected_model_memory_gib * 1.12
-
-    assert model_memory_gib < expected_model_memory_gib, (
-        f"{model_memory_gib=} higher than {expected_model_memory_gib}"
-    )
-    assert peak_memory_gib < expected_peak_memory_gib, (
-        f"{peak_memory_gib=} higher than {expected_peak_memory_gib}"
-    )
-
-
-@pytest.mark.skipif(
-    not is_quant_method_supported("fp8"),
-    reason="FP8 is not supported on this GPU type.",
-)
-def test_online_quant_load_format_dummy(
-    vllm_runner,
-    monkeypatch,
-    caplog,
-) -> None:
-    with vllm_runner(
-        "Qwen/Qwen1.5-MoE-A2.7B",
-        quantization="fp8",
-        enforce_eager=True,
-        load_format="dummy",
-    ) as llm:
-        outputs = llm.generate_greedy(["Hello my name is"], max_tokens=4)
-        print(outputs[0][1])
-
 
 @pytest.mark.skipif(
     not is_quant_method_supported("fp8"),
diff --git a/vllm/envs.py b/vllm/envs.py
index 3825610f7..7a2061b55 100755
--- a/vllm/envs.py
+++ b/vllm/envs.py
@@ -253,6 +253,7 @@ if TYPE_CHECKING:
     VLLM_LOG_MODEL_INSPECTION: bool = False
     VLLM_DEBUG_MFU_METRICS: bool = False
     VLLM_XPU_FP8_DTYPE: str = "e5m2"
+    VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT: bool = False
 
 
 def get_default_cache_root():
@@ -1614,6 +1615,11 @@ environment_variables: dict[str, Callable[[], Any]] = {
     ),
     # fp8 dtype for XPU platform
     "VLLM_XPU_FP8_DTYPE": lambda: os.environ.get("VLLM_XPU_FP8_DTYPE", "e5m2"),
+    # Offload model weights to cpu before online fp8 quantization
+    "VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT": lambda: os.environ.get(
+        "VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT", "0"
+    )
+    == "1",
 }
 
 # --8<-- [end:env-vars-definition]
diff --git a/vllm/model_executor/layers/quantization/fp8.py b/vllm/model_executor/layers/quantization/fp8.py
index 23aafd228..050e03317 100644
--- a/vllm/model_executor/layers/quantization/fp8.py
+++ b/vllm/model_executor/layers/quantization/fp8.py
@@ -84,7 +84,6 @@ from vllm.model_executor.layers.quantization.utils.w8a8_utils import (
     maybe_create_device_identity,
     normalize_e4m3fn_to_e4m3fnuz,
 )
-from vllm.model_executor.model_loader.weight_utils import initialize_single_dummy_weight
 from vllm.model_executor.parameter import (
     BlockQuantScaleParameter,
     ModelWeightParameter,
@@ -227,14 +226,9 @@ class Fp8Config(QuantizationConfig):
                 fused_mapping=self.packed_modules_mapping,
             ):
                 return UnquantizedLinearMethod()
-            if not self.is_checkpoint_fp8_serialized:
-                online_method = Fp8OnlineLinearMethod(self)
-                online_method.marlin_input_dtype = get_marlin_input_dtype(prefix)
-                return online_method
-            else:
-                offline_method = Fp8LinearMethod(self)
-                offline_method.marlin_input_dtype = get_marlin_input_dtype(prefix)
-                return offline_method
+            quant_method = Fp8LinearMethod(self)
+            quant_method.marlin_input_dtype = get_marlin_input_dtype(prefix)
+            return quant_method
         elif isinstance(layer, FusedMoE):
             if is_layer_skipped(
                 prefix=prefix,
@@ -292,23 +286,18 @@ class CopyNumelCounter(TorchDispatchMode):
         return out
 
 
-def _copy_missing_attrs(old: torch.Tensor, new: torch.Tensor) -> None:
-    """Copies any attrs present in `old` but not in `new` to `new`"""
-    new_attrs = set(dir(new))
-    attrs_to_set = {}
-    for attr in dir(old):
-        if attr not in new_attrs:
-            attrs_to_set[attr] = getattr(old, attr)
-    set_weight_attrs(new, attrs_to_set)
-
-
 class Fp8LinearMethod(LinearMethodBase):
     """Linear method for FP8.
     Supports loading FP8 checkpoints with static weight scale and
     dynamic/static activation scale.
 
+    Also supports loading quantized FP16/BF16 model checkpoints with dynamic
+    activation scaling. The weight scaling factor will be initialized after
+    the model weights are loaded.
+
     Limitations:
-    1. Only support float8_e4m3fn data type due to the limitation of
+    1. Only support per-tensor quantization due to torch._scaled_mm support.
+    2. Only support float8_e4m3fn data type due to the limitation of
        torch._scaled_mm (https://github.com/pytorch/pytorch/blob/2e48b39603411a41c5025efbe52f89560b827825/aten/src/ATen/native/cuda/Blas.cpp#L854-L856)
 
     Args:
@@ -395,43 +384,95 @@ class Fp8LinearMethod(LinearMethodBase):
                 self.weight_block_size,
             )
 
-        weight = create_fp8_weight_parameter(
-            output_size_per_partition, input_size_per_partition, weight_loader
-        )
-        layer.register_parameter("weight", weight)
-
-        # WEIGHT SCALE
-        if not self.block_quant:
-            scale = create_fp8_scale_parameter(
-                PerTensorScaleParameter,
-                output_partition_sizes,
-                input_size_per_partition,
-                None,
-                weight_loader,
+        # WEIGHT
+        if self.quant_config.is_checkpoint_fp8_serialized:
+            weight = create_fp8_weight_parameter(
+                output_size_per_partition, input_size_per_partition, weight_loader
             )
-            set_weight_attrs(scale, {"scale_type": "weight_scale"})
-            layer.register_parameter("weight_scale", scale)
         else:
-            assert not self.act_q_static
-            assert self.weight_block_size is not None
-            scale = create_fp8_scale_parameter(
-                BlockQuantScaleParameter,
-                output_partition_sizes,
-                input_size_per_partition,
-                self.weight_block_size,
-                weight_loader,
+
+            def patched_weight_loader(param, loaded_weight, *args, **kwargs):
+                # track how many elements we have updated
+                if not hasattr(layer, "_loaded_numel"):
+                    layer._loaded_numel = 0
+
+                # load the current weight chunk
+                copy_numel_counter = CopyNumelCounter()
+                with copy_numel_counter:
+                    res = weight_loader(param, loaded_weight, *args, **kwargs)  # type: ignore[misc]
+                layer._loaded_numel += copy_numel_counter.copied_numel
+
+                # if we have loaded all of the elements, call
+                # process_weights_after_loading
+                target_loaded_numel = layer.weight.numel()
+                if layer._loaded_numel == target_loaded_numel:
+                    self.process_weights_after_loading(layer)
+
+                    # Delete the bookkeeping
+                    del layer._loaded_numel
+                    # Prevent the usual `process_weights_after_loading` call from doing
+                    # anything
+                    layer._already_called_process_weights_after_loading = True
+
+                return res
+
+            # For non-serialized checkpoints, use original dtype
+            # Force offloading weights to cpu if VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT
+            # enabled, otherwise use original device config which can be gpu or cpu
+            # (may happen when cpu_offload_gb > 0)
+            weight = ModelWeightParameter(
+                data=torch.empty(
+                    output_size_per_partition,
+                    input_size_per_partition,
+                    dtype=params_dtype,
+                    device="cpu" if envs.VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT else None,
+                ),
+                input_dim=1,
+                output_dim=0,
+                weight_loader=weight_loader,
             )
-            set_weight_attrs(scale, {"scale_type": "weight_scale"})
-            # The weight_scale_inv name is intentional for deepseekv3
-            layer.register_parameter("weight_scale_inv", scale)
+        layer.register_parameter("weight", weight)
 
-        # INPUT ACTIVATION SCALE
-        if self.act_q_static:
-            scale = create_fp8_input_scale(output_partition_sizes, weight_loader)
-            set_weight_attrs(scale, {"scale_type": "input_scale"})
-            layer.register_parameter("input_scale", scale)
+        # If checkpoint is serialized fp8, load them.
+        # Otherwise, wait until process_weights_after_loading.
+        if self.quant_config.is_checkpoint_fp8_serialized:
+            # WEIGHT SCALE
+            if not self.block_quant:
+                scale = create_fp8_scale_parameter(
+                    PerTensorScaleParameter,
+                    output_partition_sizes,
+                    input_size_per_partition,
+                    None,
+                    weight_loader,
+                )
+                set_weight_attrs(scale, {"scale_type": "weight_scale"})
+                layer.register_parameter("weight_scale", scale)
+            else:
+                assert not self.act_q_static
+                assert self.weight_block_size is not None
+                scale = create_fp8_scale_parameter(
+                    BlockQuantScaleParameter,
+                    output_partition_sizes,
+                    input_size_per_partition,
+                    self.weight_block_size,
+                    weight_loader,
+                )
+                set_weight_attrs(scale, {"scale_type": "weight_scale"})
+                # The weight_scale_inv name is intentional for deepseekv3
+                layer.register_parameter("weight_scale_inv", scale)
+
+            # INPUT ACTIVATION SCALE
+            if self.act_q_static:
+                scale = create_fp8_input_scale(output_partition_sizes, weight_loader)
+                set_weight_attrs(scale, {"scale_type": "input_scale"})
+                layer.register_parameter("input_scale", scale)
+            else:
+                layer.register_parameter("input_scale", None)
 
     def process_weights_after_loading(self, layer: Module) -> None:
+        if getattr(layer, "_already_called_process_weights_after_loading", False):
+            return
+
         size_k_first = True
         input_scale = None
         # TODO(rob): refactor block quant into separate class.
@@ -449,24 +490,31 @@ class Fp8LinearMethod(LinearMethodBase):
 
         # If checkpoint not serialized fp8, quantize the weights.
         else:
+            if not self.quant_config.is_checkpoint_fp8_serialized:
+                qweight, weight_scale = ops.scaled_fp8_quant(layer.weight, scale=None)
+                weight = qweight.t()
+
             # If checkpoint is fp8 per-tensor, handle that there are N scales for N
             # shards in a fused module
-            weight = layer.weight
-            weight_scale = layer.weight_scale
-
-            # If using w8a8, torch._scaled_mm needs per tensor, so
-            # requantize the logical shards as a single weight.
-            if not self.use_marlin:
-                weight, weight_scale, input_scale = process_fp8_weight_tensor_strategy(
-                    weight,
-                    weight_scale,
-                    layer.logical_widths,
-                    getattr(layer, "input_scale", None),
-                )
-                if self.act_q_static:
-                    assert input_scale is not None
-                    input_scale = input_scale.max()
-            weight = weight.t()
+            else:
+                weight = layer.weight
+                weight_scale = layer.weight_scale
+
+                # If using w8a8, torch._scaled_mm needs per tensor, so
+                # requantize the logical shards as a single weight.
+                if not self.use_marlin:
+                    weight, weight_scale, input_scale = (
+                        process_fp8_weight_tensor_strategy(
+                            weight,
+                            weight_scale,
+                            layer.logical_widths,
+                            getattr(layer, "input_scale", None),
+                        )
+                    )
+                    if self.act_q_static:
+                        assert input_scale is not None
+                        input_scale = input_scale.max()
+                weight = weight.t()
 
             # Update layer with new values.
             replace_parameter(layer, "weight", weight.data)
@@ -568,121 +616,6 @@ class Fp8LinearMethod(LinearMethodBase):
         )
 
 
-class Fp8OnlineLinearMethod(Fp8LinearMethod):
-    """Online version of Fp8LinearMethod, loads the fp16/bf16 checkpoint
-    and quantized the weights during loading."""
-
-    def create_weights(
-        self,
-        layer: torch.nn.Module,
-        input_size_per_partition: int,
-        output_partition_sizes: list[int],
-        input_size: int,
-        output_size: int,
-        params_dtype: torch.dtype,
-        **extra_weight_attrs,
-    ):
-        output_size_per_partition = sum(output_partition_sizes)
-        weight_loader = extra_weight_attrs.get("weight_loader")
-        layer.logical_widths = output_partition_sizes
-        layer.input_size_per_partition = input_size_per_partition
-        layer.output_size_per_partition = output_size_per_partition
-        layer.orig_dtype = params_dtype
-        layer.weight_block_size = None
-
-        # WEIGHT
-        def patched_weight_loader(param, loaded_weight, *args, **kwargs):
-            # track how many elements we have updated
-            if not hasattr(layer, "_loaded_numel"):
-                layer._loaded_numel = 0
-
-                # when the first `loaded_weight` is about to be
-                # loaded to `param`, materialize `param` just-in-time
-                weight = ModelWeightParameter(
-                    data=torch.empty_like(layer.weight, device=layer._load_device),
-                    input_dim=1,
-                    output_dim=0,
-                    weight_loader=patched_weight_loader,
-                )
-                _copy_missing_attrs(layer.weight, weight)
-                layer.register_parameter("weight", weight)
-
-            # refresh the reference to `param` to reflect just-in-time
-            # materialization
-            param = layer.weight
-
-            # load the current weight chunk
-            copy_numel_counter = CopyNumelCounter()
-            with copy_numel_counter:
-                res = weight_loader(param, loaded_weight, *args, **kwargs)  # type: ignore[misc]
-            layer._loaded_numel += copy_numel_counter.copied_numel
-
-            # if we have loaded all of the elements, call
-            # process_weights_after_loading
-            target_loaded_numel = layer.weight.numel()
-            if layer._loaded_numel == target_loaded_numel:
-                self.process_weights_after_loading(layer)
-
-                # Delete the bookkeeping
-                del layer._loaded_numel
-                # Prevent the usual `process_weights_after_loading` call from doing
-                # anything
-                layer._already_called_process_weights_after_loading = True
-
-            return res
-
-        weight = ModelWeightParameter(
-            data=torch.empty(
-                output_size_per_partition,
-                input_size_per_partition,
-                # materialized just-in-time in `patched_weight_loader`
-                device="meta",
-                dtype=params_dtype,
-            ),
-            input_dim=1,
-            output_dim=0,
-            weight_loader=patched_weight_loader,
-        )
-        # stash the correct device for `patched_weight_loader`
-        layer._load_device = torch.get_default_device()
-        layer.register_parameter("weight", weight)
-
-    def process_weights_after_loading(self, layer: Module) -> None:
-        if getattr(layer, "_already_called_process_weights_after_loading", False):
-            return
-
-        # deferred initialization of randomly initialized weights for the
-        # `--load_format dummy` feature
-        if layer.weight.device == torch.device("meta"):
-            weight = ModelWeightParameter(
-                data=torch.empty_like(layer.weight, device=layer._load_device),
-                input_dim=1,
-                output_dim=0,
-                weight_loader=layer.weight.weight_loader,
-            )
-            _copy_missing_attrs(layer.weight, weight)
-            layer.register_parameter("weight", weight)
-            initialize_single_dummy_weight(layer.weight)
-
-        # TODO(future): support block_quant in online quant path
-        assert not self.block_quant
-
-        layer.input_scale = None
-        qweight, weight_scale = ops.scaled_fp8_quant(layer.weight, scale=None)
-        weight = qweight.t()
-
-        # Update layer with new values.
-        replace_parameter(layer, "weight", weight.data)
-        replace_parameter(layer, "weight_scale", weight_scale.data)
-
-        if self.use_marlin:
-            size_k_first = True
-            prepare_fp8_layer_for_marlin(
-                layer, size_k_first, input_dtype=self.marlin_input_dtype
-            )
-            # Activations not quantized for marlin.
-
-
 class Fp8MoEMethod(FusedMoEMethodBase):
     """MoE method for FP8.
     Supports loading FP8 checkpoints with static weight scale and
@@ -786,6 +719,7 @@ class Fp8MoEMethod(FusedMoEMethodBase):
                 2 * intermediate_size_per_partition,
                 hidden_size,
                 dtype=params_dtype,
+                device="cpu" if envs.VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT else None,
             ),
             requires_grad=False,
         )
@@ -798,6 +732,7 @@ class Fp8MoEMethod(FusedMoEMethodBase):
                 hidden_size,
                 intermediate_size_per_partition,
                 dtype=params_dtype,
+                device="cpu" if envs.VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT else None,
             ),
             requires_grad=False,
         )
@@ -1183,45 +1118,13 @@ class Fp8OnlineMoEMethod(Fp8MoEMethod):
         weight_loader = extra_weight_attrs["weight_loader"]
         # create a new holder to prevent modifying behavior of any other
         # objects which might depend on the old one
-        new_extra_weight_attrs = extra_weight_attrs
+        # new_extra_weight_attrs = extra_weight_attrs
 
         def patched_weight_loader(param, loaded_weight, *args, **kwargs):
             # add a counter to track how many elements we have updated
             if not hasattr(layer, "_loaded_numel"):
                 layer._loaded_numel = 0
 
-                # save the ids of original w13 and w2 so that we can
-                # distinguish which one `param` should map to further
-                # down in this file
-                layer._w13_weight_orig_id = id(layer.w13_weight)
-                layer._w2_weight_orig_id = id(layer.w2_weight)
-
-                # when the first `loaded_weight` is about to be
-                # loaded to `param`, materialize `param` just-in-time
-
-                w13_weight = torch.nn.Parameter(
-                    torch.empty_like(layer.w13_weight, device=layer._load_device),
-                    requires_grad=False,
-                )
-                set_weight_attrs(w13_weight, extra_weight_attrs)
-                _copy_missing_attrs(layer.w13_weight, w13_weight)
-                layer.register_parameter("w13_weight", w13_weight)
-
-                w2_weight = torch.nn.Parameter(
-                    torch.empty_like(layer.w2_weight, device=layer._load_device),
-                    requires_grad=False,
-                )
-                set_weight_attrs(w2_weight, extra_weight_attrs)
-                _copy_missing_attrs(layer.w2_weight, w2_weight)
-                layer.register_parameter("w2_weight", w2_weight)
-
-            # refresh the reference to `param` to reflect just-in-time
-            # materialization
-            if id(param) == layer._w13_weight_orig_id:
-                param = layer.w13_weight
-            elif id(param) == layer._w2_weight_orig_id:
-                param = layer.w2_weight
-
             # load the current weight chunk
             copy_numel_counter = CopyNumelCounter()
             with copy_numel_counter:
@@ -1236,25 +1139,18 @@ class Fp8OnlineMoEMethod(Fp8MoEMethod):
 
                 # Delete the bookkeeping
                 del layer._loaded_numel
-                del layer._w13_weight_orig_id
-                del layer._w2_weight_orig_id
                 # Prevent the usual `process_weights_after_loading` call
                 # from doing anything
                 layer._already_called_process_weights_after_loading = True
 
             return res
 
-        new_extra_weight_attrs["weight_loader"] = patched_weight_loader
-        extra_weight_attrs = new_extra_weight_attrs
-
         # WEIGHTS
         w13_weight = torch.nn.Parameter(
             torch.empty(
                 num_experts,
                 2 * intermediate_size_per_partition,
                 hidden_size,
-                # materialized just-in-time in `patched_weight_loader`
-                device="meta",
                 dtype=params_dtype,
             ),
             requires_grad=False,
@@ -1267,16 +1163,12 @@ class Fp8OnlineMoEMethod(Fp8MoEMethod):
                 num_experts,
                 hidden_size,
                 intermediate_size_per_partition,
-                # materialized just-in-time in `patched_weight_loader`
-                device="meta",
                 dtype=params_dtype,
             ),
             requires_grad=False,
         )
         layer.register_parameter("w2_weight", w2_weight)
         set_weight_attrs(w2_weight, extra_weight_attrs)
-        # stash the correct device for `patched_weight_loader`
-        layer._load_device = torch.get_default_device()
 
         # WEIGHT_SCALES
         # Allocate 2 scales for w1 and w3 respectively.
@@ -1299,31 +1191,6 @@ class Fp8OnlineMoEMethod(Fp8MoEMethod):
         if getattr(layer, "_already_called_process_weights_after_loading", False):
             return
 
-        # deferred initialization of randomly initialized weights for the
-        # `--load_format dummy` feature
-        if layer.w13_weight.device == torch.device("meta"):
-            w13_weight = torch.nn.Parameter(
-                torch.empty_like(layer.w13_weight, device=layer._load_device),
-                requires_grad=False,
-            )
-            set_weight_attrs(
-                w13_weight, {"weight_loader": layer.w13_weight.weight_loader}
-            )
-            _copy_missing_attrs(layer.w13_weight, w13_weight)
-            layer.register_parameter("w13_weight", w13_weight)
-            initialize_single_dummy_weight(layer.w13_weight)
-        if layer.w2_weight.device == torch.device("meta"):
-            w2_weight = torch.nn.Parameter(
-                torch.empty_like(layer.w2_weight, device=layer._load_device),
-                requires_grad=False,
-            )
-            set_weight_attrs(
-                w2_weight, {"weight_loader": layer.w2_weight.weight_loader}
-            )
-            _copy_missing_attrs(layer.w2_weight, w2_weight)
-            layer.register_parameter("w2_weight", w2_weight)
-            initialize_single_dummy_weight(layer.w2_weight)
-
         # If checkpoint is fp16, quantize in place.
         fp8_dtype = current_platform.fp8_dtype()
         w13 = torch.empty_like(layer.w13_weight, dtype=fp8_dtype)
diff --git a/vllm/model_executor/layers/quantization/ipex_quant.py b/vllm/model_executor/layers/quantization/ipex_quant.py
index 587bb5ff7..2d2c07d04 100644
--- a/vllm/model_executor/layers/quantization/ipex_quant.py
+++ b/vllm/model_executor/layers/quantization/ipex_quant.py
@@ -8,9 +8,9 @@ from packaging import version
 from safetensors.torch import _TYPES as _SAFETENSORS_TO_TORCH_DTYPE
 from torch.nn import Module
 
+import vllm.envs as envs
 from vllm._ipex_ops import ipex_ops as ops
 from vllm.model_executor.layers.fused_moe import (
-    FusedMoE,
     FusedMoEConfig,
     FusedMoEMethodBase,
     FusedMoeWeightScaleSupported,
@@ -19,6 +19,7 @@ from vllm.model_executor.layers.fused_moe.config import FusedMoEQuantConfig
 from vllm.model_executor.layers.fused_moe.fused_moe_router import (
     FusedMoERouter,
 )
+from vllm.model_executor.layers.fused_moe.layer import FusedMoE
 from vllm.model_executor.layers.linear import (
     LinearBase,
     UnquantizedLinearMethod,
@@ -30,16 +31,14 @@ from vllm.model_executor.layers.quantization import (
 from vllm.model_executor.layers.quantization.awq import AWQLinearMethod
 from vllm.model_executor.layers.quantization.fp8 import (
     Fp8Config,
-    Fp8OnlineLinearMethod,
+    Fp8LinearMethod,
     Fp8OnlineMoEMethod,
-    _copy_missing_attrs,
 )
 from vllm.model_executor.layers.quantization.gptq import GPTQLinearMethod
 from vllm.model_executor.layers.quantization.utils.gptq_utils import (
     get_linear_quant_method,
 )
 from vllm.model_executor.layers.quantization.utils.quant_utils import is_layer_skipped
-from vllm.model_executor.model_loader.weight_utils import initialize_single_dummy_weight
 from vllm.model_executor.utils import replace_parameter, set_weight_attrs
 from vllm.platforms import current_platform
 from vllm.scalar_type import scalar_types
@@ -390,7 +389,7 @@ class IPEXAWQLinearMethod(AWQLinearMethod):
         return out.reshape(x.shape[:-1] + (layer.ipex_output_size,))
 
 
-class XPUFp8LinearMethod(Fp8OnlineLinearMethod):
+class XPUFp8LinearMethod(Fp8LinearMethod):
     def __init__(self, quant_config: Fp8Config):
         super().__init__(quant_config)
 
@@ -428,53 +427,98 @@ class XPUFp8MoEMethod(Fp8OnlineMoEMethod):
         super().__init__(quant_config, layer)
         self.quant_config = quant_config
 
+    def create_weights(
+        self,
+        layer: Module,
+        num_experts: int,
+        hidden_size: int,
+        intermediate_size_per_partition: int,
+        params_dtype: torch.dtype,
+        **extra_weight_attrs,
+    ):
+        layer.intermediate_size_per_partition = intermediate_size_per_partition
+        layer.hidden_size = hidden_size
+        layer.num_experts = num_experts
+        layer.orig_dtype = params_dtype
+        layer.weight_block_size = None
+        # WEIGHTS
+        w13_weight = torch.nn.Parameter(
+            torch.empty(
+                num_experts,
+                2 * intermediate_size_per_partition,
+                hidden_size,
+                dtype=params_dtype,
+                device="cpu" if envs.VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT else None,
+            ),
+            requires_grad=False,
+        )
+        layer.register_parameter("w13_weight", w13_weight)
+        set_weight_attrs(w13_weight, extra_weight_attrs)
+
+        w2_weight = torch.nn.Parameter(
+            torch.empty(
+                num_experts,
+                hidden_size,
+                intermediate_size_per_partition,
+                dtype=params_dtype,
+                device="cpu" if envs.VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT else None,
+            ),
+            requires_grad=False,
+        )
+        layer.register_parameter("w2_weight", w2_weight)
+        set_weight_attrs(w2_weight, extra_weight_attrs)
+
+        # Allocate 2 scales for w1 and w3 respectively.
+        # They will be combined to a single scale after weight loading.
+        w13_weight_scale = torch.nn.Parameter(
+            torch.ones(num_experts, 2, dtype=torch.float32), requires_grad=False
+        )
+        w2_weight_scale = torch.nn.Parameter(
+            torch.ones(num_experts, dtype=torch.float32), requires_grad=False
+        )
+        layer.register_parameter("w13_weight_scale", w13_weight_scale)
+        layer.register_parameter("w2_weight_scale", w2_weight_scale)
+
+        extra_weight_attrs.update(
+            {"quant_method": FusedMoeWeightScaleSupported.TENSOR.value}
+        )
+        # INPUT_SCALES
+        layer.w13_input_scale = None
+        layer.w2_input_scale = None
+
     def process_weights_after_loading(self, layer: Module) -> None:
         if getattr(layer, "_already_called_process_weights_after_loading", False):
             return
-        # deferred initialization of randomly initialized weights for the
-        # `--load_format dummy` feature
-        if layer.w13_weight.device == torch.device("meta"):
-            w13_weight = torch.nn.Parameter(
-                torch.empty_like(layer.w13_weight, device=layer._load_device),
-                requires_grad=False,
+        if not self.quant_config.is_checkpoint_fp8_serialized:
+            fp8_dtype = current_platform.fp8_dtype()
+            w13_weight = torch.empty_like(
+                layer.w13_weight.data, device="xpu", dtype=fp8_dtype
             )
-            set_weight_attrs(
-                w13_weight, {"weight_loader": layer.w13_weight.weight_loader}
+            w2_weight = torch.empty_like(
+                layer.w2_weight.data, device="xpu", dtype=fp8_dtype
             )
-            _copy_missing_attrs(layer.w13_weight, w13_weight)
-            layer.register_parameter("w13_weight", w13_weight)
-            initialize_single_dummy_weight(layer.w13_weight)
-        if layer.w2_weight.device == torch.device("meta"):
-            w2_weight = torch.nn.Parameter(
-                torch.empty_like(layer.w2_weight, device=layer._load_device),
+
+            # Re-initialize w13_scale because we directly quantize
+            # merged w13 weights and generate a single scaling factor.
+            layer.w13_weight_scale = torch.nn.Parameter(
+                torch.ones(
+                    layer.local_num_experts,
+                    dtype=torch.float32,
+                    device=w13_weight.device,
+                ),
                 requires_grad=False,
             )
-            set_weight_attrs(
-                w2_weight, {"weight_loader": layer.w2_weight.weight_loader}
-            )
-            _copy_missing_attrs(layer.w2_weight, w2_weight)
-            layer.register_parameter("w2_weight", w2_weight)
-            initialize_single_dummy_weight(layer.w2_weight)
-
-        # If checkpoint is fp16, quantize in place.
-        fp8_dtype = current_platform.fp8_dtype()
-        w13 = torch.empty_like(layer.w13_weight, dtype=fp8_dtype)
-        w2 = torch.empty_like(layer.w2_weight, dtype=fp8_dtype)
-        w13_scale = layer.w13_weight_scale
-        w2_scale = layer.w2_weight_scale
-
-        for expert in range(layer.local_num_experts):
-            w13[expert, :, :], w13_scale[expert] = ops.scaled_fp8_quant(
-                layer.w13_weight[expert, :, :]
-            )
-            w2[expert, :, :], w2_scale[expert] = ops.scaled_fp8_quant(
-                layer.w2_weight[expert, :, :]
-            )
-
-        replace_parameter(layer, "w13_weight", w13)
-        replace_parameter(layer, "w2_weight", w2)
-        replace_parameter(layer, f"w13_{self.weight_scale_name}", w13_scale)
-        replace_parameter(layer, f"w2_{self.weight_scale_name}", w2_scale)
+            for expert in range(layer.local_num_experts):
+                w13_weight[expert, :, :], layer.w13_weight_scale[expert] = (
+                    ops.scaled_fp8_quant(layer.w13_weight.data[expert, :, :])
+                )
+                w2_weight[expert, :, :], layer.w2_weight_scale[expert] = (
+                    ops.scaled_fp8_quant(layer.w2_weight.data[expert, :, :])
+                )
+            replace_parameter(layer, "w13_weight", w13_weight)
+            replace_parameter(layer, "w2_weight", w2_weight)
+            replace_parameter(layer, "w13_weight_scale", layer.w13_weight_scale)
+            replace_parameter(layer, "w2_weight_scale", layer.w2_weight_scale)
 
         import intel_extension_for_pytorch as ipex
 
diff --git a/vllm/model_executor/model_loader/base_loader.py b/vllm/model_executor/model_loader/base_loader.py
index cf9305bb8..2238b0cfe 100644
--- a/vllm/model_executor/model_loader/base_loader.py
+++ b/vllm/model_executor/model_loader/base_loader.py
@@ -13,8 +13,6 @@ from vllm.model_executor.model_loader.utils import (
     initialize_model,
     process_weights_after_loading,
 )
-from vllm.platforms import current_platform
-from vllm.utils.mem_utils import format_gib
 from vllm.utils.torch_utils import set_default_torch_dtype
 
 logger = init_logger(__name__)
@@ -58,17 +56,6 @@ class BaseModelLoader(ABC):
             logger.debug("Loading weights on %s ...", load_device)
             # Quantization does not happen in `load_weights` but after it
             self.load_weights(model, model_config)
-
-            # Log peak GPU memory after loading weights. This is needed
-            # to have test coverage on peak memory for online quantization.
-            if current_platform.is_cuda():
-                peak_memory = torch.cuda.max_memory_allocated()
-                logger.info_once(
-                    "Peak GPU memory after loading weights: %s GiB",
-                    format_gib(peak_memory),
-                    scope="local",
-                )
-
             process_weights_after_loading(model, model_config, target_device)
 
         return model.eval()
diff --git a/vllm/model_executor/model_loader/dummy_loader.py b/vllm/model_executor/model_loader/dummy_loader.py
index 156071f1d..b2a934ce5 100644
--- a/vllm/model_executor/model_loader/dummy_loader.py
+++ b/vllm/model_executor/model_loader/dummy_loader.py
@@ -25,4 +25,4 @@ class DummyModelLoader(BaseModelLoader):
     def load_weights(self, model: nn.Module, model_config: ModelConfig) -> None:
         # NOTE(woosuk): For accurate performance evaluation, we assign
         # random values to the weights.
-        initialize_dummy_weights(model, model_config)
+        initialize_dummy_weights(model)
diff --git a/vllm/model_executor/model_loader/utils.py b/vllm/model_executor/model_loader/utils.py
index 08d7a851a..24b66eb91 100644
--- a/vllm/model_executor/model_loader/utils.py
+++ b/vllm/model_executor/model_loader/utils.py
@@ -13,6 +13,7 @@ from typing_extensions import assert_never
 
 from vllm.attention.layer import Attention, MLAAttention
 from vllm.config import ModelConfig, VllmConfig, set_current_vllm_config
+from vllm.envs import VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT
 from vllm.logger import init_logger
 from vllm.model_executor.layers.quantization.base_config import (
     QuantizationConfig,
@@ -138,26 +139,29 @@ def device_loading_context(module: torch.nn.Module, target_device: torch.device)
         yield module
 
     finally:
-        # Restore parameters to their original devices, ignoring new parameters
-        pin_memory = is_pin_memory_available()
-        for name, p in module.named_parameters():
-            if name in original_device_states:
-                original_device: torch.device = original_device_states[name]
-                if original_device.type == "cpu":
-                    # `torch.empty_like` does not support `pin_memory` argument
-                    cpu_data = torch.empty_strided(
-                        size=p.data.size(),
-                        stride=p.data.stride(),
-                        dtype=p.data.dtype,
-                        layout=p.data.layout,
-                        device="cpu",
-                        pin_memory=pin_memory,
-                    )
-                    cpu_data.copy_(p.data)
-                    p.data = cpu_data
-                else:
-                    p.data = p.data.to(original_device)
-        # New parameters or parameters already on target device are untouched
+        # If weights were loaded onto the CPU for FP8 online quantization, there
+        # is no need to move them back to the original device.
+        if not VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT:
+            # Restore parameters to their original devices, ignoring new parameters # noqa: E501
+            pin_memory = is_pin_memory_available()
+            for name, p in module.named_parameters():
+                if name in original_device_states:
+                    original_device: torch.device = original_device_states[name]
+                    if original_device.type == "cpu":
+                        # `torch.empty_like` does not support `pin_memory` argument # noqa: E501
+                        cpu_data = torch.empty_strided(
+                            size=p.data.size(),
+                            stride=p.data.stride(),
+                            dtype=p.data.dtype,
+                            layout=p.data.layout,
+                            device="cpu",
+                            pin_memory=pin_memory,
+                        )
+                        cpu_data.copy_(p.data)
+                        p.data = cpu_data
+                    else:
+                        p.data = p.data.to(original_device)
+            # New parameters or parameters already on target device are untouched # noqa: E501
 
 
 _MODEL_ARCH_BY_HASH = dict[int, tuple[type[nn.Module], str]]()
diff --git a/vllm/model_executor/model_loader/weight_utils.py b/vllm/model_executor/model_loader/weight_utils.py
index 6a45c3305..02f10eb2a 100644
--- a/vllm/model_executor/model_loader/weight_utils.py
+++ b/vllm/model_executor/model_loader/weight_utils.py
@@ -1042,7 +1042,6 @@ def composed_weight_loader(
 
 def initialize_dummy_weights(
     model: torch.nn.Module,
-    model_config: ModelConfig,
     low: float = -1e-3,
     high: float = 1e-3,
     seed: int = 1234,
@@ -1059,61 +1058,41 @@ def initialize_dummy_weights(
     is fixed, the random values generated by this function only depends on
     the parameter's number of elements and its data type.
     """
-    # TODO(future PR): make the check below more generic as more online
-    # quant backends are added
-    is_fp8_py_quant = model_config.quantization == "fp8"
-
     for param in model.state_dict().values():
-        if is_fp8_py_quant and param.device == torch.device("meta"):
-            # for fp8.py's online quantization, dummy weight init will happen
-            # in `process_weights_after_loading`.
-            # TODO(future PR): consider refactoring dummy model init to compose
-            # better with online quantization
-            continue
-
-        initialize_single_dummy_weight(param, low, high, seed)
-
+        if torch.is_floating_point(param):
+            if current_platform.is_tpu():
+                generator = torch.Generator(device="cpu")
+                generator.manual_seed(seed)
+                # Note: The param.uniform_ function cannot be used in this
+                # context because it demands more TPU HBM than directly copying
+                # from a CPU tensor.
+                # Note: We avoid using torch.rank_like as it doesn't currently
+                # support the generator argument.
+                param.copy_(
+                    (high - low)
+                    * torch.rand(
+                        param.shape,
+                        generator=generator,
+                        dtype=param.dtype,
+                        layout=param.layout,
+                        requires_grad=param.requires_grad,
+                        device="cpu",
+                    )
+                    + low
+                )
+                torch._sync(param)
+                continue
 
-def initialize_single_dummy_weight(
-    param: torch.Tensor,
-    low: float = -1e-3,
-    high: float = 1e-3,
-    seed: int = 1234,
-) -> None:
-    if torch.is_floating_point(param):
-        if current_platform.is_tpu():
-            generator = torch.Generator(device="cpu")
+            generator = torch.Generator(device=param.data.device)
             generator.manual_seed(seed)
-            # Note: The param.uniform_ function cannot be used in this
-            # context because it demands more TPU HBM than directly copying
-            # from a CPU tensor.
-            # Note: We avoid using torch.rank_like as it doesn't currently
-            # support the generator argument.
-            param.copy_(
-                (high - low)
-                * torch.rand(
-                    param.shape,
-                    generator=generator,
-                    dtype=param.dtype,
-                    layout=param.layout,
-                    requires_grad=param.requires_grad,
-                    device="cpu",
-                )
-                + low
-            )
-            torch._sync(param)
-            return
-
-        generator = torch.Generator(device=param.data.device)
-        generator.manual_seed(seed)
-        if torch.finfo(param.data.dtype).bits < 16:
-            # uniform_ doesn't support < 16-bit datatypes (FP8)
-            dtype = param.data.dtype
-            tmp_param = param.data.to(torch.float16)
-            tmp_param = tmp_param.uniform_(low, high, generator=generator).to(dtype)
-            param.data.copy_(tmp_param)
-        else:
-            param.uniform_(low, high, generator=generator)
+            if torch.finfo(param.data.dtype).bits < 16:
+                # uniform_ doesn't support < 16-bit datatypes (FP8)
+                dtype = param.data.dtype
+                tmp_param = param.data.to(torch.float16)
+                tmp_param = tmp_param.uniform_(low, high, generator=generator).to(dtype)
+                param.data.copy_(tmp_param)
+            else:
+                param.uniform_(low, high, generator=generator)
 
 
 def maybe_remap_kv_scale_name(name: str, params_dict: dict) -> str | None:
-- 
2.43.0


From 5253f29ebfb5625365974d06022bc8fb7fc4005d Mon Sep 17 00:00:00 2001
From: Zhan Xue <zhan.xue@intel.com>
Date: Thu, 12 Feb 2026 04:59:40 +0800
Subject: [PATCH 36/36] enable xpu gdr

Signed-off-by: Zhan Xue <zhan.xue@intel.com>
---
 .../kv_transfer/kv_connector/v1/nixl_connector.py          | 7 ++++++-
 1 file changed, 6 insertions(+), 1 deletion(-)

diff --git a/vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py b/vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py
index 7916d1e02..1f4f8de4d 100644
--- a/vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py
+++ b/vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py
@@ -118,7 +118,10 @@ _NIXL_SUPPORTED_DEVICE = {
         "cpu",
     ),
     "tpu": ("cpu",),
-    "xpu": ("cpu",),
+    "xpu": (
+            "cpu",
+            "xpu",
+    ),
     "cpu": ("cpu",),
 }
 # support for oot platform by providing mapping in current_platform
@@ -886,6 +889,8 @@ class NixlConnectorWorker:
                 nixl_memory_type = "VRAM"
             elif self.kv_buffer_device == "cpu":
                 nixl_memory_type = "DRAM"
+            elif self.kv_buffer_device == "xpu":
+                self.nixl_memory_type = "VRAM"
         if nixl_memory_type is None:
             raise RuntimeError(
                 f"{self.device_type} with {self.kv_buffer_device} kv_buffer "
-- 
2.43.0

