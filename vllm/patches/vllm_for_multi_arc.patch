diff --git a/.buildkite/lm-eval-harness/run-lm-eval-gsm-vllm-baseline.sh b/.buildkite/lm-eval-harness/run-lm-eval-gsm-vllm-baseline.sh
index b98d42aa7..b2a1ebef2 100644
--- a/.buildkite/lm-eval-harness/run-lm-eval-gsm-vllm-baseline.sh
+++ b/.buildkite/lm-eval-harness/run-lm-eval-gsm-vllm-baseline.sh
@@ -46,6 +46,6 @@ while getopts "m:b:l:f:t:" OPT; do
 done
 
 lm_eval --model vllm \
-  --model_args "pretrained=$MODEL,tensor_parallel_size=$TP_SIZE,add_bos_token=true,trust_remote_code=true,max_model_len=4096" \
+  --model_args "pretrained=$MODEL,tensor_parallel_size=$TP_SIZE,add_bos_token=true,distributed_executor_backend=mp,trust_remote_code=true,max_model_len=4096,enforce_eager=true,max_num_batched_tokens=4096" \
   --tasks gsm8k --num_fewshot "$FEWSHOT" --limit "$LIMIT" \
   --batch_size "$BATCH_SIZE"
diff --git a/.github/workflows/ci.yaml b/.github/workflows/ci.yaml
new file mode 100644
index 000000000..aef250abe
--- /dev/null
+++ b/.github/workflows/ci.yaml
@@ -0,0 +1,185 @@
+name: Run Intel XPU BMG CI
+
+on:
+  pull_request:
+    branches:
+      - '**release**'  
+    types: [opened, synchronize, reopened]  # 
+
+jobs:
+  run-xpu-BMG-CI:
+    if: |
+        github.event_name == 'pull_request' ||
+        (github.event_name == 'issue_comment' && 
+        github.event.issue.pull_request &&
+        contains(github.event.comment.body, '/BMG_CI'))
+    runs-on: BMG 
+
+    steps:
+    - name: Fix workspace permissions
+      run: |
+        sudo chown -R $(whoami):$(whoami) "$GITHUB_WORKSPACE"
+        sudo chmod -R 755 "$GITHUB_WORKSPACE"
+        sudo rm -f "$GITHUB_WORKSPACE/.git/index.lock" || true
+
+    - name: Checkout QA_ci (test code)  #
+      uses: actions/checkout@v4
+      with:
+        ref: QA_ci
+        path: qa_ci_code
+
+    - name: Checkout PR + Release Branch (DUT code)
+      uses: actions/checkout@v4
+      with:
+        ref: ${{ github.event.pull_request.head.ref }}
+        path: target_code
+        fetch-depth: 0  # 
+
+    - name: Merge PR into Release Branch
+      run: |
+        cd target_code
+        git fetch origin ${{ github.base_ref }}
+        git merge origin/${{ github.base_ref }} --no-commit
+      shell: bash
+
+    - name: Build docker image
+      run: |
+        echo "start to build image"
+        cd target_code
+        if [ -n "${{ github.event.pull_request.number }}" ]; then
+          image_name="vllm_xpu_ci_${{ github.event.pull_request.number }}"
+        else
+          image_name="vllm_xpu_ci_$(echo $GITHUB_REF | awk -F '/' '{print $3}')"
+        fi
+        image_name=$(echo "$image_name" | tr '[:upper:]' '[:lower:]')
+        #!/bin/bash
+
+        # Configuration
+        MAX_RETRIES=6                      # Maximum number of retry attempts
+        TIMEOUT=1800                       # 30-minute timeout per attempt (in seconds)
+        LOG_FILE="docker_build.log"         # Log file path
+
+        # Proxy configurations - add more if needed
+        PROXIES=(
+          "http://child-prc.intel.com:913"    # First fallback
+          "http://proxy.ims.intel.com:911"    # Primary proxy
+          "http://child-prc.intel.com:913"    # First fallback
+        )
+
+        # No-proxy configuration
+        NO_PROXY=".intel.com,intel.com,localhost,127.0.0.1"
+        #docker builder prune -f  #clean cache     
+        docker builder prune --all --force
+
+        #Loop through proxy configurations
+        for (( attempt=1; attempt<=$MAX_RETRIES; attempt++ )); do
+          proxy_index=$(( (attempt-1) % ${#PROXIES[@]} ))
+          proxy=${PROXIES[$proxy_index]}
+          echo "=== Attempt $attempt/$MAX_RETRIES (Proxy: $proxy) ===" | tee -a "$LOG_FILE"
+          
+          if [ $attempt -eq 1 ]; then
+            # First attempt without no_proxy
+            timeout $TIMEOUT docker build \
+              --build-arg http_proxy=$proxy \
+              --build-arg https_proxy=$proxy \
+              -f docker/Dockerfile.xpu \
+              -t "$image_name" \
+              --shm-size=4g . 2>&1 | tee -a "$LOG_FILE"
+          else
+            # Subsequent attempts with no_proxy
+            timeout $TIMEOUT docker build \
+              --build-arg http_proxy=$proxy \
+              --build-arg https_proxy=$proxy \
+              --build-arg no_proxy="$NO_PROXY" \
+              -f docker/Dockerfile.xpu \
+              -t "$image_name" \
+              --shm-size=4g . 2>&1 | tee -a "$LOG_FILE"
+          fi
+
+          # Check if build succeeded
+          if [ ${PIPESTATUS[0]} -eq 0 ]; then
+            echo "=== Build succeeded on attempt $attempt ===" | tee -a "$LOG_FILE"
+            exit 0
+          fi
+        done
+
+        echo "=== ERROR: All $MAX_RETRIES attempts failed. Check $LOG_FILE for details. ===" | tee -a "$LOG_FILE"
+        exit 1
+
+    - name: Prepare environment (clean up old processes and containers)
+      run: |
+        echo "Killing any process on port 8000..."
+        lsof -t -i:8000 | xargs -r kill -9 || true
+
+        echo "Killing old vllm server processes..."
+        pkill -f "python3 -m vllm.entrypoints.openai.api_server" || true
+
+        echo "Removing old container if exists..."
+        docker rm -f vllm_internal_ci || true
+    
+    - name: Run benchmark inside local Docker image
+      run: |
+        # Reuse the image_name from previous step
+        if [ -n "${{ github.event.pull_request.number }}" ]; then
+          image_name="vllm_xpu_ci_${{ github.event.pull_request.number }}"
+        else
+          image_name="vllm_xpu_ci_$(echo $GITHUB_REF | awk -F '/' '{print $3}')"
+        fi
+        image_name=$(echo "$image_name" | tr '[:upper:]' '[:lower:]')
+
+        echo "Running benchmark using image: $image_name"
+        docker run -t --rm --name vllm_internal_ci --shm-size 10g \
+          --net=host \
+          --ipc=host \
+          --privileged \
+          -v ${HOME}/actions-runner/_work/vllm-xpu/vllm-xpu/qa_ci_code:/WORKSPACE \
+          -v /dev/dri/by-path:/dev/dri/by-path \
+          -v ${HOME}/.cache:/root/.cache/ \
+          -e http_proxy=${http_proxy:-"http://proxy-dmz.intel.com:912"} \
+          -e https_proxy=${http_proxy:-"http://proxy-dmz.intel.com:912"} \
+          -e no_proxy=${no_proxy:-"127.0.0.1,localhost"} \
+          --device /dev/dri:/dev/dri \
+          -w /workspace \
+          --entrypoint='' \
+          --mount type=bind,source="$HOME/.secrets/my_token",target=/run/secrets/my_token,readonly \
+          $image_name \
+          bash -c "bash /WORKSPACE/.buildkite/nightly-benchmarks/scripts/CI_run_server_benchmarks.sh BMG || true; chown -R \$(id -u):\$(id -g) /WORKSPACE"
+
+    - name: Validate server benchmark results
+      run: |
+            python3 ${HOME}/actions-runner/_work/vllm-xpu/vllm-xpu/qa_ci_code/.buildkite/nightly-benchmarks/scripts/analyze_benchmark_results_final.py --test-selector BMG
+            cat ${HOME}/actions-runner/_work/vllm-xpu/vllm-xpu/qa_ci_code/benchmarks/results/benchmark_analysis_final.json
+
+    - name: Fix permissions
+      run: sudo chmod -R 755 ${{ runner.workspace }}/vllm-xpu/qa_ci_code/benchmarks/results/
+    
+    - name: Debug path
+      run: ls -la ${{ runner.workspace }}/vllm-xpu/qa_ci_code/benchmarks/results/
+
+    - name: Upload benchmark results
+      if: always()
+      uses: actions/upload-artifact@v4
+      with:
+        name: benchmark-results
+        path: ${{ runner.workspace }}/vllm-xpu/qa_ci_code/benchmarks/results/
+
+    - name: Analyze and validate benchmark results
+      if: always()
+      run: |
+        RESULTS_FILE="$HOME/actions-runner/_work/vllm-xpu/vllm-xpu/qa_ci_code/benchmarks/results/benchmark_analysis_final.json"
+        if [ ! -f "$RESULTS_FILE" ]; then
+          echo "‚ùå Benchmark analysis file not found!"
+          exit 1
+        fi
+        
+        echo "üìä Benchmark Results:"
+        cat "$RESULTS_FILE"
+        FAILURES=$(jq -r '.[] | select(.function != "pass") | .case_name' "$RESULTS_FILE")
+        
+        if [ -n "$FAILURES" ]; then
+          echo "‚ùå Failed cases detected:"
+          echo "$FAILURES"
+          exit 1
+        else
+          echo "‚úÖ All benchmarks passed"
+        fi
diff --git a/.github/workflows/ci_pvc.yaml b/.github/workflows/ci_pvc.yaml
new file mode 100644
index 000000000..eaa2f332a
--- /dev/null
+++ b/.github/workflows/ci_pvc.yaml
@@ -0,0 +1,181 @@
+name: Run Intel XPU PVC CI
+
+on:
+  pull_request:
+    branches:
+      - '**release**' 
+    types: [opened, synchronize, reopened]  # 
+
+jobs:
+  run-xpu-PVC-CI:
+    runs-on: PVC
+
+    steps:
+    - name: Fix workspace permissions
+      run: |
+        sudo chown -R $(whoami):$(whoami) "$GITHUB_WORKSPACE"
+        sudo chmod -R 755 "$GITHUB_WORKSPACE"
+        sudo rm -f "$GITHUB_WORKSPACE/.git/index.lock" || true
+
+    - name: Checkout QA_ci (test code)  # 
+      uses: actions/checkout@v4
+      with:
+        ref: QA_ci
+        path: qa_ci_code
+
+    - name: Checkout PR + Release Branch (DUT code)
+      uses: actions/checkout@v4
+      with:
+        ref: ${{ github.event.pull_request.head.ref }}
+        path: target_code
+        fetch-depth: 0  # 
+
+    # 
+    - name: Merge PR into Release Branch
+      run: |
+        cd target_code
+        git fetch origin ${{ github.base_ref }}
+        git merge origin/${{ github.base_ref }} --no-commit
+      shell: bash
+
+    - name: Build docker image
+      run: |
+        echo "start to build image"
+        cd target_code
+        if [ -n "${{ github.event.pull_request.number }}" ]; then
+          image_name="vllm_xpu_ci_${{ github.event.pull_request.number }}"
+        else
+          image_name="vllm_xpu_ci_$(echo $GITHUB_REF | awk -F '/' '{print $3}')"
+        fi
+        image_name=$(echo "$image_name" | tr '[:upper:]' '[:lower:]')
+        #!/bin/bash
+
+        # Configuration
+        MAX_RETRIES=6                      # Maximum number of retry attempts
+        TIMEOUT=1800                       # 30-minute timeout per attempt (in seconds)
+        LOG_FILE="docker_build.log"         # Log file path
+
+        # Proxy configurations - add more if needed
+        PROXIES=(
+          "http://proxy.ims.intel.com:911"    # Primary proxy
+          "http://child-prc.intel.com:913"    # First fallback
+        )
+
+        # No-proxy configuration
+        NO_PROXY=".intel.com,intel.com,localhost,127.0.0.1"
+        #docker builder prune -f  #clean cache  
+        docker builder prune --all --force
+  
+        # Loop through proxy configurations
+        for (( attempt=1; attempt<=$MAX_RETRIES; attempt++ )); do
+                  proxy_index=$(( (attempt-1) % ${#PROXIES[@]} ))
+                  proxy=${PROXIES[$proxy_index]}
+                  echo "=== Attempt $attempt/$MAX_RETRIES (Proxy: $proxy) ===" | tee -a "$LOG_FILE"
+                  
+                  if [ $attempt -eq 1 ]; then
+                    # First attempt without no_proxy
+                    timeout $TIMEOUT docker build \
+                      --build-arg http_proxy=$proxy \
+                      --build-arg https_proxy=$proxy \
+                      -f docker/Dockerfile.xpu \
+                      -t "$image_name" \
+                      --shm-size=4g . 2>&1 | tee -a "$LOG_FILE"
+                  else
+                    # Subsequent attempts with no_proxy
+                    timeout $TIMEOUT docker build \
+                      --build-arg http_proxy=$proxy \
+                      --build-arg https_proxy=$proxy \
+                      --build-arg no_proxy="$NO_PROXY" \
+                      -f docker/Dockerfile.xpu \
+                      -t "$image_name" \
+                      --shm-size=4g . 2>&1 | tee -a "$LOG_FILE"
+                  fi
+
+                  # Check if build succeeded
+                  if [ ${PIPESTATUS[0]} -eq 0 ]; then
+                    echo "=== Build succeeded on attempt $attempt ===" | tee -a "$LOG_FILE"
+                    exit 0
+                  fi
+                done
+
+        echo "=== ERROR: All $MAX_RETRIES attempts failed. Check $LOG_FILE for details. ===" | tee -a "$LOG_FILE"
+        exit 1
+
+    - name: Prepare environment (clean up old processes and containers)
+      run: |
+        echo "Killing any process on port 8000..."
+        lsof -t -i:8000 | xargs -r kill -9 || true
+
+        echo "Killing old vllm server processes..."
+        pkill -f "python3 -m vllm.entrypoints.openai.api_server" || true
+
+        echo "Removing old container if exists..."
+        docker rm -f vllm_internal_ci || true
+    
+    - name: Run benchmark inside local Docker image
+      run: |
+        # Reuse the image_name from previous step
+        if [ -n "${{ github.event.pull_request.number }}" ]; then
+          image_name="vllm_xpu_ci_${{ github.event.pull_request.number }}"
+        else
+          image_name="vllm_xpu_ci_$(echo $GITHUB_REF | awk -F '/' '{print $3}')"
+        fi
+        image_name=$(echo "$image_name" | tr '[:upper:]' '[:lower:]')
+
+        echo "Running benchmark using image: $image_name"
+        docker run -t --rm --name vllm_internal_ci --shm-size 10g \
+          --net=host \
+          --ipc=host \
+          --privileged \
+          -v $HOME/actions-runner/_work/vllm-xpu/vllm-xpu/qa_ci_code:/WORKSPACE \
+          -v /dev/dri/by-path:/dev/dri/by-path \
+          -v /mnt/data3:/root/.cache/ \
+          -e http_proxy=${http_proxy:-"http://proxy-dmz.intel.com:912"} \
+          -e https_proxy=${http_proxy:-"http://proxy-dmz.intel.com:912"} \
+          -e no_proxy=${no_proxy:-"127.0.0.1,localhost"} \
+          --device /dev/dri:/dev/dri \
+          -w /workspace \
+          --entrypoint='' \
+          --mount type=bind,source="$HOME/.secrets/my_token",target=/run/secrets/my_token,readonly \
+          $image_name \
+          bash -c "IPEX_FMHA_V3=0 bash /WORKSPACE/.buildkite/nightly-benchmarks/scripts/CI_run_server_benchmarks.sh "PVC" || true; chown -R \$(id -u):\$(id -g) /WORKSPACE"
+
+    - name: Validate server benchmark results
+      run: |
+        python3 $HOME/actions-runner/_work/vllm-xpu/vllm-xpu/qa_ci_code/.buildkite/nightly-benchmarks/scripts/analyze_benchmark_results_final.py --test-selector "PVC"
+        cat $HOME/actions-runner/_work/vllm-xpu/vllm-xpu/qa_ci_code/benchmarks/results/benchmark_analysis_final.json
+
+    - name: Fix permissions
+      run: sudo chmod -R 755 ${{ runner.workspace }}/vllm-xpu/qa_ci_code/benchmarks/results/
+    
+    - name: Debug path
+      run: ls -la ${{ runner.workspace }}/vllm-xpu/qa_ci_code/benchmarks/results/
+
+    - name: Upload benchmark results
+      if: always()
+      uses: actions/upload-artifact@v4
+      with:
+        name: benchmark-results
+        path: ${{ runner.workspace }}/vllm-xpu/qa_ci_code/benchmarks/results/
+
+    - name: Analyze and validate benchmark results
+      if: always()
+      run: |
+        RESULTS_FILE="$HOME/actions-runner/_work/vllm-xpu/vllm-xpu/qa_ci_code/benchmarks/results/benchmark_analysis_final.json"
+        if [ ! -f "$RESULTS_FILE" ]; then
+          echo "‚ùå Benchmark analysis file not found!"
+          exit 1
+        fi
+        
+        echo "üìä Benchmark Results:"
+        cat "$RESULTS_FILE"
+        
+        FAILURES=$(jq -r '.[] | select(.function != "pass") | .case_name' "$RESULTS_FILE")
+        
+        if [ -n "$FAILURES" ]; then
+          echo "‚ùå Failed cases detected:"
+          echo "$FAILURES"
+          exit 1
+        else
+          echo "‚úÖ All benchmarks passed"
+        fi
diff --git a/.github/workflows/cleanup_pr_body.yml b/.github/workflows/cleanup_pr_body.yml
deleted file mode 100644
index d5c6b8d43..000000000
--- a/.github/workflows/cleanup_pr_body.yml
+++ /dev/null
@@ -1,31 +0,0 @@
-name: Cleanup PR Body
-
-on:
-  pull_request_target:
-    types: [opened, reopened, edited]
-
-permissions:
-  pull-requests: write
-
-jobs:
-  update-description:
-    runs-on: ubuntu-latest
-
-    steps:
-      - name: Checkout repository
-        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
-
-      - name: Set up Python
-        uses: actions/setup-python@42375524e23c412d93fb67b49958b491fce71c38 # v5.4.0
-        with:
-          python-version: '3.12'
-
-      - name: Install Python dependencies
-        run: |
-          python3 -m pip install --upgrade pip
-          python3 -m pip install regex
-
-      - name: Update PR description
-        env:
-          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
-        run: bash .github/scripts/cleanup_pr_body.sh "${{ github.event.number }}"
diff --git a/.github/workflows/reminder_comment.yml b/.github/workflows/reminder_comment.yml
deleted file mode 100644
index 16ae1aadb..000000000
--- a/.github/workflows/reminder_comment.yml
+++ /dev/null
@@ -1,27 +0,0 @@
-name: PR Reminder Comment Bot
-permissions:
-  pull-requests: write
-on:
-  pull_request_target:
-    types: [opened]
-jobs:
-  pr_reminder:
-    runs-on: ubuntu-latest
-    steps:
-      - name: Remind to run full CI on PR
-        uses: actions/github-script@60a0d83039c74a4aee543508d2ffcb1c3799cdea # v7.0.1
-        with:
-          script: |
-            github.rest.issues.createComment({
-              owner: context.repo.owner,
-              repo: context.repo.repo,
-              issue_number: context.issue.number,
-              body: 'üëã Hi! Thank you for contributing to the vLLM project.\n\n' +
-                'üí¨ Join our developer Slack at https://slack.vllm.ai to discuss your PR in #pr-reviews, coordinate on features in #feat- channels, or join special interest groups in #sig- channels.\n\n' +
-                'Just a reminder: PRs would not trigger full CI run by default. Instead, it would only run `fastcheck` CI which starts running only a small and essential subset of CI tests to quickly catch errors. You can run other CI tests on top of those by going to your `fastcheck` build on Buildkite UI (linked in the PR checks section) and unblock them. If you do not have permission to unblock, ping `simon-mo` or `khluu` to add you in our Buildkite org.\n\n' +
-                'Once the PR is approved and ready to go, your PR reviewer(s) can run CI to test the changes comprehensively before merging.\n\n' +
-                'To run CI, PR reviewers can either: Add `ready` label to the PR or enable auto-merge.\n\n' +
-                'üöÄ'
-            })
-        env:
-          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
diff --git a/CMakeLists.txt b/CMakeLists.txt
index 98ed682fe..5dd6e907c 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -95,6 +95,10 @@ if (NOT VLLM_TARGET_DEVICE STREQUAL "cuda" AND
     NOT VLLM_TARGET_DEVICE STREQUAL "rocm")
     if (VLLM_TARGET_DEVICE STREQUAL "cpu")
         include(${CMAKE_CURRENT_LIST_DIR}/cmake/cpu_extension.cmake)
+    elseif(VLLM_TARGET_DEVICE STREQUAL "xpu")
+        message(STATUS "Building XPU")
+        set(VLLM_GPU_LANG "SYCL")
+        include(${CMAKE_CURRENT_LIST_DIR}/cmake/xpu_extension.cmake)
     else()
         return()
     endif()
diff --git a/benchmarks/backend_request_func.py b/benchmarks/backend_request_func.py
index c7229dbb8..72531f3fc 100644
--- a/benchmarks/backend_request_func.py
+++ b/benchmarks/backend_request_func.py
@@ -18,7 +18,7 @@ from transformers import AutoTokenizer, PreTrainedTokenizer, PreTrainedTokenizer
 # NOTE(simon): do not import vLLM here so the benchmark script
 # can run without vLLM installed.
 
-AIOHTTP_TIMEOUT = aiohttp.ClientTimeout(total=6 * 60 * 60)
+AIOHTTP_TIMEOUT = aiohttp.ClientTimeout(total=2 * 6 * 60 * 60)
 
 
 @dataclass
diff --git a/benchmarks/benchmark_serving.py b/benchmarks/benchmark_serving.py
index c597fb106..5bad6645b 100644
--- a/benchmarks/benchmark_serving.py
+++ b/benchmarks/benchmark_serving.py
@@ -256,10 +256,11 @@ async def benchmark(
         raise ValueError(f"Unknown backend: {backend}")
 
     print("Starting initial single prompt test run...")
+    # set test_output_len=10 to avoid long prompt test run
     test_prompt, test_prompt_len, test_output_len, test_mm_content = (
         input_requests[0].prompt,
         input_requests[0].prompt_len,
-        input_requests[0].expected_output_len,
+        10,
         input_requests[0].multi_modal_data,
     )
 
diff --git a/benchmarks/benchmark_throughput.py b/benchmarks/benchmark_throughput.py
index 14461121f..e9b9f0b77 100644
--- a/benchmarks/benchmark_throughput.py
+++ b/benchmarks/benchmark_throughput.py
@@ -44,6 +44,7 @@ def run_vllm(
     n: int,
     engine_args: EngineArgs,
     disable_detokenize: bool = False,
+    do_profile: bool = False,
 ) -> tuple[float, Optional[list[RequestOutput]]]:
     from vllm import LLM, SamplingParams
 
@@ -89,10 +90,14 @@ def run_vllm(
     outputs = None
     if not use_beam_search:
         start = time.perf_counter()
+        if do_profile:
+            llm.start_profile()
         outputs = llm.generate(
             prompts, sampling_params, lora_request=lora_requests, use_tqdm=True
         )
         end = time.perf_counter()
+        if do_profile:
+            llm.stop_profile()
     else:
         assert lora_requests is None, "BeamSearch API does not support LoRA"
         prompts = [request.prompt for request in requests]
@@ -410,6 +415,7 @@ def main(args: argparse.Namespace):
                 args.n,
                 EngineArgs.from_cli_args(args),
                 args.disable_detokenize,
+                args.profile
             )
     elif args.backend == "hf":
         assert args.tensor_parallel_size == 1
@@ -647,6 +653,10 @@ def create_argument_parser():
     parser.add_argument(
         "--num-prompts", type=int, default=1000, help="Number of prompts to process."
     )
+    parser.add_argument("--profile",
+                        action='store_true',
+                        default=False,
+                        help="whether run with profiler.")
     parser.add_argument(
         "--hf-max-batch-size",
         type=int,
diff --git a/cmake/utils.cmake b/cmake/utils.cmake
index 621179a70..9e1f4e9c7 100644
--- a/cmake/utils.cmake
+++ b/cmake/utils.cmake
@@ -445,7 +445,7 @@ function (define_gpu_extension_target GPU_MOD_NAME)
     GPU
     "WITH_SOABI"
     "DESTINATION;LANGUAGE;USE_SABI"
-    "SOURCES;ARCHITECTURES;COMPILE_FLAGS;INCLUDE_DIRECTORIES;LIBRARIES")
+    "SOURCES;ARCHITECTURES;COMPILE_FLAGS;INCLUDE_DIRECTORIES;LIBRARIES;LINK_FLAGS")
 
   # Add hipify preprocessing step when building with HIP/ROCm.
   if (GPU_LANGUAGE STREQUAL "HIP")
@@ -487,6 +487,11 @@ function (define_gpu_extension_target GPU_MOD_NAME)
 
   target_link_libraries(${GPU_MOD_NAME} PRIVATE torch ${GPU_LIBRARIES})
 
+  if (GPU_LANGUAGE STREQUAL "SYCL")
+    target_compile_options(${GPU_MOD_NAME} PRIVATE ${GPU_COMPILE_FLAGS})
+    target_link_options(${GPU_MOD_NAME} PRIVATE ${GPU_LINK_FLAGS})
+  endif()
+
   # Don't use `TORCH_LIBRARIES` for CUDA since it pulls in a bunch of
   # dependencies that are not necessary and may not be installed.
   if (GPU_LANGUAGE STREQUAL "CUDA")
diff --git a/cmake/xpu_extension.cmake b/cmake/xpu_extension.cmake
new file mode 100644
index 000000000..fd671a6bf
--- /dev/null
+++ b/cmake/xpu_extension.cmake
@@ -0,0 +1,62 @@
+set(CMAKE_EXPORT_COMPILE_COMMANDS ON)
+
+#
+# Define environment variables for special configurations
+#
+# TODO: detect Intel GPU Architecture(PVC or Arc) to add AOT flag.
+
+#
+# Check the compile flags
+#
+# append_cmake_prefix_path("intel_extension_for_pytorch" "intel_extension_for_pytorch.cmake_prefix_path")
+# find_package(IPEX REQUIRED)
+# IPEX will overwrite TORCH_LIBRARIES, so re-add torch_python lib.
+append_torchlib_if_found(torch_python)
+# include_directories(${IPEX_INCLUDE_DIRS})
+set(CMPLR_ROOT $ENV{CMPLR_ROOT})
+set(CMAKE_CXX_COMPILER icpx)
+set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -Wno-narrowing")
+set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -O3")
+set(VLLM_EXTRA_INCLUDE_DIRECTORIES ${CMPLR_ROOT}/include/sycl)
+
+list(APPEND VLLM_GPU_FLAGS "-fsycl" "-fsycl-targets=spir64")
+list(APPEND VLLM_GPU_LINK_FLAGS "-fsycl" "-fsycl-targets=spir64")
+list(APPEND VLLM_LINK_LIBRARIES "sycl" "OpenCL" "pthread" "m" "dl" "dnnl" )
+
+#
+# Define extension targets
+#
+
+#
+# _C extension
+#
+set(VLLM_EXT_SRC
+    "csrc/xpu/activation_xpu.cpp"
+    "csrc/xpu/attention_xpu.cpp"
+    "csrc/xpu/attention_xpu_fp8.cpp"
+    "csrc/xpu/cache_ops_xpu.cpp"
+    "csrc/xpu/cache_ops_xpu_fp8.cpp"
+    "csrc/xpu/gemm_kernels_xpu.cpp"
+    "csrc/xpu/layernorm_xpu.cpp"
+    "csrc/xpu/pos_encoding_xpu.cpp"
+    "csrc/xpu/utils.cpp"
+    "csrc/xpu/fused_moe.cpp"
+    "csrc/xpu/pybind.cpp")
+
+define_gpu_extension_target(
+    _C
+    DESTINATION vllm
+    LANGUAGE ${VLLM_GPU_LANG}
+    SOURCES ${VLLM_EXT_SRC}
+    COMPILE_FLAGS ${VLLM_GPU_FLAGS}
+    LINK_FLAGS ${VLLM_GPU_LINK_FLAGS}
+    ARCHITECTURES ${VLLM_GPU_ARCHES}
+    INCLUDE_DIRECTORIES ${VLLM_EXTRA_INCLUDE_DIRECTORIES}
+    LIBRARIES ${VLLM_LINK_LIBRARIES}
+    WITH_SOABI
+)
+
+add_custom_target(default_xpu)
+message(STATUS "Enabling C extension.")
+add_dependencies(default_xpu _C)
+
diff --git a/csrc/xpu/activation_xpu.cpp b/csrc/xpu/activation_xpu.cpp
new file mode 100644
index 000000000..6f98ddbb3
--- /dev/null
+++ b/csrc/xpu/activation_xpu.cpp
@@ -0,0 +1,278 @@
+// clang-format off
+#ifdef VLLM_DEV
+#undef __SYCL_DEVICE_ONLY__
+#endif
+#include <sycl/sycl.hpp>
+// clang-format on
+#include "xpu_types.h"
+
+#include <torch/extension.h>
+#include "utils.h"
+
+template <typename T>
+__inline__ T silu_xpu(const T& x) {
+  // x * sigmoid(x)
+  return (T)(((float)x) / (1.0f + sycl::exp((float)-x)));
+}
+
+template<typename T>
+__inline__ T gelu_xpu(const T& x) {
+  // Equivalent to PyTorch GELU with 'none' approximation.
+  // Refer to:
+  // https://github.com/pytorch/pytorch/blob/8ac9b20d4b090c213799e81acf48a55ea8d437d6/aten/src/ATen/native/cuda/ActivationGeluKernel.cu#L38
+  const float f = (float) x;
+  constexpr float ALPHA = M_SQRT1_2;
+  return (T) (f * 0.5f * (1.0f + sycl::erf(f * ALPHA)));
+}
+
+template<typename T>
+__inline__ T gelu_tanh_xpu(const T& x) {
+  const float f = (float) x;
+  constexpr float BETA = M_SQRT2 * M_2_SQRTPI * 0.5f;
+  constexpr float KAPPA = 0.044715;
+  float x_cube = f * f * f;
+  float inner = BETA * (f + KAPPA * x_cube);
+  return (T) (0.5f * f * (1.0f + ::tanhf(inner)));
+}
+
+template <typename scalar_t>
+void silu_and_mul_kernel(
+    scalar_t* __restrict__ out, // [..., d]
+    const scalar_t* __restrict__ input, // [..., 2, d]
+    const int d,
+    const sycl::nd_item<3>& item_ct1) {
+  const int64_t token_idx = item_ct1.get_group(2);
+  for (int64_t idx = item_ct1.get_local_id(2); idx < d;
+       idx += item_ct1.get_local_range(2)) {
+    const scalar_t x = input[token_idx * 2 * d + idx];
+    const scalar_t y = input[token_idx * 2 * d + d + idx];
+    out[token_idx * d + idx] = silu_xpu(x) * y;
+  }
+}
+
+template <typename scalar_t>
+void gelu_and_mul_kernel(
+    scalar_t* __restrict__ out, // [..., d]
+    const scalar_t* __restrict__ input, // [..., 2, d]
+    const int d,
+    const sycl::nd_item<3>& item_ct1) {
+  const int64_t token_idx = item_ct1.get_group(2);
+  for (int64_t idx = item_ct1.get_local_id(2); idx < d;
+       idx += item_ct1.get_local_range(2)) {
+    const scalar_t x = input[token_idx * 2 * d + idx];
+    const scalar_t y = input[token_idx * 2 * d + d + idx];
+    out[token_idx * d + idx] = gelu_xpu(x) * y;
+  }
+}
+
+template <typename scalar_t>
+void gelu_tanh_and_mul_kernel(
+    scalar_t* __restrict__ out, // [..., d]
+    const scalar_t* __restrict__ input, // [..., 2, d]
+    const int d,
+    const sycl::nd_item<3>& item_ct1) {
+  const int64_t token_idx = item_ct1.get_group(2);
+  for (int64_t idx = item_ct1.get_local_id(2); idx < d;
+       idx += item_ct1.get_local_range(2)) {
+    const scalar_t x = input[token_idx * 2 * d + idx];
+    const scalar_t y = input[token_idx * 2 * d + d + idx];
+    out[token_idx * d + idx] = gelu_tanh_xpu(x) * y;
+  }
+}
+
+
+template <typename scalar_t>
+void call_silu_and_mul_kernel(
+    int num_tokens,
+    int d,
+    const scalar_t* __restrict__ input,
+    scalar_t* __restrict__ output) {
+  using sycl_t = vllm::xpu::SyclTypeTrait<scalar_t>::Type;
+  sycl::range<3> grid(1, 1, num_tokens);
+  sycl::range<3> block(1, 1, std::min(d, 1024));
+  auto& queue = vllm::xpu::vllmGetQueue();
+  queue.submit([&](sycl::handler& cgh) {
+    cgh.parallel_for(
+        sycl::nd_range<3>(grid * block, block), [=](sycl::nd_item<3> item_ct1) {
+          silu_and_mul_kernel<sycl_t>(
+              (sycl_t*)output, (const sycl_t*)input, d, item_ct1);
+        });
+  });
+}
+
+template <typename scalar_t>
+void call_gelu_and_mul_kernel(
+    int num_tokens,
+    int d,
+    const scalar_t* __restrict__ input,
+    scalar_t* __restrict__ output) {
+  using sycl_t = vllm::xpu::SyclTypeTrait<scalar_t>::Type;
+  sycl::range<3> grid(1, 1, num_tokens);
+  sycl::range<3> block(1, 1, std::min(d, 1024));
+  auto& queue = vllm::xpu::vllmGetQueue();
+  queue.submit([&](sycl::handler& cgh) {
+    cgh.parallel_for(
+        sycl::nd_range<3>(grid * block, block), [=](sycl::nd_item<3> item_ct1) {
+          gelu_and_mul_kernel<sycl_t>(
+              (sycl_t*)output, (const sycl_t*)input, d, item_ct1);
+        });
+  });
+}
+
+template <typename scalar_t>
+void call_gelu_tanh_and_mul_kernel(
+    int num_tokens,
+    int d,
+    const scalar_t* __restrict__ input,
+    scalar_t* __restrict__ output) {
+  using sycl_t = vllm::xpu::SyclTypeTrait<scalar_t>::Type;
+  sycl::range<3> grid(1, 1, num_tokens);
+  sycl::range<3> block(1, 1, std::min(d, 1024));
+  auto& queue = vllm::xpu::vllmGetQueue();
+  queue.submit([&](sycl::handler& cgh) {
+    cgh.parallel_for(
+        sycl::nd_range<3>(grid * block, block), [=](sycl::nd_item<3> item_ct1) {
+          gelu_tanh_and_mul_kernel<sycl_t>(
+              (sycl_t*)output, (const sycl_t*)input, d, item_ct1);
+        });
+  });
+}
+
+void silu_and_mul(torch::Tensor& out, torch::Tensor& input) {
+  int num_tokens = input.numel() / input.size(-1);
+  int d = input.size(-1) / 2;
+
+  VLLM_XPU_DISPATCH_FLOATING_TYPES(
+      input.scalar_type(), "call_silu_and_mul_kernel", [&] {
+        call_silu_and_mul_kernel(
+            num_tokens,
+            d,
+            input.data_ptr<scalar_t>(),
+            out.data_ptr<scalar_t>());
+      });
+}
+
+// Element-wise activation kernel template.
+template <typename scalar_t, scalar_t (*ACT_FN)(const scalar_t&)>
+void activation_kernel(
+    scalar_t* __restrict__ out, // [..., d]
+    const scalar_t* __restrict__ input, // [..., d]
+    const int d,
+    const sycl::nd_item<3>& item_ct1) {
+  const int64_t token_idx = item_ct1.get_group(2);
+  for (int64_t idx = item_ct1.get_local_id(2); idx < d;
+       idx += item_ct1.get_local_range(2)) {
+    const scalar_t x = VLLM_LDG(&input[token_idx * d + idx]);
+    out[token_idx * d + idx] = ACT_FN(x);
+  }
+}
+
+template <typename T>
+__inline__ T gelu_new_kernel(const T& x) {
+  const float x3 = (float)(x * x * x);
+  const T t = (T)tanhf((T)(0.79788456f * (float)(x + (T)(0.044715f * x3))));
+  return ((T)0.5) * x * (((T)1.0) + t);
+}
+
+template <typename T>
+__inline__ T gelu_fast_kernel(const T& x) {
+  const float f = (float)x;
+  const T t =
+      (T)tanhf(((T)(f * 0.79788456f)) * (((T)1.0) + (T)(0.044715f * f) * x));
+  return ((T)0.5) * x * (((T)1.0) + t);
+}
+
+template <typename scalar_t>
+void call_gelu_new_activation_kernel(torch::Tensor& out, torch::Tensor& input) {
+  using sycl_t = vllm::xpu::SyclTypeTrait<scalar_t>::Type;
+  int d = input.size(-1);
+  int64_t num_tokens = input.numel() / d;
+  auto out_ptr = out.data_ptr<scalar_t>();
+  auto input_ptr = input.data_ptr<scalar_t>();
+  sycl::range<3> grid(1, 1, num_tokens);
+  sycl::range<3> block(1, 1, std::min(d, 1024));
+  auto& queue = vllm::xpu::vllmGetQueue();
+  queue.submit([&](sycl::handler& cgh) {
+    cgh.parallel_for(
+        sycl::nd_range<3>(grid * block, block), [=](sycl::nd_item<3> item_ct1) {
+          activation_kernel<sycl_t, gelu_new_kernel>(
+              (sycl_t* __restrict__)out_ptr,
+              (const sycl_t* __restrict__)input_ptr,
+              d,
+              item_ct1);
+        });
+  });
+}
+
+template <typename scalar_t>
+void call_gelu_fast_activation_kernel(
+    torch::Tensor& out,
+    torch::Tensor& input) {
+  using sycl_t = vllm::xpu::SyclTypeTrait<scalar_t>::Type;
+  int d = input.size(-1);
+  int64_t num_tokens = input.numel() / d;
+  auto out_ptr = out.data_ptr<scalar_t>();
+  auto input_ptr = input.data_ptr<scalar_t>();
+  sycl::range<3> grid(1, 1, num_tokens);
+  sycl::range<3> block(1, 1, std::min(d, 1024));
+  auto& queue = vllm::xpu::vllmGetQueue();
+  queue.submit([&](sycl::handler& cgh) {
+    cgh.parallel_for(
+        sycl::nd_range<3>(grid * block, block), [=](sycl::nd_item<3> item_ct1) {
+          activation_kernel<sycl_t, gelu_fast_kernel>(
+              (sycl_t* __restrict__)out_ptr,
+              (const sycl_t* __restrict__)input_ptr,
+              d,
+              item_ct1);
+        });
+  });
+}
+
+void gelu_new(torch::Tensor& out, torch::Tensor& input) {
+  VLLM_XPU_DISPATCH_FLOATING_TYPES(
+      out.scalar_type(), "call_gelu_new_activation_kernel", [&] {
+        call_gelu_new_activation_kernel<scalar_t>(out, input);
+      });
+}
+
+void gelu_fast(torch::Tensor& out, torch::Tensor& input) {
+  VLLM_XPU_DISPATCH_FLOATING_TYPES(
+      out.scalar_type(), "call_gelu_fast_activation_kernel", [&] {
+        call_gelu_fast_activation_kernel<scalar_t>(
+            out, input);
+      });
+}
+
+void gelu_and_mul(
+  torch::Tensor& out,      // [..., d]
+  torch::Tensor& input)    // [..., 2 * d]
+{
+    int num_tokens = input.numel() / input.size(-1);
+  int d = input.size(-1) / 2;
+
+  VLLM_XPU_DISPATCH_FLOATING_TYPES(
+      input.scalar_type(), "call_gelu_and_mul_kernel", [&] {
+        call_gelu_and_mul_kernel(
+            num_tokens,
+            d,
+            input.data_ptr<scalar_t>(),
+            out.data_ptr<scalar_t>());
+      });
+}
+
+void gelu_tanh_and_mul(
+  torch::Tensor& out,      // [..., d]
+  torch::Tensor& input)    // [..., 2 * d]
+{
+    int num_tokens = input.numel() / input.size(-1);
+  int d = input.size(-1) / 2;
+
+  VLLM_XPU_DISPATCH_FLOATING_TYPES(
+      input.scalar_type(), "call_gelu_tanh_and_mul_kernel", [&] {
+        call_gelu_tanh_and_mul_kernel(
+            num_tokens,
+            d,
+            input.data_ptr<scalar_t>(),
+            out.data_ptr<scalar_t>());
+      });
+}
\ No newline at end of file
diff --git a/csrc/xpu/attention_generic.h b/csrc/xpu/attention_generic.h
new file mode 100644
index 000000000..ab3688c82
--- /dev/null
+++ b/csrc/xpu/attention_generic.h
@@ -0,0 +1,64 @@
+/*
+ * Copyright (c) 2023, The vLLM team.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+#pragma once
+
+#include <dpct/dpct.hpp>
+#include <stdint.h>
+#include <sycl/sycl.hpp>
+
+namespace vllm {
+
+// A vector type to store Q, K, V elements.
+template <typename T, int VEC_SIZE>
+struct Vec {};
+
+// A vector type to store FP32 accumulators.
+template <typename T>
+struct FloatVec {};
+
+// Template vector operations.
+template <typename Acc, typename A, typename B>
+inline Acc mul(A a, B b);
+
+template <typename T>
+inline float sum(T v);
+
+template <typename T>
+inline float dot(T a, T b) {
+  return sum(mul<T, T, T>(a, b));
+}
+
+template <typename A, typename T>
+inline float dot(T a, T b) {
+  return sum(mul<A, T, T>(a, b));
+}
+
+template <typename T>
+inline void zero(T& dst) {
+  constexpr int WORDS = (sizeof(T) / 4) == 0 ? 1 : (sizeof(T) / 4);
+  union {
+    T raw;
+    uint32_t words[WORDS];
+  } tmp;
+
+#pragma unroll
+  for (int ii = 0; ii < WORDS; ++ii) {
+    tmp.words[ii] = 0u;
+  }
+  dst = tmp.raw;
+}
+
+} // namespace vllm
\ No newline at end of file
diff --git a/csrc/xpu/attention_xpu.cpp b/csrc/xpu/attention_xpu.cpp
new file mode 100644
index 000000000..97d5c0c21
--- /dev/null
+++ b/csrc/xpu/attention_xpu.cpp
@@ -0,0 +1,3031 @@
+// clang-format off
+#ifdef VLLM_DEV
+#undef __SYCL_DEVICE_ONLY__
+#endif
+#include <sycl/sycl.hpp>
+#include <dpct/dpct.hpp>
+#include <ext/intel/esimd.hpp>
+
+// clang-format on
+#include <float.h>
+#include <torch/extension.h>
+#include <stdexcept>
+#include "utils.h"
+#include "xpu_types.h"
+// #include "dtype_bfloat16.dp.hpp"
+#include "dtype_float16.h"
+#include "dtype_float32.h"
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+#include <c10/xpu/XPUStream.h>
+#endif
+
+#include <functional>
+// #include <ipex.h>
+
+#define WARP_SIZE 32
+#define MAX(a, b) ((a) > (b) ? (a) : (b))
+#define MIN(a, b) ((a) < (b) ? (a) : (b))
+#define DIVIDE_ROUND_UP(a, b) (((a) + (b)-1) / (b))
+using namespace sycl::ext::intel::esimd;
+
+template<typename T>
+static inline T attn_softcapping(T qk, float attn_logit_softcapping) {
+    qk = qk / attn_logit_softcapping;
+    qk = (sycl::exp(qk) - sycl::exp(-qk)) / (sycl::exp(qk) + sycl::exp(-qk));
+    qk = qk * attn_logit_softcapping;
+    return qk;
+}
+
+template <typename T>
+struct Float_Trait {
+  using Type = T;
+};
+
+template <>
+struct Float_Trait<c10::Half> {
+  using Type = uint16_t;
+};
+
+template <>
+struct Float_Trait<c10::BFloat16> {
+  using Type = sycl::ext::oneapi::bfloat16;
+};
+
+namespace vllm {
+
+// Q*K^T operation.
+template <int THREAD_GROUP_SIZE, typename Vec, int N>
+inline float qk_dot_(
+    const Vec* q,
+    const Vec* k,
+    const sycl::nd_item<3>& item_ct1) {
+  using A_vec = typename FloatVec<Vec>::Type;
+  // Compute the parallel products for Q*K^T (treat vector lanes separately).
+  A_vec qk_vec = mul<A_vec, Vec, Vec>(q[0], k[0]);
+#pragma unroll
+  for (int ii = 1; ii < N; ++ii) {
+    qk_vec = fma(q[ii], k[ii], qk_vec);
+  }
+
+  // Finalize the reduction across lanes.
+  float qk = sum(qk_vec);
+#pragma unroll
+  for (int mask = THREAD_GROUP_SIZE / 2; mask >= 1; mask /= 2) {
+    
+    qk += dpct::permute_sub_group_by_xor(
+        item_ct1.get_sub_group(), qk, mask);
+  }
+  return qk;
+}
+
+template <typename T, int THREAD_GROUP_SIZE>
+struct Qk_dot {
+  template <typename Vec, int N>
+  static inline float dot(
+      const Vec* q,
+      const Vec* k,
+      const sycl::nd_item<3>& item_ct1) {
+    return qk_dot_<THREAD_GROUP_SIZE, Vec, N>(q, k, item_ct1);
+  }
+};
+
+template <int NUM_WARPS>
+inline float block_sum(
+    float* red_smem,
+    float sum,
+    const sycl::nd_item<3>& item_ct1) {
+  // Decompose the thread index into warp / lane.
+  int warp = item_ct1.get_local_id(2) / WARP_SIZE;
+  int lane = item_ct1.get_local_id(2) % WARP_SIZE;
+
+  // Compute the sum per warp.
+#pragma unroll
+  for (int mask = WARP_SIZE / 2; mask >= 1; mask /= 2) {
+    
+    /*
+    DPCT1096:42: The right-most dimension of the work-group used in the SYCL
+    kernel that calls this function may be less than "32". The function
+    "dpct::permute_sub_group_by_xor" may return an unexpected result on the CPU
+    device. Modify the size of the work-group to ensure that the value of the
+    right-most dimension is a multiple of "32".
+    */
+    sum += dpct::permute_sub_group_by_xor(
+        item_ct1.get_sub_group(), sum, mask);
+  }
+
+  // Warp leaders store the data to shared memory.
+  if (lane == 0) {
+    red_smem[warp] = sum;
+  }
+
+  // Make sure the data is in shared memory.
+  
+  item_ct1.barrier(sycl::access::fence_space::local_space);
+
+  // The warps compute the final sums.
+  if (lane < NUM_WARPS) {
+    sum = red_smem[lane];
+  }
+
+  // Parallel reduction inside the warp.
+#pragma unroll
+  for (int mask = NUM_WARPS / 2; mask >= 1; mask /= 2) {
+    
+    /*
+    DPCT1096:43: The right-most dimension of the work-group used in the SYCL
+    kernel that calls this function may be less than "32". The function
+    "dpct::permute_sub_group_by_xor" may return an unexpected result on the CPU
+    device. Modify the size of the work-group to ensure that the value of the
+    right-most dimension is a multiple of "32".
+    */
+    sum += dpct::permute_sub_group_by_xor(
+        item_ct1.get_sub_group(), sum, mask);
+  }
+
+  // Broadcast to other threads.
+  
+  /*
+  DPCT1096:44: The right-most dimension of the work-group used in the SYCL
+  kernel that calls this function may be less than "32". The function
+  "dpct::select_from_sub_group" may return an unexpected result on the CPU
+  device. Modify the size of the work-group to ensure that the value of the
+  right-most dimension is a multiple of "32".
+  */
+  return dpct::select_from_sub_group(
+        item_ct1.get_sub_group(), sum, 0);
+}
+
+template <typename scalar_t, int GS, int HD>
+void context_attention_kernel_v1_reshaped(
+    void* query, void* key, void* value, const void* block_tables,
+    const float scale, const void* query_start_loc, const void* seq_lens,
+    const void* context_lens, const int block_size,
+    // const int x,  // x in kv_cache
+    void* out,    // output
+    const int block_table_stride_batch, const int block_table_stride_seq,
+    const int query_stride_bs, const int query_stride_head,
+    const int query_stride_dim, const int k_cache_stride_tokens,
+    const int k_cache_stride_head, const int k_cache_stride_block_size,
+    const int k_cache_stride_dim,
+    const int v_cache_stride_tokens, const int v_cache_stride_head,
+    const int v_cache_stride_block_size, const int v_cache_stride_dim,
+    const int out_stride_tokens, const int out_stride_head,
+    const int num_queries_per_kv, const int max_input_length,
+    const int batch_size, const int num_heads) {
+  static_assert(GS * HD * sizeof(scalar_t) * 2 < 64 * 1024);
+
+  const size_t key_slm_offset = 0;
+  const size_t value_slm_offset = GS * HD * sizeof(scalar_t);
+  sycl::queue& queue = vllm::xpu::vllmGetQueue();
+
+  // Get the maximum seq_lens
+  sycl::range<3> global_size(batch_size, num_heads,
+                             (max_input_length + GS - 1) / GS * GS);
+  sycl::range<3> local_size(1, 1, GS);
+
+  auto cgf = [&](sycl::handler& handle) {
+    handle.parallel_for(
+        sycl::nd_range<3>(global_size, local_size),
+        [=](sycl::nd_item<3> item) SYCL_ESIMD_KERNEL {
+          slm_init<GS * HD * sizeof(scalar_t) * 2>();
+
+          const size_t bsz_idx = item.get_global_id(0);
+          const size_t head_idx = item.get_global_id(1);
+          // Assuming we have 32 query head and 8 kv_heads. Then
+          // num_queries_per_group should be 4 For head_idx 13, then
+          // kv_head_idx = 13 / 4 = 3, which is correct
+          const size_t kv_head_idx = head_idx / num_queries_per_kv;
+          const int32_t seq_idx = item.get_global_id(2);
+          const size_t gid = item.get_group(2);
+          const size_t tid = item.get_local_id(2);
+
+          // const int64_t * seq_len = (const int64_t *) seq_lens;
+          const int32_t* seq_len = (const int32_t*)seq_lens;
+          int32_t seq_bound = seq_len[bsz_idx];
+
+          const int32_t* query_loc = (const int32_t*)query_start_loc;
+          // There is a possibility that the current token index pass
+          // over the seq_len, therefore: token_idx is the position in
+          // the query
+          int32_t token_idx =
+              query_loc[bsz_idx] + std::min(seq_idx, seq_bound - 1);
+
+          const int32_t* context_len_pointer = (const int32_t*)context_lens;
+
+          const int* block_tables_ptr = (const int*)block_tables;
+          const int* block_table =
+              block_tables_ptr + bsz_idx * block_table_stride_batch;
+          // I guess this context_len should be 0...
+          const int32_t context_len = context_len_pointer[bsz_idx];
+
+          // Position in the sequence
+          // context + seq_idx
+          // const int32_t token_position =
+          //     context_len + std::min(seq_idx, seq_bound - 1);
+          const int32_t token_position = context_len + seq_idx;
+
+          const scalar_t* query_head = (const scalar_t*)query +
+                                       token_idx * query_stride_bs +
+                                       head_idx * query_stride_head;
+          // Target output
+          scalar_t* out_head =
+              (scalar_t*)out +
+              (query_loc[bsz_idx] + seq_idx) * out_stride_tokens +
+              head_idx * out_stride_head;
+
+          int32_t context_groups = context_len / GS;
+
+          // Each token load its query_row
+          simd<scalar_t, HD> query_row =
+              block_load<scalar_t, HD>(query_head) * scale;
+          simd<scalar_t, HD> accv = 0;
+          simd<scalar_t, GS> softmaxv = 0;
+          scalar_t max_attn = -sycl::detail::max_v<scalar_t>();
+
+          // ################# Handle n * GS context part ######################
+          int32_t n = context_len / GS;
+          int32_t context_offset = context_len % GS;
+
+          for (int32_t group = 0; group < n; ++group) {
+            size_t target_key_position = group * GS + tid;
+            int which_block = target_key_position / block_size;
+            int which_slot = target_key_position % block_size;
+
+            int physical_block_number = block_table[which_block];
+            // Now key shape is [num_blocks, num_heads, block_size, head_dim]
+            const scalar_t* key_head =
+                (const scalar_t*)key +
+                physical_block_number * k_cache_stride_tokens +
+                kv_head_idx * k_cache_stride_head +
+                which_slot * k_cache_stride_block_size;
+            simd<scalar_t, HD> key_row = block_load<scalar_t, HD>(key_head);
+            slm_block_store(key_slm_offset + tid * HD * sizeof(scalar_t), key_row);
+
+            const scalar_t* value_head =
+                (const scalar_t*)value +
+                physical_block_number * v_cache_stride_tokens +
+                kv_head_idx * v_cache_stride_head + which_slot * v_cache_stride_block_size;
+            simd<scalar_t, HD> value_row = block_load<scalar_t, HD>(value_head);
+            slm_block_store(value_slm_offset + tid * HD * sizeof(scalar_t),
+                            value_row);
+            barrier();
+
+            // Calculate QK^T for this group...
+            simd<scalar_t, GS> attnv;
+#pragma unroll
+            for (size_t r = 0; r < GS; ++r) {
+              simd<scalar_t, HD> key_row = slm_block_load<scalar_t, HD>(
+                  key_slm_offset + r * HD * sizeof(scalar_t));
+              scalar_t attn =
+                  sycl::ext::intel::esimd::detail::sum<scalar_t, scalar_t, HD>(
+                      query_row * key_row);
+              attnv[r] = attn;
+            }
+            scalar_t new_max_attn =
+                std::max(hmax<scalar_t, scalar_t, GS>(attnv), max_attn);
+            scalar_t attn_exp = exp(max_attn - new_max_attn);
+            accv = accv * attn_exp;
+            softmaxv = softmaxv * attn_exp;
+            max_attn = new_max_attn;
+            const simd<scalar_t, GS> attn_expv = exp(attnv - max_attn);
+#pragma unorll
+            for (size_t r = 0; r < GS; ++r) {
+              simd<scalar_t, HD> value_row = slm_block_load<scalar_t, HD>(
+                  value_slm_offset + r * HD * sizeof(scalar_t));
+              accv += value_row * attn_expv[r];
+            }
+            softmaxv += attn_expv;
+            barrier();
+          }
+
+          // ########## End for handling context n * GS part ###########
+
+          // ########## Handle n * GS ################
+          for (size_t group = 0; group < gid; ++group) {
+            // 1. begins to load each position's key and value
+            size_t target_key_position = context_len + group * GS + tid;
+            int which_block = target_key_position / block_size;
+            int which_slot = target_key_position % block_size;
+
+            int physical_block_number = block_table[which_block];
+            const scalar_t* key_head =
+                (const scalar_t*)key +
+                physical_block_number * k_cache_stride_tokens +
+                kv_head_idx * k_cache_stride_head +
+                which_slot * k_cache_stride_block_size;
+            simd<scalar_t, HD> key_row = block_load<scalar_t, HD>(key_head);
+            slm_block_store(key_slm_offset + tid * HD * sizeof(scalar_t),
+                            key_row);
+            const scalar_t* value_head =
+                (const scalar_t*)value +
+                physical_block_number * v_cache_stride_tokens +
+                kv_head_idx * v_cache_stride_head + which_slot * v_cache_stride_block_size;
+            simd<scalar_t, HD> value_row = block_load<scalar_t, HD>(value_head);
+            slm_block_store(value_slm_offset + tid * HD * sizeof(scalar_t),
+                            value_row);
+            barrier();
+            simd<scalar_t, GS> attnv;
+#pragma unroll
+            for (size_t r = 0; r < GS; ++r) {
+              simd<scalar_t, HD> key_row = slm_block_load<scalar_t, HD>(
+                  key_slm_offset + r * HD * sizeof(scalar_t));
+              scalar_t attn =
+                  sycl::ext::intel::esimd::detail::sum<scalar_t, scalar_t, HD>(
+                      query_row * key_row);
+              attnv[r] = attn;
+            }
+
+            scalar_t new_max_attn =
+                std::max(hmax<scalar_t, scalar_t, GS>(attnv), max_attn);
+            scalar_t attn_exp = exp(max_attn - new_max_attn);
+            accv = accv * attn_exp;
+
+            softmaxv = softmaxv * attn_exp;
+            max_attn = new_max_attn;
+            const simd<scalar_t, GS> attn_expv = exp(attnv - max_attn);
+#pragma unroll
+            for (size_t r = 0; r < GS; ++r) {
+              simd<scalar_t, HD> value_row = slm_block_load<scalar_t, HD>(
+                  value_slm_offset + r * HD * sizeof(scalar_t));
+              accv += value_row * attn_expv[r];
+            }
+            softmaxv += attn_expv;
+            barrier();
+          }
+
+          // ######### End of handle n * GS part ##########
+
+          // ################ Handle offset part ####################
+          scalar_t softmax =
+              sycl::ext::intel::esimd::detail::sum<scalar_t, scalar_t, GS>(
+                  softmaxv);
+
+          // ########### handle context offset ############
+          if (tid < context_offset) {
+            size_t target_key_position = n * GS + tid;
+            int which_block = target_key_position / block_size;
+            int which_slot = target_key_position % block_size;
+
+            int physical_block_number = block_table[which_block];
+            const scalar_t* key_head =
+                (const scalar_t*)key +
+                physical_block_number * k_cache_stride_tokens +
+                kv_head_idx * k_cache_stride_head +
+                which_slot * k_cache_stride_block_size;
+            simd<scalar_t, HD> key_row = block_load<scalar_t, HD>(key_head);
+            slm_block_store(key_slm_offset + tid * HD * sizeof(scalar_t),
+                            key_row);
+
+            const scalar_t* value_head =
+                (const scalar_t*)value +
+                physical_block_number * v_cache_stride_tokens +
+                kv_head_idx * v_cache_stride_head +
+                which_slot * v_cache_stride_block_size;
+            simd<scalar_t, HD> value_row = block_load<scalar_t, HD>(value_head);
+            slm_block_store(value_slm_offset + tid * HD * sizeof(scalar_t),
+                            value_row);
+          }
+
+          barrier();
+
+          if (token_position < seq_bound) {
+#pragma unroll
+            for (size_t r = 0; r < context_offset; ++r) {
+              simd<scalar_t, HD> key_row = slm_block_load<scalar_t, HD>(
+                  key_slm_offset + r * HD * sizeof(scalar_t));
+              simd<scalar_t, HD> value_row = slm_block_load<scalar_t, HD>(
+                  value_slm_offset + r * HD * sizeof(scalar_t));
+              scalar_t attn =
+                  sycl::ext::intel::esimd::detail::sum<scalar_t, scalar_t, HD>(
+                      query_row * key_row);
+              if (attn <= max_attn) {
+                scalar_t attn_exp =
+                    sycl::ext::intel::esimd::exp(attn - max_attn);
+                accv += value_row * attn_exp;
+                softmax += attn_exp;
+              } else {
+                scalar_t attn_exp =
+                    sycl::ext::intel::esimd::exp(max_attn - attn);
+                accv = accv * attn_exp + value_row;
+                softmax = softmax * attn_exp + 1;
+                max_attn = attn;
+              }
+            }
+          }
+          barrier();
+
+          // ############## handle seq offset #################
+          if (token_position < seq_bound) {
+            const int64_t which_block =
+                static_cast<int64_t>(token_position / block_size);
+            const int64_t which_slot =
+                static_cast<int64_t>(token_position % block_size);
+
+            const int64_t physical_block_number =
+                static_cast<int64_t>(block_table[which_block]);
+
+            const scalar_t* key_head =
+                (const scalar_t*)key +
+                physical_block_number * k_cache_stride_tokens +
+                kv_head_idx * k_cache_stride_head +
+                which_slot * k_cache_stride_block_size;
+            simd<scalar_t, HD> key_row = block_load<scalar_t, HD>(key_head);
+            slm_block_store(key_slm_offset + tid * HD * sizeof(scalar_t),
+                            key_row);
+
+            // [num_blocks, num_kv_heads, head_size, block_size]
+            const scalar_t* value_head =
+                (const scalar_t*)value +
+                physical_block_number * v_cache_stride_tokens +
+                kv_head_idx * v_cache_stride_head +
+                which_slot * v_cache_stride_block_size;
+            simd<scalar_t, HD> value_row = block_load<scalar_t, HD>(value_head);
+            slm_block_store(value_slm_offset + tid * HD * sizeof(scalar_t),
+                            value_row);
+          }
+          barrier();
+
+          if (token_position < seq_bound) {
+            for (size_t r = 0; r <= tid; ++r) {
+              simd<scalar_t, HD> key_row = slm_block_load<scalar_t, HD>(
+                  key_slm_offset + r * HD * sizeof(scalar_t));
+              simd<scalar_t, HD> value_row = slm_block_load<scalar_t, HD>(
+                  value_slm_offset + r * HD * sizeof(scalar_t));
+              scalar_t attn =
+                  sycl::ext::intel::esimd::detail::sum<scalar_t, scalar_t, HD>(
+                      query_row * key_row);
+              if (attn <= max_attn) {
+                scalar_t attn_exp =
+                    sycl::ext::intel::esimd::exp(attn - max_attn);
+                accv += value_row * attn_exp;
+                softmax += attn_exp;
+              } else {
+                scalar_t attn_exp =
+                    sycl::ext::intel::esimd::exp(max_attn - attn);
+                accv = accv * attn_exp + value_row;
+                softmax = softmax * attn_exp + 1;
+                max_attn = attn;
+              }
+            }
+
+            if (softmax > 0) {
+              simd<scalar_t, HD> result = accv / softmax;
+              block_store(out_head, result);
+            } else {
+              simd<scalar_t, HD> result = 0;
+              block_store(out_head, result);
+            }
+          }
+          // ######## Ending of handling seq offset ##########
+        });
+  };
+  queue.submit(cgf);
+}
+
+// How about implement a first edition that can be used with non-chunked
+// prefill requests, so that we can make sure the reference for heads is
+// correct
+template <typename scalar_t, int GS, int HD>
+void context_attention_kernel_v1(
+    void* query, void* key, void* value, const void* block_tables,
+    const float scale, const void* query_start_loc, const void* seq_lens,
+    const void* context_lens, const int block_size,
+    const int x,  // x in kv_cache
+    void* out,    // output
+    const int block_table_stride_batch, const int block_table_stride_seq,
+    const int query_stride_bs, const int query_stride_head,
+    const int query_stride_dim, const int k_cache_stride_tokens,
+    const int k_cache_stride_head, const int k_cache_stride_dim,
+    const int k_cache_stride_block_size, const int k_cache_stride_x,
+    const int v_cache_stride_tokens, const int v_cache_stride_head,
+    const int v_cache_stride_dim, const int v_cache_stride_block_size,
+    const int out_stride_tokens, const int out_stride_head,
+    const int num_queries_per_kv, const int max_input_length,
+    const int batch_size, const int num_heads) {
+  static_assert(GS * HD * sizeof(scalar_t) * 2 < 64 * 1024);
+
+  const size_t key_slm_offset = 0;
+  const size_t value_slm_offset = GS * HD * sizeof(scalar_t);
+  sycl::queue& queue = vllm::xpu::vllmGetQueue();
+
+  // Get the maximum seq_lens
+  sycl::range<3> global_size(batch_size, num_heads,
+                             (max_input_length + GS - 1) / GS * GS);
+  sycl::range<3> local_size(1, 1, GS);
+
+  auto cgf = [&](sycl::handler& handle) {
+    handle.parallel_for(
+        sycl::nd_range<3>(global_size, local_size),
+        [=](sycl::nd_item<3> item) SYCL_ESIMD_KERNEL {
+          slm_init<GS * HD * sizeof(scalar_t) * 2>();
+
+          const size_t bsz_idx = item.get_global_id(0);
+          const size_t head_idx = item.get_global_id(1);
+          // Assuming we have 32 query head and 8 kv_heads. Then
+          // num_queries_per_group should be 4 For head_idx 13, then
+          // kv_head_idx = 13 / 4 = 3, which is correct
+          const size_t kv_head_idx = head_idx / num_queries_per_kv;
+          const int32_t seq_idx = item.get_global_id(2);
+          const size_t gid = item.get_group(2);
+          const size_t tid = item.get_local_id(2);
+
+          // const int64_t * seq_len = (const int64_t *) seq_lens;
+          const int32_t* seq_len = (const int32_t*)seq_lens;
+          int32_t seq_bound = seq_len[bsz_idx];
+
+          const int32_t* query_loc = (const int32_t*)query_start_loc;
+          // There is a possibility that the current token index pass
+          // over the seq_len, therefore: token_idx is the position in
+          // the query
+          int32_t token_idx =
+              query_loc[bsz_idx] + std::min(seq_idx, seq_bound - 1);
+
+          const int32_t* context_len_pointer = (const int32_t*)context_lens;
+
+          const int* block_tables_ptr = (const int*)block_tables;
+          const int* block_table =
+              block_tables_ptr + bsz_idx * block_table_stride_batch;
+          // I guess this context_len should be 0...
+          const int32_t context_len = context_len_pointer[bsz_idx];
+
+          // Position in the sequence
+          // context + seq_idx
+          // const int32_t token_position =
+          //     context_len + std::min(seq_idx, seq_bound - 1);
+          const int32_t token_position = context_len + seq_idx;
+
+          // static const CONSTANT char FMT[] =
+          //     "Invoke target function...\n ";
+
+          // sycl::ext::oneapi::experimental::printf(FMT);
+          // static const CONSTANT char FMT[] =
+          //     "GroupID = %6d bsz_idx = %6d seq_len = %6d seq_idx =
+          //     %6d" "local_id = "
+          //     "%6d "
+          //     "token_idx = %6d "
+          //     "context_len = %6d "
+          //     "v_cache_stride_head_dim = %6d "
+          //     "token_position = %6d\n";
+          // sycl::ext::oneapi::experimental::printf(
+          //     FMT, gid, bsz_idx, seq_bound, seq_idx, tid,
+          //     token_idx, context_len, v_cache_stride_dim,
+          //     token_position);
+
+          const scalar_t* query_head = (const scalar_t*)query +
+                                       token_idx * query_stride_bs +
+                                       head_idx * query_stride_head;
+          // Target output
+          scalar_t* out_head =
+              (scalar_t*)out +
+              (query_loc[bsz_idx] + seq_idx) * out_stride_tokens +
+              head_idx * out_stride_head;
+
+          int32_t context_groups = context_len / GS;
+
+          // Each token load its query_row
+          simd<scalar_t, HD> query_row =
+              block_load<scalar_t, HD>(query_head) * scale;
+          simd<scalar_t, HD> accv = 0;
+          simd<scalar_t, GS> softmaxv = 0;
+          scalar_t max_attn = -sycl::detail::max_v<scalar_t>();
+
+          // ################# Handle n * GS context part ######################
+          int32_t n = context_len / GS;
+          int32_t context_offset = context_len % GS;
+
+          for (int32_t group = 0; group < n; ++group) {
+            size_t target_key_position = group * GS + tid;
+            int which_block = target_key_position / block_size;
+            int which_slot = target_key_position % block_size;
+
+            int physical_block_number = block_table[which_block];
+            const scalar_t* key_head =
+                (const scalar_t*)key +
+                physical_block_number * k_cache_stride_tokens +
+                kv_head_idx * k_cache_stride_head +
+                which_slot * k_cache_stride_block_size;
+            for (int i = 0; i < HD / x; i++) {
+              // Load 8 elements, decided by x
+              simd<scalar_t, 8> key_row =
+                  block_load<scalar_t, 8>(key_head + i * k_cache_stride_dim);
+              slm_block_store(key_slm_offset + tid * HD * sizeof(scalar_t) +
+                                  8 * i * sizeof(scalar_t),
+                              key_row);
+            }
+
+            const scalar_t* value_head =
+                (const scalar_t*)value +
+                physical_block_number * v_cache_stride_tokens +
+                kv_head_idx * v_cache_stride_head + which_slot;
+            for (int i = 0; i < HD; i++) {
+              scalar_t temp_value = value_head[i * v_cache_stride_dim];
+              slm_scalar_store<scalar_t>(value_slm_offset +
+                                             tid * HD * sizeof(scalar_t) +
+                                             i * sizeof(scalar_t),
+                                         temp_value);
+            }
+            barrier();
+
+            // Calculate QK^T for this group...
+            simd<scalar_t, GS> attnv;
+#pragma unroll
+            for (size_t r = 0; r < GS; ++r) {
+              simd<scalar_t, HD> key_row = slm_block_load<scalar_t, HD>(
+                  key_slm_offset + r * HD * sizeof(scalar_t));
+              scalar_t attn =
+                  sycl::ext::intel::esimd::detail::sum<scalar_t, scalar_t, HD>(
+                      query_row * key_row);
+              attnv[r] = attn;
+            }
+            scalar_t new_max_attn =
+                std::max(hmax<scalar_t, scalar_t, GS>(attnv), max_attn);
+            scalar_t attn_exp = exp(max_attn - new_max_attn);
+            accv = accv * attn_exp;
+            softmaxv = softmaxv * attn_exp;
+            max_attn = new_max_attn;
+            const simd<scalar_t, GS> attn_expv = exp(attnv - max_attn);
+#pragma unorll
+            for (size_t r = 0; r < GS; ++r) {
+              simd<scalar_t, HD> value_row = slm_block_load<scalar_t, HD>(
+                  value_slm_offset + r * HD * sizeof(scalar_t));
+              accv += value_row * attn_expv[r];
+            }
+            softmaxv += attn_expv;
+            barrier();
+          }
+
+          // ########## End for handling context n * GS part ###########
+
+          // ########## Handle n * GS ################
+          for (size_t group = 0; group < gid; ++group) {
+            // 1. begins to load each position's key and value
+            size_t target_key_position = context_len + group * GS + tid;
+            int which_block = target_key_position / block_size;
+            int which_slot = target_key_position % block_size;
+
+            int physical_block_number = block_table[which_block];
+            const scalar_t* key_head =
+                (const scalar_t*)key +
+                physical_block_number * k_cache_stride_tokens +
+                kv_head_idx * k_cache_stride_head +
+                which_slot * k_cache_stride_block_size;
+            for (int i = 0; i < HD / x; i++) {
+              // Load 8 elements
+              simd<scalar_t, 8> key_row =
+                  block_load<scalar_t, 8>(key_head + i * k_cache_stride_dim);
+              slm_block_store(key_slm_offset + tid * HD * sizeof(scalar_t) +
+                                  8 * i * sizeof(scalar_t),
+                              key_row);
+            }
+
+            const scalar_t* value_head =
+                (const scalar_t*)value +
+                physical_block_number * v_cache_stride_tokens +
+                kv_head_idx * v_cache_stride_head + which_slot;
+            for (int i = 0; i < HD; i++) {
+              scalar_t temp_value = value_head[i * v_cache_stride_dim];
+              slm_scalar_store<scalar_t>(value_slm_offset +
+                                             tid * HD * sizeof(scalar_t) +
+                                             i * sizeof(scalar_t),
+                                         temp_value);
+            }
+            barrier();
+            simd<scalar_t, GS> attnv;
+#pragma unroll
+            for (size_t r = 0; r < GS; ++r) {
+              simd<scalar_t, HD> key_row = slm_block_load<scalar_t, HD>(
+                  key_slm_offset + r * HD * sizeof(scalar_t));
+              scalar_t attn =
+                  sycl::ext::intel::esimd::detail::sum<scalar_t, scalar_t, HD>(
+                      query_row * key_row);
+              attnv[r] = attn;
+            }
+
+            scalar_t new_max_attn =
+                std::max(hmax<scalar_t, scalar_t, GS>(attnv), max_attn);
+            scalar_t attn_exp = exp(max_attn - new_max_attn);
+            accv = accv * attn_exp;
+
+            softmaxv = softmaxv * attn_exp;
+            max_attn = new_max_attn;
+            const simd<scalar_t, GS> attn_expv = exp(attnv - max_attn);
+#pragma unroll
+            for (size_t r = 0; r < GS; ++r) {
+              simd<scalar_t, HD> value_row = slm_block_load<scalar_t, HD>(
+                  value_slm_offset + r * HD * sizeof(scalar_t));
+              accv += value_row * attn_expv[r];
+            }
+            softmaxv += attn_expv;
+            barrier();
+          }
+
+          // ######### End of handle n * GS part ##########
+
+          // ################ Handle offset part ####################
+          scalar_t softmax =
+              sycl::ext::intel::esimd::detail::sum<scalar_t, scalar_t, GS>(
+                  softmaxv);
+
+          // ########### handle context offset ############
+          if (tid < context_offset) {
+            size_t target_key_position = n * GS + tid;
+            int which_block = target_key_position / block_size;
+            int which_slot = target_key_position % block_size;
+
+            int physical_block_number = block_table[which_block];
+            const scalar_t* key_head =
+                (const scalar_t*)key +
+                physical_block_number * k_cache_stride_tokens +
+                kv_head_idx * k_cache_stride_head +
+                which_slot * k_cache_stride_block_size;
+            for (int i = 0; i < HD / x; i++) {
+              // Load 8 elements
+              simd<scalar_t, 8> key_row =
+                  block_load<scalar_t, 8>(key_head + i * k_cache_stride_dim);
+              slm_block_store(key_slm_offset + tid * HD * sizeof(scalar_t) +
+                                  8 * i * sizeof(scalar_t),
+                              key_row);
+            }
+
+            const scalar_t* value_head =
+                (const scalar_t*)value +
+                physical_block_number * v_cache_stride_tokens +
+                kv_head_idx * v_cache_stride_head + which_slot;
+            for (int i = 0; i < HD; i++) {
+              // Seems to have an error here
+              scalar_t temp_value = value_head[i * v_cache_stride_dim];
+              slm_scalar_store<scalar_t>(value_slm_offset +
+                                             tid * HD * sizeof(scalar_t) +
+                                             i * sizeof(scalar_t),
+                                         temp_value);
+            }
+          }
+
+          barrier();
+
+          if (token_position < seq_bound) {
+#pragma unroll
+            for (size_t r = 0; r < context_offset; ++r) {
+              simd<scalar_t, HD> key_row = slm_block_load<scalar_t, HD>(
+                  key_slm_offset + r * HD * sizeof(scalar_t));
+              simd<scalar_t, HD> value_row = slm_block_load<scalar_t, HD>(
+                  value_slm_offset + r * HD * sizeof(scalar_t));
+              scalar_t attn =
+                  sycl::ext::intel::esimd::detail::sum<scalar_t, scalar_t, HD>(
+                      query_row * key_row);
+              if (attn <= max_attn) {
+                scalar_t attn_exp =
+                    sycl::ext::intel::esimd::exp(attn - max_attn);
+                accv += value_row * attn_exp;
+                softmax += attn_exp;
+              } else {
+                scalar_t attn_exp =
+                    sycl::ext::intel::esimd::exp(max_attn - attn);
+                accv = accv * attn_exp + value_row;
+                softmax = softmax * attn_exp + 1;
+                max_attn = attn;
+              }
+            }
+          }
+          barrier();
+
+          // ############## handle seq offset #################
+          if (token_position < seq_bound) {
+            const int64_t which_block =
+                static_cast<int64_t>(token_position / block_size);
+            const int64_t which_slot =
+                static_cast<int64_t>(token_position % block_size);
+
+            const int64_t physical_block_number =
+                static_cast<int64_t>(block_table[which_block]);
+
+            const scalar_t* key_head =
+                (const scalar_t*)key +
+                physical_block_number * k_cache_stride_tokens +
+                kv_head_idx * k_cache_stride_head +
+                which_slot * k_cache_stride_block_size;
+
+            for (int i = 0; i < HD / x; i++) {
+              // Load 8 elements
+              simd<scalar_t, 8> key_row =
+                  block_load<scalar_t, 8>(key_head + i * k_cache_stride_dim);
+              slm_block_store(key_slm_offset + tid * HD * sizeof(scalar_t) +
+                                  8 * i * sizeof(scalar_t),
+                              key_row);
+            }
+
+            // [num_blocks, num_kv_heads, head_size, block_size]
+            const scalar_t* value_head =
+                (const scalar_t*)value +
+                physical_block_number * v_cache_stride_tokens +
+                kv_head_idx * v_cache_stride_head + which_slot;
+            for (int i = 0; i < HD; i++) {
+              scalar_t temp_value = value_head[i * v_cache_stride_dim];
+              slm_scalar_store<scalar_t>(value_slm_offset +
+                                             tid * HD * sizeof(scalar_t) +
+                                             i * sizeof(scalar_t),
+                                         temp_value);
+            }
+          }
+          barrier();
+
+          if (token_position < seq_bound) {
+            for (size_t r = 0; r <= tid; ++r) {
+              simd<scalar_t, HD> key_row = slm_block_load<scalar_t, HD>(
+                  key_slm_offset + r * HD * sizeof(scalar_t));
+              simd<scalar_t, HD> value_row = slm_block_load<scalar_t, HD>(
+                  value_slm_offset + r * HD * sizeof(scalar_t));
+              scalar_t attn =
+                  sycl::ext::intel::esimd::detail::sum<scalar_t, scalar_t, HD>(
+                      query_row * key_row);
+              if (attn <= max_attn) {
+                scalar_t attn_exp =
+                    sycl::ext::intel::esimd::exp(attn - max_attn);
+                accv += value_row * attn_exp;
+                softmax += attn_exp;
+              } else {
+                scalar_t attn_exp =
+                    sycl::ext::intel::esimd::exp(max_attn - attn);
+                accv = accv * attn_exp + value_row;
+                softmax = softmax * attn_exp + 1;
+                max_attn = attn;
+              }
+            }
+
+            if (softmax > 0) {
+              simd<scalar_t, HD> result = accv / softmax;
+              block_store(out_head, result);
+            } else {
+              simd<scalar_t, HD> result = 0;
+              block_store(out_head, result);
+            }
+          }
+          // ######## Ending of handling seq offset ##########
+        });
+  };
+  queue.submit(cgf);
+}
+
+template <typename T, int GS, int HD>
+void context_attention_kernel_v2(
+    void* query, void* key, void* value, const void* block_tables,
+    const float scale, const void* query_start_loc, const void* seq_lens,
+    const void* context_lens, const int block_size,
+    const int x,  // x in kv_cache
+    void* out,    // output
+    const int block_table_stride_batch, const int block_table_stride_seq,
+    const int query_stride_bs, const int query_stride_head,
+    const int query_stride_dim, const int k_cache_stride_tokens,
+    const int k_cache_stride_head, const int k_cache_stride_dim,
+    const int k_cache_stride_block_size, const int k_cache_stride_x,
+    const int v_cache_stride_tokens, const int v_cache_stride_head,
+    const int v_cache_stride_dim, const int v_cache_stride_block_size,
+    const int out_stride_tokens, const int out_stride_head,
+    const int num_queries_per_kv, const int max_input_length,
+    const int batch_size, const int num_heads, const int num_tokens,
+    const int max_context_len, const int max_q_len) {
+  constexpr int BLOCK_SIZE = 8;
+  constexpr int NUM_THREADS = 128;
+  // Each wrap handles one context block, therefore, each thread_group_size is
+  // this.
+  constexpr int THREAD_GROUP_SIZE = MAX(WARP_SIZE / BLOCK_SIZE, 1);
+  // Each query, and key thread_group loads 16 bytes
+  // Assume TGS=4 then 16 / 4 / sizeof(half) = 2
+  constexpr int VEC_SIZE = MAX(16 / (THREAD_GROUP_SIZE * sizeof(T)), 1);
+  using sycl_t = vllm::xpu::SyclTypeTrait<T>::Type;
+  using Q_Vec = typename Vec<sycl_t, VEC_SIZE>::Type;
+
+  // Assuming HD = 128, TGS = 2, then 128 / 2 / 2 = 32
+  int num_vecs_per_thread = HD / THREAD_GROUP_SIZE / VEC_SIZE;
+  sycl_t* out_p = reinterpret_cast<sycl_t*>(out);
+  sycl_t* query_ptr = reinterpret_cast<sycl_t*>(query);
+  sycl_t* key_cache_ptr = reinterpret_cast<sycl_t*>(key);
+  sycl_t* value_cache_ptr = reinterpret_cast<sycl_t*>(value);
+  const int* query_loc_ptr = reinterpret_cast<const int*>(query_start_loc);
+  const int* block_tables_ptr = reinterpret_cast<const int*>(block_tables);
+  const int* context_lens_ptr = reinterpret_cast<const int*>(context_lens);
+  const int* seq_lens_ptr = reinterpret_cast<const int*>(seq_lens);
+
+  constexpr int NUM_WARPS = NUM_THREADS / WARP_SIZE;
+  int padded_max_context_len =
+      DIVIDE_ROUND_UP(max_context_len + 1 + max_q_len, BLOCK_SIZE) * BLOCK_SIZE;
+  int logits_size = padded_max_context_len * sizeof(float);
+  int outputs_size = (NUM_WARPS / 2) * HD * sizeof(float);
+  // Python-side check in
+  // vllm.worker.worker._check_if_can_support_max_seq_len Keep that in
+  // sync with the logic here!
+  int shared_mem_size = std::max(logits_size, outputs_size);
+  // WARN: we have changed this...
+  sycl::range<3> grid(batch_size, num_heads, max_q_len);
+  // One work-group that is executing on the device
+  sycl::range<3> block(1, 1, NUM_THREADS);
+  sycl::queue& queue = vllm::xpu::vllmGetQueue();
+
+  auto cgf = [&](sycl::handler& handle) {
+    sycl::local_accessor<uint8_t, 1> dpct_local_acc_ct1(
+        sycl::range<1>(shared_mem_size), handle);
+    sycl::local_accessor<Q_Vec, 1> q_vecs_acc_ct1(
+        sycl::range<1>(THREAD_GROUP_SIZE * num_vecs_per_thread), handle);
+    sycl::local_accessor<float, 1> red_smem_acc_ct1(
+        sycl::range<1>(2 * NUM_WARPS), handle);
+
+    handle.parallel_for(
+        sycl::nd_range<3>(grid * block, block),
+        [=](sycl::nd_item<3> item_ct1) [[intel::reqd_sub_group_size(32)]] {
+          const int bsz_idx = item_ct1.get_group(0);
+          const int seq_idx = item_ct1.get_group(2);
+          constexpr bool USE_PARTITIONING = false;
+          int context_len = context_lens_ptr[bsz_idx] + seq_idx;
+          const int seq_len = seq_lens_ptr[bsz_idx];
+          uint8_t* dpct_local = dpct_local_acc_ct1.get_pointer();
+          Q_Vec* q_vecs = q_vecs_acc_ct1.get_pointer();
+          float* red_smem = red_smem_acc_ct1.get_pointer();
+
+          // output_stream << "Original context_len: " <<
+          // context_lens_ptr[bsz_idx] << sycl::endl; output_stream <<
+          // "Batch_idx: " << bsz_idx << " Seq_idx: " << seq_idx
+          //     << " Context_len: " << context_len << " Original context_len: "
+          //     << context_lens_ptr[bsz_idx] << " Seq_len: " << seq_len
+          //     << " Max input length: " << max_input_length
+          //     << sycl::endl;
+          if (context_len >= seq_len) {
+            return;
+          }
+
+          context_len = context_len + 1;
+
+          const int num_context_blocks =
+              DIVIDE_ROUND_UP(context_len, BLOCK_SIZE);
+          const int num_blocks_per_partition = num_context_blocks;
+
+          const int start_block_idx = 0;
+          const int end_block_idx =
+              MIN(start_block_idx + num_context_blocks, num_context_blocks);
+
+          const int num_blocks = end_block_idx - start_block_idx;
+          const int start_token_idx = start_block_idx * BLOCK_SIZE;
+          const int end_token_idx =
+              MIN(start_token_idx + num_blocks * BLOCK_SIZE, context_len);
+          const int num_tokens = end_token_idx - start_token_idx;
+          constexpr int THREAD_GROUP_SIZE = MAX(WARP_SIZE / BLOCK_SIZE, 1);
+          constexpr int NUM_THREAD_GROUPS =
+              NUM_THREADS /
+              THREAD_GROUP_SIZE;  // Note: This assumes THREAD_GROUP_SIZE
+          constexpr int NUM_TOKENS_PER_THREAD_GROUP =
+              DIVIDE_ROUND_UP(BLOCK_SIZE, WARP_SIZE);
+          constexpr int NUM_WARPS = NUM_THREADS / WARP_SIZE;
+          const int thread_idx = item_ct1.get_local_id(2);
+          const int warp_idx = thread_idx / WARP_SIZE;
+          const int lane = thread_idx % WARP_SIZE;
+          const int head_idx = item_ct1.get_group(1);
+          const int num_heads = item_ct1.get_group_range(1);
+          const int kv_head_idx = head_idx / num_queries_per_kv;
+          // TODO: consider alibi_slope later
+          constexpr int NUM_ELEMS_PER_THREAD = HD / THREAD_GROUP_SIZE;
+          constexpr int NUM_VECS_PER_THREAD = NUM_ELEMS_PER_THREAD / VEC_SIZE;
+          const int thread_group_idx = thread_idx / THREAD_GROUP_SIZE;
+          const int thread_group_offset = thread_idx % THREAD_GROUP_SIZE;
+          const sycl_t* q_ptr =
+              query_ptr + (query_loc_ptr[bsz_idx] + seq_idx) * query_stride_bs +
+              head_idx * HD;
+
+#pragma unroll
+          for (int i = thread_group_idx; i < NUM_VECS_PER_THREAD;
+               i += NUM_THREAD_GROUPS) {
+            const int vec_idx = thread_group_offset + i * THREAD_GROUP_SIZE;
+            q_vecs[thread_group_offset * NUM_VECS_PER_THREAD + i] =
+                *reinterpret_cast<const Q_Vec*>(q_ptr + vec_idx * VEC_SIZE);
+          }
+          // Loaded q_vecs
+          item_ct1.barrier(sycl::access::fence_space::local_space);
+          auto shared_mem = (char*)dpct_local;
+          float* logits = reinterpret_cast<float*>(shared_mem);
+          constexpr int x = 16 / sizeof(sycl_t);
+          float qk_max = -FLT_MAX;
+          const int* block_table =
+              block_tables_ptr + bsz_idx * block_table_stride_batch;
+
+          // Loading key
+          for (int block_idx = start_block_idx + warp_idx;
+               block_idx < end_block_idx; block_idx += NUM_WARPS) {
+            const int64_t physical_block_number =
+                static_cast<int64_t>(block_table[block_idx]);
+            for (int i = 0; i < NUM_TOKENS_PER_THREAD_GROUP; i++) {
+              const int physical_block_offset =
+                  (thread_group_idx + i * WARP_SIZE) % BLOCK_SIZE;
+              const int token_idx =
+                  block_idx * BLOCK_SIZE + physical_block_offset;
+
+              Q_Vec k_vecs[NUM_VECS_PER_THREAD];
+
+#pragma unroll
+              for (int j = 0; j < NUM_VECS_PER_THREAD; j++) {
+                const sycl_t* k_ptr =
+                    key_cache_ptr +
+                    physical_block_number * k_cache_stride_tokens +
+                    kv_head_idx * k_cache_stride_head +
+                    physical_block_offset * x;
+
+                const int vec_idx = thread_group_offset + j * THREAD_GROUP_SIZE;
+                const int offset1 = (vec_idx * VEC_SIZE) / x;
+                const int offset2 = (vec_idx * VEC_SIZE) % x;
+                k_vecs[j] = *reinterpret_cast<const Q_Vec*>(
+                    k_ptr + offset1 * BLOCK_SIZE * x + offset2);
+              }
+
+              // Compute dot product.
+              // This includes a reduction across the threads in the
+              // same thread group. Q_Vec_t
+              // q_vec_[NUM_VECS_PER_THREAD] = q_vecs +
+              // thread_group_offset * THREAD_GROUP_SIZE;
+              float qk = scale *
+                         Qk_dot<sycl_t, THREAD_GROUP_SIZE>::template dot<
+                             Q_Vec, NUM_VECS_PER_THREAD>(
+                             q_vecs + thread_group_offset * NUM_VECS_PER_THREAD,
+                             k_vecs, item_ct1);
+
+              if (thread_group_offset == 0) {
+                // Store the partial reductions to shared memory.
+                // NOTE(woosuk): It is required to zero out the
+                // masked logits.
+                const bool mask = token_idx > context_len;
+                logits[token_idx - start_token_idx] = mask ? 0.f : qk;
+                qk_max = mask ? qk_max : sycl::fmax(qk_max, qk);
+              }
+            }
+          }
+#pragma unroll
+          for (int mask = WARP_SIZE / 2; mask >= THREAD_GROUP_SIZE; mask /= 2) {
+            /*
+            DPCT1096:38: The right-most dimension of the work-group used
+            in the SYCL kernel that calls this function may be less than
+            "32". The function "dpct::permute_sub_group_by_xor" may
+            return an unexpected result on the CPU device. Modify the
+            size of the work-group to ensure that the value of the
+            right-most dimension is a multiple of "32".
+            */
+            qk_max =
+                sycl::fmax(qk_max, dpct::permute_sub_group_by_xor(
+                                       item_ct1.get_sub_group(), qk_max, mask));
+          }
+          if (lane == 0) {
+            red_smem[warp_idx] = qk_max;
+          }
+          item_ct1.barrier(sycl::access::fence_space::local_space);
+          // TODO(woosuk): Refactor this part.
+          // Get the max qk value for the sequence.
+          qk_max = lane < NUM_WARPS ? red_smem[lane] : -FLT_MAX;
+#pragma unroll
+          for (int mask = NUM_WARPS / 2; mask >= 1; mask /= 2) {
+            /*
+            DPCT1096:39: The right-most dimension of the work-group used
+            in the SYCL kernel that calls this function may be less than
+            "32". The function "dpct::permute_sub_group_by_xor" may
+            return an unexpected result on the CPU device. Modify the
+            size of the work-group to ensure that the value of the
+            right-most dimension is a multiple of "32".
+            */
+            qk_max =
+                sycl::fmax(qk_max, dpct::permute_sub_group_by_xor(
+                                       item_ct1.get_sub_group(), qk_max, mask));
+          }
+          qk_max =
+              dpct::select_from_sub_group(item_ct1.get_sub_group(), qk_max, 0);
+
+          // Get the sum of the exp values.
+          float exp_sum = 0.f;
+          for (int i = thread_idx; i < num_tokens; i += NUM_THREADS) {
+            float val = sycl::exp(logits[i] - qk_max);
+            logits[i] = val;
+            exp_sum += val;
+          }
+          exp_sum =
+              block_sum<NUM_WARPS>(&red_smem[NUM_WARPS], exp_sum, item_ct1);
+          // Compute softmax.
+          const float inv_sum = 1.f / (exp_sum + 1e-6f);
+#pragma unroll
+          for (int i = thread_idx; i < num_tokens; i += NUM_THREADS) {
+            logits[i] *= inv_sum;
+          }
+
+          item_ct1.barrier(sycl::access::fence_space::local_space);
+          constexpr int V_VEC_SIZE = MIN(16 / sizeof(sycl_t), BLOCK_SIZE);
+          using V_vec = typename Vec<sycl_t, V_VEC_SIZE>::Type;
+          using L_vec = typename Vec<sycl_t, V_VEC_SIZE>::Type;
+          using Float_L_vec = typename FloatVec<L_vec>::Type;
+          constexpr int NUM_V_VECS_PER_ROW = BLOCK_SIZE / V_VEC_SIZE;
+          constexpr int NUM_ROWS_PER_ITER = WARP_SIZE / NUM_V_VECS_PER_ROW;
+          constexpr int NUM_ROWS_PER_THREAD =
+              DIVIDE_ROUND_UP(HD, NUM_ROWS_PER_ITER);
+          // NOTE(woosuk): We use FP32 for the accumulator for better
+          // accuracy.
+          float accs[NUM_ROWS_PER_THREAD];
+#pragma unroll
+          for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
+            accs[i] = 0.f;
+          }
+
+          sycl_t zero_value;
+          zero(zero_value);
+          for (int block_idx = start_block_idx + warp_idx;
+               block_idx < end_block_idx; block_idx += NUM_WARPS) {
+            // NOTE(woosuk): The block number is stored in int32.
+            // However, we cast it to int64 because int32 can lead to
+            // overflow when this variable is multiplied by large
+            // numbers (e.g., kv_block_stride).
+            const int64_t physical_block_number =
+                static_cast<int64_t>(block_table[block_idx]);
+            const int physical_block_offset =
+                (lane % NUM_V_VECS_PER_ROW) * V_VEC_SIZE;
+            const int token_idx =
+                block_idx * BLOCK_SIZE + physical_block_offset;
+            L_vec logits_vec;
+            vllm::from_float(
+                logits_vec, *reinterpret_cast<Float_L_vec*>(logits + token_idx -
+                                                            start_token_idx));
+
+            const sycl_t* v_ptr =
+                value_cache_ptr +
+                physical_block_number * v_cache_stride_tokens +
+                kv_head_idx * v_cache_stride_head;
+#pragma unroll
+            for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
+              const int row_idx =
+                  lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;
+              if (row_idx < HD) {
+                const int offset = row_idx * BLOCK_SIZE + physical_block_offset;
+                V_vec v_vec = *reinterpret_cast<const V_vec*>(v_ptr + offset);
+                if (block_idx == num_context_blocks - 1) {
+                  // NOTE(woosuk): When v_vec contains the tokens
+                  // that are out of the context, we should
+                  // explicitly zero out the values since they may
+                  // contain NaNs. See
+                  // https://github.com/vllm-project/vllm/issues/641#issuecomment-1682544472
+                  sycl_t* v_vec_ptr = reinterpret_cast<sycl_t*>(&v_vec);
+#pragma unroll
+                  for (int j = 0; j < V_VEC_SIZE; j++) {
+                    v_vec_ptr[j] =
+                        token_idx + j < context_len ? v_vec_ptr[j] : zero_value;
+                  }
+                }
+                accs[i] += vllm::dot(logits_vec, v_vec);
+              }
+            }
+          }
+      // Perform reduction within each warp.
+#pragma unroll
+          for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
+            float acc = accs[i];
+#pragma unroll
+            for (int mask = NUM_V_VECS_PER_ROW / 2; mask >= 1; mask /= 2) {
+              /*
+              DPCT1096:41: The right-most dimension of the work-group
+              used in the SYCL kernel that calls this function may be
+              less than "32". The function
+              "dpct::permute_sub_group_by_xor" may return an
+              unexpected result on the CPU device. Modify the size of
+              the work-group to ensure that the value of the
+              right-most dimension is a multiple of "32".
+              */
+              acc += dpct::permute_sub_group_by_xor(item_ct1.get_sub_group(),
+                                                    acc, mask);
+            }
+            accs[i] = acc;
+          }
+
+          // NOTE(woosuk): A barrier is required because the shared memory
+          // space for logits is reused for the output.
+
+          item_ct1.barrier(sycl::access::fence_space::local_space);
+
+          // Perform reduction across warps.
+          float* out_smem = reinterpret_cast<float*>(shared_mem);
+#pragma unroll
+          for (int i = NUM_WARPS; i > 1; i /= 2) {
+            int mid = i / 2;
+            // Upper warps write to shared memory.
+            if (warp_idx >= mid && warp_idx < i) {
+              float* dst = &out_smem[(warp_idx - mid) * HD];
+#pragma unroll
+              for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
+                const int row_idx =
+                    lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;
+                if (row_idx < HD && lane % NUM_V_VECS_PER_ROW == 0) {
+                  dst[row_idx] = accs[i];
+                }
+              }
+            }
+
+            item_ct1.barrier(sycl::access::fence_space::local_space);
+
+            // Lower warps update the output.
+            if (warp_idx < mid) {
+              const float* src = &out_smem[warp_idx * HD];
+#pragma unroll
+              for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
+                const int row_idx =
+                    lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;
+                if (row_idx < HD && lane % NUM_V_VECS_PER_ROW == 0) {
+                  accs[i] += src[row_idx];
+                }
+              }
+            }
+
+            item_ct1.barrier(sycl::access::fence_space::local_space);
+          }
+
+          // Write the final output.
+          if (warp_idx == 0) {
+            sycl_t* out_ptr =
+                out_p + (query_loc_ptr[bsz_idx] + seq_idx) * out_stride_tokens +
+                head_idx * out_stride_head;
+
+#pragma unroll
+            for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
+              const int row_idx =
+                  lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;
+              if (row_idx < HD && lane % NUM_V_VECS_PER_ROW == 0) {
+                vllm::from_float(*(out_ptr + row_idx), accs[i]);
+              }
+            }
+          }
+        });
+    // Each thread_group handles one token
+  };
+  queue.submit(cgf);
+}
+
+template <
+    typename scalar_t,
+    typename Q_Vec_t,
+    int HEAD_SIZE,
+    int BLOCK_SIZE,
+    int NUM_THREADS,
+    int VEC_SIZE,
+    int PARTITION_SIZE = 0> // Zero means no partitioning.
+void paged_attention_kernel(
+    float* __restrict__ exp_sums, // [num_seqs, num_heads, max_num_partitions]
+    float* __restrict__ max_logits, // [num_seqs, num_heads, max_num_partitions]
+    scalar_t* __restrict__ out, // [num_seqs, num_heads, max_num_partitions,
+                                // head_size]
+    const scalar_t* __restrict__ q, // [num_seqs, num_heads, head_size]
+    const scalar_t* __restrict__ k_cache, // [num_blocks, num_kv_heads,
+                                          // head_size/x, block_size, x]
+    const scalar_t* __restrict__ v_cache, // [num_blocks, num_kv_heads,
+                                          // head_size, block_size]
+    const int num_kv_heads, // [num_heads]
+    const float scale,
+    const int* __restrict__ block_tables, // [num_seqs, max_num_blocks_per_seq]
+    const int* __restrict__ context_lens, // [num_seqs]
+    const int max_num_blocks_per_seq,
+    const float* __restrict__ alibi_slopes, // [num_heads]
+    const int q_stride,
+    const int kv_block_stride,
+    const int kv_head_stride,
+    const float attn_logit_softcapping,
+    const sycl::nd_item<3>& item_ct1,
+    uint8_t* dpct_local,
+    Q_Vec_t* q_vecs,
+    float* red_smem) {
+  const int seq_idx = item_ct1.get_group(1);
+  const int partition_idx = item_ct1.get_group(0);
+  const int max_num_partitions = item_ct1.get_group_range(0);
+  constexpr bool USE_PARTITIONING = PARTITION_SIZE > 0;
+  const int context_len = context_lens[seq_idx];
+  if (USE_PARTITIONING && partition_idx * PARTITION_SIZE >= context_len) {
+    // No work to do. Terminate the thread block.
+    return;
+  }
+
+  const int num_context_blocks = DIVIDE_ROUND_UP(context_len, BLOCK_SIZE);
+  const int num_blocks_per_partition =
+      USE_PARTITIONING ? PARTITION_SIZE / BLOCK_SIZE : num_context_blocks;
+
+  // [start_block_idx, end_block_idx) is the range of blocks to process.
+  const int start_block_idx =
+      USE_PARTITIONING ? partition_idx * num_blocks_per_partition : 0;
+  const int end_block_idx =
+      MIN(start_block_idx + num_blocks_per_partition, num_context_blocks);
+  const int num_blocks = end_block_idx - start_block_idx;
+
+  // [start_token_idx, end_token_idx) is the range of tokens to process.
+  const int start_token_idx = start_block_idx * BLOCK_SIZE;
+  const int end_token_idx =
+      MIN(start_token_idx + num_blocks * BLOCK_SIZE, context_len);
+  const int num_tokens = end_token_idx - start_token_idx;
+
+  constexpr int THREAD_GROUP_SIZE = MAX(WARP_SIZE / BLOCK_SIZE, 1);
+  constexpr int NUM_THREAD_GROUPS =
+      NUM_THREADS / THREAD_GROUP_SIZE; // Note: This assumes THREAD_GROUP_SIZE
+                                       // divides NUM_THREADS
+  assert(NUM_THREADS % THREAD_GROUP_SIZE == 0);
+  constexpr int NUM_TOKENS_PER_THREAD_GROUP =
+      DIVIDE_ROUND_UP(BLOCK_SIZE, WARP_SIZE);
+  constexpr int NUM_WARPS = NUM_THREADS / WARP_SIZE;
+  const int thread_idx = item_ct1.get_local_id(2);
+  const int warp_idx = thread_idx / WARP_SIZE;
+  const int lane = thread_idx % WARP_SIZE;
+
+  const int head_idx = item_ct1.get_group(2);
+  const int num_heads = item_ct1.get_group_range(2);
+  const int num_queries_per_kv = num_heads / num_kv_heads;
+
+  const int kv_head_idx = head_idx / num_queries_per_kv;
+  ;
+  const float alibi_slope =
+      alibi_slopes == nullptr ? 0.f : alibi_slopes[head_idx];
+
+  // A vector type to store a part of a key or a query.
+  // The vector size is configured in such a way that the threads in a thread
+  // group fetch or compute 16 bytes at a time. For example, if the size of a
+  // thread group is 4 and the data type is half, then the vector size is 16 /
+  // (4 * sizeof(half)) == 2.
+
+  // constexpr int VEC_SIZE = MAX(16 / (THREAD_GROUP_SIZE * sizeof(scalar_t)),
+  // 1);
+
+  constexpr int NUM_ELEMS_PER_THREAD = HEAD_SIZE / THREAD_GROUP_SIZE;
+  constexpr int NUM_VECS_PER_THREAD = NUM_ELEMS_PER_THREAD / VEC_SIZE;
+
+  const int thread_group_idx = thread_idx / THREAD_GROUP_SIZE;
+  const int thread_group_offset = thread_idx % THREAD_GROUP_SIZE;
+
+  // Load the query to registers.
+  // Each thread in a thread group has a different part of the query.
+  // For example, if the the thread group size is 4, then the first thread in
+  // the group has 0, 4, 8, ... th vectors of the query, and the second thread
+  // has 1, 5, 9, ... th vectors of the query, and so on. NOTE(woosuk): Because
+  // q is split from a qkv tensor, it may not be contiguous.
+  const scalar_t* q_ptr = q + seq_idx * q_stride + head_idx * HEAD_SIZE;
+
+#pragma unroll
+  for (int i = thread_group_idx; i < NUM_VECS_PER_THREAD;
+       i += NUM_THREAD_GROUPS) {
+    const int vec_idx = thread_group_offset + i * THREAD_GROUP_SIZE;
+    q_vecs[thread_group_offset * NUM_VECS_PER_THREAD + i] =
+        *reinterpret_cast<const Q_Vec_t*>(q_ptr + vec_idx * VEC_SIZE);
+  }
+  /*
+  DPCT1065:5: Consider replacing sycl::nd_item::barrier() with
+  sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better
+  performance if there is no access to global memory.
+  */
+  item_ct1.barrier(sycl::access::fence_space::local_space); // TODO(naed90): possible speedup if this is replaced with
+                      // a memory wall right before we use q_vecs
+
+  // Memory planning.
+  auto shared_mem = (char*)dpct_local;
+  // NOTE(woosuk): We use FP32 for the softmax logits for better accuracy.
+  float* logits = reinterpret_cast<float*>(shared_mem);
+  // Workspace for reduction.
+
+  // x == THREAD_GROUP_SIZE * VEC_SIZE
+  // Each thread group fetches x elements from the key at a time.
+  constexpr int x = 16 / sizeof(scalar_t);
+  float qk_max = -FLT_MAX;
+
+  // Iterate over the key blocks.
+  // Each warp fetches a block of keys for each iteration.
+  // Each thread group in a warp fetches a key from the block, and computes
+  // dot product with the query.
+  const int* block_table = block_tables + seq_idx * max_num_blocks_per_seq;
+
+  for (int block_idx = start_block_idx + warp_idx; block_idx < end_block_idx;
+       block_idx += NUM_WARPS) {
+    // NOTE(woosuk): The block number is stored in int32. However, we cast it to
+    // int64 because int32 can lead to overflow when this variable is multiplied
+    // by large numbers (e.g., kv_block_stride).
+    const int64_t physical_block_number =
+        static_cast<int64_t>(block_table[block_idx]);
+
+    // Load a key to registers.
+    // Each thread in a thread group has a different part of the key.
+    // For example, if the the thread group size is 4, then the first thread in
+    // the group has 0, 4, 8, ... th vectors of the key, and the second thread
+    // has 1, 5, 9, ... th vectors of the key, and so on.
+
+    for (int i = 0; i < NUM_TOKENS_PER_THREAD_GROUP; i++) {
+      const int physical_block_offset =
+          (thread_group_idx + i * WARP_SIZE) % BLOCK_SIZE;
+      const int token_idx = block_idx * BLOCK_SIZE + physical_block_offset;
+
+      Q_Vec_t k_vecs[NUM_VECS_PER_THREAD];
+
+#pragma unroll
+      for (int j = 0; j < NUM_VECS_PER_THREAD; j++) {
+        const scalar_t* k_ptr = k_cache +
+            physical_block_number * kv_block_stride +
+            kv_head_idx * kv_head_stride + physical_block_offset * x;
+
+        const int vec_idx = thread_group_offset + j * THREAD_GROUP_SIZE;
+        const int offset1 = (vec_idx * VEC_SIZE) / x;
+        const int offset2 = (vec_idx * VEC_SIZE) % x;
+        k_vecs[j] = *reinterpret_cast<const Q_Vec_t*>(
+            k_ptr + offset1 * BLOCK_SIZE * x + offset2);
+      }
+
+      // Compute dot product.
+      // This includes a reduction across the threads in the same thread group.
+      // Q_Vec_t q_vec_[NUM_VECS_PER_THREAD] = q_vecs + thread_group_offset *
+      // THREAD_GROUP_SIZE;
+      float qk = scale *
+          Qk_dot<scalar_t, THREAD_GROUP_SIZE>::
+              template dot<Q_Vec_t, NUM_VECS_PER_THREAD>(
+                     q_vecs + thread_group_offset * NUM_VECS_PER_THREAD,
+                     k_vecs,
+                     item_ct1);
+      // Add the ALiBi bias if slopes are given.
+      qk +=
+          (alibi_slope != 0) ? alibi_slope * (token_idx - context_len + 1) : 0;
+
+      // Add the attn_logit_softcapp if given.
+      if (attn_logit_softcapping != 0.0) {
+          qk = attn_softcapping(qk, attn_logit_softcapping);
+      }
+      if (thread_group_offset == 0) {
+        // Store the partial reductions to shared memory.
+        // NOTE(woosuk): It is required to zero out the masked logits.
+        const bool mask = token_idx >= context_len;
+        logits[token_idx - start_token_idx] = mask ? 0.f : qk;
+        // Update the max value.
+        qk_max = mask ? qk_max : sycl::fmax(qk_max, qk);
+      }
+    }
+  }
+
+  // Perform reduction across the threads in the same warp to get the
+  // max qk value for each "warp" (not across the thread block yet).
+  // The 0-th thread of each thread group already has its max qk value.
+#pragma unroll
+  for (int mask = WARP_SIZE / 2; mask >= THREAD_GROUP_SIZE; mask /= 2) {
+  
+    /*
+    DPCT1096:38: The right-most dimension of the work-group used in the SYCL
+    kernel that calls this function may be less than "32". The function
+    "dpct::permute_sub_group_by_xor" may return an unexpected result on the CPU
+    device. Modify the size of the work-group to ensure that the value of the
+    right-most dimension is a multiple of "32".
+    */
+    qk_max = sycl::fmax(
+        qk_max,
+        dpct::permute_sub_group_by_xor(
+            item_ct1.get_sub_group(), qk_max, mask));
+  }
+  if (lane == 0) {
+    red_smem[warp_idx] = qk_max;
+  }
+  
+  item_ct1.barrier(sycl::access::fence_space::local_space);
+
+  // TODO(woosuk): Refactor this part.
+  // Get the max qk value for the sequence.
+  qk_max = lane < NUM_WARPS ? red_smem[lane] : -FLT_MAX;
+#pragma unroll
+  for (int mask = NUM_WARPS / 2; mask >= 1; mask /= 2) {
+    
+    /*
+    DPCT1096:39: The right-most dimension of the work-group used in the SYCL
+    kernel that calls this function may be less than "32". The function
+    "dpct::permute_sub_group_by_xor" may return an unexpected result on the CPU
+    device. Modify the size of the work-group to ensure that the value of the
+    right-most dimension is a multiple of "32".
+    */
+    qk_max = sycl::fmax(
+        qk_max,
+        dpct::permute_sub_group_by_xor(
+            item_ct1.get_sub_group(), qk_max, mask));
+  }
+  // Broadcast the max qk value to all threads.
+  
+  /*
+  DPCT1096:40: The right-most dimension of the work-group used in the SYCL
+  kernel that calls this function may be less than "32". The function
+  "dpct::select_from_sub_group" may return an unexpected result on the CPU
+  device. Modify the size of the work-group to ensure that the value of the
+  right-most dimension is a multiple of "32".
+  */
+  qk_max = dpct::select_from_sub_group(
+          item_ct1.get_sub_group(), qk_max, 0);
+
+  // Get the sum of the exp values.
+  float exp_sum = 0.f;
+  for (int i = thread_idx; i < num_tokens; i += NUM_THREADS) {
+    float val = sycl::exp(logits[i] - qk_max);
+    logits[i] = val;
+    exp_sum += val;
+  }
+  exp_sum = block_sum<NUM_WARPS>(&red_smem[NUM_WARPS], exp_sum, item_ct1);
+
+  // Compute softmax.
+  const float inv_sum = 1.f / (exp_sum + 1e-6f);
+  for (int i = thread_idx; i < num_tokens; i += NUM_THREADS) {
+    logits[i] *= inv_sum;
+  }
+  
+  item_ct1.barrier(sycl::access::fence_space::local_space);
+
+  // If partitioning is enabled, store the max logit and exp_sum.
+  if (USE_PARTITIONING && thread_idx == 0) {
+    float* max_logits_ptr = max_logits +
+        seq_idx * num_heads * max_num_partitions +
+        head_idx * max_num_partitions + partition_idx;
+    *max_logits_ptr = qk_max;
+    float* exp_sums_ptr = exp_sums + seq_idx * num_heads * max_num_partitions +
+        head_idx * max_num_partitions + partition_idx;
+    *exp_sums_ptr = exp_sum;
+  }
+
+  // Each thread will fetch 16 bytes from the value cache at a time.
+  constexpr int V_VEC_SIZE = MIN(16 / sizeof(scalar_t), BLOCK_SIZE);
+  using V_vec = typename Vec<scalar_t, V_VEC_SIZE>::Type;
+  using L_vec = typename Vec<scalar_t, V_VEC_SIZE>::Type;
+  using Float_L_vec = typename FloatVec<L_vec>::Type;
+
+  constexpr int NUM_V_VECS_PER_ROW = BLOCK_SIZE / V_VEC_SIZE;
+  constexpr int NUM_ROWS_PER_ITER = WARP_SIZE / NUM_V_VECS_PER_ROW;
+  constexpr int NUM_ROWS_PER_THREAD =
+      DIVIDE_ROUND_UP(HEAD_SIZE, NUM_ROWS_PER_ITER);
+
+  // NOTE(woosuk): We use FP32 for the accumulator for better accuracy.
+  float accs[NUM_ROWS_PER_THREAD];
+#pragma unroll
+  for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
+    accs[i] = 0.f;
+  }
+
+  scalar_t zero_value;
+  zero(zero_value);
+  for (int block_idx = start_block_idx + warp_idx; block_idx < end_block_idx;
+       block_idx += NUM_WARPS) {
+    // NOTE(woosuk): The block number is stored in int32. However, we cast it to
+    // int64 because int32 can lead to overflow when this variable is multiplied
+    // by large numbers (e.g., kv_block_stride).
+    const int64_t physical_block_number =
+        static_cast<int64_t>(block_table[block_idx]);
+    const int physical_block_offset = (lane % NUM_V_VECS_PER_ROW) * V_VEC_SIZE;
+    const int token_idx = block_idx * BLOCK_SIZE + physical_block_offset;
+    L_vec logits_vec;
+    vllm::from_float(
+        logits_vec,
+        *reinterpret_cast<Float_L_vec*>(logits + token_idx - start_token_idx));
+
+    const scalar_t* v_ptr = v_cache + physical_block_number * kv_block_stride +
+        kv_head_idx * kv_head_stride;
+#pragma unroll
+    for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
+      const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;
+      if (row_idx < HEAD_SIZE) {
+        const int offset = row_idx * BLOCK_SIZE + physical_block_offset;
+        V_vec v_vec = *reinterpret_cast<const V_vec*>(v_ptr + offset);
+        if (block_idx == num_context_blocks - 1) {
+          // NOTE(woosuk): When v_vec contains the tokens that are out of the
+          // context, we should explicitly zero out the values since they may
+          // contain NaNs. See
+          // https://github.com/vllm-project/vllm/issues/641#issuecomment-1682544472
+          scalar_t* v_vec_ptr = reinterpret_cast<scalar_t*>(&v_vec);
+#pragma unroll
+          for (int j = 0; j < V_VEC_SIZE; j++) {
+            v_vec_ptr[j] =
+                token_idx + j < context_len ? v_vec_ptr[j] : zero_value;
+          }
+        }
+        accs[i] += vllm::dot(logits_vec, v_vec);
+      }
+    }
+  }
+
+  // Perform reduction within each warp.
+#pragma unroll
+  for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
+    float acc = accs[i];
+#pragma unroll
+    for (int mask = NUM_V_VECS_PER_ROW / 2; mask >= 1; mask /= 2) {
+     
+      /*
+      DPCT1096:41: The right-most dimension of the work-group used in the SYCL
+      kernel that calls this function may be less than "32". The function
+      "dpct::permute_sub_group_by_xor" may return an unexpected result on the
+      CPU device. Modify the size of the work-group to ensure that the value of
+      the right-most dimension is a multiple of "32".
+      */
+      acc += dpct::permute_sub_group_by_xor(
+          item_ct1.get_sub_group(), acc, mask);
+    }
+    accs[i] = acc;
+  }
+
+  // NOTE(woosuk): A barrier is required because the shared memory space for
+  // logits is reused for the output.
+
+  item_ct1.barrier(sycl::access::fence_space::local_space);
+
+  // Perform reduction across warps.
+  float* out_smem = reinterpret_cast<float*>(shared_mem);
+#pragma unroll
+  for (int i = NUM_WARPS; i > 1; i /= 2) {
+    int mid = i / 2;
+    // Upper warps write to shared memory.
+    if (warp_idx >= mid && warp_idx < i) {
+      float* dst = &out_smem[(warp_idx - mid) * HEAD_SIZE];
+#pragma unroll
+      for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
+        const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;
+        if (row_idx < HEAD_SIZE && lane % NUM_V_VECS_PER_ROW == 0) {
+          dst[row_idx] = accs[i];
+        }
+      }
+    }
+    
+    item_ct1.barrier(sycl::access::fence_space::local_space);
+
+    // Lower warps update the output.
+    if (warp_idx < mid) {
+      const float* src = &out_smem[warp_idx * HEAD_SIZE];
+#pragma unroll
+      for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
+        const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;
+        if (row_idx < HEAD_SIZE && lane % NUM_V_VECS_PER_ROW == 0) {
+          accs[i] += src[row_idx];
+        }
+      }
+    }
+    
+    item_ct1.barrier(sycl::access::fence_space::local_space);
+  }
+
+  // Write the final output.
+  if (warp_idx == 0) {
+    scalar_t* out_ptr = out +
+        seq_idx * num_heads * max_num_partitions * HEAD_SIZE +
+        head_idx * max_num_partitions * HEAD_SIZE + partition_idx * HEAD_SIZE;
+#pragma unroll
+    for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
+      const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;
+      if (row_idx < HEAD_SIZE && lane % NUM_V_VECS_PER_ROW == 0) {
+        vllm::from_float(*(out_ptr + row_idx), accs[i]);
+      }
+    }
+  }
+}
+
+// Grid: (num_heads, num_seqs, 1).
+template <
+    typename scalar_t,
+    typename Q_Vec_t,
+    int HEAD_SIZE,
+    int BLOCK_SIZE,
+    int NUM_THREADS,
+    int VEC_SIZE>
+void paged_attention_v1_kernel(
+    scalar_t* __restrict__ out, // [num_seqs, num_heads, head_size]
+    const scalar_t* __restrict__ q, // [num_seqs, num_heads, head_size]
+    const scalar_t* __restrict__ k_cache, // [num_blocks, num_kv_heads,
+                                          // head_size/x, block_size, x]
+    const scalar_t* __restrict__ v_cache, // [num_blocks, num_kv_heads,
+                                          // head_size, block_size]
+    const int num_kv_heads, // [num_heads]
+    const float scale,
+    const int* __restrict__ block_tables, // [num_seqs, max_num_blocks_per_seq]
+    const int* __restrict__ context_lens, // [num_seqs]
+    const int max_num_blocks_per_seq,
+    const float* __restrict__ alibi_slopes, // [num_heads]
+    const int q_stride,
+    const int kv_block_stride,
+    const int kv_head_stride,
+    const float attn_logit_softcapping,
+    const sycl::nd_item<3>& item_ct1,
+    uint8_t* dpct_local,
+    Q_Vec_t* q_vecs,
+    float* red_smem) {
+  paged_attention_kernel<
+      scalar_t,
+      Q_Vec_t,
+      HEAD_SIZE,
+      BLOCK_SIZE,
+      NUM_THREADS,
+      VEC_SIZE>(
+      /* exp_sums */ nullptr,
+      /* max_logits */ nullptr,
+      out,
+      q,
+      k_cache,
+      v_cache,
+      num_kv_heads,
+      scale,
+      block_tables,
+      context_lens,
+      max_num_blocks_per_seq,
+      alibi_slopes,
+      q_stride,
+      kv_block_stride,
+      kv_head_stride,
+      attn_logit_softcapping,
+      item_ct1,
+      dpct_local,
+      q_vecs,
+      red_smem);
+}
+
+#define LAUNCH_ATTENTION_KERNEL(T, HEAD_SIZE, BLOCK_SIZE)      \
+  paged_attention_xpu_v1_impl<T, HEAD_SIZE, BLOCK_SIZE>::call( \
+      out_ptr,                                                 \
+      query_ptr,                                               \
+      key_cache_ptr,                                           \
+      value_cache_ptr,                                         \
+      num_kv_heads,                                            \
+      scale,                                                   \
+      block_tables_ptr,                                        \
+      context_lens_ptr,                                        \
+      max_num_blocks_per_seq,                                  \
+      alibi_slopes_ptr,                                        \
+      q_stride,                                                \
+      kv_block_stride,                                         \
+      kv_head_stride,                                          \
+      num_seqs,                                                \
+      num_heads,                                               \
+      num_blocks);
+
+#define LAUNCH_PAGED_ATTENTION_V1(HEAD_SIZE)                                \
+  event = queue.submit([&](sycl::handler& cgh) {                            \
+    sycl::local_accessor<uint8_t, 1> dpct_local_acc_ct1(                    \
+        sycl::range<1>(shared_mem_size), cgh);                              \
+    sycl::local_accessor<Q_Vec, 1> q_vecs_acc_ct1(                          \
+        sycl::range<1>(THREAD_GROUP_SIZE * num_vecs_per_thread), cgh);      \
+    sycl::local_accessor<float, 1> red_smem_acc_ct1(                        \
+        sycl::range<1>(2 * NUM_WARPS), cgh);                                \
+                                                                            \
+    auto out_ptr_ct0 = out_ptr;                                             \
+    auto query_ptr_ct1 = query_ptr;                                         \
+    auto key_cache_ptr_ct2 = key_cache_ptr;                                 \
+    auto value_cache_ptr_ct3 = value_cache_ptr;                             \
+    auto scale_ct5 = scale;                                                 \
+    auto block_tables_ptr_ct6 = block_tables_ptr;                           \
+    auto context_lens_ptr_ct7 = context_lens_ptr;                           \
+    auto max_num_blocks_per_seq_ct8 = max_num_blocks_per_seq;               \
+    auto alibi_slopes_ptr_ct9 = alibi_slopes_ptr;                           \
+    auto q_stride_ct10 = q_stride;                                          \
+    auto kv_block_stride_ct11 = kv_block_stride;                            \
+    auto kv_head_stride_ct12 = kv_head_stride;                              \
+    auto attn_logit_softcapping_ct13 = attn_logit_softcapping;              \
+                                                                            \
+    cgh.parallel_for(                                                       \
+        sycl::nd_range<3>(grid * block, block),                             \
+        [=](sycl::nd_item<3> item_ct1) [[intel::reqd_sub_group_size(32)]] { \
+          paged_attention_v1_kernel<                                        \
+              sycl_t,                                                       \
+              Q_Vec,                                                        \
+              HEAD_SIZE,                                                    \
+              BLOCK_SIZE,                                                   \
+              NUM_THREADS,                                                  \
+              VEC_SIZE>(                                                    \
+              out_ptr_ct0,                                                  \
+              query_ptr_ct1,                                                \
+              key_cache_ptr_ct2,                                            \
+              value_cache_ptr_ct3,                                          \
+              num_kv_heads,                                                 \
+              scale_ct5,                                                    \
+              block_tables_ptr_ct6,                                         \
+              context_lens_ptr_ct7,                                         \
+              max_num_blocks_per_seq_ct8,                                   \
+              alibi_slopes_ptr_ct9,                                         \
+              q_stride_ct10,                                                \
+              kv_block_stride_ct11,                                         \
+              kv_head_stride_ct12,                                          \
+              attn_logit_softcapping_ct13,                                  \
+              item_ct1,                                                     \
+              dpct_local_acc_ct1.get_pointer(),                             \
+              q_vecs_acc_ct1.get_pointer(),                                 \
+              red_smem_acc_ct1.get_pointer());                              \
+        });                                                                 \
+  });
+
+template <typename T, int BLOCK_SIZE, int NUM_THREADS = 512>
+void paged_attention_xpu_v1_impl_launcher(
+    torch::Tensor& out,
+    torch::Tensor& query,
+    torch::Tensor& key_cache,
+    torch::Tensor& value_cache,
+    int num_kv_heads,
+    float scale,
+    torch::Tensor& block_tables,
+    torch::Tensor& context_lens,
+    int max_context_len,
+    const c10::optional<torch::Tensor>& alibi_slopes,
+    const float attn_logit_softcapping) {
+  int num_seqs = query.size(0);
+  int num_heads = query.size(1);
+  int head_size = query.size(2);
+  int max_num_blocks_per_seq = block_tables.size(1);
+  int q_stride = query.stride(0);
+  int kv_block_stride = key_cache.stride(0);
+  int kv_head_stride = key_cache.stride(1);
+
+  constexpr int THREAD_GROUP_SIZE = MAX(WARP_SIZE / BLOCK_SIZE, 1);
+  constexpr int VEC_SIZE = MAX(16 / (THREAD_GROUP_SIZE * sizeof(T)), 1);
+  using sycl_t = vllm::xpu::SyclTypeTrait<T>::Type;
+  using Q_Vec = typename Vec<sycl_t, VEC_SIZE>::Type;
+
+  int num_vecs_per_thread = head_size / THREAD_GROUP_SIZE / VEC_SIZE;
+  assert(head_size % THREAD_GROUP_SIZE == 0);
+
+  // NOTE: alibi_slopes is optional.
+  const float* alibi_slopes_ptr = alibi_slopes
+      ? reinterpret_cast<const float*>(alibi_slopes.value().data_ptr())
+      : nullptr;
+
+  sycl_t* out_ptr = reinterpret_cast<sycl_t*>(out.data_ptr());
+  sycl_t* query_ptr = reinterpret_cast<sycl_t*>(query.data_ptr());
+  sycl_t* key_cache_ptr = reinterpret_cast<sycl_t*>(key_cache.data_ptr());
+  sycl_t* value_cache_ptr = reinterpret_cast<sycl_t*>(value_cache.data_ptr());
+  int* block_tables_ptr = block_tables.data_ptr<int>();
+  int* context_lens_ptr = context_lens.data_ptr<int>();
+
+  constexpr int NUM_WARPS = NUM_THREADS / WARP_SIZE;
+  int padded_max_context_len =
+      DIVIDE_ROUND_UP(max_context_len, BLOCK_SIZE) * BLOCK_SIZE;
+  
+  int logits_size = padded_max_context_len * sizeof(float);
+  int outputs_size = (NUM_WARPS / 2) * head_size * sizeof(float);
+  // Python-side check in vllm.worker.worker._check_if_can_support_max_seq_len
+  // Keep that in sync with the logic here!
+  int shared_mem_size = std::max(logits_size, outputs_size);
+
+  sycl::range<3> grid(1, num_seqs, num_heads);
+  sycl::range<3> block(1, 1, NUM_THREADS);
+  sycl::queue& queue = vllm::xpu::vllmGetQueue();
+  sycl::event event;
+
+  switch (head_size) {
+    // NOTE(woosuk): To reduce the compilation time, we only compile for the
+    // head sizes that we use in the model. However, we can easily extend this
+    // to support any head size which is a multiple of 16.
+    case 64:
+      LAUNCH_PAGED_ATTENTION_V1(64);
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+    // xpu::profiler_record(event_desc, event);  // Uncomment when needed
+#else
+    ::xpu::profiler_record("paged attn v1", event);
+#endif
+      break;
+    case 80:
+      LAUNCH_PAGED_ATTENTION_V1(80);
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+    // xpu::profiler_record(event_desc, event);  // Uncomment when needed
+#else
+    ::xpu::profiler_record("paged attn v1", event);
+#endif
+      break;
+    case 96:
+      LAUNCH_PAGED_ATTENTION_V1(96);
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+    // xpu::profiler_record(event_desc, event);  // Uncomment when needed
+#else
+    ::xpu::profiler_record("paged attn v1", event);
+#endif
+      break;
+    case 112:
+      LAUNCH_PAGED_ATTENTION_V1(112);
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+    // xpu::profiler_record(event_desc, event);  // Uncomment when needed
+#else
+    ::xpu::profiler_record("paged attn v1", event);
+#endif
+      break;
+    case 128:
+      LAUNCH_PAGED_ATTENTION_V1(128);
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+    // xpu::profiler_record(event_desc, event);  // Uncomment when needed
+#else
+    ::xpu::profiler_record("paged attn v1", event);
+#endif
+      break;
+    case 256:
+      LAUNCH_PAGED_ATTENTION_V1(256);
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+    // xpu::profiler_record(event_desc, event);  // Uncomment when needed
+#else
+    ::xpu::profiler_record("paged attn v1", event);
+#endif
+      break;
+    default:
+      TORCH_CHECK(false, "Unsupported head size: ", head_size);
+      break;
+  }
+  // queue.wait();
+}
+
+#define CALL_KERNEL_LAUNCHER(T, BLOCK_SIZE)                  \
+  vllm::paged_attention_xpu_v1_impl_launcher<T, BLOCK_SIZE>( \
+      out,                                                   \
+      query,                                                 \
+      key_cache,                                             \
+      value_cache,                                           \
+      num_kv_heads,                                          \
+      scale,                                                 \
+      block_tables,                                          \
+      context_lens,                                          \
+      max_context_len,                                       \
+      alibi_slopes,                                          \
+      attn_logit_softcapping);
+
+#define CALL_KERNEL_LAUNCHER_BLOCK_SIZE(T)                        \
+  switch (block_size) {                                           \
+    case 8:                                                      \
+      CALL_KERNEL_LAUNCHER(T, 8);                                \
+      break;                                                      \
+    case 16:                                                      \
+      CALL_KERNEL_LAUNCHER(T, 16);                                \
+      break;                                                      \
+    case 32:                                                      \
+      CALL_KERNEL_LAUNCHER(T, 32);                                \
+      break;                                                      \
+    case 64:                                                      \
+      CALL_KERNEL_LAUNCHER(T, 64);                                \
+      break;                                                      \
+    default:                                                      \
+      TORCH_CHECK(false, "Unsupported block size: ", block_size); \
+      break;                                                      \
+  }
+
+// Grid: (num_heads, num_seqs).
+template <
+    typename scalar_t,
+    int HEAD_SIZE,
+    int NUM_THREADS,
+    int PARTITION_SIZE>
+void paged_attention_v2_reduce_kernel(
+    scalar_t* __restrict__ out, // [num_seqs, num_heads, head_size]
+    const float* __restrict__ exp_sums, // [num_seqs, num_heads,
+                                        // max_num_partitions]
+    const float* __restrict__ max_logits, // [num_seqs, num_heads,
+                                          // max_num_partitions]
+    const scalar_t* __restrict__ tmp_out, // [num_seqs, num_heads,
+                                          // max_num_partitions, head_size]
+    const int* __restrict__ context_lens, // [num_seqs]
+    const int max_num_partitions,
+    const sycl::nd_item<3>& item_ct1,
+    uint8_t* dpct_local,
+    float* red_smem) {
+  const int num_heads = item_ct1.get_group_range(2);
+  const int head_idx = item_ct1.get_group(2);
+  const int seq_idx = item_ct1.get_group(1);
+  const int context_len = context_lens[seq_idx];
+  const int num_partitions = DIVIDE_ROUND_UP(context_len, PARTITION_SIZE);
+  if (num_partitions == 1) {
+    // No need to reduce. Only copy tmp_out to out.
+    scalar_t* out_ptr =
+        out + seq_idx * num_heads * HEAD_SIZE + head_idx * HEAD_SIZE;
+    const scalar_t* tmp_out_ptr = tmp_out +
+        seq_idx * num_heads * max_num_partitions * HEAD_SIZE +
+        head_idx * max_num_partitions * HEAD_SIZE;
+    for (int i = item_ct1.get_local_id(2); i < HEAD_SIZE;
+         i += item_ct1.get_local_range(2)) {
+      out_ptr[i] = tmp_out_ptr[i];
+    }
+    // Terminate the thread block.
+    return;
+  }
+
+  constexpr int NUM_WARPS = NUM_THREADS / WARP_SIZE;
+  const int warp_idx = item_ct1.get_local_id(2) / WARP_SIZE;
+  const int lane = item_ct1.get_local_id(2) % WARP_SIZE;
+
+  // Size: 2 * num_partitions.
+  auto shared_mem = (char*)dpct_local;
+  // Workspace for reduction.
+
+  // Load max logits to shared memory.
+  float* shared_max_logits = reinterpret_cast<float*>(shared_mem);
+  const float* max_logits_ptr = max_logits +
+      seq_idx * num_heads * max_num_partitions + head_idx * max_num_partitions;
+  float max_logit = -FLT_MAX;
+  for (int i = item_ct1.get_local_id(2); i < num_partitions;
+       i += item_ct1.get_local_range(2)) {
+    const float l = max_logits_ptr[i];
+    shared_max_logits[i] = l;
+    max_logit = sycl::fmax(max_logit, (float)l);
+  }
+  
+  item_ct1.barrier(sycl::access::fence_space::local_space);
+
+  // Get the global max logit.
+  // Reduce within the warp.
+#pragma unroll
+  for (int mask = WARP_SIZE / 2; mask >= 1; mask /= 2) {
+    
+    /*
+    DPCT1096:45: The right-most dimension of the work-group used in the SYCL
+    kernel that calls this function may be less than "32". The function
+    "dpct::permute_sub_group_by_xor" may return an unexpected result on the CPU
+    device. Modify the size of the work-group to ensure that the value of the
+    right-most dimension is a multiple of "32".
+    */
+    max_logit = sycl::fmax(
+        max_logit,
+        dpct::permute_sub_group_by_xor(
+            item_ct1.get_sub_group(), max_logit, mask));
+  }
+  if (lane == 0) {
+    red_smem[warp_idx] = max_logit;
+  }
+  
+  item_ct1.barrier(sycl::access::fence_space::local_space);
+  // Reduce across warps.
+  max_logit = lane < NUM_WARPS ? red_smem[lane] : -FLT_MAX;
+#pragma unroll
+  for (int mask = NUM_WARPS / 2; mask >= 1; mask /= 2) {
+    
+    /*
+    DPCT1096:46: The right-most dimension of the work-group used in the SYCL
+    kernel that calls this function may be less than "32". The function
+    "dpct::permute_sub_group_by_xor" may return an unexpected result on the CPU
+    device. Modify the size of the work-group to ensure that the value of the
+    right-most dimension is a multiple of "32".
+    */
+    max_logit = sycl::fmax(
+        max_logit,
+        dpct::permute_sub_group_by_xor(
+            item_ct1.get_sub_group(), max_logit, mask));
+  }
+  // Broadcast the max value to all threads.
+  
+  /*
+  DPCT1096:47: The right-most dimension of the work-group used in the SYCL
+  kernel that calls this function may be less than "32". The function
+  "dpct::select_from_sub_group" may return an unexpected result on the CPU
+  device. Modify the size of the work-group to ensure that the value of the
+  right-most dimension is a multiple of "32".
+  */
+  max_logit = dpct::select_from_sub_group(
+      item_ct1.get_sub_group(), max_logit, 0);
+
+  // Load rescaled exp sums to shared memory.
+  float* shared_exp_sums =
+      reinterpret_cast<float*>(shared_mem + sizeof(float) * num_partitions);
+  const float* exp_sums_ptr = exp_sums +
+      seq_idx * num_heads * max_num_partitions + head_idx * max_num_partitions;
+  float global_exp_sum = 0.0f;
+  for (int i = item_ct1.get_local_id(2); i < num_partitions;
+       i += item_ct1.get_local_range(2)) {
+    float l = shared_max_logits[i];
+    float rescaled_exp_sum = exp_sums_ptr[i] * sycl::exp(l - max_logit);
+    global_exp_sum += rescaled_exp_sum;
+    shared_exp_sums[i] = rescaled_exp_sum;
+  }
+  
+  item_ct1.barrier(sycl::access::fence_space::local_space);
+  global_exp_sum =
+      block_sum<NUM_WARPS>(&red_smem[NUM_WARPS], global_exp_sum, item_ct1);
+  const float inv_global_exp_sum = 1.0f / (global_exp_sum + 1e-6f);
+
+  // Aggregate tmp_out to out.
+  const scalar_t* tmp_out_ptr = tmp_out +
+      seq_idx * num_heads * max_num_partitions * HEAD_SIZE +
+      head_idx * max_num_partitions * HEAD_SIZE;
+  scalar_t* out_ptr =
+      out + seq_idx * num_heads * HEAD_SIZE + head_idx * HEAD_SIZE;
+#pragma unroll
+  for (int i = item_ct1.get_local_id(2); i < HEAD_SIZE; i += NUM_THREADS) {
+    float acc = 0.0f;
+    for (int j = 0; j < num_partitions; ++j) {
+      acc += to_float(tmp_out_ptr[j * HEAD_SIZE + i]) * shared_exp_sums[j] *
+          inv_global_exp_sum;
+    }
+    from_float(out_ptr[i], acc);
+  }
+}
+
+// Grid: (num_heads, num_seqs, max_num_partitions).
+template <
+    typename scalar_t,
+    typename Q_Vec_t,
+    int HEAD_SIZE,
+    int BLOCK_SIZE,
+    int NUM_THREADS,
+    int VEC_SIZE,
+    int PARTITION_SIZE>
+void paged_attention_v2_kernel(
+    float* __restrict__ exp_sums, // [num_seqs, num_heads, max_num_partitions]
+    float* __restrict__ max_logits, // [num_seqs, num_heads, max_num_partitions]
+    scalar_t* __restrict__ tmp_out, // [num_seqs, num_heads, max_num_partitions,
+                                    // head_size]
+    const scalar_t* __restrict__ q, // [num_seqs, num_heads, head_size]
+    const scalar_t* __restrict__ k_cache, // [num_blocks, num_kv_heads,
+                                          // head_size/x, block_size, x]
+    const scalar_t* __restrict__ v_cache, // [num_blocks, num_kv_heads,
+                                          // head_size, block_size]
+    const int num_kv_heads, // [num_heads]
+    const float scale,
+    const int* __restrict__ block_tables, // [num_seqs, max_num_blocks_per_seq]
+    const int* __restrict__ context_lens, // [num_seqs]
+    const int max_num_blocks_per_seq,
+    const float* __restrict__ alibi_slopes, // [num_heads]
+    const int q_stride,
+    const int kv_block_stride,
+    const int kv_head_stride,
+    const float attn_logit_softcapping,
+    const sycl::nd_item<3>& item_ct1,
+    uint8_t* dpct_local,
+    Q_Vec_t* q_vecs,
+    float* red_smem) {
+  paged_attention_kernel<
+      scalar_t,
+      Q_Vec_t,
+      HEAD_SIZE,
+      BLOCK_SIZE,
+      NUM_THREADS,
+      VEC_SIZE,
+      PARTITION_SIZE>(
+      exp_sums,
+      max_logits,
+      tmp_out,
+      q,
+      k_cache,
+      v_cache,
+      num_kv_heads,
+      scale,
+      block_tables,
+      context_lens,
+      max_num_blocks_per_seq,
+      alibi_slopes,
+      q_stride,
+      kv_block_stride,
+      kv_head_stride,
+      attn_logit_softcapping,
+      item_ct1,
+      dpct_local,
+      q_vecs,
+      red_smem);
+}
+
+#define LAUNCH_PAGED_ATTENTION_V2_FIRST_HALF(HEAD_SIZE)                     \
+  event = queue.submit([&](sycl::handler& cgh) {                            \
+    sycl::local_accessor<uint8_t, 1> dpct_local_acc_ct1(                    \
+        sycl::range<1>(shared_mem_size), cgh);                              \
+    sycl::local_accessor<Q_Vec, 1> q_vecs_acc_ct1(                          \
+        sycl::range<1>(THREAD_GROUP_SIZE * num_vecs_per_thread), cgh);      \
+    sycl::local_accessor<float, 1> red_smem_acc_ct1(                        \
+        sycl::range<1>(2 * NUM_WARPS), cgh);                                \
+                                                                            \
+    auto exp_sums_ptr_ct0 = exp_sums_ptr;                                   \
+    auto max_logits_ptr_ct1 = max_logits_ptr;                               \
+    auto tmp_out_ptr_ct2 = tmp_out_ptr;                                     \
+    auto query_ptr_ct3 = query_ptr;                                         \
+    auto key_cache_ptr_ct4 = key_cache_ptr;                                 \
+    auto value_cache_ptr_ct5 = value_cache_ptr;                             \
+    auto scale_ct7 = scale;                                                 \
+    auto block_tables_ptr_ct8 = block_tables_ptr;                           \
+    auto context_lens_ptr_ct9 = context_lens_ptr;                           \
+    auto max_num_blocks_per_seq_ct10 = max_num_blocks_per_seq;              \
+    auto alibi_slopes_ptr_ct11 = alibi_slopes_ptr;                          \
+    auto q_stride_ct12 = q_stride;                                          \
+    auto kv_block_stride_ct13 = kv_block_stride;                            \
+    auto kv_head_stride_ct14 = kv_head_stride;                              \
+    auto attn_logit_softcapping_ct15 = attn_logit_softcapping;              \
+                                                                            \
+    cgh.parallel_for(                                                       \
+        sycl::nd_range<3>(grid * block, block),                             \
+        [=](sycl::nd_item<3> item_ct1) [[intel::reqd_sub_group_size(32)]] { \
+          vllm::paged_attention_v2_kernel<                                  \
+              sycl_t,                                                       \
+              Q_Vec,                                                        \
+              HEAD_SIZE,                                                    \
+              BLOCK_SIZE,                                                   \
+              NUM_THREADS,                                                  \
+              VEC_SIZE,                                                     \
+              PARTITION_SIZE>(                                              \
+              exp_sums_ptr_ct0,                                             \
+              max_logits_ptr_ct1,                                           \
+              tmp_out_ptr_ct2,                                              \
+              query_ptr_ct3,                                                \
+              key_cache_ptr_ct4,                                            \
+              value_cache_ptr_ct5,                                          \
+              num_kv_heads,                                                 \
+              scale_ct7,                                                    \
+              block_tables_ptr_ct8,                                         \
+              context_lens_ptr_ct9,                                         \
+              max_num_blocks_per_seq_ct10,                                  \
+              alibi_slopes_ptr_ct11,                                        \
+              q_stride_ct12,                                                \
+              kv_block_stride_ct13,                                         \
+              kv_head_stride_ct14,                                          \
+              attn_logit_softcapping_ct15,                                  \
+              item_ct1,                                                     \
+              dpct_local_acc_ct1.get_pointer(),                             \
+              q_vecs_acc_ct1.get_pointer(),                                 \
+              red_smem_acc_ct1.get_pointer());                              \
+        });                                                                 \
+  });
+
+#define LAUNCH_PAGED_ATTENTION_V2_SECOND_HALF(HEAD_SIZE)                    \
+  event2 = queue.submit([&](sycl::handler& cgh) {                           \
+    sycl::local_accessor<uint8_t, 1> dpct_local_acc_ct1(                    \
+        sycl::range<1>(reduce_shared_mem_size), cgh);                       \
+    sycl::local_accessor<float, 1> red_smem_acc_ct1(                        \
+        sycl::range<1>(2 * NUM_WARPS), cgh);                                \
+                                                                            \
+    auto out_ptr_ct0 = out_ptr;                                             \
+    auto exp_sums_ptr_ct1 = exp_sums_ptr;                                   \
+    auto max_logits_ptr_ct2 = max_logits_ptr;                               \
+    auto tmp_out_ptr_ct3 = tmp_out_ptr;                                     \
+    auto context_lens_ptr_ct4 = context_lens_ptr;                           \
+    auto max_num_partitions_ct5 = max_num_partitions;                       \
+                                                                            \
+    cgh.parallel_for(                                                       \
+        sycl::nd_range<3>(reduce_grid * block, block),                      \
+        [=](sycl::nd_item<3> item_ct1) [[intel::reqd_sub_group_size(32)]] { \
+          vllm::paged_attention_v2_reduce_kernel<                           \
+              sycl_t,                                                       \
+              HEAD_SIZE,                                                    \
+              NUM_THREADS,                                                  \
+              PARTITION_SIZE>(                                              \
+              out_ptr_ct0,                                                  \
+              exp_sums_ptr_ct1,                                             \
+              max_logits_ptr_ct2,                                           \
+              tmp_out_ptr_ct3,                                              \
+              context_lens_ptr_ct4,                                         \
+              max_num_partitions_ct5,                                       \
+              item_ct1,                                                     \
+              dpct_local_acc_ct1.get_pointer(),                             \
+              red_smem_acc_ct1.get_pointer());                              \
+        });                                                                 \
+  });
+
+template <
+    typename T,
+    int BLOCK_SIZE,
+    int NUM_THREADS = 512,
+    int PARTITION_SIZE = 512>
+void paged_attention_v2_launcher(
+    torch::Tensor& out,
+    torch::Tensor& exp_sums,
+    torch::Tensor& max_logits,
+    torch::Tensor& tmp_out,
+    torch::Tensor& query,
+    torch::Tensor& key_cache,
+    torch::Tensor& value_cache,
+    int num_kv_heads,
+    float scale,
+    torch::Tensor& block_tables,
+    torch::Tensor& context_lens,
+    int max_context_len,
+    const c10::optional<torch::Tensor>& alibi_slopes,
+    const float attn_logit_softcapping) {
+  int num_seqs = query.size(0);
+  int num_heads = query.size(1);
+  int head_size = query.size(2);
+  int max_num_blocks_per_seq = block_tables.size(1);
+  int q_stride = query.stride(0);
+  int kv_block_stride = key_cache.stride(0);
+  int kv_head_stride = key_cache.stride(1);
+
+  constexpr int THREAD_GROUP_SIZE = MAX(WARP_SIZE / BLOCK_SIZE, 1);
+  assert(head_size % THREAD_GROUP_SIZE == 0);
+  constexpr int VEC_SIZE = MAX(16 / (THREAD_GROUP_SIZE * sizeof(T)), 1);
+  using sycl_t = vllm::xpu::SyclTypeTrait<T>::Type;
+  using Q_Vec = typename Vec<sycl_t, VEC_SIZE>::Type;
+
+  int num_vecs_per_thread = head_size / THREAD_GROUP_SIZE / VEC_SIZE;
+  assert(head_size % THREAD_GROUP_SIZE == 0);
+
+  // NOTE: alibi_slopes is optional.
+  const float* alibi_slopes_ptr = alibi_slopes
+      ? reinterpret_cast<const float*>(alibi_slopes.value().data_ptr())
+      : nullptr;
+
+  sycl_t* out_ptr = reinterpret_cast<sycl_t*>(out.data_ptr());
+  float* exp_sums_ptr = reinterpret_cast<float*>(exp_sums.data_ptr());
+  float* max_logits_ptr = reinterpret_cast<float*>(max_logits.data_ptr());
+  sycl_t* tmp_out_ptr = reinterpret_cast<sycl_t*>(tmp_out.data_ptr());
+  sycl_t* query_ptr = reinterpret_cast<sycl_t*>(query.data_ptr());
+  sycl_t* key_cache_ptr = reinterpret_cast<sycl_t*>(key_cache.data_ptr());
+  sycl_t* value_cache_ptr = reinterpret_cast<sycl_t*>(value_cache.data_ptr());
+  int* block_tables_ptr = block_tables.data_ptr<int>();
+  int* context_lens_ptr = context_lens.data_ptr<int>();
+
+  constexpr int NUM_WARPS = NUM_THREADS / WARP_SIZE;
+  int max_num_partitions = DIVIDE_ROUND_UP(max_context_len, PARTITION_SIZE);
+  
+  int logits_size = PARTITION_SIZE * sizeof(float);
+  int outputs_size = (NUM_WARPS / 2) * head_size * sizeof(float);
+
+  // For paged attention v2 kernel.
+  sycl::range<3> grid(max_num_partitions, num_seqs, num_heads);
+  int shared_mem_size = std::max(logits_size, outputs_size);
+  // For paged attention v2 reduce kernel.
+  sycl::range<3> reduce_grid(1, num_seqs, num_heads);
+  
+  int reduce_shared_mem_size = 2 * max_num_partitions * sizeof(float);
+
+  sycl::range<3> block(1, 1, NUM_THREADS);
+  sycl::queue& queue = vllm::xpu::vllmGetQueue();
+  sycl::event event;
+  sycl::event event2;
+  switch (head_size) {
+    // NOTE(woosuk): To reduce the compilation time, we only compile for the
+    // head sizes that we use in the model. However, we can easily extend this
+    // to support any head size which is a multiple of 16.
+    case 64:
+      LAUNCH_PAGED_ATTENTION_V2_FIRST_HALF(64);
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+    // xpu::profiler_record(event_desc, event);  // Uncomment when needed
+#else
+    ::xpu::profiler_record("paged attn v2", event);
+#endif
+      LAUNCH_PAGED_ATTENTION_V2_SECOND_HALF(64);
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+    // xpu::profiler_record(event_desc, event);  // Uncomment when needed
+#else
+    ::xpu::profiler_record("paged attn v2", event2);
+#endif
+      break;
+    case 80:
+      LAUNCH_PAGED_ATTENTION_V2_FIRST_HALF(80);
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+    // xpu::profiler_record(event_desc, event);  // Uncomment when needed
+#else
+    ::xpu::profiler_record("paged attn v2", event);
+#endif
+      LAUNCH_PAGED_ATTENTION_V2_SECOND_HALF(80);
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+    // xpu::profiler_record(event_desc, event);  // Uncomment when needed
+#else
+    ::xpu::profiler_record("paged attn v2", event2);
+#endif
+      break;
+    case 96:
+      LAUNCH_PAGED_ATTENTION_V2_FIRST_HALF(96);
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+    // xpu::profiler_record(event_desc, event);  // Uncomment when needed
+#else
+    ::xpu::profiler_record("paged attn v2", event);
+#endif
+      LAUNCH_PAGED_ATTENTION_V2_SECOND_HALF(96);
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+    // xpu::profiler_record(event_desc, event);  // Uncomment when needed
+#else
+    ::xpu::profiler_record("paged attn v2", event2);
+#endif
+      break;
+    case 112:
+      LAUNCH_PAGED_ATTENTION_V2_FIRST_HALF(112);
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+    // xpu::profiler_record(event_desc, event);  // Uncomment when needed
+#else
+    ::xpu::profiler_record("paged attn v2", event);
+#endif
+      LAUNCH_PAGED_ATTENTION_V2_SECOND_HALF(112);
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+    // xpu::profiler_record(event_desc, event);  // Uncomment when needed
+#else
+    ::xpu::profiler_record("paged attn v2", event2);
+#endif
+      break;
+    case 128:
+      LAUNCH_PAGED_ATTENTION_V2_FIRST_HALF(128);
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+    // xpu::profiler_record(event_desc, event);  // Uncomment when needed
+#else
+    ::xpu::profiler_record("paged attn v2", event);
+#endif
+      LAUNCH_PAGED_ATTENTION_V2_SECOND_HALF(128);
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+    // xpu::profiler_record(event_desc, event);  // Uncomment when needed
+#else
+    ::xpu::profiler_record("paged attn v2", event2);
+#endif
+      break;
+    case 256:
+      LAUNCH_PAGED_ATTENTION_V2_FIRST_HALF(256);
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+    // xpu::profiler_record(event_desc, event);  // Uncomment when needed
+#else
+    ::xpu::profiler_record("paged attn v2", event);
+#endif
+      LAUNCH_PAGED_ATTENTION_V2_SECOND_HALF(256);
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+    // xpu::profiler_record(event_desc, event);  // Uncomment when needed
+#else
+    ::xpu::profiler_record("paged attn v2", event2);
+#endif
+      break;
+    default:
+      TORCH_CHECK(false, "Unsupported head size: ", head_size);
+      break;
+  }
+}
+
+#define CALL_V2_LAUNCHER(T, BLOCK_SIZE)             \
+  vllm::paged_attention_v2_launcher<T, BLOCK_SIZE>( \
+      out,                                          \
+      exp_sums,                                     \
+      max_logits,                                   \
+      tmp_out,                                      \
+      query,                                        \
+      key_cache,                                    \
+      value_cache,                                  \
+      num_kv_heads,                                 \
+      scale,                                        \
+      block_tables,                                 \
+      context_lens,                                 \
+      max_context_len,                              \
+      alibi_slopes,                                 \
+      attn_logit_softcapping);
+
+#define CALL_V2_LAUNCHER_BLOCK_SIZE(T)                            \
+  switch (block_size) {                                           \
+    case 8:                                                       \
+      CALL_V2_LAUNCHER(T, 8);                                     \
+      break;                                                      \
+    case 16:                                                      \
+      CALL_V2_LAUNCHER(T, 16);                                    \
+      break;                                                      \
+    case 32:                                                      \
+      CALL_V2_LAUNCHER(T, 32);                                    \
+      break;                                                      \
+    case 64:                                                      \
+      CALL_V2_LAUNCHER(T, 64);                                    \
+      break;                                                      \
+    default:                                                      \
+      TORCH_CHECK(false, "Unsupported block size: ", block_size); \
+      break;                                                      \
+  }
+
+} // namespace vllm
+
+void paged_attention_v1(
+    torch::Tensor& out,
+    torch::Tensor& query,
+    torch::Tensor& key_cache,
+    torch::Tensor& value_cache,
+    int num_kv_heads,
+    float scale,
+    torch::Tensor& block_tables,
+    torch::Tensor& context_lens,
+    int block_size,
+    int max_context_len,
+    const c10::optional<torch::Tensor>& alibi_slopes,
+    const std::string& kv_cache_dtype,
+    const float kv_scale,
+    const float attn_logit_softcapping) {
+  VLLM_XPU_DISPATCH_FLOATING_TYPES_FLOAT_ONLY(
+      query.scalar_type(), "paged_attention_xpu_v1_impl", [&] {
+        CALL_KERNEL_LAUNCHER_BLOCK_SIZE(scalar_t);
+      });
+}
+
+void paged_attention_v2(
+    torch::Tensor& out,
+    torch::Tensor& exp_sums,
+    torch::Tensor& max_logits,
+    torch::Tensor& tmp_out,
+    torch::Tensor& query,
+    torch::Tensor& key_cache,
+    torch::Tensor& value_cache,
+    int num_kv_heads,
+    float scale,
+    torch::Tensor& block_tables,
+    torch::Tensor& context_lens,
+    int block_size,
+    int max_context_len,
+    const c10::optional<torch::Tensor>& alibi_slopes,
+    const std::string& kv_cache_dtype,
+    const float kv_scale,
+    const float attn_logit_softcapping) {
+  VLLM_XPU_DISPATCH_FLOATING_TYPES_FLOAT_ONLY(
+      query.scalar_type(), "paged_attention_xpu_v2_impl", [&] {
+        CALL_V2_LAUNCHER_BLOCK_SIZE(scalar_t);
+      });
+}
+
+torch::Tensor context_attention_forward_v2(
+    torch::Tensor query,  // [num_tokens, num_kv_head, head_dim]
+    torch::Tensor key,    // [num_tokens, num_kv_heads * head_size]
+    torch::Tensor value,  // [num_tokens, num_kv_heads * head_size]
+    torch::Tensor block_tables, torch::Tensor query_start_loc,
+    torch::Tensor seq_lens, torch::Tensor context_lens, int max_input_length,
+    int max_context_length, int max_q_length) {
+  // Currently, only support fp16 here
+  int64_t num_tokens = query.size(0);
+  int64_t num_heads = query.size(1);
+  int64_t head_dim = query.size(2);
+  int64_t batch_size = seq_lens.size(0);
+  int num_kv_heads = value.size(1);
+
+  int key_dimension = key.dim();
+  auto output = at::empty({query.size(0), query.size(1), query.size(2)},
+                          at::device(query.device()).dtype(query.dtype()));
+
+  assert(key_dimension == 5);
+  assert(query.scalar_type() == key.scalar_type() &&
+         query.scalar_type() == value.scalar_type());
+  assert(head_dim == 128);
+  assert(query.scalar_type() == at::ScalarType::Half);
+
+  int query_stride_token = query.stride(0);
+  int query_stride_head = query.stride(1);
+  int query_stride_dim = query.stride(2);
+  const float attn_scale = 1 / std::sqrt((float)head_dim);
+
+  assert(num_heads % num_kv_heads == 0);
+  int num_queries_per_kv = num_heads / num_kv_heads;
+
+
+  // key: num_blocks, num_kv_heads, head_size // x, num_blocks, x)
+  // value: [num_blocks, num_kv_heads, head_size, block_dim]
+  int block_size = value.size(3);
+  // Currently, only block_size 16 is supported...
+  assert(block_size == 16);
+  int x = key.size(4);
+  int block_table_stride_bsz = block_tables.stride(0);
+  int block_table_stride_seq = block_tables.stride(1);
+  int k_cache_stride_token = key.stride(0);
+  int k_cache_stride_head = key.stride(1);
+  int k_cache_stride_head_dim = key.stride(2);
+  int k_cache_stride_block = key.stride(3);
+  int k_cache_stride_x = key.stride(4);
+
+  int v_cache_stride_token = value.stride(0);
+  int v_cache_stride_head = value.stride(1);
+  int v_cache_stride_head_dim = value.stride(2);
+  int v_cache_stride_block = value.stride(3);
+  switch(head_dim) {
+    case 128:
+      vllm::context_attention_kernel_v2<sycl::half, 32, 128>(
+        query.data_ptr(), key.data_ptr(), value.data_ptr(),
+        block_tables.data_ptr(), attn_scale, query_start_loc.data_ptr(),
+        seq_lens.data_ptr(), context_lens.data_ptr(), block_size, x,
+        output.data_ptr(), block_table_stride_bsz, block_table_stride_seq,
+        query_stride_token, query_stride_head, query_stride_dim,
+        k_cache_stride_token, k_cache_stride_head, k_cache_stride_head_dim,
+        k_cache_stride_block, k_cache_stride_x, v_cache_stride_token,
+        v_cache_stride_head, v_cache_stride_head_dim, v_cache_stride_block,
+        output.stride(0), output.stride(1), num_queries_per_kv,
+        max_input_length, batch_size, num_heads, query.size(0),
+        max_context_length, max_q_length);
+      break;
+    case 64:
+      vllm::context_attention_kernel_v2<sycl::half, 32, 64>(
+        query.data_ptr(), key.data_ptr(), value.data_ptr(),
+        block_tables.data_ptr(), attn_scale, query_start_loc.data_ptr(),
+        seq_lens.data_ptr(), context_lens.data_ptr(), block_size, x,
+        output.data_ptr(), block_table_stride_bsz, block_table_stride_seq,
+        query_stride_token, query_stride_head, query_stride_dim,
+        k_cache_stride_token, k_cache_stride_head, k_cache_stride_head_dim,
+        k_cache_stride_block, k_cache_stride_x, v_cache_stride_token,
+        v_cache_stride_head, v_cache_stride_head_dim, v_cache_stride_block,
+        output.stride(0), output.stride(1), num_queries_per_kv,
+        max_input_length, batch_size, num_heads, query.size(0),
+        max_context_length, max_q_length);
+      break;
+    case 80:
+      vllm::context_attention_kernel_v2<sycl::half, 32, 80>(
+        query.data_ptr(), key.data_ptr(), value.data_ptr(),
+        block_tables.data_ptr(), attn_scale, query_start_loc.data_ptr(),
+        seq_lens.data_ptr(), context_lens.data_ptr(), block_size, x,
+        output.data_ptr(), block_table_stride_bsz, block_table_stride_seq,
+        query_stride_token, query_stride_head, query_stride_dim,
+        k_cache_stride_token, k_cache_stride_head, k_cache_stride_head_dim,
+        k_cache_stride_block, k_cache_stride_x, v_cache_stride_token,
+        v_cache_stride_head, v_cache_stride_head_dim, v_cache_stride_block,
+        output.stride(0), output.stride(1), num_queries_per_kv,
+        max_input_length, batch_size, num_heads, query.size(0),
+        max_context_length, max_q_length);
+      break;
+    case 96:
+      vllm::context_attention_kernel_v2<sycl::half, 32, 96>(
+        query.data_ptr(), key.data_ptr(), value.data_ptr(),
+        block_tables.data_ptr(), attn_scale, query_start_loc.data_ptr(),
+        seq_lens.data_ptr(), context_lens.data_ptr(), block_size, x,
+        output.data_ptr(), block_table_stride_bsz, block_table_stride_seq,
+        query_stride_token, query_stride_head, query_stride_dim,
+        k_cache_stride_token, k_cache_stride_head, k_cache_stride_head_dim,
+        k_cache_stride_block, k_cache_stride_x, v_cache_stride_token,
+        v_cache_stride_head, v_cache_stride_head_dim, v_cache_stride_block,
+        output.stride(0), output.stride(1), num_queries_per_kv,
+        max_input_length, batch_size, num_heads, query.size(0),
+        max_context_length, max_q_length);
+      break;
+    default: throw std::runtime_error("unsupported head_dim");
+  }
+    return output;
+}
+
+torch::Tensor context_attention_forward_v1(
+    torch::Tensor query,  // [num_tokens, num_kv_head, head_dim]
+    torch::Tensor key,    // [num_tokens, num_kv_heads * head_size]
+    torch::Tensor value,  // [num_tokens, num_kv_heads * head_size]
+    torch::Tensor block_tables, torch::Tensor query_start_loc,
+    torch::Tensor seq_lens, torch::Tensor context_lens, int max_input_length,
+    int max_context_length) {
+  // Currently, only support fp16
+  int64_t num_tokens = query.size(0);
+  int64_t num_heads = query.size(1);
+  int64_t head_dim = query.size(2);
+  int64_t batch_size = seq_lens.size(0);
+  int num_kv_heads = value.size(1);
+
+  int key_dimension = key.dim();
+  auto output = at::empty({query.size(0), query.size(1), query.size(2)},
+                          at::device(query.device()).dtype(query.dtype()));
+
+  // key should be in shape:
+  // 1. [num_blocks, num_heads, block_size, head_dim]
+  // 2. [num_blocks, num_heads, head_dim / x, block_size, x]
+  assert(key_dimension == 4 or key_dimension == 5);
+  assert(query.scalar_type() == key.scalar_type() &&
+         query.scalar_type() == value.scalar_type());
+  assert(query.scalar_type() == at::ScalarType::Half);
+
+  int query_stride_token = query.stride(0);
+  int query_stride_head = query.stride(1);
+  int query_stride_dim = query.stride(2);
+  const float attn_scale = 1 / std::sqrt((float)head_dim);
+
+  assert(num_heads % num_kv_heads == 0);
+  int num_queries_per_kv = num_heads / num_kv_heads;
+  int block_table_stride_bsz = block_tables.stride(0);
+  int block_table_stride_seq = block_tables.stride(1);
+  if (key_dimension == 4) {
+    // key/value: num_blocks, num_kv_heads, num_blocks, head_dim)
+    int block_size = value.size(2);
+    int k_cache_stride_0 = key.stride(0);
+    int k_cache_stride_1 = key.stride(1);
+    int k_cache_stride_2 = key.stride(2);
+    int k_cache_stride_3 = key.stride(3);
+
+    int v_cache_stride_0 = value.stride(0);
+    int v_cache_stride_1 = value.stride(1);
+    int v_cache_stride_2 = value.stride(2);
+    int v_cache_stride_3 = value.stride(3);
+    switch (head_dim) {
+      case 128:
+        vllm::context_attention_kernel_v1_reshaped<sycl::half, 32, 128>(
+            query.data_ptr(), key.data_ptr(), value.data_ptr(),
+            block_tables.data_ptr(), attn_scale, query_start_loc.data_ptr(),
+            seq_lens.data_ptr(), context_lens.data_ptr(), block_size,
+            output.data_ptr(), block_table_stride_bsz, block_table_stride_seq,
+            query_stride_token, query_stride_head, query_stride_dim,
+            k_cache_stride_0, k_cache_stride_1, k_cache_stride_2,
+            k_cache_stride_3, v_cache_stride_0, v_cache_stride_1,
+            v_cache_stride_2, v_cache_stride_3, output.stride(0),
+            output.stride(1), num_queries_per_kv, max_input_length, batch_size,
+            num_heads);
+        break;
+      case 64:
+        vllm::context_attention_kernel_v1_reshaped<sycl::half, 32, 64>(
+            query.data_ptr(), key.data_ptr(), value.data_ptr(),
+            block_tables.data_ptr(), attn_scale, query_start_loc.data_ptr(),
+            seq_lens.data_ptr(), context_lens.data_ptr(), block_size,
+            output.data_ptr(), block_table_stride_bsz, block_table_stride_seq,
+            query_stride_token, query_stride_head, query_stride_dim,
+            k_cache_stride_0, k_cache_stride_1, k_cache_stride_2,
+            k_cache_stride_3, v_cache_stride_0, v_cache_stride_1,
+            v_cache_stride_2, v_cache_stride_3, output.stride(0),
+            output.stride(1), num_queries_per_kv, max_input_length, batch_size,
+            num_heads);
+        break;
+      default:
+        throw std::runtime_error("unsupported head_dim");
+    }
+  } else {
+    int x = key.size(4);
+    int block_size = value.size(3);
+    int k_cache_stride_token = key.stride(0);
+    int k_cache_stride_head = key.stride(1);
+    int k_cache_stride_head_dim = key.stride(2);
+    int k_cache_stride_block = key.stride(3);
+    int k_cache_stride_x = key.stride(4);
+
+    int v_cache_stride_token = value.stride(0);
+    int v_cache_stride_head = value.stride(1);
+    int v_cache_stride_head_dim = value.stride(2);
+    int v_cache_stride_block = value.stride(3);
+    switch (head_dim) {
+      case 128:
+        vllm::context_attention_kernel_v1<sycl::half, 32, 128>(
+            query.data_ptr(), key.data_ptr(), value.data_ptr(),
+            block_tables.data_ptr(), attn_scale, query_start_loc.data_ptr(),
+            seq_lens.data_ptr(), context_lens.data_ptr(), block_size, x,
+            output.data_ptr(), block_table_stride_bsz, block_table_stride_seq,
+            query_stride_token, query_stride_head, query_stride_dim,
+            k_cache_stride_token, k_cache_stride_head, k_cache_stride_head_dim,
+            k_cache_stride_block, k_cache_stride_x, v_cache_stride_token,
+            v_cache_stride_head, v_cache_stride_head_dim, v_cache_stride_block,
+            output.stride(0), output.stride(1), num_queries_per_kv,
+            max_input_length, batch_size, num_heads);
+        break;
+      case 64:
+        vllm::context_attention_kernel_v1<sycl::half, 32, 64>(
+            query.data_ptr(), key.data_ptr(), value.data_ptr(),
+            block_tables.data_ptr(), attn_scale, query_start_loc.data_ptr(),
+            seq_lens.data_ptr(), context_lens.data_ptr(), block_size, x,
+            output.data_ptr(), block_table_stride_bsz, block_table_stride_seq,
+            query_stride_token, query_stride_head, query_stride_dim,
+            k_cache_stride_token, k_cache_stride_head, k_cache_stride_head_dim,
+            k_cache_stride_block, k_cache_stride_x, v_cache_stride_token,
+            v_cache_stride_head, v_cache_stride_head_dim, v_cache_stride_block,
+            output.stride(0), output.stride(1), num_queries_per_kv,
+            max_input_length, batch_size, num_heads);
+        break;
+      default:
+        throw std::runtime_error("unsupported head_dim");
+    }
+  }
+  return output;
+}
+
+template<typename IT, const int VS, const int HD>
+void gqa_1_kernel(
+    const void * query, // [num_seqs, num_heads, head_size]
+    const void * key,   // [num_blocks, num_kv_heads, head_size, block_size]
+    const void * value, // [num_blocks, num_kv_heads, head_size, block_size]
+    const void* block_tables, // [num_seqs, max_num_blocks_per_seq]
+    const void* context_lens, // [num_seqs]
+    void * o_a_s,
+    void * o_accs,
+    const int64_t query_bsz_stride,
+    const int64_t query_head_stride,
+    const int64_t kv_token_stride,
+    const int64_t kv_head_stride,
+    const int64_t kv_block_stride,
+    const int64_t block_table_stride_batch,
+    const int64_t o_a_s_bsz_stride,
+    const int64_t o_a_s_head_stride,
+    const int64_t o_accs_bsz_stride,
+    const int64_t o_accs_head_stride,
+    const float scale,
+    const int block_size,
+    const int bsz,
+    const int num_heads,
+    const int num_kv_heads,
+    const int block_num,
+    const at::Device & device
+) {
+    const int group_size = num_heads / num_kv_heads;
+    const int sub_rows = VS / group_size;
+    const int rem_rows = VS % group_size;
+
+    const float attn_scale = scale;
+
+    sycl::range<3> global_size(bsz, num_heads, block_num);
+    sycl::range<3> local_size(1, group_size, 1);
+
+    auto cgf = [&](sycl::handler& handle) {
+        handle.parallel_for(
+            sycl::nd_range<3>(global_size, local_size),
+            [=](sycl::nd_item<3> item) SYCL_ESIMD_KERNEL {
+                slm_init<VS * HD * sizeof(IT)>();
+
+                const int bsz_idx = item.get_global_id(0);
+                const int head_idx = item.get_global_id(1);
+                const int kv_head_idx = item.get_group(1);
+                const int tid = item.get_local_id(1);
+                const int vid = item.get_global_id(2);
+
+                const IT * query_head = (const IT *)query + bsz_idx * query_bsz_stride
+                                                          + head_idx * query_head_stride;
+                
+                IT * o_accs_head = (IT *)o_accs + bsz_idx * o_accs_bsz_stride
+                                                + head_idx * o_accs_head_stride;
+                float * o_a_s_head = (float *)o_a_s + bsz_idx * o_a_s_bsz_stride
+                                                    + head_idx * o_a_s_head_stride;
+
+                const int* block_tables_ptr = (const int*)block_tables;
+                const int* block_table =
+                    block_tables_ptr + bsz_idx * block_table_stride_batch;
+
+                const int* context_lens_ptr = (const int*)context_lens;
+                const int context_length = context_lens_ptr[bsz_idx];
+
+                simd<IT, HD> query_row = block_load<IT, HD>(query_head) * attn_scale;
+
+                // copy k_cache to slm
+                int start_row = std::min(vid * VS + tid * sub_rows + std::min(tid, rem_rows), context_length);
+                int end_row = std::min(start_row + sub_rows + (tid < rem_rows), context_length);
+                for (int r = start_row; r < end_row; ++r) {
+                    int which_block = r / block_size;
+                    int which_slot = r % block_size;
+                    int physical_block_number = block_table[which_block];
+
+                    const IT * key_head = (const IT *)key + physical_block_number * kv_token_stride +
+                      kv_head_idx * kv_head_stride +
+                      which_slot * kv_block_stride;
+
+                    simd<IT, HD> key_row = block_load<IT, HD>(key_head);
+                    slm_block_store<IT, HD>((r - vid * VS) * HD * sizeof(IT), key_row);
+                }
+                barrier();
+
+                simd<float, VS> attns = -sycl::detail::max_v<float>();
+                int row_num = (vid + 1) * VS > context_length ? context_length % VS : VS;
+                // q @ k
+                for (int r = 0; r < row_num; ++r) {
+                    simd<IT, HD> key_row = slm_block_load<IT, HD>(r * HD * sizeof(IT));
+                    float attn = sycl::ext::intel::esimd::detail::sum<float, IT, HD>(query_row * key_row);
+                    attns[r] = attn;
+                }
+
+                float max_attn = hmax<float, float, VS>(attns);
+                const simd<IT, VS> attn_exp = exp(attns - max_attn);
+                barrier();
+
+                // copy v_cache to slm
+                for (int r = start_row; r < end_row; ++r) {
+                    int which_block = r / block_size;
+                    int which_slot = r % block_size;
+                    int physical_block_number = block_table[which_block];
+
+                    const IT * value_head = (const IT *)value + physical_block_number * kv_token_stride +
+                      kv_head_idx * kv_head_stride +
+                      which_slot * kv_block_stride;
+
+                    simd<IT, HD> value_row = block_load<IT, HD>(value_head);
+                    slm_block_store<IT, HD>((r - vid * VS) * HD * sizeof(IT), value_row);
+                }
+                barrier();
+
+                // attn @ v
+                simd<IT, HD> accs = 0;
+                for (int r = 0; r < row_num; ++r) {
+                    simd<IT, HD> value_row = slm_block_load<IT, HD>(r * HD * sizeof(IT));
+                    accs = accs + value_row * attn_exp[r];
+                }
+
+                float softmax = sycl::ext::intel::esimd::detail::sum<float, float, VS>(attn_exp);
+
+                block_store<IT, HD>(o_accs_head + vid * HD, accs);
+                block_store<float, 1>(o_a_s_head + vid * 2, max_attn);
+                block_store<float, 1>(o_a_s_head + vid * 2 + 1, softmax);
+            }
+        );
+    };
+
+    utils::submit_kernel(cgf, device, "gqa kernel 1/2");
+}
+
+template<typename IT, const int GS, const int HD>
+void gqa_2_kernel(
+    void * o_a_s,
+    void * o_accs,
+    void * output,
+    const void* context_lens, // [num_seqs]
+    const int64_t o_a_s_bsz_stride,
+    const int64_t o_a_s_head_stride,
+    const int64_t o_accs_bsz_stride,
+    const int64_t o_accs_head_stride,
+    const int64_t output_bsz_stride,
+    const int64_t output_head_stride,
+    const int bsz,
+    const int num_heads,
+    const int row_block_num,
+    const at::Device & device
+) {
+    constexpr int SUB_HD = 8;
+    static_assert(HD % SUB_HD == 0);
+    static_assert(HD / SUB_HD <= GS);
+
+    const int sub_rows = row_block_num / GS;
+    const int rem_rows = row_block_num % GS;
+
+    constexpr int accs_slm_offset = 0;
+    constexpr int attn_slm_offset = GS * HD * sizeof(float);
+    constexpr int softmax_slm_offset = attn_slm_offset + GS * sizeof(float);
+
+    sycl::range<3> global_size(bsz, num_heads, GS);
+    sycl::range<3> local_size(1, 1, GS);
+
+    auto cgf = [&](sycl::handler& handle) {
+        handle.parallel_for(
+            sycl::nd_range<3>(global_size, local_size),
+            [=](sycl::nd_item<3> item) SYCL_ESIMD_KERNEL {
+                slm_init<GS * HD * sizeof(float) + GS * 2 * sizeof(float)>();
+
+                const int bsz_idx = item.get_global_id(0);
+                const int head_idx = item.get_global_id(1);
+                const int tid = item.get_global_id(2);
+
+                const int* context_lens_ptr = (const int*)context_lens;
+                const int context_length = context_lens_ptr[bsz_idx];
+                constexpr int VS = 32;
+                const int cur_row_block_num = (context_length + VS - 1) / VS;
+                const int cur_sub_rows = cur_row_block_num / GS;
+                const int cur_rem_rows = cur_row_block_num % GS;
+
+                const float * o_a_s_head = (const float *)o_a_s + bsz_idx * o_a_s_bsz_stride
+                                                                + head_idx * o_a_s_head_stride;
+                const IT * o_accs_head = (const IT *)o_accs + bsz_idx * o_accs_bsz_stride
+                                                            + head_idx * o_accs_head_stride;
+                IT * output_head = (IT *)output + bsz_idx * output_bsz_stride
+                                                + head_idx * output_head_stride;
+
+                int start_row = std::min(tid * cur_sub_rows + std::min(tid, cur_rem_rows), cur_row_block_num);
+                int end_row = std::min(start_row + cur_sub_rows + (tid < cur_rem_rows), cur_row_block_num);
+
+                float max_attn = -sycl::detail::max_v<float>();
+                float softmax = 0;
+                simd<float, HD> accs = 0;
+                for (int r = start_row; r < end_row; ++r) {
+                    float sub_attn = o_a_s_head[2 * r];
+                    float sub_softmax = o_a_s_head[2 * r + 1];
+                    simd<float, HD> sub_accs = block_load<IT, HD>(o_accs_head + r * HD);
+                    float new_max_attn = std::max(max_attn, sub_attn);
+                    float exp1 = exp(max_attn - new_max_attn);
+                    float exp2 = exp(sub_attn - new_max_attn);
+                    accs = accs * exp1 + sub_accs * exp2;
+                    softmax = softmax * exp1 + sub_softmax * exp2;
+                    max_attn = new_max_attn;
+                }
+
+                slm_block_store<float, HD>(accs_slm_offset + tid * HD * sizeof(float), accs);
+                slm_block_store<float, 1>(attn_slm_offset + tid * sizeof(float), max_attn);
+                slm_block_store<float, 1>(softmax_slm_offset + tid * sizeof(float), softmax);
+                barrier();
+
+                if (tid < HD / SUB_HD) {
+                    simd<float, GS> max_attns = slm_block_load<float, GS>(attn_slm_offset);
+                    const simd<float, GS> scales = exp(max_attns - hmax<float, float, GS>(max_attns));
+                    simd<float, GS> softmaxs = slm_block_load<float, GS>(softmax_slm_offset);
+                    float softmax_sum = sycl::ext::intel::esimd::detail::sum<float, float, GS>(softmaxs * scales);
+
+                    simd<float, SUB_HD> result = 0;
+                    #pragma unroll
+                    for (int r = 0; r < GS; ++r) {
+                        simd<float, SUB_HD> sub_accs = slm_block_load<float, SUB_HD>(
+                            accs_slm_offset + (r * HD + tid * SUB_HD) * sizeof(float)
+                        );
+                        result = result + sub_accs * scales[r];
+                    }
+                    result = result / softmax_sum;
+                    block_store<IT, SUB_HD>(output_head + tid * SUB_HD, result);
+                }
+            }
+        );
+    };
+
+    utils::submit_kernel(cgf, device, "gqa kernel 2/2");
+}
+
+using AT = at::ScalarType;
+using fp16 = sycl::half;
+template<const int VS, const int GS, const int HD>
+auto dispatch_gqa_kernel(AT it) {
+    switch (it) {
+        case AT::Float: return std::make_tuple(gqa_1_kernel<float, VS, HD>, gqa_2_kernel<float, GS, HD>);
+        case AT::Half: return std::make_tuple(gqa_1_kernel<fp16, VS, HD>, gqa_2_kernel<fp16, GS, HD>);
+        default: throw std::runtime_error("unsupported dtype, only fp32 and fp16 are supported");
+    }
+}
+
+void paged_attention_gqa(
+    torch::Tensor output,
+    torch::Tensor query,
+    torch::Tensor key_cache,
+    torch::Tensor value_cache,
+    int64_t bsz,
+    int64_t num_heads,
+    int64_t num_kv_heads,
+    float scale,
+    torch::Tensor& block_tables,
+    torch::Tensor& context_lens,
+    int block_size,
+    int64_t head_dim,
+    int max_seq_len
+) {
+    constexpr int VS = 32;
+    constexpr int GS = 32;
+
+    const int row_block_num = (max_seq_len + VS - 1) / VS;
+    auto o_a_s = torch::empty({bsz, num_heads, 1, row_block_num * 2},
+                              torch::device(query.device()).dtype(torch::kFloat32));
+    auto o_accs = torch::empty({bsz, num_heads, 1, row_block_num * head_dim},
+                               torch::device(query.device()).dtype(query.dtype()));
+
+    auto [func1, func2] = [&](){
+        switch (head_dim) {
+            case 128: return dispatch_gqa_kernel<VS, GS, 128>(query.scalar_type());
+            case 96: return dispatch_gqa_kernel<VS, GS, 96>(query.scalar_type());
+            case 80: return dispatch_gqa_kernel<VS, GS, 80>(query.scalar_type());
+            case 64: return dispatch_gqa_kernel<VS, GS, 64>(query.scalar_type());
+            default: throw std::runtime_error("unsupported head_dim, only 128, 96, 80 and 64 are supported");
+        }
+    }();
+
+    func1(
+        query.data_ptr(), key_cache.data_ptr(), value_cache.data_ptr(),
+        block_tables.data_ptr(), context_lens.data_ptr(), o_a_s.data_ptr(), o_accs.data_ptr(),
+        query.stride(0), query.stride(1), key_cache.stride(0), key_cache.stride(1), key_cache.stride(2), block_tables.stride(0),
+        o_a_s.stride(0), o_a_s.stride(1), o_accs.stride(0), o_accs.stride(1),
+        scale, block_size, bsz, num_heads, num_kv_heads, row_block_num,
+        query.device()
+    );
+
+    func2(
+        o_a_s.data_ptr(), o_accs.data_ptr(), output.data_ptr(), context_lens.data_ptr(),
+        o_a_s.stride(0), o_a_s.stride(1),
+        o_accs.stride(0), o_accs.stride(1),
+        output.stride(0), output.stride(1),
+        bsz, num_heads, row_block_num,
+        query.device()
+    );
+}
diff --git a/csrc/xpu/attention_xpu_fp8.cpp b/csrc/xpu/attention_xpu_fp8.cpp
new file mode 100644
index 000000000..a2ea5819b
--- /dev/null
+++ b/csrc/xpu/attention_xpu_fp8.cpp
@@ -0,0 +1,324 @@
+// clang-format off
+#ifdef VLLM_DEV
+#undef __SYCL_DEVICE_ONLY__
+#endif
+#include <sycl/sycl.hpp>
+#include <dpct/dpct.hpp>
+#include <ext/intel/esimd.hpp>
+#include "kv.h"
+
+// clang-format on
+#include <float.h>
+#include <torch/extension.h>
+#include <stdexcept>
+#include "utils.h"
+#include "xpu_types.h"
+// #include "dtype_bfloat16.dp.hpp"
+#include "dtype_float16.h"
+#include "dtype_float32.h"
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+#include <c10/xpu/XPUStream.h>
+#endif
+
+#include <functional>
+// #include <ipex.h>
+
+using namespace sycl::ext::intel::esimd;
+using AT = at::ScalarType;
+
+template <typename IT, const int VS, const int HD>
+void gqa_1_kernel_fp8(
+    const void* query,  // [num_seqs, num_heads, head_size]
+    const void* key,    // [num_blocks, num_kv_heads, head_size, block_size]
+    const void* value,  // [num_blocks, num_kv_heads, head_size, block_size]
+    const void* block_tables,  // [num_seqs, max_num_blocks_per_seq]
+    const void* context_lens,  // [num_seqs]
+    void* o_a_s, void* o_accs, const int64_t query_bsz_stride,
+    const int64_t query_head_stride, const int64_t kv_token_stride,
+    const int64_t kv_head_stride, const int64_t kv_block_stride,
+    const int64_t block_table_stride_batch, const int64_t o_a_s_bsz_stride,
+    const int64_t o_a_s_head_stride, const int64_t o_accs_bsz_stride,
+    const int64_t o_accs_head_stride, const float scale, const int block_size,
+    const int bsz, const int num_heads, const int num_kv_heads,
+    const int block_num, const at::Device& device) {
+  const int group_size = num_heads / num_kv_heads;
+  const int sub_rows = VS / group_size;
+  const int rem_rows = VS % group_size;
+
+  const float attn_scale = scale;
+
+  sycl::range<3> global_size(bsz, num_heads, block_num);
+  sycl::range<3> local_size(1, group_size, 1);
+
+  auto cgf = [&](sycl::handler& handle) {
+    handle.parallel_for(
+        sycl::nd_range<3>(global_size, local_size),
+        [=](sycl::nd_item<3> item) SYCL_ESIMD_KERNEL {
+          slm_init<VS * HD * sizeof(IT)>();
+
+          const int bsz_idx = item.get_global_id(0);
+          const int head_idx = item.get_global_id(1);
+          const int kv_head_idx = item.get_group(1);
+          const int tid = item.get_local_id(1);
+          const int vid = item.get_global_id(2);
+
+          const IT* query_head = (const IT*)query + bsz_idx * query_bsz_stride +
+                                 head_idx * query_head_stride;
+
+          IT* o_accs_head = (IT*)o_accs + bsz_idx * o_accs_bsz_stride +
+                            head_idx * o_accs_head_stride;
+          float* o_a_s_head = (float*)o_a_s + bsz_idx * o_a_s_bsz_stride +
+                              head_idx * o_a_s_head_stride;
+
+          const int* block_tables_ptr = (const int*)block_tables;
+          const int* block_table =
+              block_tables_ptr + bsz_idx * block_table_stride_batch;
+
+          const int* context_lens_ptr = (const int*)context_lens;
+          const int context_length = context_lens_ptr[bsz_idx];
+
+          simd<IT, HD> query_row = block_load<IT, HD>(query_head) * attn_scale;
+
+          // copy k_cache to slm
+          int start_row =
+              std::min(vid * VS + tid * sub_rows + std::min(tid, rem_rows),
+                       context_length);
+          int end_row =
+              std::min(start_row + sub_rows + (tid < rem_rows), context_length);
+          for (int r = start_row; r < end_row; ++r) {
+            int which_block = r / block_size;
+            int which_slot = r % block_size;
+            int physical_block_number = block_table[which_block];
+
+            // Load elements in uint8_t
+            const uint8_t* key_head =
+                (const uint8_t*)key + physical_block_number * kv_token_stride +
+                kv_head_idx * kv_head_stride + which_slot * kv_block_stride;
+
+            simd<uint8_t, HD> key_row = block_load<uint8_t, HD>(key_head);
+            simd<IT, HD> key_dequantized = dequantize_key_row<HD>(key_row);
+            slm_block_store<IT, HD>((r - vid * VS) * HD * sizeof(IT), key_dequantized);
+          }
+          barrier();
+
+          simd<float, VS> attns = -sycl::detail::max_v<float>();
+          int row_num =
+              (vid + 1) * VS > context_length ? context_length % VS : VS;
+          // q @ k
+          for (int r = 0; r < row_num; ++r) {
+            simd<IT, HD> key_row = slm_block_load<IT, HD>(r * HD * sizeof(IT));
+            float attn = sycl::ext::intel::esimd::detail::sum<float, IT, HD>(
+                query_row * key_row);
+            attns[r] = attn;
+          }
+
+          float max_attn = hmax<float, float, VS>(attns);
+          const simd<IT, VS> attn_exp = exp(attns - max_attn);
+          barrier();
+
+          // copy v_cache to slm
+          for (int r = start_row; r < end_row; ++r) {
+            int which_block = r / block_size;
+            int which_slot = r % block_size;
+            int physical_block_number = block_table[which_block];
+
+            const uint8_t* value_head =
+                (const uint8_t*)value + physical_block_number * kv_token_stride +
+                kv_head_idx * kv_head_stride + which_slot * kv_block_stride;
+
+            simd<uint8_t, HD> value_row = block_load<uint8_t, HD>(value_head);
+            simd<IT, HD> value_dequantized = dequantize_value_row<HD>(value_row);
+            slm_block_store<IT, HD>((r - vid * VS) * HD * sizeof(IT),
+                                    value_dequantized);
+          }
+          barrier();
+
+          // attn @ v
+          simd<IT, HD> accs = 0;
+          for (int r = 0; r < row_num; ++r) {
+            simd<IT, HD> value_row =
+                slm_block_load<IT, HD>(r * HD * sizeof(IT));
+            accs = accs + value_row * attn_exp[r];
+          }
+
+          float softmax =
+              sycl::ext::intel::esimd::detail::sum<float, float, VS>(attn_exp);
+
+          block_store<IT, HD>(o_accs_head + vid * HD, accs);
+          block_store<float, 1>(o_a_s_head + vid * 2, max_attn);
+          block_store<float, 1>(o_a_s_head + vid * 2 + 1, softmax);
+        });
+  };
+
+  utils::submit_kernel(cgf, device, "gqa kernel 1/2");
+}
+
+template <typename IT, const int GS, const int HD>
+void gqa_2_kernel_fp8(void* o_a_s, void* o_accs, void* output,
+                  const void* context_lens,  // [num_seqs]
+                  const int64_t o_a_s_bsz_stride,
+                  const int64_t o_a_s_head_stride,
+                  const int64_t o_accs_bsz_stride,
+                  const int64_t o_accs_head_stride,
+                  const int64_t output_bsz_stride,
+                  const int64_t output_head_stride, const int bsz,
+                  const int num_heads, const int row_block_num,
+                  const at::Device& device) {
+  constexpr int SUB_HD = 8;
+  static_assert(HD % SUB_HD == 0);
+  static_assert(HD / SUB_HD <= GS);
+
+  const int sub_rows = row_block_num / GS;
+  const int rem_rows = row_block_num % GS;
+
+  constexpr int accs_slm_offset = 0;
+  constexpr int attn_slm_offset = GS * HD * sizeof(float);
+  constexpr int softmax_slm_offset = attn_slm_offset + GS * sizeof(float);
+
+  sycl::range<3> global_size(bsz, num_heads, GS);
+  sycl::range<3> local_size(1, 1, GS);
+
+  auto cgf = [&](sycl::handler& handle) {
+    handle.parallel_for(
+        sycl::nd_range<3>(global_size, local_size),
+        [=](sycl::nd_item<3> item) SYCL_ESIMD_KERNEL {
+          slm_init<GS * HD * sizeof(float) + GS * 2 * sizeof(float)>();
+
+          const int bsz_idx = item.get_global_id(0);
+          const int head_idx = item.get_global_id(1);
+          const int tid = item.get_global_id(2);
+
+          const int* context_lens_ptr = (const int*)context_lens;
+          const int context_length = context_lens_ptr[bsz_idx];
+          constexpr int VS = 32;
+          const int cur_row_block_num = (context_length + VS - 1) / VS;
+          const int cur_sub_rows = cur_row_block_num / GS;
+          const int cur_rem_rows = cur_row_block_num % GS;
+
+          const float* o_a_s_head = (const float*)o_a_s +
+                                    bsz_idx * o_a_s_bsz_stride +
+                                    head_idx * o_a_s_head_stride;
+          const IT* o_accs_head = (const IT*)o_accs +
+                                  bsz_idx * o_accs_bsz_stride +
+                                  head_idx * o_accs_head_stride;
+          IT* output_head = (IT*)output + bsz_idx * output_bsz_stride +
+                            head_idx * output_head_stride;
+
+          int start_row =
+              std::min(tid * cur_sub_rows + std::min(tid, cur_rem_rows),
+                       cur_row_block_num);
+          int end_row =
+              std::min(start_row + cur_sub_rows + (tid < cur_rem_rows),
+                       cur_row_block_num);
+
+          float max_attn = -sycl::detail::max_v<float>();
+          float softmax = 0;
+          simd<float, HD> accs = 0;
+          for (int r = start_row; r < end_row; ++r) {
+            float sub_attn = o_a_s_head[2 * r];
+            float sub_softmax = o_a_s_head[2 * r + 1];
+            simd<float, HD> sub_accs = block_load<IT, HD>(o_accs_head + r * HD);
+            float new_max_attn = std::max(max_attn, sub_attn);
+            float exp1 = exp(max_attn - new_max_attn);
+            float exp2 = exp(sub_attn - new_max_attn);
+            accs = accs * exp1 + sub_accs * exp2;
+            softmax = softmax * exp1 + sub_softmax * exp2;
+            max_attn = new_max_attn;
+          }
+
+          slm_block_store<float, HD>(accs_slm_offset + tid * HD * sizeof(float),
+                                     accs);
+          slm_block_store<float, 1>(attn_slm_offset + tid * sizeof(float),
+                                    max_attn);
+          slm_block_store<float, 1>(softmax_slm_offset + tid * sizeof(float),
+                                    softmax);
+          barrier();
+
+          if (tid < HD / SUB_HD) {
+            simd<float, GS> max_attns =
+                slm_block_load<float, GS>(attn_slm_offset);
+            const simd<float, GS> scales =
+                exp(max_attns - hmax<float, float, GS>(max_attns));
+            simd<float, GS> softmaxs =
+                slm_block_load<float, GS>(softmax_slm_offset);
+            float softmax_sum =
+                sycl::ext::intel::esimd::detail::sum<float, float, GS>(
+                    softmaxs * scales);
+
+            simd<float, SUB_HD> result = 0;
+#pragma unroll
+            for (int r = 0; r < GS; ++r) {
+              simd<float, SUB_HD> sub_accs = slm_block_load<float, SUB_HD>(
+                  accs_slm_offset + (r * HD + tid * SUB_HD) * sizeof(float));
+              result = result + sub_accs * scales[r];
+            }
+            result = result / softmax_sum;
+            block_store<IT, SUB_HD>(output_head + tid * SUB_HD, result);
+          }
+        });
+  };
+
+  utils::submit_kernel(cgf, device, "gqa kernel 2/2");
+}
+
+template <const int VS, const int GS, const int HD>
+auto dispatch_gqa_kernel_fp8(AT it) {
+  switch (it) {
+    case AT::Float:
+      return std::make_tuple(gqa_1_kernel_fp8<float, VS, HD>,
+                             gqa_2_kernel_fp8<float, GS, HD>);
+    case AT::Half:
+      return std::make_tuple(gqa_1_kernel_fp8<fp16, VS, HD>,
+                             gqa_2_kernel_fp8<fp16, GS, HD>);
+    default:
+      throw std::runtime_error(
+          "unsupported dtype, only fp32 and fp16 are supported");
+  }
+}
+
+void paged_attention_gqa_fp8(torch::Tensor output, torch::Tensor query,
+                         torch::Tensor key_cache, torch::Tensor value_cache,
+                         int64_t bsz, int64_t num_heads, int64_t num_kv_heads,
+                         float scale, torch::Tensor& block_tables,
+                         torch::Tensor& context_lens, int block_size,
+                         int64_t head_dim, int max_seq_len) {
+  constexpr int VS = 32;
+  constexpr int GS = 32;
+
+  const int row_block_num = (max_seq_len + VS - 1) / VS;
+  auto o_a_s =
+      torch::empty({bsz, num_heads, 1, row_block_num * 2},
+                   torch::device(query.device()).dtype(torch::kFloat32));
+  auto o_accs =
+      torch::empty({bsz, num_heads, 1, row_block_num * head_dim},
+                   torch::device(query.device()).dtype(query.dtype()));
+
+  auto [func1, func2] = [&]() {
+    switch (head_dim) {
+      case 128:
+        return dispatch_gqa_kernel_fp8<VS, GS, 128>(query.scalar_type());
+      case 96:
+        return dispatch_gqa_kernel_fp8<VS, GS, 96>(query.scalar_type());
+      case 80:
+        return dispatch_gqa_kernel_fp8<VS, GS, 80>(query.scalar_type());
+      case 64:
+        return dispatch_gqa_kernel_fp8<VS, GS, 64>(query.scalar_type());
+      default:
+        throw std::runtime_error(
+            "unsupported head_dim, only 128, 96, 80 and 64 are supported");
+    }
+  }();
+
+  func1(query.data_ptr(), key_cache.data_ptr(), value_cache.data_ptr(),
+        block_tables.data_ptr(), context_lens.data_ptr(), o_a_s.data_ptr(),
+        o_accs.data_ptr(), query.stride(0), query.stride(1),
+        key_cache.stride(0), key_cache.stride(1), key_cache.stride(2),
+        block_tables.stride(0), o_a_s.stride(0), o_a_s.stride(1),
+        o_accs.stride(0), o_accs.stride(1), scale, block_size, bsz, num_heads,
+        num_kv_heads, row_block_num, query.device());
+
+  func2(o_a_s.data_ptr(), o_accs.data_ptr(), output.data_ptr(),
+        context_lens.data_ptr(), o_a_s.stride(0), o_a_s.stride(1),
+        o_accs.stride(0), o_accs.stride(1), output.stride(0), output.stride(1),
+        bsz, num_heads, row_block_num, query.device());
+}
diff --git a/csrc/xpu/base.hpp b/csrc/xpu/base.hpp
new file mode 100644
index 000000000..c364c62e6
--- /dev/null
+++ b/csrc/xpu/base.hpp
@@ -0,0 +1,118 @@
+#pragma once
+
+#include <sycl.hpp>
+#include <sycl/ext/intel/esimd.hpp>
+
+#include "common.h"
+
+using namespace sycl::ext::intel::esimd;
+using fp16 = sycl::half;
+
+constexpr int QK = 64;
+constexpr int SBS = 4;
+
+constexpr int BLOCK_SIZES[GGML_TYPE_COUNT] = {
+    [GGML_TYPE_Q4_0]     = QK / 2,
+    [GGML_TYPE_Q4_0_WOQ] = QK / 2,
+    [GGML_TYPE_FP8E5]  = QK,
+};
+
+constexpr int SCALE_SIZES[GGML_TYPE_COUNT] = {
+    [GGML_TYPE_Q4_0]     = sizeof(fp16),
+    [GGML_TYPE_Q4_0_WOQ] = sizeof(fp16),
+    [GGML_TYPE_FP8E5]  = 0,
+};
+
+template<int QTYPE>
+ESIMD_INLINE auto load_qblocks(const uint8_t * weight, const uint8_t * scale);
+
+template<>
+ESIMD_INLINE auto load_qblocks<GGML_TYPE_Q4_0>(const uint8_t * weight, const uint8_t * scale) {
+    constexpr int BLOCK_SIZE = BLOCK_SIZES[GGML_TYPE_Q4_0];
+    simd<uint8_t, BLOCK_SIZE * SBS> ybytes = block_load<uint8_t, BLOCK_SIZE * SBS>(weight);
+    const simd<fp16, SBS> scales = block_load<fp16, SBS>((const fp16 *)scale);
+
+    simd<fp16, QK * SBS> yvs;
+    #pragma unroll
+    for (int i = 0; i < SBS; ++i) {
+        simd<uint8_t, QK> uyv;
+        uyv.select<QK / 2, 1>(0) = ybytes.template select<QK / 2, 1>(i * QK / 2) & (uint8_t)0xF;
+        uyv.select<QK / 2, 1>(QK / 2) = ybytes.template select<QK / 2, 1>(i * QK / 2) >> (uint8_t)4;
+        yvs.template select<QK, 1>(i * QK) = (uyv.bit_cast_view<int8_t>() - (int8_t)8) * scales[i];
+    }
+    return yvs;
+}
+
+template<>
+ESIMD_INLINE auto load_qblocks<GGML_TYPE_Q4_0_WOQ>(const uint8_t * weight, const uint8_t * scale) {
+    constexpr int BLOCK_SIZE = BLOCK_SIZES[GGML_TYPE_Q4_0_WOQ];
+    simd<uint8_t, BLOCK_SIZE * SBS> ybytes = block_load<uint8_t, BLOCK_SIZE * SBS>(weight);
+    const simd<fp16, SBS> scales = block_load<fp16, SBS>((const fp16 *)scale);
+
+    simd<fp16, QK * SBS> yvs;
+    #pragma unroll
+    for (int i = 0; i < SBS; ++i) {
+        simd<uint8_t, QK> uyv;
+        uyv.select<QK / 2, 2>(0) = ybytes.template select<QK / 2, 1>(i * QK / 2) & (uint8_t)0xF;
+        uyv.select<QK / 2, 2>(1) = ybytes.template select<QK / 2, 1>(i * QK / 2) >> (uint8_t)4;
+        yvs.template select<QK, 1>(i * QK) = (uyv.bit_cast_view<int8_t>() - (int8_t)8) * scales[i];
+    }
+    return yvs;
+}
+
+
+template<>
+ESIMD_INLINE auto load_qblocks<GGML_TYPE_FP8E5>(const uint8_t * weight, const uint8_t * scale) {
+    constexpr int BLOCK_SIZE = BLOCK_SIZES[GGML_TYPE_FP8E5];
+    simd<uint8_t, BLOCK_SIZE * SBS> ybytes = block_load<uint8_t, BLOCK_SIZE * SBS>(weight);
+
+    simd<fp16, QK * SBS> yvs;
+    yvs.template bit_cast_view<uint8_t>().template select<QK * SBS, 2>(0) = 0x80;
+    yvs.template bit_cast_view<uint8_t>().template select<QK * SBS, 2>(1) = ybytes;
+    return yvs;
+}
+
+
+// C++ doesn't support function template partial specialization, so write a new version for SBS=1
+template<int QTYPE>
+ESIMD_INLINE auto load_qblock(const uint8_t * weight, const uint8_t * scale);
+
+template<>
+ESIMD_INLINE auto load_qblock<GGML_TYPE_Q4_0>(const uint8_t * weight, const uint8_t * scale) {
+    constexpr int BLOCK_SIZE = BLOCK_SIZES[GGML_TYPE_Q4_0];
+    simd<uint8_t, BLOCK_SIZE> ybytes = block_load<uint8_t, BLOCK_SIZE>(weight);
+    fp16 scales = *(const fp16 *)scale;
+
+    simd<uint8_t, QK> uyv;
+    uyv.select<QK / 2, 1>(0) = ybytes & (uint8_t)0xF;
+    uyv.select<QK / 2, 1>(QK / 2) = ybytes >> (uint8_t)4;
+    simd<fp16, QK> yv = (uyv.bit_cast_view<int8_t>() - (int8_t)8) * scales;
+
+    return yv;
+}
+
+template<>
+ESIMD_INLINE auto load_qblock<GGML_TYPE_Q4_0_WOQ>(const uint8_t * weight, const uint8_t * scale) {
+    constexpr int BLOCK_SIZE = BLOCK_SIZES[GGML_TYPE_Q4_0_WOQ];
+    simd<uint8_t, BLOCK_SIZE> ybytes = block_load<uint8_t, BLOCK_SIZE>(weight);
+    fp16 scales = *(const fp16 *)scale;
+
+    simd<uint8_t, QK> uyv;
+    uyv.select<QK / 2, 2>(0) = ybytes & (uint8_t)0xF;
+    uyv.select<QK / 2, 2>(1) = ybytes >> (uint8_t)4;
+    simd<fp16, QK> yv = (uyv.bit_cast_view<int8_t>() - (int8_t)8) * scales;
+
+    return yv;
+}
+
+
+template<>
+ESIMD_INLINE auto load_qblock<GGML_TYPE_FP8E5>(const uint8_t * weight, const uint8_t * scale) {
+    constexpr int BLOCK_SIZE = BLOCK_SIZES[GGML_TYPE_FP8E5];
+    simd<uint8_t, BLOCK_SIZE> ybytes = block_load<uint8_t, BLOCK_SIZE>(weight);
+
+    simd<fp16, QK> yvs;
+    yvs.template bit_cast_view<uint8_t>().template select<QK, 2>(0) = 0x80;
+    yvs.template bit_cast_view<uint8_t>().template select<QK, 2>(1) = ybytes;
+    return yvs;
+}
diff --git a/csrc/xpu/cache_ops_xpu.cpp b/csrc/xpu/cache_ops_xpu.cpp
new file mode 100644
index 000000000..a3451c0e7
--- /dev/null
+++ b/csrc/xpu/cache_ops_xpu.cpp
@@ -0,0 +1,579 @@
+// clang-format off
+#ifdef VLLM_DEV
+#undef __SYCL_DEVICE_ONLY__
+#endif
+#include <sycl/sycl.hpp>
+#include <dpct/dpct.hpp>
+#include <ext/intel/esimd.hpp>
+// clang-format on
+#include "xpu_types.h"
+
+#include <torch/extension.h>
+#include "utils.h"
+
+using fp16 = sycl::half;
+using namespace sycl::ext::intel::esimd;
+
+template <typename scalar_t>
+void reshape_and_cache_kernel(
+    const scalar_t* __restrict__ key, // [num_tokens, num_heads, head_size]
+    const scalar_t* __restrict__ value, // [num_tokens, num_heads, head_size]
+    scalar_t* __restrict__ key_cache, // [num_blocks, num_heads, head_size/x,
+                                      // block_size, x]
+    scalar_t* __restrict__ value_cache, // [num_blocks, num_heads, head_size,
+                                        // block_size]
+    const int64_t* __restrict__ slot_mapping, // [num_tokens]
+    const int key_stride,
+    const int value_stride,
+    const int num_heads,
+    const int head_size,
+    const int block_size,
+    const int x,
+    const sycl::nd_item<3>& item_ct1) {
+  const int64_t token_idx = item_ct1.get_group(2);
+  const int64_t slot_idx = slot_mapping[token_idx];
+  if (slot_idx < 0) {
+    // Padding token that should be ignored.
+    return;
+  }
+
+  const int64_t block_idx = slot_idx / block_size;
+  const int64_t block_offset = slot_idx % block_size;
+
+  const int n = num_heads * head_size;
+  for (int i = item_ct1.get_local_id(2); i < n;
+       i += item_ct1.get_local_range(2)) {
+    const int64_t src_key_idx = token_idx * key_stride + i;
+    const int64_t src_value_idx = token_idx * value_stride + i;
+
+    const int head_idx = i / head_size;
+    const int head_offset = i % head_size;
+    const int x_idx = head_offset / x;
+    const int x_offset = head_offset % x;
+
+    const int64_t tgt_key_idx =
+        block_idx * num_heads * (head_size / x) * block_size * x +
+        head_idx * (head_size / x) * block_size * x + x_idx * block_size * x +
+        block_offset * x + x_offset;
+    const int64_t tgt_value_idx =
+        block_idx * num_heads * head_size * block_size +
+        head_idx * head_size * block_size + head_offset * block_size +
+        block_offset;
+    key_cache[tgt_key_idx] = key[src_key_idx];
+    value_cache[tgt_value_idx] = value[src_value_idx];
+  }
+}
+
+template <typename scalar_t>
+void call_reshape_and_cache_kernel(
+    const scalar_t* __restrict__ key,
+    const scalar_t* __restrict__ value,
+    scalar_t* __restrict__ key_cache,
+    scalar_t* __restrict__ value_cache,
+    const int64_t* __restrict__ slot_mapping,
+    const int num_tokens,
+    const int key_stride,
+    const int value_stride,
+    const int num_heads,
+    const int head_size,
+    const int block_size,
+    const int x) {
+  using sycl_t = vllm::xpu::SyclTypeTrait<scalar_t>::Type;
+  sycl::range<3> grid(1, 1, num_tokens);
+  sycl::range<3> block(1, 1, std::min(num_heads * head_size, 512));
+  auto& queue = vllm::xpu::vllmGetQueue();
+  queue.submit([&](sycl::handler& cgh) {
+    cgh.parallel_for(
+        sycl::nd_range<3>(grid * block, block), [=](sycl::nd_item<3> item_ct1) {
+          reshape_and_cache_kernel<sycl_t>(
+              (const sycl_t* __restrict__)key,
+              (const sycl_t* __restrict__)value,
+              (sycl_t* __restrict__)key_cache,
+              (sycl_t* __restrict__)value_cache,
+              slot_mapping,
+              key_stride,
+              value_stride,
+              num_heads,
+              head_size,
+              block_size,
+              x,
+              item_ct1);
+        });
+  });
+}
+
+void reshape_and_cache(
+    torch::Tensor& key,
+    torch::Tensor& value,
+    torch::Tensor& key_cache,
+    torch::Tensor& value_cache,
+    torch::Tensor& slot_mapping,
+    const std::string& kv_cache_dtype,
+    const float kv_scale) {
+  int num_tokens = key.size(0);
+  int num_heads = key.size(1);
+  int head_size = key.size(2);
+  int block_size = key_cache.size(3);
+  int x = key_cache.size(4);
+
+  int key_stride = key.stride(0);
+  int value_stride = value.stride(0);
+
+  VLLM_XPU_DISPATCH_FLOATING_TYPES(
+      key.scalar_type(), "call_reshape_and_cache_kernel", [&] {
+        call_reshape_and_cache_kernel<scalar_t>(
+            key.data_ptr<scalar_t>(),
+            value.data_ptr<scalar_t>(),
+            key_cache.data_ptr<scalar_t>(),
+            value_cache.data_ptr<scalar_t>(),
+            slot_mapping.data_ptr<int64_t>(),
+            num_tokens,
+            key_stride,
+            value_stride,
+            num_heads,
+            head_size,
+            block_size,
+            x);
+      });
+}
+
+template <typename scalar_t>
+void reshape_and_cache_ipexllm_kernel(
+    const scalar_t* __restrict__ key, // [num_tokens, num_heads, head_size]
+    const scalar_t* __restrict__ value, // [num_tokens, num_heads, head_size]
+    scalar_t* __restrict__ key_cache, // [num_blocks, num_kv_heads, block_size, head_size]
+    scalar_t* __restrict__ value_cache, // [num_blocks, num_kv_heads, block_size, head_size]
+    const int64_t* __restrict__ slot_mapping, // [num_tokens]
+    const int key_stride,
+    const int value_stride,
+    const int num_heads,
+    const int head_size,
+    const int block_size,
+    const int x,
+    const sycl::nd_item<3>& item_ct1) {
+  const int64_t token_idx = item_ct1.get_group(2);
+  const int64_t slot_idx = slot_mapping[token_idx];
+  if (slot_idx < 0) {
+    // Padding token that should be ignored.
+    return;
+  }
+
+  const int64_t block_idx = slot_idx / block_size;
+  const int64_t block_offset = slot_idx % block_size;
+
+  const int n = num_heads * head_size;
+  for (int i = item_ct1.get_local_id(2); i < n;
+       i += item_ct1.get_local_range(2)) {
+    const int64_t src_key_idx = token_idx * key_stride + i;
+    const int64_t src_value_idx = token_idx * value_stride + i;
+
+    const int head_idx = i / head_size;
+    const int head_offset = i % head_size;
+
+    // const int64_t tgt_key_idx =
+    //     block_idx * num_heads * (head_size / x) * block_size * x +
+    //     head_idx * (head_size / x) * block_size * x + x_idx * block_size * x +
+    //     block_offset * x + x_offset;
+
+    // const int64_t tgt_value_idx =
+    //     block_idx * num_heads * head_size * block_size +
+    //     head_idx * head_size * block_size + head_offset * block_size +
+    //     block_offset;
+
+    const int64_t tgt_value_idx =
+        block_idx * num_heads * head_size * block_size +
+        head_idx * head_size * block_size + 
+        block_offset * head_size + 
+        head_offset;
+    const int64_t tgt_key_idx = tgt_value_idx;
+    key_cache[tgt_key_idx] = key[src_key_idx];
+    value_cache[tgt_value_idx] = value[src_value_idx];
+  }
+}
+
+template <typename scalar_t>
+void call_reshape_and_cache_ipexllm_kernel(
+    const scalar_t* __restrict__ key,
+    const scalar_t* __restrict__ value,
+    scalar_t* __restrict__ key_cache,
+    scalar_t* __restrict__ value_cache,
+    const int64_t* __restrict__ slot_mapping,
+    const int num_tokens,
+    const int key_stride,
+    const int value_stride,
+    const int num_heads,
+    const int head_size,
+    const int block_size,
+    const int x) {
+  using sycl_t = vllm::xpu::SyclTypeTrait<scalar_t>::Type;
+  sycl::range<3> grid(1, 1, num_tokens);
+  sycl::range<3> block(1, 1, std::min(num_heads * head_size, 512));
+  auto& queue = vllm::xpu::vllmGetQueue();
+  queue.submit([&](sycl::handler& cgh) {
+    cgh.parallel_for(
+        sycl::nd_range<3>(grid * block, block), [=](sycl::nd_item<3> item_ct1) {
+          reshape_and_cache_ipexllm_kernel<sycl_t>(
+              (const sycl_t* __restrict__)key,
+              (const sycl_t* __restrict__)value,
+              (sycl_t* __restrict__)key_cache,
+              (sycl_t* __restrict__)value_cache,
+              slot_mapping,
+              key_stride,
+              value_stride,
+              num_heads,
+              head_size,
+              block_size,
+              x,
+              item_ct1);
+        });
+  });
+}
+
+void reshape_and_cache_ipexllm(
+    torch::Tensor& key,
+    torch::Tensor& value,
+    torch::Tensor& key_cache,
+    torch::Tensor& value_cache,
+    torch::Tensor& slot_mapping,
+    const std::string& kv_cache_dtype,
+    const float kv_scale) {
+  int num_tokens = key.size(0);
+  int num_heads = key.size(1);
+  int head_size = key.size(2);
+  int block_size = key_cache.size(2);
+  // int x = key_cache.size(4);
+  int x = 1;
+
+  int key_stride = key.stride(0);
+  int value_stride = value.stride(0);
+
+  VLLM_XPU_DISPATCH_FLOATING_TYPES(
+      key.scalar_type(), "call_reshape_and_cache_ipexllm_kernel", [&] {
+        call_reshape_and_cache_ipexllm_kernel<scalar_t>(
+            key.data_ptr<scalar_t>(),
+            value.data_ptr<scalar_t>(),
+            key_cache.data_ptr<scalar_t>(),
+            value_cache.data_ptr<scalar_t>(),
+            slot_mapping.data_ptr<int64_t>(),
+            num_tokens,
+            key_stride,
+            value_stride,
+            num_heads,
+            head_size,
+            block_size,
+            x);
+      });
+}
+
+
+template <typename scalar_t>
+void copy_blocks_kernel(
+    int64_t* key_cache_ptrs,
+    int64_t* value_cache_ptrs,
+    const int64_t* __restrict__ block_mapping,
+    const int numel_per_block,
+    const sycl::nd_item<3>& item_ct1) {
+  const int layer_idx = item_ct1.get_group(2);
+  const int pair_idx = item_ct1.get_group(1);
+
+  scalar_t* key_cache = reinterpret_cast<scalar_t*>(key_cache_ptrs[layer_idx]);
+  scalar_t* value_cache =
+      reinterpret_cast<scalar_t*>(value_cache_ptrs[layer_idx]);
+  int64_t src_block_number = block_mapping[2 * pair_idx];
+  int64_t dst_block_number = block_mapping[2 * pair_idx + 1];
+
+  const int64_t src_block_offset = src_block_number * numel_per_block;
+  const int64_t dst_block_offset = dst_block_number * numel_per_block;
+  for (int i = item_ct1.get_local_id(2); i < numel_per_block;
+       i += item_ct1.get_local_range(2)) {
+    int64_t src_offset = src_block_offset + i;
+    int64_t dst_offset = dst_block_offset + i;
+    key_cache[dst_offset] = key_cache[src_offset];
+  }
+  for (int i = item_ct1.get_local_id(2); i < numel_per_block;
+       i += item_ct1.get_local_range(2)) {
+    int64_t src_offset = src_block_offset + i;
+    int64_t dst_offset = dst_block_offset + i;
+    value_cache[dst_offset] = value_cache[src_offset];
+  }
+}
+
+template <typename scalar_t>
+void call_copy_blocks_kernel(
+    std::vector<torch::Tensor>& key_caches,
+    std::vector<torch::Tensor>& value_caches,
+    const std::map<int64_t, std::vector<int64_t>>& block_mapping) {
+  using sycl_t = vllm::xpu::SyclTypeTrait<scalar_t>::Type;
+  int num_layers = key_caches.size();
+  TORCH_CHECK(num_layers == value_caches.size());
+  if (num_layers == 0) {
+    return;
+  }
+  torch::Device cache_device = key_caches[0].device();
+  TORCH_CHECK(cache_device.is_xpu());
+  // Create data structures for the kernel.
+  // Create an array of pointers to the key and value caches.
+  int64_t key_cache_ptrs[num_layers];
+  int64_t value_cache_ptrs[num_layers];
+  for (int layer_idx = 0; layer_idx < num_layers; ++layer_idx) {
+    key_cache_ptrs[layer_idx] =
+        reinterpret_cast<int64_t>(key_caches[layer_idx].data_ptr());
+    value_cache_ptrs[layer_idx] =
+        reinterpret_cast<int64_t>(value_caches[layer_idx].data_ptr());
+  }
+  // Create block mapping array.
+  std::vector<int64_t> block_mapping_vec;
+  for (const auto& pair : block_mapping) {
+    int64_t src_block_number = pair.first;
+    for (int64_t dst_block_number : pair.second) {
+      block_mapping_vec.push_back(src_block_number);
+      block_mapping_vec.push_back(dst_block_number);
+    }
+  }
+  int64_t* block_mapping_array = block_mapping_vec.data();
+  int num_pairs = block_mapping_vec.size() / 2;
+  // Move the data structures to the GPU.
+  // NOTE: This synchronizes the CPU and GPU.
+  torch::Tensor key_cache_ptrs_tensor =
+      torch::from_blob(key_cache_ptrs, {num_layers}, torch::kInt64)
+          .to(cache_device);
+  torch::Tensor value_cache_ptrs_tensor =
+      torch::from_blob(value_cache_ptrs, {num_layers}, torch::kInt64)
+          .to(cache_device);
+  torch::Tensor block_mapping_tensor =
+      torch::from_blob(block_mapping_array, {2 * num_pairs}, torch::kInt64)
+          .to(cache_device);
+  auto k_ptr = key_cache_ptrs_tensor.data_ptr<int64_t>();
+  auto v_ptr = value_cache_ptrs_tensor.data_ptr<int64_t>();
+  auto b_ptr = block_mapping_tensor.data_ptr<int64_t>();
+  // Launch the kernel.
+  const int numel_per_block = key_caches[0][0].numel();
+
+  sycl::range<3> grid(1, num_pairs, num_layers);
+  sycl::range<3> block(1, 1, std::min(1024, numel_per_block));
+  auto& queue = vllm::xpu::vllmGetQueue();
+  queue.submit([&](sycl::handler& cgh) {
+    cgh.parallel_for(
+        sycl::nd_range<3>(grid * block, block), [=](sycl::nd_item<3> item_ct1) {
+          copy_blocks_kernel<sycl_t>(
+              k_ptr, v_ptr, b_ptr, numel_per_block, item_ct1);
+        });
+  });
+}
+
+void copy_blocks(
+    std::vector<torch::Tensor>& key_caches,
+    std::vector<torch::Tensor>& value_caches,
+    const std::map<int64_t, std::vector<int64_t>>& block_mapping) {
+  VLLM_XPU_DISPATCH_FLOATING_TYPES(
+      key_caches[0].scalar_type(), "call_copy_blocks_kernel", [&] {
+        call_copy_blocks_kernel<scalar_t>(
+            key_caches, value_caches, block_mapping);
+      });
+}
+
+void swap_blocks(
+    torch::Tensor& src,
+    torch::Tensor& dst,
+    const std::map<int64_t, int64_t>& block_mapping) {
+  char* src_ptr = (char*)src.data_ptr();
+  char* dst_ptr = (char*)dst.data_ptr();
+
+  const int64_t block_size_in_bytes = src.element_size() * src[0].numel();
+  auto& queue = vllm::xpu::vllmGetQueue();
+
+  // NOTE(woosuk): This can be slow if the number of blocks is large.
+  for (const auto& pair : block_mapping) {
+    int64_t src_block_number = pair.first;
+    int64_t dst_block_number = pair.second;
+    int64_t src_offset = src_block_number * block_size_in_bytes;
+    int64_t dst_offset = dst_block_number * block_size_in_bytes;
+    queue.memcpy(
+        dst_ptr + dst_offset, src_ptr + src_offset, block_size_in_bytes);
+  }
+  queue.wait();
+}
+
+template <typename scalar_t>
+void gather_cached_kv_kernel(
+    scalar_t* __restrict__ key, // [num_tokens, [stride], num_heads, head_size]
+    scalar_t* __restrict__ value, // [num_tokens, [stride], num_heads,
+                                  // head_size]
+    const scalar_t* __restrict__ key_cache, // [num_blocks, num_heads,
+                                            // head_size/x, block_size, x]
+    const scalar_t* __restrict__ value_cache, // [num_blocks, num_heads,
+                                              // head_size, block_size]
+    const int* __restrict__ slot_mapping, // [num_tokens]
+    const int key_stride,
+    const int value_stride,
+    const int num_heads,
+    const int head_size,
+    const int block_size,
+    const int x,
+    const sycl::nd_item<3>& item_ct1) {
+  const int token_idx = item_ct1.get_group(2);
+  const int slot_idx = slot_mapping[token_idx];
+  const int block_idx = slot_idx / block_size;
+  const int block_offset = slot_idx % block_size;
+
+  const int num_tokens = num_heads * head_size;
+  for (int i = item_ct1.get_local_id(2); i < num_tokens;
+       i += item_ct1.get_local_range(2)) {
+    const int tgt_key_idx = token_idx * key_stride + i;
+    const int tgt_value_idx = token_idx * value_stride + i;
+
+    const int head_idx = i / head_size;
+    const int head_offset = i % head_size;
+    const int x_idx =
+        head_offset / x; // the offset of the [head_size/x] dimension
+    const int x_offset = head_offset % x;
+
+    // const int src_key_idx =
+    //     block_idx * num_heads * (head_size / x) * block_size * x +
+    //     head_idx * (head_size / x) * block_size * x + x_idx * block_size * x +
+    //     block_offset * x + x_offset;
+    // const int src_value_idx = block_idx * num_heads * head_size * block_size +
+    //     head_idx * head_size * block_size + head_offset * block_size +
+    //     block_offset;
+
+    const int src_value_idx = 
+        block_idx * num_heads * head_size * block_size + 
+        head_idx * head_size * block_size + 
+        block_offset * head_size + 
+        head_offset;
+    const int src_key_idx = src_value_idx;
+
+    key[tgt_key_idx] = VLLM_LDG(&key_cache[src_key_idx]);
+    value[tgt_value_idx] = VLLM_LDG(&value_cache[src_value_idx]);
+  }
+}
+
+template <typename scalar_t>
+void gather_cached_kv_kernel_optimized(
+    scalar_t* __restrict__ key, // [num_tokens, [stride], num_heads, head_size]
+    scalar_t* __restrict__ value, // [num_tokens, [stride], num_heads,
+                                  // head_size]
+    const scalar_t* __restrict__ key_cache, // [num_blocks, num_heads,
+                                            // head_size/x, block_size, x]
+    const scalar_t* __restrict__ value_cache, // [num_blocks, num_heads,
+                                              // head_size, block_size]
+    const int* __restrict__ slot_mapping, // [num_tokens]
+    const int key_stride,
+    const int value_stride,
+    const int num_heads,
+    const int head_size,
+    const int block_size,
+    const int x,
+    const sycl::nd_item<3>& item_ct1) {
+  const int token_idx = item_ct1.get_group(2);
+  const int slot_idx = slot_mapping[token_idx];
+  const int block_idx = slot_idx / block_size;
+  const int block_offset = slot_idx % block_size;
+
+  const int dim = num_heads * head_size;
+  assert(dim % 4 == 0); // this is true for known use cases
+  const int unroll_factor = 4;
+  const int unrolled_dim = dim / unroll_factor;
+
+  for (int i = item_ct1.get_local_id(2); i < unrolled_dim;
+       i += item_ct1.get_local_range(2)) {
+    int tgt_key_indices[unroll_factor];
+    int tgt_value_indices[unroll_factor];
+    int src_key_indices[unroll_factor];
+    int src_value_indices[unroll_factor];
+    scalar_t keys_to_store[unroll_factor];
+    scalar_t values_to_store[unroll_factor];
+
+#pragma unroll
+    for (int j = 0; j < unroll_factor; ++j) {
+      int index = i + j * unrolled_dim;
+
+      const int tgt_key_idx = token_idx * key_stride + index;
+      const int tgt_value_idx = token_idx * value_stride + index;
+
+      const int head_idx = index / head_size;
+      const int head_offset = index % head_size;
+
+      const int src_value_idx = 
+        block_idx * num_heads * head_size * block_size + 
+        head_idx * head_size * block_size + 
+        block_offset * head_size + 
+        head_offset;
+      const int src_key_idx = src_value_idx;
+
+      tgt_key_indices[j] = tgt_key_idx;
+      tgt_value_indices[j] = tgt_value_idx;
+      src_key_indices[j] = src_key_idx;
+      src_value_indices[j] = src_value_idx;
+
+      keys_to_store[j] = VLLM_LDG(&key_cache[src_key_idx]);
+      values_to_store[j] = VLLM_LDG(&value_cache[src_value_idx]);
+    }
+
+#pragma unroll
+    for (int j = 0; j < unroll_factor; ++j) {
+      key[tgt_key_indices[j]] = keys_to_store[j];
+      value[tgt_value_indices[j]] = values_to_store[j];
+    }
+  }
+}
+
+template <typename scalar_t>
+void call_gather_cached_kv_kernel_optimized(
+    torch::Tensor& key,
+    torch::Tensor& value,
+    torch::Tensor& key_cache,
+    torch::Tensor& value_cache,
+    torch::Tensor& slot_mapping) {
+  using sycl_t = vllm::xpu::SyclTypeTrait<scalar_t>::Type;
+  int num_tokens = key.size(0);
+  int num_heads = key.size(1);
+  int head_size = key.size(2);
+  int block_size = key_cache.size(2);
+  // int x = key_cache.size(4);
+  int x = 1;
+
+  int key_stride = key.stride(0);
+  int value_stride = value.stride(0);
+  auto key_ptr = key.data_ptr<scalar_t>();
+  auto value_ptr = value.data_ptr<scalar_t>();
+  auto key_cache_ptr = key_cache.data_ptr<scalar_t>();
+  auto value_cache_ptr = value_cache.data_ptr<scalar_t>();
+  auto slot_mapping_ptr = slot_mapping.data_ptr<int>();
+  sycl::range<3> grid(1, 1, num_tokens);
+  sycl::range<3> block(1, 1, std::min(num_heads * head_size, 512));
+  auto& queue = vllm::xpu::vllmGetQueue();
+  queue.submit([&](sycl::handler& cgh) {
+    cgh.parallel_for(
+        sycl::nd_range<3>(grid * block, block), [=](sycl::nd_item<3> item_ct1) {
+          gather_cached_kv_kernel_optimized<sycl_t>(
+              (sycl_t* __restrict__)key_ptr,
+              (sycl_t* __restrict__)value_ptr,
+              (const sycl_t* __restrict__)key_cache_ptr,
+              (const sycl_t* __restrict__)value_cache_ptr,
+              slot_mapping_ptr,
+              key_stride,
+              value_stride,
+              num_heads,
+              head_size,
+              block_size,
+              x,
+              item_ct1);
+        });
+  });
+}
+
+void gather_cached_kv(
+    torch::Tensor& key,
+    torch::Tensor& value,
+    torch::Tensor& key_cache,
+    torch::Tensor& value_cache,
+    torch::Tensor& slot_mapping) {
+  VLLM_XPU_DISPATCH_FLOATING_TYPES(
+      key_cache[0].scalar_type(),
+      "call_gather_cached_kv_kernel_optimized",
+      [&] {
+        call_gather_cached_kv_kernel_optimized<scalar_t>(
+            key, value, key_cache, value_cache, slot_mapping);
+      });
+}
diff --git a/csrc/xpu/cache_ops_xpu_fp8.cpp b/csrc/xpu/cache_ops_xpu_fp8.cpp
new file mode 100644
index 000000000..e4a0001fe
--- /dev/null
+++ b/csrc/xpu/cache_ops_xpu_fp8.cpp
@@ -0,0 +1,170 @@
+// clang-format off
+#ifdef VLLM_DEV
+#undef __SYCL_DEVICE_ONLY__
+#endif
+#include <sycl/sycl.hpp>
+#include <dpct/dpct.hpp>
+#include <ext/intel/esimd.hpp>
+// clang-format on
+#include "xpu_types.h"
+
+#include <torch/extension.h>
+#include "utils.h"
+#include "kv.h"
+
+using fp16 = sycl::half;
+using namespace sycl::ext::intel::esimd;
+
+// scalar_t is key.scalar_type() -> half
+template <typename scalar_t, const int HD>
+void reshape_and_cache_ipexllm_kernel_fp8(
+    const scalar_t* __restrict__ key,    // [num_tokens, num_heads, head_size]
+    const scalar_t* __restrict__ value,  // [num_tokens, num_heads, head_size]
+    uint8_t * __restrict__ key_cache,  // [num_blocks, num_kv_heads, block_size,
+                                       // head_size]
+    uint8_t * __restrict__ value_cache,        // [num_blocks, num_kv_heads,
+                                               // block_size, head_size]
+    const int64_t* __restrict__ slot_mapping,  // [num_tokens]
+    const int key_stride, const int value_stride,
+    const int key_head_stride, const int value_head_stride,
+    const int num_heads,
+    const int head_size, const int block_size, const int x,
+    const sycl::nd_item<3>& item_ct1) {
+
+  //                      New Implementation                      //
+  const size_t token_idx = item_ct1.get_global_id(0);
+  const size_t head_idx = item_ct1.get_global_id(1);
+  const int64_t slot_idx = slot_mapping[token_idx];
+  if (slot_idx < 0) {
+    return;
+  }
+  const int64_t block_idx = slot_idx / block_size;
+  const int64_t block_offset = slot_idx % block_size;
+  // The thread is responsible for the HD elements within key/value
+  const scalar_t * key_head = key + token_idx * key_stride + head_idx * key_head_stride;
+
+  const scalar_t * value_head = value + token_idx * value_stride + head_idx * value_head_stride;
+
+  uint8_t * key_output_head = key_cache + block_idx * num_heads * head_size * block_size +
+      head_idx * head_size * block_size + block_offset * head_size;
+  uint8_t * value_output_head = value_cache + block_idx * num_heads * head_size * block_size +
+      head_idx * head_size * block_size + block_offset * head_size;
+
+  simd<fp16, HD> key_row = block_load<scalar_t, HD>(key_head);
+  simd<uint8_t, HD> key_result = quantize_key_row<HD>(key_row);
+  block_store<uint8_t, HD>(key_output_head, key_result);
+
+  simd<fp16, HD> value_row = block_load<scalar_t, HD>(value_head);
+  simd<uint8_t, HD> value_result = quantize_value_row<HD>(value_row);
+  block_store<uint8_t, HD>(value_output_head, value_result);
+}
+
+
+template <typename scalar_t, const int HD>
+void call_reshape_and_cache_ipexllm_kernel_fp8(
+    const scalar_t* __restrict__ key, const scalar_t* __restrict__ value,
+    uint8_t* __restrict__ key_cache, uint8_t* __restrict__ value_cache,
+    const int64_t* __restrict__ slot_mapping, const int num_tokens,
+    const int key_stride, const int value_stride,
+    const int key_head_stride, const int value_head_stride,
+    const int num_heads,
+    const int head_size, const int block_size, const int x) {
+  using sycl_t = vllm::xpu::SyclTypeTrait<scalar_t>::Type;
+  sycl::range<3> grid(num_tokens, num_heads, 1);
+  sycl::range<3> block(1, 1, 1);
+  auto& queue = vllm::xpu::vllmGetQueue();
+  queue.submit([&](sycl::handler& cgh) {
+    cgh.parallel_for(
+        sycl::nd_range<3>(grid * block, block), [=](sycl::nd_item<3> item_ct1) SYCL_ESIMD_KERNEL {
+          reshape_and_cache_ipexllm_kernel_fp8<sycl_t, HD>(
+              (const sycl_t* __restrict__)key,
+              (const sycl_t* __restrict__)value,
+              (uint8_t* __restrict__)key_cache,
+              (uint8_t* __restrict__)value_cache, slot_mapping, key_stride,
+              value_stride, key_head_stride, value_head_stride,
+              num_heads, head_size, block_size, x, item_ct1);
+        });
+  });
+}
+
+void reshape_and_cache_ipexllm_fp8(torch::Tensor& key, torch::Tensor& value,
+                               torch::Tensor& key_cache,
+                               torch::Tensor& value_cache,
+                               torch::Tensor& slot_mapping,
+                               const std::string& kv_cache_dtype,
+                               const float kv_scale) {
+  int num_tokens = key.size(0);
+  int num_heads = key.size(1);
+  int head_size = key.size(2);
+  int block_size = key_cache.size(2);
+  // int x = key_cache.size(4);
+  int x = 1;
+
+  int key_stride = key.stride(0);
+  int value_stride = value.stride(0);
+
+  int key_head_stride = key.stride(1);
+  int value_head_stride = value.stride(1);
+
+  // This actually dispatches on scalar_type, we will then need to dispatch on Head Dim...
+switch (head_size) {
+  case 64:
+    VLLM_XPU_DISPATCH_FLOATING_TYPES(
+        key.scalar_type(), "call_reshape_and_cache_ipexllm_kernel_fp8", [&] {
+          call_reshape_and_cache_ipexllm_kernel_fp8<scalar_t, 64>(
+              key.data_ptr<scalar_t>(), value.data_ptr<scalar_t>(),
+              key_cache.data_ptr<uint8_t>(), value_cache.data_ptr<uint8_t>(),
+              slot_mapping.data_ptr<int64_t>(), num_tokens, key_stride,
+              value_stride, key_head_stride, value_head_stride, num_heads,
+              head_size, block_size, x);
+        });
+    break;
+  case 128:
+    VLLM_XPU_DISPATCH_FLOATING_TYPES(
+        key.scalar_type(), "call_reshape_and_cache_ipexllm_kernel_fp8", [&] {
+          call_reshape_and_cache_ipexllm_kernel_fp8<scalar_t, 128>(
+              key.data_ptr<scalar_t>(), value.data_ptr<scalar_t>(),
+              key_cache.data_ptr<uint8_t>(), value_cache.data_ptr<uint8_t>(),
+              slot_mapping.data_ptr<int64_t>(), num_tokens, key_stride,
+              value_stride, key_head_stride, value_head_stride, num_heads,
+              head_size, block_size, x);
+        });
+    break;
+  case 96:
+    VLLM_XPU_DISPATCH_FLOATING_TYPES(
+        key.scalar_type(), "call_reshape_and_cache_ipexllm_kernel_fp8", [&] {
+          call_reshape_and_cache_ipexllm_kernel_fp8<scalar_t, 96>(
+              key.data_ptr<scalar_t>(), value.data_ptr<scalar_t>(),
+              key_cache.data_ptr<uint8_t>(), value_cache.data_ptr<uint8_t>(),
+              slot_mapping.data_ptr<int64_t>(), num_tokens, key_stride,
+              value_stride, key_head_stride, value_head_stride, num_heads,
+              head_size, block_size, x);
+        });
+    break;
+  case 80:
+    VLLM_XPU_DISPATCH_FLOATING_TYPES(
+        key.scalar_type(), "call_reshape_and_cache_ipexllm_kernel_fp8", [&] {
+          call_reshape_and_cache_ipexllm_kernel_fp8<scalar_t, 80>(
+              key.data_ptr<scalar_t>(), value.data_ptr<scalar_t>(),
+              key_cache.data_ptr<uint8_t>(), value_cache.data_ptr<uint8_t>(),
+              slot_mapping.data_ptr<int64_t>(), num_tokens, key_stride,
+              value_stride, key_head_stride, value_head_stride, num_heads,
+              head_size, block_size, x);
+        });
+    break;
+  default:
+    TORCH_CHECK(false, "Unsupported head_dim: ", head_size);
+}
+  // VLLM_XPU_DISPATCH_FLOATING_TYPES(
+  //     key.scalar_type(), "call_reshape_and_cache_ipexllm_kernel_fp8", [&] {
+  //       call_reshape_and_cache_ipexllm_kernel_fp8<scalar_t, 128>(
+  //           key.data_ptr<scalar_t>(), value.data_ptr<scalar_t>(),
+  //           key_cache.data_ptr<uint8_t>(), value_cache.data_ptr<uint8_t>(),
+  //           slot_mapping.data_ptr<int64_t>(), num_tokens, key_stride,
+  //           value_stride, key_head_stride, value_head_stride,
+  //           num_heads, head_size, block_size, x);
+  //     });
+}
+
+
+
diff --git a/csrc/xpu/common.h b/csrc/xpu/common.h
new file mode 100644
index 000000000..17d6ef643
--- /dev/null
+++ b/csrc/xpu/common.h
@@ -0,0 +1,312 @@
+#pragma once
+
+#include <sycl.hpp>
+#include <torch/extension.h>
+
+typedef union half_t {
+    uint16_t u;
+    sycl::half f;
+} __half_t;
+
+typedef union ufloat32 {
+    unsigned u;
+    float f;
+} __float_t;
+
+#define QK4_0 64
+#define QR4_0 2
+#define QK4_1 64
+#define QR4_1 2
+#define QK5_0 64
+#define QR5_0 2
+#define QK5_1 64
+#define QR5_1 2
+#define QK8_0 64
+#define QR8_0 1
+#define QK8_1 32
+#define QR8_1 1
+#define QI8_1 (QK8_1 / (4 * QR8_1)) // 8
+#define QKFP8 64
+#define QRFP8 1
+#define QKFP6 64
+// for iq2 quantization
+#define WARP_SIZE 32
+#define QK_K 256
+#define QK4_K 32
+#define QR4_K 2
+#define QK6_K 16
+#define QKFP6_K 16
+#define QR2_XXS 8
+#define QI2_XXS (QK_K / (4*QR2_XXS)) // 8
+#define QR2_XS 8
+#define QI2_XS (QK_K / (4*QR2_XS)) // 8
+#define QR2_K 4
+#define QI2_K (QK_K / (4*QR2_K)) // 16
+#define QR1_S 8
+#define QI1_S (QK_K / (4*QR1_S)) // 8
+
+typedef struct {
+    sycl::half d;          // delta
+    uint8_t qs[QK4_0 / 2];    // nibbles / quants
+} block_q4_0;
+
+typedef struct {
+    uint8_t qs[QK4_0 / 2];    // nibbles / quants
+} block_q4_0_qs;
+
+typedef struct {
+    uint8_t qs[QK4_1 / 2];    // nibbles / quants
+} block_q4_1_qs;
+
+typedef struct {
+    sycl::half d;              // delta
+    sycl::half m;              // min
+    uint8_t qs[QK4_1 / 2];     // nibbles / quants
+} block_q4_1;
+
+typedef struct {
+    sycl::half d;
+    uint8_t qh[8];
+    uint8_t qs[QK5_0 / 2];
+} block_q5_0;
+
+typedef struct {
+    sycl::half d;          // delta
+    sycl::half m;          // min
+    uint8_t qh[8];         // 5-th bit of quants
+    uint8_t qs[QK5_1 / 2]; // nibbles / quants
+} block_q5_1;
+
+typedef struct {
+    sycl::half d;           // delta
+    uint8_t qh[8];          // 3-th bit of quants
+    uint8_t qs[QK4_0 / 4];  // nibbles / quants
+} block_nf3;
+
+typedef struct {
+    uint8_t qh[8];          // 3-th bit of quants
+    uint8_t qs[QK4_0 / 4];  // nibbles / quants
+} block_nf3_qs;
+
+typedef struct {
+    float d;       // delta
+    int8_t qs[QK8_0];   // quants
+} block_q8_0;
+
+typedef struct {
+    int8_t qs[QK8_0];   // quants
+} block_q8_0_qs;
+
+typedef struct {
+    sycl::half d;
+    sycl::half sum;
+    int8_t  qs[QK8_1];      // quants
+} block_q8_1;
+
+typedef struct {
+    uint8_t qs[QKFP8];
+} block_fp8_qs;
+
+typedef struct {
+    float d;
+    uint8_t qs[QKFP8];
+} block_fp8;
+
+typedef struct {
+    sycl::half d;
+    uint16_t qs[QK_K/8]; // 32
+} block_iq2_xxs;
+
+typedef struct {
+    sycl::half d;
+    uint16_t qs[QK_K/8]; // 32
+    uint8_t  scales[QK_K/32]; // 8
+} block_iq2_xs;
+
+typedef struct {
+    uint8_t scales[QK_K/16]; // scales and mins, quantized with 4 bits
+    uint8_t qs[QK_K/4];      // quants
+    sycl::half d;            // super-block scale for quantized scales
+    sycl::half min;          // super-block min for quantized mins
+} block_q2_K;
+
+typedef struct {
+    sycl::half d;                 // super-block scale for quantized scales
+    sycl::half dmin;              // super-block scale for quantized mins
+    uint8_t scales[16];           // scales and mins, quantized with 8 bits
+    uint8_t qs[QK_K/2];           // 4--bit quants
+} block_q4_K;
+
+typedef struct {
+    uint8_t qs[QK_K/2];            // 4-bit quants
+} block_q4_K_qs;
+
+typedef struct {
+    uint8_t qs[QK4_K/2];            // 4-bit quants
+} block_q4_K_qs_block;
+
+typedef struct {
+    uint8_t scales[16];            // scales and mins, quantized with 8 bits
+} block_q4_K_scales;
+
+typedef struct {
+    sycl::half d;               // super-block scale for quantized scales
+    sycl::half dmin;            // super-block scale for quantized mins
+    uint8_t scales[12];         // scales and mins, quantized with 6 bits
+    uint8_t qh[QK_K/8];          // quants, high bit
+    uint8_t qs[QK_K/2];          // quants, low 4 bits
+} block_q5_K;
+
+typedef struct {
+    uint8_t ql[QK_K/2];   // quants, lower 4 bits
+    uint8_t qh[QK_K/4];   // quants, upper 2 bits
+    int8_t  scales[QK_K/16]; // scales
+    sycl::half d;            // delta
+} block_q6_K;
+
+typedef struct {
+    uint32_t qh[QK_K/16];      // quants, upper 2 bits
+} block_q6_K_qh;
+
+typedef struct {
+    uint32_t ql[QK_K/8];      // quants, lower 4 bits
+} block_q6_K_ql;
+
+typedef struct {
+    int8_t  scales[QK_K/16]; // scales, quantized with 8 bits
+} block_q6_K_scales;
+
+typedef struct {
+    uint8_t ql[QK_K/2];       // quants, lower 4 bits
+    uint8_t qh[QK_K/4];       // quants, upper 2 bits
+    int8_t  scales[QK_K/16];  // scales, quantized with 8 bits
+    sycl::half d;            // super-block scale
+} block_fp6_K;
+static_assert(sizeof(block_fp6_K) == sizeof(sycl::half) + QK_K / 16 + 3*QK_K/4, "wrong fp6_K block size/padding");
+
+typedef struct {
+    uint32_t ql[QK_K/8];      // quants, lower 4 bits
+} block_fp6_k_ql;
+
+typedef struct {
+    uint32_t qh[QK_K/16];     // quants, upper 2 bits
+} block_fp6_k_qh;
+
+typedef struct {
+    int8_t scales[QK_K/16];  // scales, quantized with 8 bits, 16
+} block_fp6_k_scales;
+
+typedef struct {
+    uint32_t ql[QKFP6_K/8];     // upper 2 bits, 2
+} block_base_fp6_k_ql;
+
+typedef struct {
+    uint32_t qh[QKFP6_K/16];     // upper 2 bits, 1
+} block_base_fp6_k_qh;
+
+#define NGRID_IQ1S 2048
+#define IQ1S_DELTA 0.125f
+#define IQ1M_DELTA 0.125f
+
+typedef struct {
+    sycl::half d;
+    uint8_t  qs[QK_K/8];
+    uint16_t qh[QK_K/32];
+} block_iq1_s;
+
+// 1.8125 bpw
+typedef struct {
+    uint8_t  qs[QK_K/8];      // grid index, low 8 bits
+    uint8_t  qh[QK_K/16];     // grid index, high 3 bits + grid shift bit (for two groups of 8)
+    uint8_t  scales[QK_K/32]; // 4-bit block scales
+} block_iq1_m;
+
+typedef struct {
+    uint8_t ql[QKFP6/2];      // lower 4 bits, 32
+    uint8_t qh[QKFP6/4];      // upper 2 bits, 16
+    sycl::half  d;            // delta
+} block_fp6;
+
+typedef struct {
+    uint32_t qh[QKFP6/16];     // upper 2 bits, 4
+} block_fp6_32_qh;
+
+typedef struct {
+    uint32_t ql[QKFP6/8];      // lower 4 bits, 8
+} block_fp6_32_ql;
+
+enum ggml_type {
+    GGML_TYPE_Q4_0 = 2,
+    GGML_TYPE_Q4_1 = 3,
+    GGML_TYPE_Q5_0 = 6,
+    GGML_TYPE_Q5_1 = 7,
+    GGML_TYPE_Q8_0 = 8,
+    GGML_TYPE_Q8_1 = 9,
+    GGML_TYPE_NF4 = 10,
+    GGML_TYPE_NF3 = 11,
+    GGML_TYPE_FP8E4 = 15,
+    GGML_TYPE_FP4 = 16,
+    GGML_TYPE_FP8E5 = 19,
+    GGML_TYPE_IQ2_XXS = 21,
+    GGML_TYPE_IQ2_XS = 22,
+    GGML_TYPE_Q2_K = 23,
+    GGML_TYPE_IQ1_S = 24,
+    GGML_TYPE_IQ1_M = 25,
+    GGML_TYPE_Q6_K = 26,
+    GGML_TYPE_Q4_K = 27,
+    GGML_TYPE_Q5_K = 28,
+    GGML_TYPE_FP6 = 29,
+    GGML_TYPE_FP6_K = 30,
+    GGML_TYPE_Q4_0_WOQ = 34,
+    GGML_TYPE_COUNT
+};
+
+static const int GGML_BLCK_SIZE[GGML_TYPE_COUNT] = {
+    [GGML_TYPE_Q4_0] = QK4_0,
+    [GGML_TYPE_Q4_1] = QK4_1,
+    [GGML_TYPE_Q5_0] = QK5_0,
+    [GGML_TYPE_Q5_1] = QK5_1,
+    [GGML_TYPE_NF4]  = QK4_0,
+    [GGML_TYPE_NF3]  = QK4_0,
+    [GGML_TYPE_Q8_0] = QK8_0,
+    [GGML_TYPE_Q8_1] = QK8_1,
+    [GGML_TYPE_FP8E4]  = QKFP8,
+    [GGML_TYPE_FP4]  = QK4_0,
+    [GGML_TYPE_FP6]  = QKFP6,
+    [GGML_TYPE_FP8E5]  = QKFP8,
+    [GGML_TYPE_IQ2_XXS] = QK_K,
+    [GGML_TYPE_IQ2_XS] = QK_K,
+    [GGML_TYPE_Q2_K] = QK_K,
+    [GGML_TYPE_IQ1_S] = QK_K,
+    [GGML_TYPE_IQ1_M] = QK_K,
+    [GGML_TYPE_Q6_K] = QK_K,
+    [GGML_TYPE_Q4_K] = QK_K,
+    [GGML_TYPE_Q5_K] = QK_K,
+    [GGML_TYPE_FP6_K] = QK_K,
+    [GGML_TYPE_Q4_0_WOQ] = QK4_0,
+};
+
+static const size_t GGML_TYPE_SIZE[GGML_TYPE_COUNT] = {
+    [GGML_TYPE_Q4_0] = sizeof(block_q4_0),
+    [GGML_TYPE_Q4_1] = sizeof(block_q4_1),
+    [GGML_TYPE_Q5_0] = sizeof(block_q5_1),
+    [GGML_TYPE_Q5_1] = sizeof(block_q5_1),
+    [GGML_TYPE_NF4]  = sizeof(block_q4_0),
+    [GGML_TYPE_NF3]  = sizeof(block_nf3),
+    [GGML_TYPE_Q8_0] = sizeof(block_q8_0),
+    [GGML_TYPE_Q8_1] = sizeof(block_q8_1),
+    [GGML_TYPE_FP8E4]= sizeof(block_fp8),
+    [GGML_TYPE_FP4]  = sizeof(block_q4_0),
+    [GGML_TYPE_FP6]  = sizeof(block_fp6),
+    [GGML_TYPE_FP8E5]  = sizeof(block_fp8),
+    [GGML_TYPE_IQ2_XXS] = sizeof(block_iq2_xxs),
+    [GGML_TYPE_IQ2_XS] = sizeof(block_iq2_xs),
+    [GGML_TYPE_Q2_K] = sizeof(block_q2_K),
+    [GGML_TYPE_IQ1_S] = sizeof(block_iq1_s),
+    [GGML_TYPE_IQ1_M] = sizeof(block_iq1_m),
+    [GGML_TYPE_Q6_K] = sizeof(block_q6_K),
+    [GGML_TYPE_Q4_K] = sizeof(block_q4_K),
+    [GGML_TYPE_Q5_K] = sizeof(block_q5_K),
+    [GGML_TYPE_FP6_K] = sizeof(block_fp6_K),
+    [GGML_TYPE_Q4_0_WOQ] = sizeof(block_q4_0),
+};
diff --git a/csrc/xpu/dequantize.h b/csrc/xpu/dequantize.h
new file mode 100644
index 000000000..9a967312e
--- /dev/null
+++ b/csrc/xpu/dequantize.h
@@ -0,0 +1,74 @@
+#include <dpct/dpct.hpp>
+#include <sycl/sycl.hpp>
+#include "utils.h"
+/*
+Adapted from https://github.com/mit-han-lab/llm-awq
+Modified from NVIDIA FasterTransformer:
+https://github.com/NVIDIA/FasterTransformer/blob/main/src/fastertransformer/cutlass_extensions/include/cutlass_extensions/interleaved_numeric_conversion.h
+@article{lin2023awq,
+  title={AWQ: Activation-aware Weight Quantization for LLM Compression and
+Acceleration}, author={Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang,
+Shang and Dang, Xingyu and Han, Song}, journal={arXiv}, year={2023}
+}
+*/
+
+#pragma once
+
+namespace vllm {
+namespace awq {
+
+sycl::uint4 dequantize_s4_to_fp16x2(uint32_t const& source) {
+  sycl::uint4 result;
+
+  uint32_t* h = reinterpret_cast<uint32_t*>(&result);
+  uint32_t const i4s = reinterpret_cast<uint32_t const&>(source);
+
+  // First, we extract the i4s and construct an intermediate fp16 number.
+  static constexpr uint32_t immLut = (0xf0 & 0xcc) | 0xaa;
+  static constexpr uint32_t BOTTOM_MASK = 0x000f000f;
+  static constexpr uint32_t TOP_MASK = 0x00f000f0;
+  static constexpr uint32_t I4s_TO_F16s_MAGIC_NUM = 0x64006400;
+
+  // Note that the entire sequence only requires 1 shift instruction. This is
+  // thanks to the register packing format and the fact that we force our
+  // integers to be unsigned, and account for this in the fp16 subtractions. In
+  // addition, I exploit the fact that sub and fma have the same throughput in
+  // order to convert elt_23 and elt_67 to fp16 without having to shift them to
+  // the bottom bits before hand.
+
+  // Shift right by 8 to now consider elt_45 and elt_67. Issue first to hide RAW
+  // dependency if we issue immediately before required.
+  const uint32_t top_i4s = i4s >> 8;
+  h[0] = (i4s & BOTTOM_MASK) | I4s_TO_F16s_MAGIC_NUM;
+  h[1] = (i4s & TOP_MASK) | I4s_TO_F16s_MAGIC_NUM;
+  h[2] = (top_i4s & BOTTOM_MASK) | I4s_TO_F16s_MAGIC_NUM;
+  h[3] = (top_i4s & TOP_MASK) | I4s_TO_F16s_MAGIC_NUM;
+
+  // This is the half2 {1032, 1032} represented as an integer.
+  // static constexpr uint32_t FP16_TOP_MAGIC_NUM = 0x64086408;
+  // Haotian: subtract {1024, 1024} instead, we do not need to map to [-8, 7]
+  static constexpr uint32_t FP16_TOP_MAGIC_NUM = 0x64006400;
+  // This is the half2 {1 / 16, 1 / 16} represented as an integer.
+  static constexpr uint32_t ONE_SIXTEENTH = 0x2c002c00;
+  // This is the half2 {-72, -72} represented as an integer.
+  // static constexpr uint32_t NEG_72 = 0xd480d480;
+  // Haotian: Let's use {-64, -64}.
+  static constexpr uint32_t NEG_64 = 0xd400d400;
+  *(sycl::half2*)(&h[0]) = sycl_half_sub2(
+      *(sycl::half2*)(&h[0]), *(sycl::half2*)(&FP16_TOP_MAGIC_NUM));
+  *(sycl::half2*)(&h[1]) = sycl_half_fma2(
+      *(sycl::half2*)(&h[1]),
+      *(sycl::half2*)(&ONE_SIXTEENTH),
+      *(sycl::half2*)(&NEG_64));
+  *(sycl::half2*)(&h[2]) = sycl_half_sub2(
+      *(sycl::half2*)(&h[2]), *(sycl::half2*)(&FP16_TOP_MAGIC_NUM));
+  *(sycl::half2*)(&h[3]) = sycl_half_fma2(
+      *(sycl::half2*)(&h[3]),
+      *(sycl::half2*)(&ONE_SIXTEENTH),
+      *(sycl::half2*)(&NEG_64));
+
+  return result;
+}
+
+} // namespace awq
+} // namespace vllm
\ No newline at end of file
diff --git a/csrc/xpu/dtype_float16.h b/csrc/xpu/dtype_float16.h
new file mode 100644
index 000000000..1b9c1f248
--- /dev/null
+++ b/csrc/xpu/dtype_float16.h
@@ -0,0 +1,458 @@
+/*
+ * Adapted from
+ * https://github.com/NVIDIA/FasterTransformer/blob/release/v5.3_tag/src/fastertransformer/kernels/decoder_masked_multihead_attention/decoder_masked_multihead_attention_template.hpp
+ * and
+ * https://github.com/NVIDIA/FasterTransformer/blob/release/v5.3_tag/src/fastertransformer/kernels/decoder_masked_multihead_attention_utils.h
+ * Copyright (c) 2023, The vLLM team.
+ * Copyright (c) 2020-2023, NVIDIA CORPORATION.  All rights reserved.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+#pragma once
+
+#include <dpct/dpct.hpp>
+#include <sycl/sycl.hpp>
+#include "attention_generic.h"
+#include "dtype_float32.h"
+#include "utils.h"
+
+#include <stdint.h>
+
+namespace vllm {
+
+// FP16 vector types for Q, K, V.
+template <>
+struct Vec<sycl::half, 1> {
+  using Type = sycl::half;
+};
+template <>
+struct Vec<sycl::half, 2> {
+  using Type = sycl::half2;
+};
+template <>
+struct Vec<sycl::half, 4> {
+  using Type = sycl::half4;
+};
+template <>
+struct Vec<sycl::half, 8> {
+  using Type = sycl::half8;
+};
+
+template <>
+struct FloatVec<sycl::half> {
+  using Type = float;
+};
+template <>
+struct FloatVec<sycl::half2> {
+  using Type = sycl::float2;
+};
+
+template <>
+struct FloatVec<sycl::half4> {
+  using Type = Float4_;
+};
+template <>
+struct FloatVec<sycl::half8> {
+  using Type = Float8_;
+};
+
+// Utility functions for type conversions.
+inline sycl::half2 h0_h0(sycl::half a) {
+  return sycl::half2{a, a};
+}
+
+inline float half_to_float(sycl::half h) {
+  return float(h);
+}
+
+inline sycl::float2 half2_to_float2(sycl::half2 v) {
+
+  return sycl::float2(half_to_float(v.x()), half_to_float(v.y()));
+}
+
+inline sycl::half float_to_half(float f) {
+  return sycl::half(f);
+}
+
+inline sycl::half2 float2_to_half2(sycl::float2 f) {
+  return sycl::half2{float_to_half(f.x()), float_to_half(f.y())};
+}
+
+// Vector addition.
+inline sycl::half add(sycl::half a, sycl::half b) {
+  return sycl_half_add(a,b);
+}
+
+inline sycl::half2 add(sycl::half2 a, sycl::half2 b) {
+  auto val = sycl_half_add2(a, b);
+  return (val);
+}
+
+inline sycl::half4 add(sycl::half4 a, sycl::half4 b) {
+  sycl::half4 c;
+  c.x() = add(a.x(), b.x());
+  c.y() = add(a.y(), b.y());
+  c.z() = add(a.z(), b.z());
+  c.w() = add(a.w(), b.w());
+  return c;
+}
+
+inline sycl::half8 add(sycl::half8 a, sycl::half8 b) {
+  sycl::half8 c;
+  c.s0() = add(a.s0(), b.s0());
+  c.s1() = add(a.s1(), b.s1());
+  c.s2() = add(a.s2(), b.s2());
+  c.s3() = add(a.s3(), b.s3());
+  c.s4() = add(a.s4(), b.s4());
+  c.s5() = add(a.s5(), b.s5());
+  c.s6() = add(a.s6(), b.s6());
+  c.s7() = add(a.s7(), b.s7());
+  return c;
+}
+
+inline sycl::float2 add(sycl::half2 a, sycl::float2 fb) {
+  sycl::float2 fa = half2_to_float2(a);
+  return add(fa, fb);
+}
+
+inline Float4_ add(sycl::half4 a, Float4_ fb) {
+  Float4_ fc;
+  fc.x = add(sycl::half2{a.x(), a.y()}, fb.x);
+  fc.y = add(sycl::half2{a.z(), a.w()}, fb.y);
+  return fc;
+}
+
+inline Float8_ add(sycl::half8 a, Float8_ fb) {
+  Float8_ fc;
+  fc.x = add(sycl::half2{a.s0(), a.s1()}, fb.x);
+  fc.y = add(sycl::half2{a.s2(), a.s3()}, fb.y);
+  fc.z = add(sycl::half2{a.s4(), a.s5()}, fb.z);
+  fc.w = add(sycl::half2{a.s6(), a.s7()}, fb.w);
+  return fc;
+}
+
+// Vector multiplication.
+template <>
+inline sycl::half mul(sycl::half a, sycl::half b) {
+  auto val = sycl_half_mul((a), (b));
+  return (val);
+}
+
+template <>
+inline sycl::half2 mul(sycl::half2 a, sycl::half2 b) {
+  auto val = sycl_half_mul2((a), (b));
+  return (val);
+}
+
+template <>
+inline sycl::half2 mul(sycl::half a, sycl::half2 b) {
+  return mul<sycl::half2, sycl::half2, sycl::half2>(h0_h0(a), b);
+}
+
+
+template <>
+inline sycl::half4 mul(sycl::half4 a, sycl::half4 b) {
+  sycl::half4 c;
+  c.x() = mul<sycl::half, sycl::half, sycl::half>(a.x(), b.x());
+  c.y() = mul<sycl::half, sycl::half, sycl::half>(a.y(), b.y());
+  c.z() = mul<sycl::half, sycl::half, sycl::half>(a.z(), b.z());
+  c.w() = mul<sycl::half, sycl::half, sycl::half>(a.w(), b.w());
+  return c;
+}
+
+template <>
+inline sycl::half4 mul(sycl::half a, sycl::half4 b) {
+  sycl::half4 c;
+  c.x() = mul<sycl::half, sycl::half, sycl::half>(a, b.x());
+  c.y() = mul<sycl::half, sycl::half, sycl::half>(a, b.y());
+  c.z() = mul<sycl::half, sycl::half, sycl::half>(a, b.z());
+  c.w() = mul<sycl::half, sycl::half, sycl::half>(a, b.w());
+  return c;
+}
+
+template <>
+inline sycl::half8 mul(sycl::half8 a, sycl::half8 b) {
+  sycl::half8 c;
+  c.s0() = mul<sycl::half, sycl::half, sycl::half>(a.s0(), b.s0());
+  c.s1() = mul<sycl::half, sycl::half, sycl::half>(a.s1(), b.s1());
+  c.s2() = mul<sycl::half, sycl::half, sycl::half>(a.s2(), b.s2());
+  c.s3() = mul<sycl::half, sycl::half, sycl::half>(a.s3(), b.s3());
+  c.s4() = mul<sycl::half, sycl::half, sycl::half>(a.s4(), b.s4());
+  c.s5() = mul<sycl::half, sycl::half, sycl::half>(a.s5(), b.s5());
+  c.s6() = mul<sycl::half, sycl::half, sycl::half>(a.s6(), b.s6());
+  c.s7() = mul<sycl::half, sycl::half, sycl::half>(a.s7(), b.s7());
+  return c;
+}
+
+template <>
+inline sycl::half8 mul(sycl::half a, sycl::half8 b) {
+  sycl::half8 c;
+  c.s0() = mul<sycl::half, sycl::half, sycl::half>(a, b.s0());
+  c.s1() = mul<sycl::half, sycl::half, sycl::half>(a, b.s1());
+  c.s2() = mul<sycl::half, sycl::half, sycl::half>(a, b.s2());
+  c.s3() = mul<sycl::half, sycl::half, sycl::half>(a, b.s3());
+  c.s4() = mul<sycl::half, sycl::half, sycl::half>(a, b.s4());
+  c.s5() = mul<sycl::half, sycl::half, sycl::half>(a, b.s5());
+  c.s6() = mul<sycl::half, sycl::half, sycl::half>(a, b.s6());
+  c.s7() = mul<sycl::half, sycl::half, sycl::half>(a, b.s7());
+  return c;
+}
+
+template <>
+inline float mul(sycl::half a, sycl::half b) {
+  float fa = half_to_float(a);
+  float fb = half_to_float(b);
+  return fa * fb;
+}
+
+template <>
+inline sycl::float2 mul(sycl::half2 a, sycl::half2 b) {
+  sycl::float2 fa = half2_to_float2(a);
+  sycl::float2 fb = half2_to_float2(b);
+  return mul<sycl::float2, sycl::float2, sycl::float2>(fa, fb);
+}
+
+template <>
+inline sycl::float2 mul(sycl::half a, sycl::half2 b) {
+  return mul<sycl::float2, sycl::half2, sycl::half2>(h0_h0(a), b);
+}
+
+template <>
+inline Float4_ mul(sycl::half4 a, sycl::half4 b) {
+  Float4_ fc;
+  fc.x = mul<sycl::float2, sycl::half2, sycl::half2>(
+      sycl::half2{a.x(), a.y()}, sycl::half2{b.x(), b.y()});
+  fc.y = mul<sycl::float2, sycl::half2, sycl::half2>(
+      sycl::half2{a.z(), a.w()}, sycl::half2{b.z(), b.w()});
+  return fc;
+}
+
+template <>
+inline Float4_ mul(sycl::half a, sycl::half4 b) {
+  sycl::half2 s = h0_h0(a);
+  Float4_ fc;
+
+  fc.x =
+      mul<sycl::float2, sycl::half2, sycl::half2>(s, sycl::half2{b.x(), b.y()});
+  fc.y =
+      mul<sycl::float2, sycl::half2, sycl::half2>(s, sycl::half2{b.z(), b.w()});
+  return fc;
+}
+
+template <>
+inline Float8_ mul(sycl::half8 a, sycl::half8 b) {
+  Float8_ fc;
+  fc.x = mul<sycl::float2, sycl::half2, sycl::half2>(
+      sycl::half2{a.s0(), a.s1()}, sycl::half2{b.s0(), b.s1()});
+  fc.y = mul<sycl::float2, sycl::half2, sycl::half2>(
+      sycl::half2{a.s2(), a.s3()}, sycl::half2{b.s2(), b.s3()});
+  fc.z = mul<sycl::float2, sycl::half2, sycl::half2>(
+      sycl::half2{a.s4(), a.s5()}, sycl::half2{b.s4(), b.s5()});
+  fc.w = mul<sycl::float2, sycl::half2, sycl::half2>(
+      sycl::half2{a.s6(), a.s7()}, sycl::half2{b.s6(), b.s7()});
+  return fc;
+}
+
+template <>
+inline Float8_ mul(sycl::half a, sycl::half8 b) {
+  sycl::half2 s = h0_h0(a);
+  Float8_ fc;
+  fc.x = mul<sycl::float2, sycl::half2, sycl::half2>(
+      s, sycl::half2{b.s0(), b.s1()});
+  fc.y = mul<sycl::float2, sycl::half2, sycl::half2>(
+      s, sycl::half2{b.s2(), b.s3()});
+  fc.z = mul<sycl::float2, sycl::half2, sycl::half2>(
+      s, sycl::half2{b.s4(), b.s5()});
+  fc.w = mul<sycl::float2, sycl::half2, sycl::half2>(
+      s, sycl::half2{b.s6(), b.s7()});
+  return fc;
+}
+
+// Vector fused multiply-add.
+inline sycl::half2 fma(sycl::half2 a, sycl::half2 b, sycl::half2 c) {
+  auto val = sycl_half_fma2((a), (b), (c));
+  return (val);
+}
+
+inline sycl::half2 fma(sycl::half a, sycl::half2 b, sycl::half2 c) {
+  return fma(h0_h0(a), b, c);
+}
+
+inline sycl::half4 fma(sycl::half4 a, sycl::half4 b, sycl::half4 c) {
+  sycl::half4 d;
+  d.x() = fma(a.x(), b.x(), c.x());
+  d.y() = fma(a.y(), b.y(), c.y());
+  d.z() = fma(a.z(), b.z(), c.z());
+  d.w() = fma(a.w(), b.w(), c.w());
+  return d;
+}
+
+inline sycl::half4 fma(sycl::half a, sycl::half4 b, sycl::half4 c) {
+  sycl::half4 s = sycl::half4{a, a, a, a};
+  return fma(s, b, c);
+}
+
+inline sycl::half8 fma(sycl::half8 a, sycl::half8 b, sycl::half8 c) {
+  sycl::half8 d;
+  d.s0() = fma(a.s0(), b.s0(), c.s0());
+  d.s1() = fma(a.s1(), b.s1(), c.s1());
+  d.s2() = fma(a.s2(), b.s2(), c.s2());
+  d.s3() = fma(a.s3(), b.s3(), c.s3());
+  d.s4() = fma(a.s4(), b.s4(), c.s4());
+  d.s5() = fma(a.s5(), b.s5(), c.s5());
+  d.s6() = fma(a.s6(), b.s6(), c.s6());
+  d.s7() = fma(a.s7(), b.s7(), c.s7());
+  return d;
+}
+
+inline sycl::half8 fma(sycl::half a, sycl::half8 b, sycl::half8 c) {
+  sycl::half8 d;
+  d.s0() = fma(a, b.s0(), c.s0());
+  d.s1() = fma(a, b.s1(), c.s1());
+  d.s2() = fma(a, b.s2(), c.s2());
+  d.s3() = fma(a, b.s3(), c.s3());
+  d.s4() = fma(a, b.s4(), c.s4());
+  d.s5() = fma(a, b.s5(), c.s5());
+  d.s6() = fma(a, b.s6(), c.s6());
+  d.s7() = fma(a, b.s7(), c.s7());
+  return d;
+}
+
+inline float fma(sycl::half a, sycl::half b, float fc) {
+  float fa = half_to_float(a);
+  float fb = half_to_float(b);
+  return sycl::fma(fa, fb, fc);
+}
+
+inline sycl::float2 fma(sycl::half2 a, sycl::half2 b, sycl::float2 fc) {
+  sycl::float2 fa = half2_to_float2(a);
+  sycl::float2 fb = half2_to_float2(b);
+  return fma(fa, fb, fc);
+}
+
+inline sycl::float2 fma(sycl::half a, sycl::half2 b, sycl::float2 fc) {
+  return fma(h0_h0(a), b, fc);
+}
+
+inline Float4_ fma(sycl::half4 a, sycl::half4 b, Float4_ fc) {
+  Float4_ fd;
+  fd.x = fma(sycl::half2{a.x(), a.y()}, sycl::half2{b.x(), b.y()}, fc.x);
+  fd.y = fma(sycl::half2{a.z(), a.w()}, sycl::half2{b.z(), b.w()}, fc.y);
+  return fd;
+}
+
+inline Float4_ fma(sycl::half a, sycl::half4 b, Float4_ fc) {
+  sycl::half4 s = sycl::half4{a, a, a, a};
+
+  return fma(s, b, fc);
+}
+
+inline Float8_ fma(sycl::half8 a, sycl::half8 b, Float8_ fc) {
+  Float8_ fd;
+  fd.x = fma(sycl::half2{a.s0(), a.s1()}, sycl::half2{b.s0(), b.s1()}, fc.x);
+  fd.y = fma(sycl::half2{a.s2(), a.s3()}, sycl::half2{b.s2(), b.s3()}, fc.y);
+  fd.z = fma(sycl::half2{a.s4(), a.s5()}, sycl::half2{b.s4(), b.s5()}, fc.z);
+  fd.w = fma(sycl::half2{a.s6(), a.s7()}, sycl::half2{b.s6(), b.s7()}, fc.w);
+  return fd;
+}
+
+inline Float8_ fma(sycl::half a, sycl::half8 b, Float8_ fc) {
+  sycl::half8 s = sycl::half8{a, a, a, a, a, a, a, a};
+
+  return fma(s, b, fc);
+}
+
+// Vector sum.
+template <>
+inline float sum(sycl::half v) {
+  return half_to_float(v);
+}
+
+template <>
+inline float sum(sycl::half2 v) {
+  sycl::float2 tmp = half2_to_float2(v);
+  return tmp.x() + tmp.y();
+}
+
+template <>
+inline float sum(sycl::half4 v) {
+  sycl::half2 c = add(sycl::half2{v.x(), v.y()}, sycl::half2{v.z(), v.w()});
+  return sum(c);
+}
+
+template <>
+inline float sum(sycl::half8 v) {
+  return add(
+      sum(sycl::half4{v.s0(), v.s1(), v.s2(), v.s3()}),
+      sum(sycl::half4{v.s4(), v.s5(), v.s6(), v.s7()}));
+}
+
+inline void from_float(sycl::half& dst, float src) {
+  dst = sycl::half(src);
+}
+
+inline void from_float(sycl::half2& dst, sycl::float2 src) {
+  dst = float2_to_half2(src);
+}
+
+inline void from_float(sycl::half4& dst, Float4_ src) {
+  sycl::half2 h0 = float2_to_half2(src.x);
+  sycl::half2 h1 = float2_to_half2(src.y);
+  dst.x() = h0.x();
+  dst.y() = h0.y();
+  dst.z() = h1.x();
+  dst.w() = h1.y();
+}
+
+inline void from_float(sycl::half8& dst, Float8_ src) {
+  dst.s0() = float2_to_half2(src.x).x();
+  dst.s1() = float2_to_half2(src.x).y();
+  dst.s2() = float2_to_half2(src.y).x();
+  dst.s3() = float2_to_half2(src.y).y();
+  dst.s4() = float2_to_half2(src.z).x();
+  dst.s5() = float2_to_half2(src.z).y();
+  dst.s6() = float2_to_half2(src.w).x();
+  dst.s7() = float2_to_half2(src.w).y();
+}
+
+// From float16 to float32.
+inline float to_float(sycl::half u) {
+  return half_to_float(u);
+}
+
+inline sycl::float2 to_float(sycl::half2 u) {
+  return half2_to_float2(u);
+}
+
+inline Float4_ to_float(sycl::half4 u) {
+  Float4_ tmp;
+  tmp.x = half2_to_float2(sycl::half2{u.x(), u.y()});
+  tmp.y = half2_to_float2(sycl::half2{u.z(), u.w()});
+  return tmp;
+}
+
+inline Float8_ to_float(sycl::half8 u) {
+  Float8_ tmp;
+  tmp.x = half2_to_float2(sycl::half2{u.s0(), u.s1()});
+  tmp.y = half2_to_float2(sycl::half2{u.s2(), u.s3()});
+  tmp.z = half2_to_float2(sycl::half2{u.s4(), u.s5()});
+  tmp.w = half2_to_float2(sycl::half2{u.s6(), u.s7()});
+  return tmp;
+}
+
+// Zero-out a variable.
+inline void zero(sycl::half& dst) {
+  dst = sycl::half(0);
+}
+
+} // namespace vllm
\ No newline at end of file
diff --git a/csrc/xpu/dtype_float32.h b/csrc/xpu/dtype_float32.h
new file mode 100644
index 000000000..7b70e4efc
--- /dev/null
+++ b/csrc/xpu/dtype_float32.h
@@ -0,0 +1,268 @@
+/*
+ * Adapted from https://github.com/NVIDIA/FasterTransformer/blob/release/v5.3_tag/src/fastertransformer/kernels/decoder_masked_multihead_attention/decoder_masked_multihead_attention_template.hpp
+ * and https://github.com/NVIDIA/FasterTransformer/blob/release/v5.3_tag/src/fastertransformer/kernels/decoder_masked_multihead_attention_utils.h
+ * Copyright (c) 2023, The vLLM team.
+ * Copyright (c) 2020-2023, NVIDIA CORPORATION.  All rights reserved.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+#pragma once
+
+#include <sycl/sycl.hpp>
+#include <dpct/dpct.hpp>
+#include "attention_generic.h"
+
+#include <stdint.h>
+
+namespace vllm {
+
+// Define custom FP32 vector data types.
+struct Float4_ {
+  sycl::float2 x;
+  sycl::float2 y;
+};
+
+struct Float8_ {
+  sycl::float2 x;
+  sycl::float2 y;
+  sycl::float2 z;
+  sycl::float2 w;
+};
+
+// FP32 vector types for Q, K, V.
+template<>
+struct Vec<float, 1> {
+  using Type = float;
+};
+template<>
+struct Vec<float, 2> {
+  using Type = sycl::float2;
+};
+template<>
+struct Vec<float, 4> {
+  using Type = sycl::float4;
+};
+
+// FP32 accumulator vector types corresponding to Vec.
+template<>
+struct FloatVec<float> {
+  using Type = float;
+};
+template <> struct FloatVec<sycl::float2> {
+  using Type = sycl::float2;
+};
+template <> struct FloatVec<sycl::float4> {
+  using Type = sycl::float4;
+};
+
+// Vector addition.
+inline float add(float a, float b) {
+  return a + b;
+}
+
+inline sycl::float2 add(sycl::float2 a, sycl::float2 b) {
+  sycl::float2 c;
+  c.x() = add(a.x(), b.x());
+  c.y() = add(a.y(), b.y());
+  return c;
+}
+
+inline sycl::float4 add(sycl::float4 a, sycl::float4 b) {
+  sycl::float4 c;
+  c.x() = add(a.x(), b.x());
+  c.y() = add(a.y(), b.y());
+  c.z() = add(a.z(), b.z());
+  c.w() = add(a.w(), b.w());
+  return c;
+}
+
+// Vector multiplication.
+template<>
+inline float mul<float, float>(float a, float b) {
+  return a * b;
+}
+
+template <> inline sycl::float2 mul(sycl::float2 a, sycl::float2 b) {
+  sycl::float2 c;
+  c.x() = a.x() * b.x();
+  c.y() = a.y() * b.y();
+  return c;
+}
+
+template <> inline sycl::float2 mul(float a, sycl::float2 b) {
+  sycl::float2 c;
+  c.x() = a * b.x();
+  c.y() = a * b.y();
+  return c;
+}
+
+template <> inline sycl::float4 mul(sycl::float4 a, sycl::float4 b) {
+  sycl::float4 c;
+  c.x() = a.x() * b.x();
+  c.y() = a.y() * b.y();
+  c.z() = a.z() * b.z();
+  c.w() = a.w() * b.w();
+  return c;
+}
+
+template <> inline sycl::float4 mul(float a, sycl::float4 b) {
+  sycl::float4 c;
+  c.x() = a * b.x();
+  c.y() = a * b.y();
+  c.z() = a * b.z();
+  c.w() = a * b.w();
+  return c;
+}
+
+// Vector fused multiply-add.
+inline float fma(float a, float b, float c) {
+  return a * b + c;
+}
+
+inline sycl::float2 fma(sycl::float2 a, sycl::float2 b, sycl::float2 c) {
+  sycl::float2 d;
+  d.x() = fma(a.x(), b.x(), c.x());
+  d.y() = fma(a.y(), b.y(), c.y());
+  return d;
+}
+
+inline sycl::float2 fma(float a, sycl::float2 b, sycl::float2 c) {
+  sycl::float2 d;
+  d.x() = fma(a, b.x(), c.x());
+  d.y() = fma(a, b.y(), c.y());
+  return d;
+}
+
+inline sycl::float4 fma(sycl::float4 a, sycl::float4 b, sycl::float4 c) {
+  sycl::float4 d;
+  d.x() = fma(a.x(), b.x(), c.x());
+  d.y() = fma(a.y(), b.y(), c.y());
+  d.z() = fma(a.z(), b.z(), c.z());
+  d.w() = fma(a.w(), b.w(), c.w());
+  return d;
+}
+
+inline sycl::float4 fma(float a, sycl::float4 b, sycl::float4 c) {
+  sycl::float4 d;
+  d.x() = fma(a, b.x(), c.x());
+  d.y() = fma(a, b.y(), c.y());
+  d.z() = fma(a, b.z(), c.z());
+  d.w() = fma(a, b.w(), c.w());
+  return d;
+}
+
+inline Float4_ fma(float a, Float4_ b, Float4_ c) {
+  Float4_ d;
+  d.x = fma(a, b.x, c.x);
+  d.y = fma(a, b.y, c.y);
+  return d;
+}
+
+inline Float8_ fma(float a, Float8_ b, Float8_ c) {
+  Float8_ d;
+  d.x = fma(a, b.x, c.x);
+  d.y = fma(a, b.y, c.y);
+  d.z = fma(a, b.z, c.z);
+  d.w = fma(a, b.w, c.w);
+  return d;
+}
+
+// Vector sum.
+template<>
+inline float sum(float v) {
+  return v;
+}
+
+template <> inline float sum(sycl::float2 v) {
+  return v.x() + v.y();
+}
+
+template <> inline float sum(sycl::float4 v) {
+  return v.x() + v.y() + v.z() + v.w();
+}
+
+template<>
+inline float sum(Float4_ v) {
+  return v.x.x() + v.x.y() + v.y.x() + v.y.y();
+}
+
+template<>
+inline float sum(Float8_ v) {
+  return v.x.x() + v.x.y() + v.y.x() + v.y.y() + v.z.x() + v.z.y() + v.w.x() +
+         v.w.y();
+}
+
+// Vector dot product.
+inline float dot(float a, float b) {
+  return a * b;
+}
+
+inline float dot(sycl::float2 a, sycl::float2 b) {
+  sycl::float2 c = mul<sycl::float2, sycl::float2, sycl::float2>(a, b);
+  return c.x() + c.y();
+}
+
+inline float dot(Float4_ a, Float4_ b) {
+  sycl::float2 acc = mul<sycl::float2, sycl::float2, sycl::float2>(a.x, b.x);
+  acc = fma(a.y, b.y, acc);
+  return acc.x() + acc.y();
+}
+
+inline float dot(Float8_ a, Float8_ b) {
+  sycl::float2 acc = mul<sycl::float2, sycl::float2, sycl::float2>(a.x, b.x);
+  acc = fma(a.y, b.y, acc);
+  acc = fma(a.z, b.z, acc);
+  acc = fma(a.w, b.w, acc);
+  return acc.x() + acc.y();
+}
+
+// From float to float.
+inline void from_float(float& dst, float src) {
+  dst = src;
+}
+
+inline void from_float(sycl::float2 &dst, sycl::float2 src) {
+  dst = src;
+}
+
+inline void from_float(sycl::float4 &dst, sycl::float4 src) {
+  dst = src;
+}
+
+// From float to float.
+inline float to_float(float u) {
+  return u;
+}
+
+inline sycl::float2 to_float(sycl::float2 u) {
+  return u;
+}
+
+inline sycl::float4 to_float(sycl::float4 u) {
+  return u;
+}
+
+inline Float4_ to_float(Float4_ u) {
+  return u;
+}
+
+inline Float8_ to_float(Float8_ u) {
+  return u;
+}
+
+// Zero-out a variable.
+inline void zero(float& dst) {
+  dst = 0.f;
+}
+
+} // namespace vllm
\ No newline at end of file
diff --git a/csrc/xpu/fused_moe.cpp b/csrc/xpu/fused_moe.cpp
new file mode 100644
index 000000000..3a39d0e13
--- /dev/null
+++ b/csrc/xpu/fused_moe.cpp
@@ -0,0 +1,269 @@
+#include "utils.h"
+#include "base.hpp"
+
+using ST = at::ScalarType;
+
+#include <sycl/sycl.hpp>
+#include "xpu_types.h"
+#include <torch/extension.h>
+
+template <typename T>
+__inline__ T silu_xpu(const T& x) {
+  // x * sigmoid(x)
+  return (T)(((float)x) / (1.0f + sycl::exp((float)-x)));
+}
+
+template <typename scalar_t>
+void silu_and_mul_kernel(
+    scalar_t* __restrict__ out, // [..., d]
+    const scalar_t* __restrict__ input, // [..., 2, d]
+    const int d,
+    const sycl::nd_item<3>& item_ct1) {
+  const int64_t token_idx = item_ct1.get_group(2);
+  for (int64_t idx = item_ct1.get_local_id(2); idx < d;
+       idx += item_ct1.get_local_range(2)) {
+    const scalar_t x = input[token_idx * 2 * d + idx];
+    const scalar_t y = input[token_idx * 2 * d + d + idx];
+    out[token_idx * d + idx] = silu_xpu(x) * y;
+  }
+}
+
+template <typename scalar_t>
+void call_silu_and_mul_kernel(
+    int num_tokens,
+    int d,
+    const scalar_t* __restrict__ input,
+    scalar_t* __restrict__ output) {
+  using sycl_t = vllm::xpu::SyclTypeTrait<scalar_t>::Type;
+  sycl::range<3> grid(1, 1, num_tokens);
+  sycl::range<3> block(1, 1, std::min(d, 1024));
+  auto& queue = vllm::xpu::vllmGetQueue();
+  queue.submit([&](sycl::handler& cgh) {
+    cgh.parallel_for(
+        sycl::nd_range<3>(grid * block, block), [=](sycl::nd_item<3> item_ct1) {
+          silu_and_mul_kernel<sycl_t>(
+              (sycl_t*)output, (const sycl_t*)input, d, item_ct1);
+        });
+  });
+}
+
+void _silu_and_mul(torch::Tensor& out, torch::Tensor& input) {
+  int num_tokens = input.numel() / input.size(-1);
+  int d = input.size(-1) / 2;
+
+  VLLM_XPU_DISPATCH_FLOATING_TYPES(
+      input.scalar_type(), "call_silu_and_mul_kernel", [&] {
+        call_silu_and_mul_kernel(
+            num_tokens,
+            d,
+            input.data_ptr<scalar_t>(),
+            out.data_ptr<scalar_t>());
+      });
+}
+
+template <typename IT, const int VS, const int GS, const int ES, const int QTYPE>
+static void moe_forward_kernel(
+    const void* input_ptr,
+    const int64_t* indexs,
+    const uint64_t* qweights,
+    void * output_ptr,
+    const int num_tokens,
+    const int state_size,
+    const int output_size,
+    at::Device device
+) {
+    static_assert(ES == 8 || ES == 16 || ES == 32);
+    assert(output_size % VS == 0);
+
+    const int nb = state_size / QK;
+    const int nsb = nb / SBS;
+
+    constexpr int BLOCK_SIZE = BLOCK_SIZES[QTYPE];
+    constexpr int SCALE_SIZE = SCALE_SIZES[QTYPE];
+
+    sycl::range<2> global_size(num_tokens, output_size / VS * GS);
+    sycl::range<2> local_size(1, GS);
+
+    auto cgf = [&](sycl::handler& handle) {
+        handle.parallel_for(
+            sycl::nd_range<2>(global_size, local_size),
+            [=](sycl::nd_item<2> item) SYCL_ESIMD_KERNEL {
+                slm_init<GS * VS * sizeof(float)>();
+
+                const int eid = item.get_global_id(0);
+                const int tid = item.get_local_id(1);
+                const int vid = item.get_group(1) * VS;
+
+                if (indexs[eid] >= 0) {
+                    const uint8_t* weight = (const uint8_t *)(qweights[indexs[eid]]);
+                    const uint8_t* scales = weight + (int64_t)output_size * nb * BLOCK_SIZE;
+                    const IT* input = static_cast<const IT *>(input_ptr) + eid * state_size;
+                    IT* output = static_cast<IT *>(output_ptr) + eid * output_size;
+
+                    const uint8_t * weight_base = weight + nb * BLOCK_SIZE * vid;
+                    const uint8_t * scale_base = scales + nb * SCALE_SIZE * vid;
+
+                    simd<IT, VS * ES> accvs{};
+
+                    for (int s = tid; s < nsb; s += GS) {
+                        simd<IT, SBS * QK> xvs = block_load<IT, SBS * QK>(input + s * SBS * QK);
+
+                        #pragma unroll
+                        for (int v = 0; v < VS; ++v) {
+                            simd<fp16, SBS * QK> yvs = load_qblocks<QTYPE>(
+                                weight_base + v * nb * BLOCK_SIZE + s * SBS * BLOCK_SIZE,
+                                scale_base + v * nb * SCALE_SIZE + s * SBS * SCALE_SIZE
+                            );
+
+                            #pragma unroll
+                            for (int i = 0; i < SBS * QK; i += ES) {
+                                accvs.template select<ES, 1>(v * ES) +=
+                                    xvs.template select<ES, 1>(i) *
+                                    yvs.template select<ES, 1>(i);
+                            }
+                        }
+                    }
+
+                    for (int b = nsb * SBS + tid; b < nb; b += GS) {
+                        simd<IT, QK> xv = block_load<IT, QK>(input + b * QK);
+
+                        #pragma unroll
+                        for (int v = 0; v < VS; ++v) {
+                            simd<fp16, QK> yv = load_qblock<QTYPE>(
+                                weight_base + v * nb * BLOCK_SIZE + b * BLOCK_SIZE,
+                                scale_base + v * nb * SCALE_SIZE + b * SCALE_SIZE
+                            );
+
+                            #pragma unroll
+                            for (int i = 0; i < QK; i += ES) {
+                                accvs.template select<ES, 1>(v * ES) +=
+                                    xv.template select<ES, 1>(i) *
+                                    yv.template select<ES, 1>(i);
+                            }
+                        }
+                    }
+
+                    simd<float, VS> accs;
+                    #pragma unroll
+                    for(int v = 0; v < VS; ++v) {
+                        accs[v] = sycl::ext::intel::esimd::detail::sum<float, IT, ES>(
+                            accvs.template select<ES, 1>(v * ES)
+                        );
+                    }
+
+                    slm_block_store<float, VS>(tid * VS * sizeof(float), accs);
+
+                    barrier();
+
+                    if (tid == 0) {
+                        #pragma unroll
+                        for (int i = 1; i < GS; ++i) {
+                            accs += slm_block_load<float, VS>(i * VS * sizeof(float));
+                        }
+
+                        block_store<IT, VS>(output + vid, accs);
+                    }
+                }
+
+                
+            }
+        );
+    };
+
+    utils::submit_kernel(cgf, device, "moe forward down kernel");
+}
+
+
+template <int QTYPE>
+static auto dispatch_moe_forward(ST scalar_t) {
+    switch (scalar_t) {
+        case ST::Float: return std::make_tuple(moe_forward_kernel<float, 4, 4, 16, QTYPE>);
+        case ST::Half: return std::make_tuple(moe_forward_kernel<fp16, 4, 4, 32, QTYPE>);
+        default: throw std::runtime_error("unsupported dtype, only fp32 and fp16 are supported");
+    }
+}
+
+
+torch::Tensor moe_forward(
+    torch::Tensor input,
+    torch::Tensor indexs,
+    torch::Tensor qweights_attr,
+    int64_t state_size,
+    int64_t output_size,
+    int64_t qtype
+) {
+    auto [func] = [&] () {
+        switch (qtype) {
+            case GGML_TYPE_Q4_0:
+                return dispatch_moe_forward<GGML_TYPE_Q4_0>(input.scalar_type());
+            case GGML_TYPE_Q4_0_WOQ:
+                return dispatch_moe_forward<GGML_TYPE_Q4_0_WOQ>(input.scalar_type());
+            case GGML_TYPE_FP8E5:
+                return dispatch_moe_forward<GGML_TYPE_FP8E5>(input.scalar_type());
+            default: throw std::runtime_error("unsupported qtype: " + std::to_string(qtype));
+        }
+    } ();
+
+    int64_t num_tokens = indexs.numel();
+
+    torch::Tensor output = torch::zeros({num_tokens, output_size},
+                                    torch::device(input.device()).dtype(input.dtype()));
+
+    func(
+        input.data_ptr(), indexs.data_ptr<int64_t>(),
+        qweights_attr.data_ptr<uint64_t>(), output.data_ptr(),
+        num_tokens, state_size, output_size, input.device()
+    );
+
+    return output;
+}
+
+
+torch::Tensor fused_moe_forward(
+    torch::Tensor input,
+    torch::Tensor indexs,
+    torch::Tensor qweights1_attr,
+    torch::Tensor qweights2_attr,
+    int64_t hidden_size,
+    int64_t intermediate_size,
+    int64_t qtype
+) {
+    auto [gmm_func] = [&] () {
+        switch (qtype) {
+            case GGML_TYPE_Q4_0:
+                return dispatch_moe_forward<GGML_TYPE_Q4_0>(input.scalar_type());
+            case GGML_TYPE_Q4_0_WOQ:
+                return dispatch_moe_forward<GGML_TYPE_Q4_0_WOQ>(input.scalar_type());
+            case GGML_TYPE_FP8E5:
+                return dispatch_moe_forward<GGML_TYPE_FP8E5>(input.scalar_type());
+            default: throw std::runtime_error("unsupported qtype: " + std::to_string(qtype));
+        }
+    } ();
+
+    int64_t num_tokens = indexs.numel();
+
+    torch::Tensor w1_output = torch::zeros({num_tokens, intermediate_size * 2},
+                                    torch::device(input.device()).dtype(input.dtype()));
+    
+    torch::Tensor tmp = torch::zeros({num_tokens, intermediate_size},
+                                    torch::device(input.device()).dtype(input.dtype()));
+    
+    torch::Tensor w2_output = torch::zeros({num_tokens, hidden_size},
+                                    torch::device(input.device()).dtype(input.dtype()));
+
+    gmm_func(
+        input.data_ptr(), indexs.data_ptr<int64_t>(),
+        qweights1_attr.data_ptr<uint64_t>(), w1_output.data_ptr(),
+        num_tokens, hidden_size, intermediate_size * 2, input.device()
+    );
+
+    _silu_and_mul(tmp, w1_output);
+
+    gmm_func(
+        tmp.data_ptr(), indexs.data_ptr<int64_t>(),
+        qweights2_attr.data_ptr<uint64_t>(), w2_output.data_ptr(),
+        num_tokens, intermediate_size, hidden_size, input.device()
+    );
+
+    return w2_output;
+}
diff --git a/csrc/xpu/gemm_kernels_xpu.cpp b/csrc/xpu/gemm_kernels_xpu.cpp
new file mode 100644
index 000000000..d96aa5880
--- /dev/null
+++ b/csrc/xpu/gemm_kernels_xpu.cpp
@@ -0,0 +1,125 @@
+/*
+Adapted from https://github.com/mit-han-lab/llm-awq
+@article{lin2023awq,
+  title={AWQ: Activation-aware Weight Quantization for LLM Compression and
+Acceleration}, author={Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang,
+Shang and Dang, Xingyu and Han, Song}, journal={arXiv}, year={2023}
+}
+ */
+
+#include <dpct/dpct.hpp>
+#include <sycl/sycl.hpp>
+#include <torch/extension.h>
+//#include <c10/cuda/CUDAGuard.h>
+#include "dequantize.h"
+#include "utils.h"
+#include "xpu_types.h"
+
+void awq_dequantize_impl(
+    int* __restrict__ input,
+    sycl::half* __restrict__ scaling_factors,
+    int* __restrict__ zeros,
+    sycl::half* __restrict__ output,
+    int G,
+    sycl::nd_item<3> item_ct1) {
+  int j_factors1 = 4;
+  int row_stride2 = 4;
+  int split_k_iters = 1;
+  sycl::half2 ZERO_HALF2{0, 0};
+  sycl::half input_shared[8];
+
+  int N = item_ct1.get_local_range(2) * item_ct1.get_group_range(2);
+  int col = item_ct1.get_group(2) * item_ct1.get_local_range(2) +
+      item_ct1.get_local_id(2);
+  int row = item_ct1.get_group(1) * item_ct1.get_local_range(1) +
+      item_ct1.get_local_id(1);
+  int index1 = 8 * col + 8 * row * N;
+  sycl::half* output_ptr2 = output + index1;
+
+  int index2 = col + row * N;
+  int* input_ptr2 = input + index2;
+
+  int index3 = col + (int)(row / G) * N;
+  int* zeros_ptr2 = zeros + index3;
+  int index4 = 8 * col + (int)(row / G) * N * 8;
+  sycl::half* scale_loaded = scaling_factors + index4;
+
+  uint32_t zeros_loaded = *(uint32_t*)(zeros_ptr2);
+  sycl::uint4 zero_loaded_u4 = vllm::awq::dequantize_s4_to_fp16x2(zeros_loaded);
+  // sycl::uint4 scale_loaded_u4 = *(sycl::uint4*)(scaling_factors_ptr2);
+  // int j = 0;
+
+  uint32_t input_loaded = *(uint32_t*)(input_ptr2);
+  sycl::uint4 input_loaded_fp16 =
+      vllm::awq::dequantize_s4_to_fp16x2(input_loaded);
+
+  sycl::half2* input_loaded_h2 = (sycl::half2*)(&input_loaded_fp16);
+  sycl::half2* zero_loaded_h2 = (sycl::half2*)(&zero_loaded_u4);
+  sycl::half2* scale_loaded_h2 = (sycl::half2*)scale_loaded;
+  for (int i = 0; i < 4; i++) {
+    input_loaded_h2[i] = sycl_half_sub2(input_loaded_h2[i], zero_loaded_h2[i]);
+    input_loaded_h2[i] =
+        sycl_half_fma2(input_loaded_h2[i], scale_loaded_h2[i], ZERO_HALF2);
+  }
+  *(sycl::uint4*)(input_shared) = input_loaded_fp16;
+
+  for (int i = 0; i < 8; ++i) {
+    *(output_ptr2 + i) = input_shared[i];
+  }
+}
+
+torch::Tensor awq_dequantize(
+    torch::Tensor _kernel,
+    torch::Tensor _scaling_factors,
+    torch::Tensor _zeros,
+    int split_k_iters,
+    int thx,
+    int thy) {
+  int in_c = _kernel.size(0);
+  int qout_c = _kernel.size(1);
+  int out_c = qout_c * 8;
+  int G = in_c / _scaling_factors.size(0);
+
+  int x_thread = thx;
+  int y_thread = thy;
+
+  int x_blocks = 1;
+  int y_blocks = 1;
+  if (thx == 0) {
+    x_thread = qout_c;
+  }
+  if (thy == 0) {
+    y_thread = in_c;
+  }
+  if (thx == 0 && thy == 0) {
+    x_thread = 8;
+    y_thread = 8;
+    x_blocks = (int)(qout_c / 8);
+    y_blocks = (int)(in_c / 8);
+  }
+
+  auto options = torch::TensorOptions()
+                     .dtype(_scaling_factors.dtype())
+                     .device(_scaling_factors.device());
+  at::Tensor _de_kernel = torch::empty({in_c, out_c}, options);
+  auto kernel = reinterpret_cast<int*>(_kernel.data_ptr<int>());
+  auto de_kernel =
+      reinterpret_cast<sycl::half*>(_de_kernel.data_ptr<at::Half>());
+  auto scaling_factors =
+      reinterpret_cast<sycl::half*>(_scaling_factors.data_ptr<at::Half>());
+  auto zeros = reinterpret_cast<int*>(_zeros.data_ptr<int>());
+
+  sycl::range<3> num_blocks(1, y_blocks, x_blocks);
+  sycl::range<3> threads_per_block(1, y_thread, x_thread);
+  auto& queue = vllm::xpu::vllmGetQueue();
+
+  queue.submit([&](sycl::handler& cgh) {
+    cgh.parallel_for(
+        sycl::nd_range<3>(num_blocks * threads_per_block, threads_per_block),
+        [=](sycl::nd_item<3> item_ct1) {
+          awq_dequantize_impl(
+              kernel, scaling_factors, zeros, de_kernel, G, item_ct1);
+        });
+  });
+  return _de_kernel;
+}
\ No newline at end of file
diff --git a/csrc/xpu/kv.h b/csrc/xpu/kv.h
new file mode 100644
index 000000000..9616ad7ef
--- /dev/null
+++ b/csrc/xpu/kv.h
@@ -0,0 +1,76 @@
+#pragma once
+
+#include <torch/extension.h>
+#include <ext/intel/esimd.hpp>
+
+using fp16 = sycl::half;
+
+constexpr uint8_t FP16_EXP_OFFSET = 15;
+constexpr uint8_t K_EXP_OFFSET = 9;
+constexpr uint8_t V_EXP_OFFSET = 12;
+constexpr uint8_t K_OFFSET = (FP16_EXP_OFFSET - K_EXP_OFFSET) << 3;
+constexpr uint8_t V_OFFSET = (FP16_EXP_OFFSET - V_EXP_OFFSET) << 3;
+constexpr uint16_t K_MAX =
+    (uint16_t)0x3FC0 + ((uint16_t)(FP16_EXP_OFFSET - K_EXP_OFFSET) << 10);
+constexpr uint16_t K_MIN =
+    (uint16_t)0x0040 + ((uint16_t)(FP16_EXP_OFFSET - K_EXP_OFFSET) << 10);
+constexpr uint16_t V_MAX =
+    (uint16_t)0x3FC0 + ((uint16_t)(FP16_EXP_OFFSET - V_EXP_OFFSET) << 10);
+constexpr uint16_t V_MIN =
+    (uint16_t)0x0040 + ((uint16_t)(FP16_EXP_OFFSET - V_EXP_OFFSET) << 10);
+
+template <const int HD>
+ESIMD_INLINE __ESIMD_NS::simd<uint8_t, HD> quantize_key_row(
+    __ESIMD_NS::simd<fp16, HD> key_row) {
+  const __ESIMD_NS::simd<fp16, HD> kmax = sycl::bit_cast<fp16, uint16_t>(K_MAX);
+  const __ESIMD_NS::simd<fp16, HD> kmin = sycl::bit_cast<fp16, uint16_t>(K_MIN);
+  __ESIMD_NS::simd<fp16, HD> key =
+      __ESIMD_NS::max(__ESIMD_NS::min(__ESIMD_NS::abs(key_row), kmax), kmin);
+  key.template bit_cast_view<uint16_t>() <<= 1;
+  __ESIMD_NS::simd<uint8_t, HD> sign =
+      key_row.template bit_cast_view<uint8_t>().template select<HD, 2>(1) &
+      (uint8_t)0x80;
+  return (key.template bit_cast_view<uint8_t>().template select<HD, 2>(1) -
+          K_OFFSET) |
+         sign;
+}
+
+template <const int HD>
+ESIMD_INLINE __ESIMD_NS::simd<uint8_t, HD> quantize_value_row(
+    __ESIMD_NS::simd<fp16, HD> value_row) {
+  const __ESIMD_NS::simd<fp16, HD> vmax = sycl::bit_cast<fp16, uint16_t>(V_MAX);
+  const __ESIMD_NS::simd<fp16, HD> vmin = sycl::bit_cast<fp16, uint16_t>(V_MIN);
+  __ESIMD_NS::simd<fp16, HD> value =
+      __ESIMD_NS::max(__ESIMD_NS::min(__ESIMD_NS::abs(value_row), vmax), vmin);
+  value.template bit_cast_view<uint16_t>() <<= 1;
+  __ESIMD_NS::simd<uint8_t, HD> sign =
+      value_row.template bit_cast_view<uint8_t>().template select<HD, 2>(1) &
+      (uint8_t)0x80;
+  return (value.template bit_cast_view<uint8_t>().template select<HD, 2>(1) -
+          V_OFFSET) |
+         sign;
+}
+
+template <const int HD>
+ESIMD_INLINE __ESIMD_NS::simd<fp16, HD> dequantize_key_row(
+    const __ESIMD_NS::simd<uint8_t, HD>& key_row) {
+  __ESIMD_NS::simd<uint16_t, HD> result = 0x80;
+  result.template bit_cast_view<uint8_t>().template select<HD, 2>(1) =
+      (key_row & (uint8_t)0x7F) + K_OFFSET;
+  result >>= 1;
+  __ESIMD_NS::simd<uint8_t, HD> sign = key_row & (uint8_t)0x80;
+  result.template bit_cast_view<uint8_t>().template select<HD, 2>(1) |= sign;
+  return result.template bit_cast_view<fp16>();
+}
+
+template <const int HD>
+ESIMD_INLINE __ESIMD_NS::simd<fp16, HD> dequantize_value_row(
+    const __ESIMD_NS::simd<uint8_t, HD>& value_row) {
+  __ESIMD_NS::simd<uint16_t, HD> result = 0x80;
+  result.template bit_cast_view<uint8_t>().template select<HD, 2>(1) =
+      (value_row & (uint8_t)0x7F) + V_OFFSET;
+  result >>= 1;
+  __ESIMD_NS::simd<uint8_t, HD> sign = value_row & (uint8_t)0x80;
+  result.template bit_cast_view<uint8_t>().template select<HD, 2>(1) |= sign;
+  return result.template bit_cast_view<fp16>();
+}
\ No newline at end of file
diff --git a/csrc/xpu/layernorm_xpu.cpp b/csrc/xpu/layernorm_xpu.cpp
new file mode 100644
index 000000000..9a6a2af0a
--- /dev/null
+++ b/csrc/xpu/layernorm_xpu.cpp
@@ -0,0 +1,188 @@
+// clang-format off
+#ifdef VLLM_DEV
+#undef __SYCL_DEVICE_ONLY__
+#endif
+#include <sycl/sycl.hpp>
+#include <dpct/dpct.hpp>
+
+#include <torch/extension.h>
+#include <algorithm>
+#include "utils.h"
+#include "xpu_types.h"
+#include "reduction_utils.h"
+
+namespace vllm {
+
+template <typename scalar_t>
+void rms_norm_kernel(
+    scalar_t* __restrict__ out, // [..., hidden_size]
+    const scalar_t* __restrict__ input, // [..., hidden_size]
+    const scalar_t* __restrict__ weight, // [hidden_size]
+    const float epsilon,
+    const int num_tokens,
+    const int hidden_size,
+    const sycl::nd_item<3>& item_ct1,
+    float* s_variance,
+    float* shared_vals) {
+  float variance = 0.0f;
+
+  for (int idx = item_ct1.get_local_id(2); idx < hidden_size;
+       idx += item_ct1.get_local_range(2)) {
+    const float x = (float)input[item_ct1.get_group(2) * hidden_size + idx];
+    variance += x * x;
+  }
+
+  variance = blockReduceSum<float>(variance, item_ct1, shared_vals);
+  if (item_ct1.get_local_id(2) == 0) {
+    *s_variance = sycl::rsqrt(variance / hidden_size + epsilon);
+  }
+
+  // item_ct1.barrier();
+  item_ct1.barrier(sycl::access::fence_space::local_space);
+
+  for (int idx = item_ct1.get_local_id(2); idx < hidden_size;
+       idx += item_ct1.get_local_range(2)) {
+    float x = (float)input[item_ct1.get_group(2) * hidden_size + idx];
+    out[item_ct1.get_group(2) * hidden_size + idx] =
+        ((scalar_t)(x * (*s_variance))) * weight[idx];
+  }
+}
+
+template <typename scalar_t>
+void call_rms_norm_kernel(
+    torch::Tensor& out,
+    torch::Tensor& input,
+    torch::Tensor& weight,
+    float epsilon) {
+  using sycl_t = vllm::xpu::SyclTypeTrait<scalar_t>::Type;
+  int hidden_size = input.size(-1);
+  int num_tokens = input.numel() / hidden_size;
+  auto out_ptr = out.data_ptr<scalar_t>();
+  auto input_ptr = input.data_ptr<scalar_t>();
+  auto weight_ptr = weight.data_ptr<scalar_t>();
+  sycl::range<3> grid(1, 1, num_tokens);
+  sycl::range<3> block(1, 1, std::min(hidden_size, 1024));
+  auto& queue = vllm::xpu::vllmGetQueue();
+  queue.submit([&](sycl::handler& cgh) {
+    sycl::local_accessor<float, 1> shared_vals( sycl::range<1>(32), cgh);
+    sycl::local_accessor<float, 1> s_variance( sycl::range<1>(1), cgh);
+    cgh.parallel_for(
+        sycl::nd_range<3>(grid * block, block),
+        [=](sycl::nd_item<3> item_ct1) [[intel::reqd_sub_group_size(32)]] {
+          rms_norm_kernel<sycl_t>(
+              (sycl_t*)out_ptr,
+              (const sycl_t*)input_ptr,
+              (const sycl_t*)weight_ptr,
+              epsilon,
+              num_tokens,
+              hidden_size,
+              item_ct1,
+              s_variance.get_pointer(),
+              shared_vals.get_pointer());
+        });
+  });
+}
+
+
+template <typename scalar_t>
+void fused_add_rms_norm_kernel(
+    scalar_t* __restrict__ input,   // [..., hidden_size]
+    scalar_t* __restrict__ residual,        // [..., hidden_size]
+    const scalar_t* __restrict__ weight, // [hidden_size]
+    const float epsilon,
+    const int num_tokens,
+    const int hidden_size,
+    const sycl::nd_item<3>& item_ct1,
+    float* s_variance,
+    float* shared_vals) {
+  float variance = 0.0f;
+
+  for (int idx = item_ct1.get_local_id(2); idx < hidden_size;
+       idx += item_ct1.get_local_range(2)) {
+    float x = (float)input[item_ct1.get_group(2) * hidden_size + idx];
+    x+=(float)residual[item_ct1.get_group(2) * hidden_size + idx];
+    variance += x * x;
+    residual[item_ct1.get_group(2) * hidden_size + idx] = (scalar_t)x;
+  }
+
+  variance = blockReduceSum<float>(variance, item_ct1, shared_vals);
+  if (item_ct1.get_local_id(2) == 0) {
+    *s_variance = sycl::rsqrt(variance / hidden_size + epsilon);
+  }
+
+  // item_ct1.barrier();
+  item_ct1.barrier(sycl::access::fence_space::local_space);
+
+  for (int idx = item_ct1.get_local_id(2); idx < hidden_size;
+       idx += item_ct1.get_local_range(2)) {
+    float x = (float)residual[item_ct1.get_group(2) * hidden_size + idx];
+    input[item_ct1.get_group(2) * hidden_size + idx] =
+        ((scalar_t)(x * (*s_variance))) * weight[idx];
+  }
+}
+
+template <typename scalar_t>
+void call_fused_add_rms_norm_kernel(
+    torch::Tensor& input,
+    torch::Tensor& residual,
+    torch::Tensor& weight,
+    float epsilon){
+  using sycl_t = vllm::xpu::SyclTypeTrait<scalar_t>::Type;
+  int hidden_size = input.size(-1);
+  int num_tokens = input.numel() / hidden_size;
+  auto input_ptr = input.data_ptr<scalar_t>();
+  auto residual_ptr = residual.data_ptr<scalar_t>();
+  auto weight_ptr = weight.data_ptr<scalar_t>();
+  sycl::range<3> grid(1, 1, num_tokens);
+  sycl::range<3> block(1, 1, std::min(hidden_size, 1024));
+  auto& queue = vllm::xpu::vllmGetQueue();
+  queue.submit([&](sycl::handler& cgh) {
+    sycl::local_accessor<float, 1> shared_vals( sycl::range<1>(32), cgh);
+    sycl::local_accessor<float, 1> s_variance( sycl::range<1>(1), cgh);
+    cgh.parallel_for(
+        sycl::nd_range<3>(grid * block, block), [=](sycl::nd_item<3> item_ct1)[[intel::reqd_sub_group_size(32)]] {
+          fused_add_rms_norm_kernel<sycl_t>(
+              (sycl_t*)input_ptr,
+              (sycl_t*)residual_ptr,
+              (const sycl_t*)weight_ptr,
+              epsilon,
+              num_tokens,
+              hidden_size,
+              item_ct1,
+              s_variance.get_pointer(),
+              shared_vals.get_pointer());
+        });
+  });
+}
+
+} // namespace vllm
+
+void rms_norm(
+    torch::Tensor& out,
+    torch::Tensor& input,
+    torch::Tensor& weight,
+    float epsilon) {
+  VLLM_XPU_DISPATCH_FLOATING_TYPES(
+      input.scalar_type(), "call_rms_norm_kernel", [&] {
+        vllm::call_rms_norm_kernel<scalar_t>(out, input, weight, epsilon);
+      });
+}
+
+void fused_add_rms_norm(
+    torch::Tensor& input,
+    torch::Tensor& residual,
+    torch::Tensor& weight,
+    float epsilon) {
+  int hidden_size = input.size(-1);
+  int num_tokens = input.numel() / hidden_size;
+
+  VLLM_XPU_DISPATCH_FLOATING_TYPES(
+      input.scalar_type(), "call_fused_add_rms_norm_kernel", [&] {
+        vllm::call_fused_add_rms_norm_kernel<scalar_t>(
+            input,
+            residual,
+            weight,
+               epsilon);
+      });
+}
+
diff --git a/csrc/xpu/pos_encoding_xpu.cpp b/csrc/xpu/pos_encoding_xpu.cpp
new file mode 100644
index 000000000..3232cacbc
--- /dev/null
+++ b/csrc/xpu/pos_encoding_xpu.cpp
@@ -0,0 +1,333 @@
+// clang-format off
+#ifdef VLLM_DEV
+#undef __SYCL_DEVICE_ONLY__
+#endif
+#include <sycl/sycl.hpp>
+// clang-format on
+#include "xpu_types.h"
+
+#include <torch/extension.h>
+#include "utils.h"
+
+template <typename scalar_t, bool IS_NEOX>
+inline void apply_rotary_embedding(
+    scalar_t* __restrict__ arr,
+    const scalar_t* __restrict__ cos_ptr,
+    const scalar_t* __restrict__ sin_ptr,
+    int rot_offset,
+    int embed_dim) {
+  int x_index, y_index;
+  scalar_t cos, sin;
+  if (IS_NEOX) {
+    // GPT-NeoX style rotary embedding.
+    x_index = rot_offset;
+    y_index = embed_dim + rot_offset;
+    cos = VLLM_LDG(cos_ptr + x_index);
+    sin = VLLM_LDG(sin_ptr + x_index);
+  } else {
+    // GPT-J style rotary embedding.
+    x_index = 2 * rot_offset;
+    y_index = 2 * rot_offset + 1;
+    cos = VLLM_LDG(cos_ptr + x_index / 2);
+    sin = VLLM_LDG(sin_ptr + x_index / 2);
+  }
+
+  const scalar_t x = arr[x_index];
+  const scalar_t y = arr[y_index];
+  arr[x_index] = x * cos - y * sin;
+  arr[y_index] = y * cos + x * sin;
+}
+
+template <typename scalar_t, bool IS_NEOX>
+void rotary_embedding_kernel(
+    const int64_t* __restrict__ positions, // [batch_size, seq_len] or
+                                           // [num_tokens]
+    scalar_t* __restrict__ query, // [batch_size, seq_len, num_heads, head_size]
+                                  // or [num_tokens, num_heads, head_size]
+    scalar_t* __restrict__ key, // [batch_size, seq_len, num_kv_heads,
+                                // head_size] or [num_tokens, num_kv_heads,
+                                // head_size]
+    const scalar_t* __restrict__ cos_sin_cache, // [max_position, 2, rot_dim //
+                                                // 2]
+    const int rot_dim,
+    const int query_stride,
+    const int key_stride,
+    const int num_heads,
+    const int num_kv_heads,
+    const int head_size,
+    const sycl::nd_item<3>& item_ct1) {
+  // Each thread block is responsible for one token.
+  const int token_idx = item_ct1.get_group(2);
+  int64_t pos = positions[token_idx];
+  const scalar_t* cache_ptr = cos_sin_cache + pos * rot_dim;
+
+  const int embed_dim = rot_dim / 2;
+  const scalar_t* cos_ptr = cache_ptr;
+  const scalar_t* sin_ptr = cache_ptr + embed_dim;
+
+  const int nq = num_heads * embed_dim;
+  for (int i = item_ct1.get_local_id(2); i < nq;
+       i += item_ct1.get_local_range(2)) {
+    const int head_idx = i / embed_dim;
+    const int token_head = token_idx * query_stride + head_idx * head_size;
+    const int rot_offset = i % embed_dim;
+    apply_rotary_embedding<scalar_t, IS_NEOX>(
+        query + token_head, cos_ptr, sin_ptr, rot_offset, embed_dim);
+  }
+
+  const int nk = num_kv_heads * embed_dim;
+  for (int i = item_ct1.get_local_id(2); i < nk;
+       i += item_ct1.get_local_range(2)) {
+    const int head_idx = i / embed_dim;
+    const int token_head = token_idx * key_stride + head_idx * head_size;
+    const int rot_offset = i % embed_dim;
+    apply_rotary_embedding<scalar_t, IS_NEOX>(
+        key + token_head, cos_ptr, sin_ptr, rot_offset, embed_dim);
+  }
+}
+
+template <typename scalar_t, bool IS_NEOX>
+void batched_rotary_embedding_kernel(
+    const int64_t* __restrict__ positions, // [batch_size, seq_len] or
+                                           // [num_tokens]
+    scalar_t* __restrict__ query, // [batch_size, seq_len, num_heads, head_size]
+                                  // or [num_tokens, num_heads, head_size]
+    scalar_t* __restrict__ key, // [batch_size, seq_len, num_kv_heads,
+                                // head_size] or [num_tokens, num_kv_heads,
+                                // head_size]
+    const scalar_t* __restrict__ cos_sin_cache, // [max_position, 2, rot_dim //
+                                                // 2]
+    const int64_t* __restrict__ cos_sin_cache_offsets,  // [batch_size, seq_len] or [num_tokens]
+    const int rot_dim,
+    const int query_stride,
+    const int key_stride,
+    const int num_heads,
+    const int num_kv_heads,
+    const int head_size,
+    const sycl::nd_item<3>& item_ct1) {
+  // Each thread block is responsible for one token.
+  const int token_idx = item_ct1.get_group(2);
+  int64_t cos_sin_cache_offset = cos_sin_cache_offsets[token_idx];
+  int64_t pos = positions[token_idx];
+  const scalar_t* cache_ptr = cos_sin_cache + (cos_sin_cache_offset + pos) * rot_dim;
+
+  const int embed_dim = rot_dim / 2;
+  const scalar_t* cos_ptr = cache_ptr;
+  const scalar_t* sin_ptr = cache_ptr + embed_dim;
+
+  const int nq = num_heads * embed_dim;
+  for (int i = item_ct1.get_local_id(2); i < nq;
+       i += item_ct1.get_local_range(2)) {
+    const int head_idx = i / embed_dim;
+    const int token_head = token_idx * query_stride + head_idx * head_size;
+    const int rot_offset = i % embed_dim;
+    apply_rotary_embedding<scalar_t, IS_NEOX>(
+        query + token_head, cos_ptr, sin_ptr, rot_offset, embed_dim);
+  }
+
+  const int nk = num_kv_heads * embed_dim;
+  for (int i = item_ct1.get_local_id(2); i < nk;
+       i += item_ct1.get_local_range(2)) {
+    const int head_idx = i / embed_dim;
+    const int token_head = token_idx * key_stride + head_idx * head_size;
+    const int rot_offset = i % embed_dim;
+    apply_rotary_embedding<scalar_t, IS_NEOX>(
+        key + token_head, cos_ptr, sin_ptr, rot_offset, embed_dim);
+  }
+}
+
+template <typename scalar_t>
+void call_rotary_embedding_kernel(
+    const int64_t* __restrict__ positions, // [num_tokens]
+    scalar_t* __restrict__ query, // [num_tokens, num_heads, head_size]
+    scalar_t* __restrict__ key, // [num_tokens, num_kv_heads, head_size]
+    const scalar_t* __restrict__ cos_sin_cache, // [max_position, 2, rot_dim //
+                                                // 2]
+    const int rot_dim,
+    const int query_stride,
+    const int key_stride,
+    const int num_heads,
+    const int num_kv_heads,
+    const int head_size,
+    const int num_tokens,
+    const int sin_cos_dim,
+    bool is_neox) {
+  using sycl_t = vllm::xpu::SyclTypeTrait<scalar_t>::Type;
+  sycl::range<3> grid(1, 1, num_tokens);
+  sycl::range<3> block(1, 1, std::min(num_heads * rot_dim / 2, 512));
+  auto& queue = vllm::xpu::vllmGetQueue();
+  if (is_neox) {
+    queue.submit([&](sycl::handler& cgh) {
+      cgh.parallel_for(
+          sycl::nd_range<3>(grid * block, block),
+          [=](sycl::nd_item<3> item_ct1) {
+            rotary_embedding_kernel<sycl_t, true>(
+                positions,
+                (sycl_t* __restrict__)query,
+                (sycl_t* __restrict__)key,
+                (const sycl_t* __restrict__)cos_sin_cache,
+                rot_dim,
+                query_stride,
+                key_stride,
+                num_heads,
+                num_kv_heads,
+                head_size,
+                item_ct1);
+          });
+    });
+  } else {
+    queue.submit([&](sycl::handler& cgh) {
+      cgh.parallel_for(
+          sycl::nd_range<3>(grid * block, block),
+          [=](sycl::nd_item<3> item_ct1) {
+            rotary_embedding_kernel<sycl_t, false>(
+                positions,
+                (sycl_t* __restrict__)query,
+                (sycl_t* __restrict__)key,
+                (const sycl_t* __restrict__)cos_sin_cache,
+                rot_dim,
+                query_stride,
+                key_stride,
+                num_heads,
+                num_kv_heads,
+                head_size,
+                item_ct1);
+          });
+    });
+  }
+}
+
+template <typename scalar_t>
+void call_batched_rotary_embedding_kernel(
+    const int64_t* __restrict__ positions, // [num_tokens]
+    scalar_t* __restrict__ query, // [num_tokens, num_heads, head_size]
+    scalar_t* __restrict__ key, // [num_tokens, num_kv_heads, head_size]
+    const scalar_t* __restrict__ cos_sin_cache, // [max_position, 2, rot_dim //
+                                                // 2]
+    const int64_t* __restrict__ cos_sin_cache_offsets,  // [batch_size, seq_len] or [num_tokens]
+    const int rot_dim,
+    const int query_stride,
+    const int key_stride,
+    const int num_heads,
+    const int num_kv_heads,
+    const int head_size,
+    const int num_tokens,
+    const int sin_cos_dim,
+    bool is_neox) {
+  using sycl_t = vllm::xpu::SyclTypeTrait<scalar_t>::Type;
+  sycl::range<3> grid(1, 1, num_tokens);
+  sycl::range<3> block(1, 1, std::min(num_heads * rot_dim / 2, 512));
+  auto& queue = vllm::xpu::vllmGetQueue();
+  if (is_neox) {
+    queue.submit([&](sycl::handler& cgh) {
+      cgh.parallel_for(
+          sycl::nd_range<3>(grid * block, block),
+          [=](sycl::nd_item<3> item_ct1) {
+            batched_rotary_embedding_kernel<sycl_t, true>(
+                positions,
+                (sycl_t* __restrict__)query,
+                (sycl_t* __restrict__)key,
+                (const sycl_t* __restrict__)cos_sin_cache,
+                cos_sin_cache_offsets,
+                rot_dim,
+                query_stride,
+                key_stride,
+                num_heads,
+                num_kv_heads,
+                head_size,
+                item_ct1);
+          });
+    });
+  } else {
+    queue.submit([&](sycl::handler& cgh) {
+      cgh.parallel_for(
+          sycl::nd_range<3>(grid * block, block),
+          [=](sycl::nd_item<3> item_ct1) {
+            batched_rotary_embedding_kernel<sycl_t, false>(
+                positions,
+                (sycl_t* __restrict__)query,
+                (sycl_t* __restrict__)key,
+                (const sycl_t* __restrict__)cos_sin_cache,
+                cos_sin_cache_offsets,
+                rot_dim,
+                query_stride,
+                key_stride,
+                num_heads,
+                num_kv_heads,
+                head_size,
+                item_ct1);
+          });
+    });
+  }
+}
+
+void rotary_embedding(
+    torch::Tensor& positions,
+    torch::Tensor& query,
+    torch::Tensor& key,
+    int head_size,
+    torch::Tensor& cos_sin_cache,
+    bool is_neox) {
+
+  int num_tokens = query.numel() / query.size(-1);
+  int rot_dim = cos_sin_cache.size(1);
+  int num_heads = query.size(-1) / head_size;
+  int num_kv_heads = key.size(-1) / head_size;
+  int key_stride = key.stride(-2);
+  int query_stride = query.stride(-2);
+  int cos_sin_dim = cos_sin_cache.size(0);
+
+  VLLM_XPU_DISPATCH_FLOATING_TYPES(
+      query.scalar_type(), "call_rotary_embedding_kernel", [&] {
+        call_rotary_embedding_kernel<scalar_t>(
+            positions.data_ptr<int64_t>(),
+            query.data_ptr<scalar_t>(),
+            key.data_ptr<scalar_t>(),
+            cos_sin_cache.data_ptr<scalar_t>(),
+            rot_dim,
+            query_stride,
+            key_stride,
+            num_heads,
+            num_kv_heads,
+            head_size,
+            num_tokens,
+            cos_sin_dim,
+            is_neox);
+      });
+}
+
+void batched_rotary_embedding(
+  torch::Tensor& positions,
+  torch::Tensor& query,
+  torch::Tensor& key,
+  int head_size,
+  torch::Tensor& cos_sin_cache,
+  bool is_neox,
+  int rot_dim,
+  torch::Tensor& cos_sin_cache_offsets) {
+  int64_t num_tokens = cos_sin_cache_offsets.size(0);
+  int num_heads = query.size(-1) / head_size;
+  int num_kv_heads = key.size(-1) / head_size;
+  int key_stride = key.stride(-2);
+  int query_stride = query.stride(-2);
+  int cos_sin_dim = cos_sin_cache.size(0);
+
+  VLLM_XPU_DISPATCH_FLOATING_TYPES(
+    query.scalar_type(), "call_batched_rotary_embedding_kernel", [&] {
+      call_batched_rotary_embedding_kernel<scalar_t>(
+          positions.data_ptr<int64_t>(),
+          query.data_ptr<scalar_t>(),
+          key.data_ptr<scalar_t>(),
+          cos_sin_cache.data_ptr<scalar_t>(),
+          cos_sin_cache_offsets.data_ptr<int64_t>(),
+          rot_dim,
+          query_stride,
+          key_stride,
+          num_heads,
+          num_kv_heads,
+          head_size,
+          num_tokens,
+          cos_sin_dim,
+          is_neox);
+    });
+}
\ No newline at end of file
diff --git a/csrc/xpu/pybind.cpp b/csrc/xpu/pybind.cpp
new file mode 100644
index 000000000..bf9e94612
--- /dev/null
+++ b/csrc/xpu/pybind.cpp
@@ -0,0 +1,112 @@
+// #include "cache.h"
+#include "xpu_ops.h"
+#include <torch/extension.h>
+
+PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
+  // vLLM custom ops
+  pybind11::module ops = m.def_submodule("ops", "vLLM custom operators");
+
+  // Attention ops
+  ops.def(
+    "paged_attention_v1",
+    &paged_attention_v1,
+    "Compute the attention between an input query and the cached keys/values using PagedAttention.");
+  ops.def(
+    "paged_attention_v2",
+    &paged_attention_v2,
+    "PagedAttention V2.");
+
+  ops.def("context_attention_forward_v1", &context_attention_forward_v1,
+          "Context attention forward_v1");
+
+  ops.def("context_attention_forward_v2", &context_attention_forward_v2,
+          "Context attention forward_v2");
+
+  ops.def(
+    "paged_attention_gqa",
+    &paged_attention_gqa,
+    "PagedAttention GQA.");
+
+  ops.def("paged_attention_gqa_fp8", &paged_attention_gqa_fp8, "PagedAttention GQA fp8.");
+
+  // Activation ops
+  ops.def(
+    "silu_and_mul",
+    &silu_and_mul,
+    "Activation function used in SwiGLU.");
+  ops.def(
+    "gelu_and_mul",
+    &gelu_and_mul,
+    "Activation function used in GeGLU with `none` approximation.");
+  ops.def(
+    "gelu_tanh_and_mul",
+    &gelu_tanh_and_mul,
+    "Activation function used in GeGLU with `tanh` approximation.");
+  ops.def(
+    "gelu_new",
+    &gelu_new,
+    "GELU implementation used in GPT-2.");
+  ops.def(
+    "gelu_fast",
+    &gelu_fast,
+    "Approximate GELU implementation.");
+
+  // Layernorm
+  ops.def(
+    "rms_norm",
+    &rms_norm,
+    "Apply Root Mean Square (RMS) Normalization to the input tensor.");
+
+  ops.def(
+    "fused_add_rms_norm",
+    &fused_add_rms_norm,
+    "In-place fused Add and RMS Normalization");
+
+  // Rotary embedding
+  ops.def(
+    "rotary_embedding",
+    &rotary_embedding,
+    "Apply GPT-NeoX or GPT-J style rotary embedding to query and key");
+
+  // Cache ops
+  pybind11::module cache_ops = m.def_submodule("cache_ops", "vLLM cache ops");
+  cache_ops.def(
+    "swap_blocks",
+    &swap_blocks,
+    "Swap in (out) the cache blocks from src to dst");
+  cache_ops.def(
+    "copy_blocks",
+    &copy_blocks,
+    "Copy the cache blocks from src to dst");
+  cache_ops.def(
+    "reshape_and_cache",
+    &reshape_and_cache,
+    "Reshape the key and value tensors and cache them");
+  cache_ops.def(
+    "reshape_and_cache_ipexllm",
+    &reshape_and_cache_ipexllm,
+    "Reshape the key and value tensors and cache them for ipex_llm");
+
+  cache_ops.def(
+    "reshape_and_cache_ipexllm_fp8",
+    &reshape_and_cache_ipexllm_fp8,
+    "Reshape the key and value tensors and cache them for ipex_llm with fp8");
+
+  // Quant
+  ops.def(
+    "awq_dequantize",
+    &awq_dequantize,
+    "dequant method for awq");
+
+
+  ops.def(
+    "moe_forward",
+    &moe_forward,
+    "PagedAttention GQA.");
+
+  ops.def(
+    "fused_moe_forward",
+    &fused_moe_forward,
+    "PagedAttention GQA.");
+
+}
diff --git a/csrc/xpu/reduction_utils.h b/csrc/xpu/reduction_utils.h
new file mode 100644
index 000000000..93c64d759
--- /dev/null
+++ b/csrc/xpu/reduction_utils.h
@@ -0,0 +1,56 @@
+/*
+ * Copyright (c) 2023, The vLLM team.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+#pragma once
+
+#include <dpct/dpct.hpp>
+#include <stdint.h>
+#include <sycl/sycl.hpp>
+
+namespace vllm {
+
+template <typename T>
+__inline__ T warpReduceSum(T val, const sycl::nd_item<3>& item_ct1) {
+#pragma unroll
+  for (int mask = 16; mask > 0; mask >>= 1)
+    val += dpct::permute_sub_group_by_xor(
+        item_ct1.get_sub_group(), val, mask, 32);
+  return val;
+}
+
+/* Calculate the sum of all elements in a block */
+template<typename T>
+__inline__ T blockReduceSum(T val, const sycl::nd_item<3> &item_ct1, T *shared) {
+
+  int lane = item_ct1.get_local_id(2) & 0x1f;
+  int wid = item_ct1.get_local_id(2) >> 5;
+
+  val = warpReduceSum<T>(val, item_ct1);
+
+  if (lane == 0) {
+    shared[wid] = val;
+  }
+  item_ct1.barrier();
+
+  // Modify from blockDim.x << 5 to blockDim.x / 32. to prevent
+  // blockDim.x is not divided by 32
+  val = (item_ct1.get_local_id(2) < (item_ct1.get_local_range(2) / 32.f))
+            ? shared[lane]
+            : (T)(0.0f);
+  val = warpReduceSum<T>(val, item_ct1);
+  return val;
+}
+
+} // namespace vllm
\ No newline at end of file
diff --git a/csrc/xpu/utils.cpp b/csrc/xpu/utils.cpp
new file mode 100644
index 000000000..5f613af55
--- /dev/null
+++ b/csrc/xpu/utils.cpp
@@ -0,0 +1,34 @@
+#include "utils.h"
+#include <sycl/ext/intel/math.hpp>
+
+sycl::half sycl_half_mul(sycl::half a, sycl::half b) {
+  return sycl::ext::intel::math::hmul(a, b);
+}
+sycl::half sycl_half_add(sycl::half a, sycl::half b) {
+  return sycl::ext::intel::math::hadd(a, b);
+}
+sycl::half sycl_half_sub(sycl::half a, sycl::half b) {
+  return sycl::ext::intel::math::hsub(a, b);
+}
+sycl::half sycl_half_fma(sycl::half a, sycl::half b, sycl::half c) {
+  return sycl::ext::intel::math::hfma(a, b, c);
+}
+
+sycl::half2 sycl_half_mul2(sycl::half2 a, sycl::half2 b) {
+  return sycl::ext::intel::math::hmul2(a, b);
+}
+sycl::half2 sycl_half_add2(sycl::half2 a, sycl::half2 b) {
+  return sycl::ext::intel::math::hadd2(a, b);
+}
+sycl::half2 sycl_half_sub2(sycl::half2 a, sycl::half2 b) {
+  return sycl::ext::intel::math::hsub2(a, b);
+}
+
+sycl::half2 sycl_half_fma2(sycl::half2 a, sycl::half2 b, sycl::half2 c) {
+  return sycl::ext::intel::math::hfma2(a, b, c);
+}
+
+int get_max_shared_memory_per_block_device_attribute(int device_id) {
+  const sycl::device& device = vllm::xpu::vllmGetQueue().get_device();
+  return device.get_info<sycl::info::device::local_mem_size>();
+}
diff --git a/csrc/xpu/utils.h b/csrc/xpu/utils.h
new file mode 100644
index 000000000..fa3ead51c
--- /dev/null
+++ b/csrc/xpu/utils.h
@@ -0,0 +1,82 @@
+#pragma once
+
+#include <sycl/sycl.hpp>
+#include <functional>
+#include <memory>
+// #include <ipex.h>
+#include <ATen/ATen.h>
+#include <torch/torch.h>
+
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+#include <c10/xpu/XPUStream.h>
+#endif
+
+
+#define VLLM_LDG(arg) *(arg)
+namespace vllm {
+namespace xpu {
+
+static inline sycl::queue& vllmGetQueue() {
+  auto device_type = c10::DeviceType::XPU;
+  c10::impl::VirtualGuardImpl impl(device_type);
+  c10::Stream c10_stream = impl.getStream(c10::Device(device_type));
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+  return at::xpu::XPUStream(c10_stream).queue();
+#else
+  return ::xpu::get_queue_from_stream(c10_stream);
+#endif
+}
+template <typename T>
+struct SyclTypeTrait{
+  using Type = T;
+};
+
+template <>
+struct SyclTypeTrait<c10::Half>{
+  using Type = sycl::half;
+};
+
+template <>
+struct SyclTypeTrait<c10::BFloat16>{
+  using Type = sycl::ext::oneapi::bfloat16;
+};
+
+
+} // namespace xpu
+
+} // namespace vllm
+
+SYCL_EXTERNAL sycl::half sycl_half_mul(sycl::half a, sycl::half b);
+SYCL_EXTERNAL sycl::half sycl_half_add(sycl::half a, sycl::half b);
+SYCL_EXTERNAL sycl::half sycl_half_sub(sycl::half a, sycl::half b);
+SYCL_EXTERNAL sycl::half sycl_half_fma(sycl::half a, sycl::half b, sycl::half c);
+
+SYCL_EXTERNAL sycl::half2 sycl_half_mul2(sycl::half2 a, sycl::half2 b);
+SYCL_EXTERNAL sycl::half2 sycl_half_add2(sycl::half2 a, sycl::half2 b);
+SYCL_EXTERNAL sycl::half2 sycl_half_sub2(sycl::half2 a, sycl::half2 b);
+SYCL_EXTERNAL sycl::half2 sycl_half_fma2(sycl::half2 a, sycl::half2 b, sycl::half2 c);
+
+int get_max_shared_memory_per_block_device_attribute(int device_id);
+
+namespace utils {
+    static inline sycl::queue& get_queue(const at::Device& device) {
+        c10::impl::VirtualGuardImpl impl(device.type());
+        c10::Stream c10_stream = impl.getStream(c10::Device(device));
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+        return at::xpu::XPUStream(c10_stream).queue();
+#else
+        return ::xpu::get_queue_from_stream(c10_stream);
+#endif
+    }
+
+    static inline sycl::event submit_kernel(std::function<void(sycl::handler&)> kernel, const at::Device& device, const char * desc) {
+        sycl::queue& queue = get_queue(device);
+        sycl::event event = queue.submit(kernel);
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+        // xpu::profiler_record(desc, event);
+#else
+        ::xpu::profiler_record(desc, event);
+#endif
+        return event;
+    }
+}
diff --git a/csrc/xpu/xpu_ops.h b/csrc/xpu/xpu_ops.h
new file mode 100644
index 000000000..603d4f23d
--- /dev/null
+++ b/csrc/xpu/xpu_ops.h
@@ -0,0 +1,194 @@
+#pragma once
+#include <torch/extension.h>
+
+void rotary_embedding(torch::Tensor &positions, torch::Tensor &query,
+                          torch::Tensor &key, int head_size,
+                          torch::Tensor &cos_sin_cache, bool is_neox);
+void batched_rotary_embedding(
+  torch::Tensor& positions,
+  torch::Tensor& query,
+  torch::Tensor& key,
+  int head_size,
+  torch::Tensor& cos_sin_cache,
+  bool is_neox,
+  int rot_dim,
+  torch::Tensor& cos_sin_cache_offsets);
+
+void silu_and_mul(torch::Tensor &out, torch::Tensor &input);
+void gelu_and_mul(torch::Tensor &out, torch::Tensor &input);
+
+void gelu_new(torch::Tensor &out, torch::Tensor &input);
+
+void gelu_fast(torch::Tensor &out, torch::Tensor &input);
+
+
+void gelu_tanh_and_mul(
+  torch::Tensor& out,
+  torch::Tensor& input);
+
+void paged_attention_v1(
+    torch::Tensor &out, torch::Tensor &query, torch::Tensor &key_cache,
+    torch::Tensor &value_cache, int num_kv_heads, float scale,
+    torch::Tensor &block_tables, torch::Tensor &context_lens, int block_size,
+    int max_context_len, const c10::optional<torch::Tensor> &alibi_slopes,
+    const std::string& kv_cache_dtype, const float kv_scale, const float attn_logit_softcapping);
+
+void paged_attention_v2(
+    torch::Tensor &out, torch::Tensor &exp_sums, torch::Tensor &max_logits,
+    torch::Tensor &tmp_out, torch::Tensor &query, torch::Tensor &key_cache,
+    torch::Tensor &value_cache, int num_kv_heads, float scale,
+    torch::Tensor &block_tables, torch::Tensor &context_lens, int block_size,
+    int max_context_len, const c10::optional<torch::Tensor> &alibi_slopes,
+    const std::string& kv_cache_dtype, const float kv_scale, const float attn_logit_softcapping);
+
+torch::Tensor context_attention_forward_v1(
+    torch::Tensor query,  // [num_tokens, num_kv_head, head_dim]
+    torch::Tensor key,    // [num_tokens, num_kv_heads * head_size]
+    torch::Tensor value,  // [num_tokens, num_kv_heads * head_size]
+    torch::Tensor block_tables, torch::Tensor query_start_loc,
+    torch::Tensor seq_lens, torch::Tensor context_lens, int max_input_length,
+    int max_context_length);
+
+torch::Tensor context_attention_forward_v2(
+    torch::Tensor query,  // [num_tokens, num_kv_head, head_dim]
+    torch::Tensor key,    // [num_tokens, num_kv_heads * head_size]
+    torch::Tensor value,  // [num_tokens, num_kv_heads * head_size]
+    torch::Tensor block_tables, torch::Tensor query_start_loc,
+    torch::Tensor seq_lens, torch::Tensor context_lens, int max_input_length,
+    int max_context_length, int max_q_length);
+
+void copy_blocks(
+    std::vector<torch::Tensor> &key_caches,
+    std::vector<torch::Tensor> &value_caches,
+    const std::map<int64_t, std::vector<int64_t>> &block_mapping);
+
+void reshape_and_cache(torch::Tensor &key, torch::Tensor &value,
+                           torch::Tensor &key_cache, torch::Tensor &value_cache,
+                           torch::Tensor &slot_mapping,
+                           const std::string& kv_cache_dtype, const float kv_scale);
+void reshape_and_cache_ipexllm(torch::Tensor &key, torch::Tensor &value,
+                           torch::Tensor &key_cache, torch::Tensor &value_cache,
+                           torch::Tensor &slot_mapping,
+                           const std::string& kv_cache_dtype, const float kv_scale);
+
+void reshape_and_cache_ipexllm_fp8(torch::Tensor& key, torch::Tensor& value,
+                                   torch::Tensor& key_cache,
+                                   torch::Tensor& value_cache,
+                                   torch::Tensor& slot_mapping,
+                                   const std::string& kv_cache_dtype,
+                                   const float kv_scale);
+
+void moe_align_block_size(
+  torch::Tensor topk_ids,
+  int num_experts,
+  int block_size,
+  torch::Tensor sorted_token_ids,
+  torch::Tensor experts_ids,
+  torch::Tensor num_tokens_post_pad) {
+  TORCH_CHECK(false, "moe_align_block_size is not supported on XPU.");
+}
+void swap_blocks(torch::Tensor &src, torch::Tensor &dst,
+                     const std::map<int64_t, int64_t> &block_mapping);
+
+void gather_cached_kv(torch::Tensor &key, torch::Tensor &value,
+                          torch::Tensor &key_cache, torch::Tensor &value_cache,
+                          torch::Tensor &slot_mapping);
+
+void convert_fp8_e5m2(torch::Tensor& src_cache, torch::Tensor& dst_cache) {
+  TORCH_CHECK(false, "Quantization is not supported on XPU.");
+}
+
+void rms_norm(torch::Tensor &out, torch::Tensor &input,
+                  torch::Tensor &weight, float epsilon);
+
+void fused_add_rms_norm(torch::Tensor &input, torch::Tensor &residual,
+                            torch::Tensor &weight, float epsilon);
+
+torch::Tensor awq_gemm(torch::Tensor _in_feats, torch::Tensor _kernel,
+                       torch::Tensor _scaling_factors, torch::Tensor _zeros,
+                       int split_k_iters) {
+  TORCH_CHECK(false, "awq_gemm is not supported on XPU.");                            
+}
+
+torch::Tensor marlin_gemm(
+    torch::Tensor& a, 
+    torch::Tensor& b_q_weight,
+    torch::Tensor& b_scales, 
+    torch::Tensor& workspace,
+    int64_t size_m, 
+    int64_t size_n, 
+    int64_t size_k) {
+  TORCH_CHECK(false, "marlin_gemm is not supported on XPU.");                            
+}
+
+torch::Tensor awq_dequantize(torch::Tensor _kernel, 
+    torch::Tensor _scaling_factors,
+    torch::Tensor _zeros,
+    int split_k_iters,
+    int thx,
+    int thy);
+
+void squeezellm_gemm(torch::Tensor vec, torch::Tensor mat,
+                         torch::Tensor mul, torch::Tensor lookup_table) {
+  TORCH_CHECK(false, "squeezellm_gemm is not supported on XPU.");
+}
+
+torch::Tensor gptq_gemm(
+  torch::Tensor a,
+  torch::Tensor b_q_weight,
+  torch::Tensor b_gptq_qzeros,
+  torch::Tensor b_gptq_scales,
+  torch::Tensor b_g_idx,
+  bool use_exllama,
+  int bit) {
+  TORCH_CHECK(false, "gptq_gemm is not supported on XPU.");
+}
+
+void gptq_shuffle(
+  torch::Tensor q_weight,
+  torch::Tensor q_perm,
+  int bit) {
+  TORCH_CHECK(false, "gptq_shuffle is not supported on XPU.");
+}
+
+void paged_attention_gqa(
+    torch::Tensor output,
+    torch::Tensor query,
+    torch::Tensor key_cache,
+    torch::Tensor value_cache,
+    int64_t bsz,
+    int64_t num_heads,
+    int64_t num_kv_heads,
+    float scale,
+    torch::Tensor& block_tables,
+    torch::Tensor& context_lens,
+    int block_size,
+    int64_t head_dim,
+    int max_seq_len
+);
+
+
+torch::Tensor moe_forward(
+    torch::Tensor input,
+    torch::Tensor indexs,
+    torch::Tensor qweights_attr,
+    int64_t state_size,
+    int64_t output_size,
+    int64_t qtype
+);
+
+torch::Tensor fused_moe_forward(
+    torch::Tensor input,
+    torch::Tensor indexs,
+    torch::Tensor qweights1_attr,
+    torch::Tensor qweights2_attr,
+    int64_t hidden_size,
+    int64_t intermediate_size,
+    int64_t qtype
+);
+void paged_attention_gqa_fp8(torch::Tensor output, torch::Tensor query,
+                         torch::Tensor key_cache, torch::Tensor value_cache,
+                         int64_t bsz, int64_t num_heads, int64_t num_kv_heads,
+                         float scale, torch::Tensor& block_tables,
+                         torch::Tensor& context_lens, int block_size,
+                         int64_t head_dim, int max_seq_len);
diff --git a/csrc/xpu/xpu_types.h b/csrc/xpu/xpu_types.h
new file mode 100644
index 000000000..23f5b805c
--- /dev/null
+++ b/csrc/xpu/xpu_types.h
@@ -0,0 +1,25 @@
+
+#ifndef XPU_TYPES_H
+#define XPU_TYPES_H
+
+#include <torch/extension.h>
+
+// FIXME: FP16 is not fully supported in Torch-CPU
+#define VLLM_XPU_DISPATCH_CASE_FLOATING_TYPES(...)     \
+  AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__) \
+  AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)  \
+  AT_DISPATCH_CASE(at::ScalarType::BFloat16, __VA_ARGS__)
+
+#define VLLM_XPU_DISPATCH_CASE_FLOATING_TYPES_FLOAT_ONLY(...) \
+  AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)        \
+  AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)
+
+#define VLLM_XPU_DISPATCH_FLOATING_TYPES(TYPE, NAME, ...) \
+  AT_DISPATCH_SWITCH(                                     \
+      TYPE, NAME, VLLM_XPU_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__))
+
+#define VLLM_XPU_DISPATCH_FLOATING_TYPES_FLOAT_ONLY(TYPE, NAME, ...) \
+  AT_DISPATCH_SWITCH(                                     \
+      TYPE, NAME, VLLM_XPU_DISPATCH_CASE_FLOATING_TYPES_FLOAT_ONLY(__VA_ARGS__))
+
+#endif
\ No newline at end of file
diff --git a/docker/Dockerfile.xpu b/docker/Dockerfile.xpu
index 7d5a589eb..25a9fd7cd 100644
--- a/docker/Dockerfile.xpu
+++ b/docker/Dockerfile.xpu
@@ -1,9 +1,10 @@
-# oneapi 2025.0.2 docker base image use rolling 2448 package. https://dgpu-docs.intel.com/releases/packages.html?release=Rolling+2448.13&os=Ubuntu+22.04, and we don't need install driver manually.
-FROM intel/deep-learning-essentials:2025.0.2-0-devel-ubuntu22.04 AS vllm-base
+FROM intel/deep-learning-essentials:2025.1.3-0-devel-ubuntu24.04 AS vllm-base
 
-RUN rm /etc/apt/sources.list.d/intel-graphics.list
+RUN wget -O- https://apt.repos.intel.com/intel-gpg-keys/GPG-PUB-KEY-INTEL-SW-PRODUCTS.PUB | gpg --dearmor | tee /usr/share/keyrings/oneapi-archive-keyring.gpg > /dev/null && \
+    echo "deb [signed-by=/usr/share/keyrings/oneapi-archive-keyring.gpg] https://apt.repos.intel.com/oneapi all main" | tee /etc/apt/sources.list.d/oneAPI.list && \
+    add-apt-repository -y ppa:kobuk-team/intel-graphics
 
-RUN apt-get update -y && \
+RUN apt clean && apt-get update -y && \
     apt-get install -y --no-install-recommends --fix-missing \
     curl \
     ffmpeg \
@@ -14,15 +15,29 @@ RUN apt-get update -y && \
     libgl1 \
     lsb-release \
     numactl \
-    python3 \
-    python3-dev \
-    python3-pip \
-    wget
+    wget \
+    vim \
+    python3.12 \
+    python3.12-dev \
+    python3-pip
+
+RUN update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.12 1
+RUN update-alternatives --install /usr/bin/python python /usr/bin/python3.12 1
+
+RUN apt install -y libze1=1.23.1-1~24.04~ppa1 libze-dev=1.23.1-1~24.04~ppa1 libze-intel-gpu1=25.27.34303.9-1~24.04~ppa1 intel-opencl-icd=25.27.34303.9-1~24.04~ppa1 libze-intel-gpu-raytracing=1.1.0-114~u24.04
+
+RUN wget https://github.com/uxlfoundation/oneCCL/releases/download/2021.15.4/intel-oneccl-2021.15.4.11_offline.sh
+RUN bash intel-oneccl-2021.15.4.11_offline.sh -a --silent --eula accept && echo "source /opt/intel/oneapi/setvars.sh --force" >> /root/.bashrc
+SHELL ["bash", "-c"]
+CMD ["bash", "-c", "source /root/.bashrc && exec bash"]
 
 WORKDIR /workspace/vllm
 COPY requirements/xpu.txt /workspace/vllm/requirements/xpu.txt
 COPY requirements/common.txt /workspace/vllm/requirements/common.txt
 
+# suppress the python externally managed environment error
+RUN python3 -m pip config set global.break-system-packages true
+
 RUN --mount=type=cache,target=/root/.cache/pip \
     pip install --no-cache-dir \
     -r requirements/xpu.txt
@@ -47,10 +62,11 @@ FROM vllm-base AS vllm-openai
 
 # install additional dependencies for openai api server
 RUN --mount=type=cache,target=/root/.cache/pip \
-    pip install accelerate hf_transfer pytest pytest_asyncio lm_eval[api] modelscope
+    pip install accelerate hf_transfer pytest pytest_asyncio lm_eval[api] 'modelscope!=1.15.0'
+
+RUN --mount=type=cache,target=/root/.cache/pip \
+    pip uninstall oneccl oneccl-devel -y
 
-ENV VLLM_USAGE_SOURCE production-docker-image \
-    TRITON_XPU_PROFILE 1
 # install development dependencies (for testing)
 RUN python3 -m pip install -e tests/vllm_test_utils
 ENTRYPOINT ["python3", "-m", "vllm.entrypoints.openai.api_server"]
diff --git a/docs/features/quantization/fp8.md b/docs/features/quantization/fp8.md
index 0661933ac..469d88a05 100644
--- a/docs/features/quantization/fp8.md
+++ b/docs/features/quantization/fp8.md
@@ -134,4 +134,4 @@ print(result[0].outputs[0].text)
 ```
 
 !!! warning
-    Currently, we load the model at original precision before quantizing down to 8-bits, so you need enough memory to load the whole model.
+    Currently, by default we load the model at original precision before quantizing down to 8-bits, so you need enough memory to load the whole model. To avoid this, adding `VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT=1` can allow offloading weights to cpu before quantization and quantized weights will be kept in device.
diff --git a/docs/models/supported_models.md b/docs/models/supported_models.md
index c8b6c6c86..404045306 100644
--- a/docs/models/supported_models.md
+++ b/docs/models/supported_models.md
@@ -592,7 +592,8 @@ Specified using `--task generate`.
 | `GraniteSpeechForConditionalGeneration` | Granite Speech | T + A | `ibm-granite/granite-speech-3.3-8b` | ‚úÖÔ∏é | ‚úÖÔ∏é | ‚úÖÔ∏é |
 | `H2OVLChatModel` | H2OVL | T + I<sup>E+</sup> | `h2oai/h2ovl-mississippi-800m`, `h2oai/h2ovl-mississippi-2b`, etc. | | ‚úÖÔ∏é | ‚úÖÔ∏é |
 | `Idefics3ForConditionalGeneration` | Idefics3 | T + I | `HuggingFaceM4/Idefics3-8B-Llama3`, etc. | ‚úÖÔ∏é | | ‚úÖÔ∏é |
-| `InternVLChatModel` | InternVL 3.0, InternVideo 2.5, InternVL 2.5, Mono-InternVL, InternVL 2.0 | T + I<sup>E+</sup> + (V<sup>E+</sup>) | `OpenGVLab/InternVL3-9B`, `OpenGVLab/InternVideo2_5_Chat_8B`, `OpenGVLab/InternVL2_5-4B`, `OpenGVLab/Mono-InternVL-2B`, `OpenGVLab/InternVL2-4B`, etc. | ‚úÖÔ∏é | ‚úÖÔ∏é | ‚úÖÔ∏é |
+| `InternS1ForConditionalGeneration` | Intern-S1 | T + I<sup>E+</sup> + V<sup>E+</sup> | `internlm/Intern-S1`, etc. | ‚úÖÔ∏é | ‚úÖÔ∏é | ‚úÖÔ∏é |
+| `InternVLChatModel` | InternVL 3.5, InternVL 3.0, InternVideo 2.5, InternVL 2.5, Mono-InternVL, InternVL 2.0 | T + I<sup>E+</sup> + (V<sup>E+</sup>) | `OpenGVLab/InternVL3_5-14B`, `OpenGVLab/InternVL3-9B`, `OpenGVLab/InternVideo2_5_Chat_8B`, `OpenGVLab/InternVL2_5-4B`, `OpenGVLab/Mono-InternVL-2B`, `OpenGVLab/InternVL2-4B`, etc. | ‚úÖÔ∏é | ‚úÖÔ∏é | ‚úÖÔ∏é |
 | `KeyeForConditionalGeneration` | Keye-VL-8B-Preview | T + I<sup>E+</sup> + V<sup>E+</sup> | `Kwai-Keye/Keye-VL-8B-Preview` | | | ‚úÖÔ∏é |
 | `KimiVLForConditionalGeneration` | Kimi-VL-A3B-Instruct, Kimi-VL-A3B-Thinking | T + I<sup>+</sup> | `moonshotai/Kimi-VL-A3B-Instruct`, `moonshotai/Kimi-VL-A3B-Thinking` | | | ‚úÖÔ∏é |
 | `Llama4ForConditionalGeneration` | Llama 4 | T + I<sup>+</sup> | `meta-llama/Llama-4-Scout-17B-16E-Instruct`, `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8`, `meta-llama/Llama-4-Maverick-17B-128E-Instruct`, etc. | | ‚úÖÔ∏é | ‚úÖÔ∏é |
@@ -602,7 +603,7 @@ Specified using `--task generate`.
 | `LlavaNextVideoForConditionalGeneration` | LLaVA-NeXT-Video | T + V | `llava-hf/LLaVA-NeXT-Video-7B-hf`, etc. | | ‚úÖÔ∏é | ‚úÖÔ∏é |
 | `LlavaOnevisionForConditionalGeneration` | LLaVA-Onevision | T + I<sup>+</sup> + V<sup>+</sup> | `llava-hf/llava-onevision-qwen2-7b-ov-hf`, `llava-hf/llava-onevision-qwen2-0.5b-ov-hf`, etc. | | ‚úÖÔ∏é | ‚úÖÔ∏é |
 | `MiniCPMO` | MiniCPM-O | T + I<sup>E+</sup> + V<sup>E+</sup> + A<sup>E+</sup> | `openbmb/MiniCPM-o-2_6`, etc. | ‚úÖÔ∏é | ‚úÖÔ∏é | ‚úÖÔ∏é |
-| `MiniCPMV` | MiniCPM-V | T + I<sup>E+</sup> + V<sup>E+</sup> | `openbmb/MiniCPM-V-2` (see note), `openbmb/MiniCPM-Llama3-V-2_5`, `openbmb/MiniCPM-V-2_6`, etc. | ‚úÖÔ∏é | | ‚úÖÔ∏é |
+| `MiniCPMV` | MiniCPM-V | T + I<sup>E+</sup> + V<sup>E+</sup> | `openbmb/MiniCPM-V-2` (see note), `openbmb/MiniCPM-Llama3-V-2_5`, `openbmb/MiniCPM-V-2_6`, `openbmb/MiniCPM-V-4`, `openbmb/MiniCPM-V-4_5`, etc. | ‚úÖÔ∏é | | ‚úÖÔ∏é |
 | `MiniMaxVL01ForConditionalGeneration` | MiniMax-VL | T + I<sup>E+</sup> | `MiniMaxAI/MiniMax-VL-01`, etc. | | ‚úÖÔ∏é | ‚úÖÔ∏é |
 | `Mistral3ForConditionalGeneration` | Mistral3 (HF Transformers) | T + I<sup>+</sup> | `mistralai/Mistral-Small-3.1-24B-Instruct-2503`, etc. | ‚úÖÔ∏é | ‚úÖÔ∏é | ‚úÖÔ∏é |
 | `MllamaForConditionalGeneration` | Llama 3.2 | T + I<sup>+</sup> | `meta-llama/Llama-3.2-90B-Vision-Instruct`, `meta-llama/Llama-3.2-11B-Vision`, etc. | | | |
@@ -646,6 +647,15 @@ Specified using `--task generate`.
 
     This limitation exists because the model's mixed attention pattern (bidirectional for images, causal otherwise) is not yet supported by vLLM's attention backends.
 
+!!! note
+    `Gemma3nForConditionalGeneration` is only supported on V1 due to shared KV caching and it depends on `timm>=1.0.17` to make use of its
+    MobileNet-v5 vision backbone.
+  
+    Performance is not yet fully optimized mainly due to:
+  
+    - Both audio and vision MM encoders use `transformers.AutoModel` implementation.  
+    - There's no PLE caching or out-of-memory swapping support, as described in [Google's blog](https://developers.googleblog.com/en/introducing-gemma-3n/). These features might be too model-specific for vLLM, and swapping in particular may be better suited for constrained setups.
+
 !!! note
     Only `InternVLChatModel` with Qwen2.5 text backbone (`OpenGVLab/InternVL3-2B`, `OpenGVLab/InternVL2.5-1B` etc) has video inputs support currently.
 
diff --git a/examples/offline_inference/basic/reward.py b/examples/offline_inference/basic/reward.py
new file mode 100644
index 000000000..aec3481d2
--- /dev/null
+++ b/examples/offline_inference/basic/reward.py
@@ -0,0 +1,55 @@
+# SPDX-License-Identifier: Apache-2.0
+# SPDX-FileCopyrightText: Copyright contributors to the vLLM project
+
+from argparse import Namespace
+
+from vllm import LLM, EngineArgs
+from vllm.utils import FlexibleArgumentParser
+
+
+def parse_args():
+    parser = FlexibleArgumentParser()
+    parser = EngineArgs.add_cli_args(parser)
+    # Set example specific arguments
+    parser.set_defaults(
+        model="internlm/internlm2-1_8b-reward",
+        #runner="pooling",
+        task="reward",
+        enforce_eager=True,
+        max_model_len=1024,
+        trust_remote_code=True,
+    )
+    return parser.parse_args()
+
+
+def main(args: Namespace):
+    # Sample prompts.
+    prompts = [
+        "Hello, my name is",
+        "The president of the United States is",
+        "The capital of France is",
+        "The future of AI is",
+    ]
+
+    # Create an LLM.
+    # You should pass runner="pooling" for reward models
+    llm = LLM(**vars(args))
+
+    # Generate rewards. The output is a list of PoolingRequestOutput.
+    outputs = llm.reward(prompts)
+
+    # Print the outputs.
+    print("\nGenerated Outputs:\n" + "-" * 60)
+    for prompt, output in zip(prompts, outputs):
+        rewards = output.outputs.data
+        rewards_trimmed = (
+            (str(rewards[:16])[:-1] + ", ...]") if len(rewards) > 16 else rewards
+        )
+        print(f"Prompt: {prompt!r} \nReward: {rewards_trimmed} (size={len(rewards)})")
+        print("-" * 60)
+
+
+if __name__ == "__main__":
+    args = parse_args()
+    main(args)
+
diff --git a/examples/offline_inference/multilora_inference.py b/examples/offline_inference/multilora_inference.py
index f0c00bcaa..c8fa36295 100644
--- a/examples/offline_inference/multilora_inference.py
+++ b/examples/offline_inference/multilora_inference.py
@@ -30,7 +30,7 @@ def create_test_prompts(
         (
             "A robot may not injure a human being",
             SamplingParams(
-                temperature=0.0, logprobs=1, prompt_logprobs=1, max_tokens=128
+                temperature=0.0, logprobs=1, max_tokens=128
             ),
             None,
         ),
@@ -46,7 +46,7 @@ def create_test_prompts(
             SamplingParams(
                 temperature=0.0,
                 logprobs=1,
-                prompt_logprobs=1,
+                #prompt_logprobs=1,
                 max_tokens=128,
                 stop_token_ids=[32003],
             ),
@@ -57,7 +57,7 @@ def create_test_prompts(
             SamplingParams(
                 temperature=0.0,
                 logprobs=1,
-                prompt_logprobs=1,
+                #prompt_logprobs=1,
                 max_tokens=128,
                 stop_token_ids=[32003],
             ),
@@ -99,14 +99,14 @@ def initialize_engine() -> LLMEngine:
     #   numbers will cause higher memory usage. If you know that all LoRAs will
     #   use the same rank, it is recommended to set this as low as possible.
     # max_cpu_loras: controls the size of the CPU LoRA cache.
-    engine_args = EngineArgs(
-        model="meta-llama/Llama-2-7b-hf",
-        enable_lora=True,
-        max_loras=1,
-        max_lora_rank=8,
-        max_cpu_loras=2,
-        max_num_seqs=256,
-    )
+    engine_args = EngineArgs(model="meta-llama/Llama-2-7b-hf",
+                             enable_lora=True,
+                             max_loras=1,
+                             max_lora_rank=8,
+                             max_cpu_loras=2,
+                             max_num_seqs=256,
+                             enforce_eager=True,
+                             block_size=64)
     return LLMEngine.from_engine_args(engine_args)
 
 
diff --git a/examples/offline_inference/prithvi_geospatial_mae.py b/examples/offline_inference/prithvi_geospatial_mae.py
index 4fdc7a3cf..b6007b9f4 100644
--- a/examples/offline_inference/prithvi_geospatial_mae.py
+++ b/examples/offline_inference/prithvi_geospatial_mae.py
@@ -3,12 +3,12 @@
 import argparse
 import datetime
 import os
-import re
 from typing import Union
 
 import albumentations
 import numpy as np
 import rasterio
+import regex as re
 import torch
 from einops import rearrange
 from terratorch.datamodules import Sen1Floods11NonGeoDataModule
diff --git a/examples/offline_inference/vision_language.py b/examples/offline_inference/vision_language.py
index e4811c023..fe4393bcf 100644
--- a/examples/offline_inference/vision_language.py
+++ b/examples/offline_inference/vision_language.py
@@ -389,6 +389,39 @@ def run_tarsier(questions: list[str], modality: str) -> ModelRequestData:
     )
 
 
+# Intern-S1
+def run_interns1(questions: list[str], modality: str) -> ModelRequestData:
+    model_name = "internlm/Intern-S1"
+
+    engine_args = EngineArgs(
+        model=model_name,
+        trust_remote_code=True,
+        max_model_len=8192,
+        max_num_seqs=2,
+        limit_mm_per_prompt={modality: 1},
+        enforce_eager=True,
+    )
+
+    if modality == "image":
+        placeholder = "<IMG_CONTEXT>"
+    elif modality == "video":
+        placeholder = "<video>"
+
+    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
+    messages = [
+        [{"role": "user", "content": f"{placeholder}\n{question}"}]
+        for question in questions
+    ]
+    prompts = tokenizer.apply_chat_template(
+        messages, tokenize=False, add_generation_prompt=True
+    )
+
+    return ModelRequestData(
+        engine_args=engine_args,
+        prompts=prompts,
+    )
+
+
 # InternVL
 def run_internvl(questions: list[str], modality: str) -> ModelRequestData:
     model_name = "OpenGVLab/InternVL3-2B"
@@ -1080,7 +1113,9 @@ def run_qwen2_5_vl(questions: list[str], modality: str) -> ModelRequestData:
             "max_pixels": 1280 * 28 * 28,
             "fps": 1,
         },
-        limit_mm_per_prompt={modality: 1},
+        limit_mm_per_prompt={"image": 1},
+        enforce_eager=True,
+        disable_mm_preprocessor_cache=args.disable_mm_preprocessor_cache,
     )
 
     if modality == "image":
diff --git a/examples/offline_inference/vision_language_multi_image.py b/examples/offline_inference/vision_language_multi_image.py
index eb4f3b6c8..ca31dc3e0 100644
--- a/examples/offline_inference/vision_language_multi_image.py
+++ b/examples/offline_inference/vision_language_multi_image.py
@@ -799,12 +799,14 @@ def load_qwen2_5_vl(question: str, image_urls: list[str]) -> ModelRequestData:
         )
         smart_resize = None
 
-    model_name = "Qwen/Qwen2.5-VL-3B-Instruct"
+    model_name = "Qwen/Qwen2.5-VL-7B-Instruct"
 
     engine_args = EngineArgs(
         model=model_name,
         max_model_len=32768 if smart_resize is None else 4096,
-        max_num_seqs=5,
+        max_num_seqs=2,
+        enforce_eager=True,
+        gpu_memory_utilization=0.8,
         limit_mm_per_prompt={"image": len(image_urls)},
     )
 
diff --git a/requirements/tpu.txt b/requirements/tpu.txt
index 354771482..d86f643d3 100644
--- a/requirements/tpu.txt
+++ b/requirements/tpu.txt
@@ -10,6 +10,7 @@ jinja2>=3.1.6
 ray[default]
 ray[data]
 setuptools==78.1.0
+nixl==0.3.0
 
 # Install torch_xla
 --pre
diff --git a/requirements/xpu.txt b/requirements/xpu.txt
index 0d95dc571..170c09928 100644
--- a/requirements/xpu.txt
+++ b/requirements/xpu.txt
@@ -11,14 +11,10 @@ jinja2>=3.1.6
 datasets # for benchmark scripts
 numba == 0.60.0 # v0.61 doesn't support Python 3.9. Required for N-gram speculative decoding
 
-torch==2.7.0+xpu
+
+torch == 2.8.0
 torchaudio
 torchvision
-pytorch-triton-xpu
 --extra-index-url=https://download.pytorch.org/whl/xpu
 
-# Please refer xpu doc, we need manually install intel-extension-for-pytorch 2.6.10+xpu due to there are some conflict dependencies with torch 2.6.0+xpu
-# FIXME: This will be fix in ipex 2.7. just leave this here for awareness.
-intel-extension-for-pytorch==2.7.10+xpu
-oneccl_bind_pt==2.7.0+xpu
---extra-index-url=https://pytorch-extension.intel.com/release-whl/stable/xpu/us/
+intel-extension-for-pytorch @ https://intel-extension-for-pytorch.s3.us-east-1.amazonaws.com/ipex_dev/xpu/intel_extension_for_pytorch-2.8.10.post0%2Bxpu-cp312-cp312-linux_x86_64.whl
diff --git a/run_benchmark_with_profile.sh b/run_benchmark_with_profile.sh
new file mode 100644
index 000000000..fe4dbc268
--- /dev/null
+++ b/run_benchmark_with_profile.sh
@@ -0,0 +1,3 @@
+export VLLM_TORCH_PROFILER_DIR=$PWD/profile
+
+VLLM_USE_V1=1 python3 benchmarks/benchmark_throughput.py --model facebook/opt-125m --dataset_name random  --enforce-eager --max-num-seqs 32 --gpu-memory-util 0.8 --num-prompts 16 --max-model-len 2000 --input-len 1024 --output-len 10 --max-num-batched-tokens 32768  --disable-sliding-window --dtype float16 --profile
\ No newline at end of file
diff --git a/setup.py b/setup.py
index d46e678e7..9951f49bc 100644
--- a/setup.py
+++ b/setup.py
@@ -148,6 +148,7 @@ class cmake_build_ext(build_ext):
         cmake_args = [
             '-DCMAKE_BUILD_TYPE={}'.format(cfg),
             '-DVLLM_TARGET_DEVICE={}'.format(VLLM_TARGET_DEVICE),
+            "-DCMAKE_CXX_STANDARD=17",
         ]
 
         verbose = envs.VERBOSE
@@ -442,7 +443,7 @@ def _is_xpu() -> bool:
 
 
 def _build_custom_ops() -> bool:
-    return _is_cuda() or _is_hip() or _is_cpu()
+    return _is_cuda() or _is_hip() or _is_cpu() or _is_xpu()
 
 
 def get_rocm_version():
diff --git a/tests/conftest.py b/tests/conftest.py
index a18dbf58c..fd4956bdb 100644
--- a/tests/conftest.py
+++ b/tests/conftest.py
@@ -1062,8 +1062,17 @@ class VllmRunner:
         return [req_output.outputs.score for req_output in req_outputs]
 
     def apply_model(self, func: Callable[[nn.Module], _R]) -> list[_R]:
-        executor = self.llm.llm_engine.model_executor
-        return executor.apply_model(func)
+        if hasattr(self.llm.llm_engine, "model_executor"):
+            # This works either in V0 or in V1 with
+            # VLLM_ENABLE_V1_MULTIPROCESSING=0
+            executor = self.llm.llm_engine.model_executor
+            return executor.apply_model(func)
+
+        # This works in V1 with VLLM_ALLOW_INSECURE_SERIALIZATION=1
+        def _apply_model(self):
+            return func(self.get_model())
+
+        return self.llm.llm_engine.collective_rpc(_apply_model)
 
     def __enter__(self):
         return self
diff --git a/tests/model_executor/test_model_load_with_params.py b/tests/model_executor/test_model_load_with_params.py
index aae9a4d1e..667a63e76 100644
--- a/tests/model_executor/test_model_load_with_params.py
+++ b/tests/model_executor/test_model_load_with_params.py
@@ -22,10 +22,12 @@ REVISION_ROBERTA = os.environ.get("REVISION", "main")
 
 @pytest.mark.skipif(current_platform.is_rocm(),
                     reason="Xformers backend is not supported on ROCm.")
-def test_model_loading_with_params(vllm_runner):
+def test_model_loading_with_params(vllm_runner, monkeypatch):
     """
     Test parameter weight loading with tp>1.
     """
+    # to use apply_model
+    monkeypatch.setenv("VLLM_ALLOW_INSECURE_SERIALIZATION", "1")
     with vllm_runner(model_name=MODEL_NAME,
                      revision=REVISION,
                      dtype="float16",
@@ -61,10 +63,12 @@ def test_model_loading_with_params(vllm_runner):
 
 @pytest.mark.skipif(current_platform.is_rocm(),
                     reason="Xformers backend is not supported on ROCm.")
-def test_roberta_model_loading_with_params(vllm_runner):
+def test_roberta_model_loading_with_params(vllm_runner, monkeypatch):
     """
     Test parameter weight loading with tp>1.
     """
+    # to use apply_model
+    monkeypatch.setenv("VLLM_ALLOW_INSECURE_SERIALIZATION", "1")
     with vllm_runner(model_name=MODEL_NAME_ROBERTA,
                      revision=REVISION_ROBERTA,
                      dtype="float16",
@@ -101,10 +105,12 @@ def test_roberta_model_loading_with_params(vllm_runner):
 
 @pytest.mark.skipif(current_platform.is_rocm(),
                     reason="Xformers backend is not supported on ROCm.")
-def test_facebook_roberta_model_loading_with_params(vllm_runner):
+def test_facebook_roberta_model_loading_with_params(vllm_runner, monkeypatch):
     """
     Test loading roberta-base model with no lm_head.
     """
+    # to use apply_model
+    monkeypatch.setenv("VLLM_ALLOW_INSECURE_SERIALIZATION", "1")
     model_name = "FacebookAI/roberta-base"
     with vllm_runner(model_name=model_name,
                      dtype="float16",
diff --git a/tests/models/language/pooling/test_embedding.py b/tests/models/language/pooling/test_embedding.py
index cc9e4102d..ba42e389f 100644
--- a/tests/models/language/pooling/test_embedding.py
+++ b/tests/models/language/pooling/test_embedding.py
@@ -39,17 +39,9 @@ def v1(run_with_both_engines):
         pytest.param("ssmits/Qwen2-7B-Instruct-embed-base",
                      marks=[pytest.mark.skip_v0, pytest.mark.cpu_model]),
         # [Encoder-only]
-        pytest.param(
-            "BAAI/bge-base-en-v1.5",
-            marks=[
-                # CPU only supports V1
-                pytest.mark.core_model,
-                pytest.mark.skip_v1
-            ]),
-        pytest.param("sentence-transformers/all-MiniLM-L12-v2",
-                     marks=[pytest.mark.skip_v1]),
-        pytest.param("intfloat/multilingual-e5-small",
-                     marks=[pytest.mark.skip_v1]),
+        pytest.param("BAAI/bge-base-en-v1.5", marks=[pytest.mark.core_model]),
+        pytest.param("sentence-transformers/all-MiniLM-L12-v2"),
+        pytest.param("intfloat/multilingual-e5-small"),
         pytest.param("Alibaba-NLP/gte-Qwen2-1.5B-instruct",
                      marks=[pytest.mark.skip_v1]),
         # [Cross-Encoder]
diff --git a/tests/models/language/pooling/test_jina.py b/tests/models/language/pooling/test_jina.py
index 16c711407..a4681baa5 100644
--- a/tests/models/language/pooling/test_jina.py
+++ b/tests/models/language/pooling/test_jina.py
@@ -23,6 +23,14 @@ RERANK_MODELS = [
 ]
 
 
+@pytest.fixture(autouse=True)
+def v1(run_with_both_engines):
+    # Simple autouse wrapper to run both engines for each test
+    # This can be promoted up to conftest.py to run for every
+    # test in a package
+    pass
+
+
 @pytest.mark.parametrize("model_info", EMBEDDING_MODELS)
 def test_embed_models_mteb(hf_runner, vllm_runner,
                            model_info: EmbedModelInfo) -> None:
diff --git a/tests/models/multimodal/processing/test_common.py b/tests/models/multimodal/processing/test_common.py
index fd5842523..321a156a6 100644
--- a/tests/models/multimodal/processing/test_common.py
+++ b/tests/models/multimodal/processing/test_common.py
@@ -272,10 +272,15 @@ def _test_processing_correctness_one(
     "THUDM/GLM-4.1V-9B-Thinking",
     "ibm-granite/granite-speech-3.3-2b",
     "h2oai/h2ovl-mississippi-800m",
+    "internlm/Intern-S1",
     "OpenGVLab/InternVL2-1B",
     "OpenGVLab/InternVL3-1B",
     "HuggingFaceM4/Idefics3-8B-Llama3",
     "HuggingFaceTB/SmolVLM2-2.2B-Instruct",
+    "OpenGVLab/InternVL3_5-1B",
+    "OpenGVLab/InternVL3_5-GPT-OSS-20B-A4B-Preview",
+    "OpenGVLab/InternVL3_5-30B-A3B",
+    "Kwai-Keye/Keye-VL-8B-Preview",
     "moonshotai/Kimi-VL-A3B-Instruct",
     "meta-llama/Llama-4-Scout-17B-16E-Instruct",
     "llava-hf/llava-1.5-7b-hf",
diff --git a/tests/models/multimodal/processing/test_tensor_schema.py b/tests/models/multimodal/processing/test_tensor_schema.py
new file mode 100644
index 000000000..2d8cd49ed
--- /dev/null
+++ b/tests/models/multimodal/processing/test_tensor_schema.py
@@ -0,0 +1,264 @@
+# SPDX-License-Identifier: Apache-2.0
+# SPDX-FileCopyrightText: Copyright contributors to the vLLM project
+from collections.abc import Iterable
+from functools import partial
+from typing import Any, Union
+from unittest.mock import patch
+
+import numpy as np
+import pytest
+from mistral_common.protocol.instruct.messages import (ImageChunk, TextChunk,
+                                                       UserMessage)
+from mistral_common.protocol.instruct.request import ChatCompletionRequest
+from PIL import Image
+
+from vllm.config import ModelConfig
+from vllm.engine.llm_engine import LLMEngine as V0LLMEngine
+from vllm.inputs import InputProcessingContext
+from vllm.multimodal import (MULTIMODAL_REGISTRY, BatchedTensorInputs,
+                             MultiModalKwargs)
+from vllm.multimodal.processing import BaseMultiModalProcessor
+from vllm.multimodal.utils import group_mm_kwargs_by_modality
+from vllm.transformers_utils.tokenizer import cached_tokenizer_from_config
+from vllm.utils import GiB_bytes, is_list_of, set_default_torch_num_threads
+from vllm.v1.core.kv_cache_utils import get_kv_cache_config
+from vllm.v1.engine.core import EngineCore as V1EngineCore
+
+from ....conftest import VllmRunner
+from ...registry import _MULTIMODAL_EXAMPLE_MODELS, HF_EXAMPLE_MODELS
+from ...utils import dummy_hf_overrides
+
+ARCH_TO_SKIP = {
+    "MolmoForCausalLM": "incompatible requirements",
+}
+ARCH_NEEDS_EXTRAS = [
+    "InternVLChatModel",
+    "Idefics3ForConditionalGeneration",
+    "LlavaForConditionalGeneration",
+    "MiniCPMV",
+    "PaliGemmaForConditionalGeneration",
+]
+REPO_ID_TO_SKIP = {
+    "nm-testing/pixtral-12b-FP8-dynamic": "duplicated test",
+    # FIXME(Isotr0py): enable GPT-OSS based InternVL3.5 model
+    # after support PP for GPT-OSS
+    "OpenGVLab/InternVL3_5-GPT-OSS-20B-A4B-Preview": "Broken model",
+}
+
+ImageInput = list[Image.Image]
+VideoInput = Union[list[Image.Image], list[np.ndarray],
+                   list[tuple[np.ndarray, dict[str, Any]]]]
+AudioInput = list[tuple[np.ndarray, int]]
+
+
+def _resize_data(_data: Union[Image.Image, np.ndarray],
+                 size_factor: float) -> Union[Image.Image, np.ndarray]:
+    assert size_factor <= 1, "Size factor must be less than 1"
+    # Image input
+    if isinstance(_data, Image.Image):
+        W, H = _data.width, _data.height
+        W, H = map(lambda x: int(x * size_factor), (W, H))
+        return _data.resize((W, H))
+    # Video input with PIL Images
+    elif is_list_of(_data, Image.Image):
+        W, H = next(iter(_data)).width, next(iter(_data)).height
+        T = len(_data)
+        T, W, H = map(lambda x: max(int(x * size_factor), 1), (T, W, H))
+        return [d.resize((W, H)) for d in _data[:T]]
+    # Video input with numpy arrays
+    elif isinstance(_data, np.ndarray) and _data.ndim >= 4:
+        T, H, W, C = _data.shape[-4:]
+        T, H, W = map(lambda x: max(int(x * size_factor), 1), (T, H, W))
+        return _data[..., :T, :H, :W, :C]
+    # Audio input
+    elif isinstance(_data, np.ndarray) and _data.ndim == 1:
+        return _data[:int(len(_data) * size_factor)]
+    raise AssertionError("This line should be unreachable.")
+
+
+def resize_mm_data(
+    data: Union[ImageInput, VideoInput, AudioInput],
+    size_factors: tuple[float,
+                        ...]) -> Union[ImageInput, VideoInput, AudioInput]:
+    size_factors = size_factors[:len(data)]
+    if is_list_of(data, (Image.Image, np.ndarray, list)):
+        return [_resize_data(d, s) for d, s in zip(data, size_factors)]
+    elif is_list_of(data, tuple):
+        return [(_resize_data(d, s), meta)
+                for (d, meta), s in zip(data, size_factors)]
+    raise ValueError("Unsupported multimodal data type.")
+
+
+def create_batched_mm_kwargs(
+    model_config: ModelConfig,
+    processor: BaseMultiModalProcessor,
+    size_factors: tuple[float, ...] = (1.0, 0.5, 0.25),
+) -> Iterable[tuple[str, int, BatchedTensorInputs]]:
+    processing_info = processor.info
+    dummy_inputs = processor.dummy_inputs
+    supported_mm_limits = processing_info.get_supported_mm_limits()
+    mm_counts = {
+        modality: 3 if limit is None else limit
+        for modality, limit in supported_mm_limits.items()
+    }
+    processor_inputs = dummy_inputs.get_dummy_processor_inputs(
+        seq_len=model_config.max_model_len,
+        mm_counts=mm_counts,
+    )
+    mm_data = processor_inputs.mm_data
+    resized_mm_data = {
+        modality: resize_mm_data(data, size_factors)
+        for modality, data in mm_data.items()
+    }
+    # Mistral chat outputs tokens directly, rather than text prompts
+    if model_config.tokenizer_mode == "mistral":
+        images = resized_mm_data.get("image", [])
+        request = ChatCompletionRequest(messages=[
+            UserMessage(content=[
+                TextChunk(text=""),
+                *(ImageChunk(image=image) for image in images),
+            ]),
+        ])
+        tokenizer = processing_info.get_tokenizer()
+        res = tokenizer.mistral.encode_chat_completion(request)
+        prompt = res.tokens
+    else:
+        prompt = processor_inputs.prompt
+    mm_kwargs = processor.apply(
+        prompt=prompt,
+        mm_data=resized_mm_data,
+        hf_processor_mm_kwargs=processor_inputs.hf_processor_mm_kwargs,
+        tokenization_kwargs=processor_inputs.tokenization_kwargs,
+    )["mm_kwargs"]
+    items = [
+        item for modality in supported_mm_limits
+        for item in mm_kwargs[modality]
+    ]
+    return group_mm_kwargs_by_modality(items)
+
+
+def get_model_id_to_test(
+        model_arch_list: Iterable[str]) -> list[tuple[str, str]]:
+    filtered_results = []
+    for model_arch in model_arch_list:
+        model_info = HF_EXAMPLE_MODELS.get_hf_info(model_arch)
+        if model_info.extras and model_arch in ARCH_NEEDS_EXTRAS:
+            available_repos = list(
+                map(lambda model_id: (model_arch, model_id),
+                    [model_info.default, *model_info.extras.values()]))
+            filtered_results.extend(available_repos)
+        else:
+            filtered_results.append((model_arch, model_info.default))
+    return filtered_results
+
+
+@pytest.mark.parametrize(
+    "model_arch, model_id",
+    get_model_id_to_test(_MULTIMODAL_EXAMPLE_MODELS.keys()))
+def test_model_tensor_schema(model_arch: str, model_id: str,
+                             vllm_runner: type[VllmRunner], monkeypatch):
+    if model_arch in ARCH_TO_SKIP:
+        pytest.skip(f"Skipping {model_arch} due to {ARCH_TO_SKIP[model_arch]}")
+    if model_id in REPO_ID_TO_SKIP:
+        pytest.skip(f"Skipping {model_id} due to {REPO_ID_TO_SKIP[model_id]}")
+
+    model_info = HF_EXAMPLE_MODELS.get_hf_info(model_arch)
+    model_info.check_available_online(on_fail="skip")
+    model_info.check_transformers_version(on_fail="skip",
+                                          check_max_version=False)
+
+    hf_overrides_fn = partial(dummy_hf_overrides,
+                              model_arch=model_arch,
+                              exist_overrides=model_info.hf_overrides)
+
+    model_config = ModelConfig(
+        model_id,
+        tokenizer=model_info.tokenizer or model_id,
+        tokenizer_mode=model_info.tokenizer_mode,
+        revision=model_info.revision,
+        trust_remote_code=model_info.trust_remote_code,
+        hf_overrides=model_info.hf_overrides,
+    )
+    model_cls = MULTIMODAL_REGISTRY._get_model_cls(model_config)
+    factories = MULTIMODAL_REGISTRY._processor_factories[model_cls]
+
+    if not any(
+            hasattr(model_cls, f"_parse_and_validate_{m}_input")
+            for m in ["image", "video", "audio"]):
+        pytest.skip(f"{model_arch} does not support tensor schema validation.")
+
+    ctx = InputProcessingContext(
+        model_config,
+        tokenizer=cached_tokenizer_from_config(model_config),
+    )
+    processing_info = factories.info(ctx)
+    supported_mm_limits = processing_info.get_supported_mm_limits()
+    limit_mm_per_prompt = {
+        modality: 3 if limit is None else limit
+        for modality, limit in supported_mm_limits.items()
+    }
+
+    # Avoid calling model.forward()
+    def _initialize_kv_caches_v0(self) -> None:
+        self.cache_config.num_gpu_blocks = 0
+        self.cache_config.num_cpu_blocks = 0
+
+    def _initialize_kv_caches_v1(self, vllm_config):
+        kv_cache_specs = self.model_executor.get_kv_cache_specs()
+        scheduler_kv_cache_config = get_kv_cache_config(
+            vllm_config,
+            kv_cache_specs[0],
+            10 * GiB_bytes,
+        )
+
+        # gpu_blocks (> 0), cpu_blocks, scheduler_kv_cache_config
+        return 1, 0, scheduler_kv_cache_config
+
+    with (patch.object(V0LLMEngine, "_initialize_kv_caches",
+                       _initialize_kv_caches_v0),
+          patch.object(V1EngineCore, "_initialize_kv_caches",
+                       _initialize_kv_caches_v1), monkeypatch.context() as m):
+        m.setenv("VLLM_ALLOW_INSECURE_SERIALIZATION", "1")
+        if model_info.v0_only:
+            m.setenv("VLLM_USE_V1", "0")
+
+        # TODO(Isotr0py): Can we avoid initializing engine?
+        with (
+                set_default_torch_num_threads(1),
+                vllm_runner(
+                    model_id,
+                    tokenizer_name=model_info.tokenizer,
+                    tokenizer_mode=model_info.tokenizer_mode,
+                    revision=model_info.revision,
+                    trust_remote_code=model_info.trust_remote_code,
+                    max_model_len=model_info.max_model_len,
+                    load_format="dummy",
+                    hf_overrides=hf_overrides_fn,
+                    limit_mm_per_prompt=limit_mm_per_prompt,
+                    enforce_eager=True,
+                ) as vllm_model,
+        ):
+            model_config = vllm_model.llm.llm_engine.model_config
+            llm_engine = vllm_model.llm.llm_engine
+
+            if hasattr(llm_engine, "processor"):
+                # v1 processor
+                mm_registry = llm_engine.processor.mm_registry
+            else:
+                # v0 input_preprocessor
+                mm_registry = llm_engine.input_preprocessor.mm_registry
+
+            processor = mm_registry.create_processor(model_config)
+
+            def validate_model_input(model, modality: str,
+                                     mm_kwargs: MultiModalKwargs):
+                method_name = f"_parse_and_validate_{modality}_input"
+                if hasattr(model, method_name):
+                    getattr(model, method_name)(**mm_kwargs)
+
+            for modality, _, mm_kwargs in create_batched_mm_kwargs(
+                    model_config, processor):
+                valid_func = partial(validate_model_input,
+                                     modality=modality,
+                                     mm_kwargs=mm_kwargs)
+                vllm_model.apply_model(valid_func)
diff --git a/tests/models/registry.py b/tests/models/registry.py
index 84ca0bc60..6d32122a4 100644
--- a/tests/models/registry.py
+++ b/tests/models/registry.py
@@ -267,6 +267,9 @@ _TEXT_GENERATION_EXAMPLE_MODELS = {
     "Qwen3ForCausalLM": _HfExamplesInfo("Qwen/Qwen3-8B"),
     "Qwen3MoeForCausalLM": _HfExamplesInfo("Qwen/Qwen3-30B-A3B"),
     "RWForCausalLM": _HfExamplesInfo("tiiuae/falcon-40b"),
+    "SeedOssForCausalLM": _HfExamplesInfo("ByteDance-Seed/Seed-OSS-36B-Instruct", # noqa: E501
+                                          trust_remote_code=True,
+                                          is_available_online=False),
     "StableLMEpochForCausalLM": _HfExamplesInfo("stabilityai/stablelm-zephyr-3b"),  # noqa: E501
     "StableLmForCausalLM": _HfExamplesInfo("stabilityai/stablelm-3b-4e1t"),
     "Starcoder2ForCausalLM": _HfExamplesInfo("bigcode/starcoder2-3b"),
@@ -373,7 +376,10 @@ _MULTIMODAL_EXAMPLE_MODELS = {
                                       transformers_version_reason="HF model is not compatible."),  # noqa: E501
     "InternVLChatModel": _HfExamplesInfo("OpenGVLab/InternVL2-1B",
                                          extras={"2B": "OpenGVLab/InternVL2-2B",
-                                                 "3.0": "OpenGVLab/InternVL3-1B"},  # noqa: E501
+                                                 "3.0": "OpenGVLab/InternVL3-1B",   # noqa: E501
+                                                 "3.5-qwen3": "OpenGVLab/InternVL3_5-1B",   # noqa: E501
+                                                 "3.5-qwen3moe": "OpenGVLab/InternVL3_5-30B-A3B",   # noqa: E501
+                                                 "3.5-gptoss": "OpenGVLab/InternVL3_5-GPT-OSS-20B-A4B-Preview"},  # noqa: E501
                                          trust_remote_code=True),
     "Idefics3ForConditionalGeneration": _HfExamplesInfo("HuggingFaceM4/Idefics3-8B-Llama3",  # noqa: E501
                                                         {"tiny": "HuggingFaceTB/SmolVLM-256M-Instruct"}),  # noqa: E501
@@ -397,7 +403,7 @@ _MULTIMODAL_EXAMPLE_MODELS = {
     "MiniCPMO": _HfExamplesInfo("openbmb/MiniCPM-o-2_6",
                                 trust_remote_code=True),
     "MiniCPMV": _HfExamplesInfo("openbmb/MiniCPM-Llama3-V-2_5",
-                                extras={"2.6": "openbmb/MiniCPM-V-2_6"},  # noqa: E501
+                                extras={"2.6": "openbmb/MiniCPM-V-2_6", "4.0": "openbmb/MiniCPM-V-4", "4.5": "openbmb/MiniCPM-V-4_5"},  # noqa: E501
                                 trust_remote_code=True),
     "MiniMaxVL01ForConditionalGeneration": _HfExamplesInfo("MiniMaxAI/MiniMax-VL-01", # noqa: E501
                                               trust_remote_code=True,
diff --git a/tests/quantization/test_cpu_offload.py b/tests/quantization/test_cpu_offload.py
index 08d9573ec..82a0e0cd8 100644
--- a/tests/quantization/test_cpu_offload.py
+++ b/tests/quantization/test_cpu_offload.py
@@ -1,4 +1,4 @@
-# SPDX-License-Identifier: Apache-2.0
+# SPDX-License-Identifier: Apache-2.0
 # SPDX-FileCopyrightText: Copyright contributors to the vLLM project
 
 # Expanded quantized model tests for CPU offloading
@@ -11,6 +11,16 @@ from tests.quantization.utils import is_quant_method_supported
 from ..utils import compare_two_settings
 
 
+@pytest.mark.skipif(not is_quant_method_supported("fp8"),
+                    reason="fp8 is not supported on this GPU type.")
+def test_offload_weights_before_quant_fp8():
+    # Test quantization of an unquantized checkpoint
+    compare_two_settings("meta-llama/Llama-3.2-1B-Instruct",
+                         ["--quantization", "fp8"], ["--quantization", "fp8"],
+                         {"VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT": "1"},
+                         max_wait_seconds=480)
+
+
 @pytest.mark.skipif(not is_quant_method_supported("fp8"),
                     reason="fp8 is not supported on this GPU type.")
 def test_cpu_offload_fp8():
diff --git a/tests/quantization/test_ipex_quant.py b/tests/quantization/test_ipex_quant.py
index 34b1b6c2e..4c8082646 100644
--- a/tests/quantization/test_ipex_quant.py
+++ b/tests/quantization/test_ipex_quant.py
@@ -25,7 +25,7 @@ DTYPE = ["bfloat16"]
 @pytest.mark.parametrize("model", MODELS)
 @pytest.mark.parametrize("dtype", DTYPE)
 def test_ipex_quant(vllm_runner, model, dtype):
-    with vllm_runner(model, dtype=dtype) as llm:
+    with vllm_runner(model, dtype=dtype, enforce_eager=True, block_size=64) as llm:
         output = llm.generate_greedy(["The capital of France is"],
                                      max_tokens=32)
     assert output
diff --git a/tests/tool_use/test_seed_oss_tool_parser.py b/tests/tool_use/test_seed_oss_tool_parser.py
new file mode 100644
index 000000000..d85bc9bbf
--- /dev/null
+++ b/tests/tool_use/test_seed_oss_tool_parser.py
@@ -0,0 +1,459 @@
+# SPDX-License-Identifier: Apache-2.0
+# SPDX-FileCopyrightText: Copyright contributors to the vLLM project
+# ruff: noqa: E501
+
+import json
+from collections.abc import Generator
+from typing import Optional
+
+import pytest
+
+from vllm.entrypoints.openai.protocol import (ChatCompletionRequest,
+                                              ChatCompletionToolsParam,
+                                              DeltaMessage, FunctionCall,
+                                              ToolCall)
+from vllm.entrypoints.openai.tool_parsers import SeedOssToolParser
+from vllm.transformers_utils.detokenizer import detokenize_incrementally
+from vllm.transformers_utils.tokenizer import AnyTokenizer, get_tokenizer
+
+# Use a common model that is likely to be available
+MODEL = "ByteDance-Seed/Seed-OSS-36B-Instruct"
+
+
+@pytest.fixture(scope="module")
+def seed_oss_tokenizer():
+    return get_tokenizer(tokenizer_name=MODEL, trust_remote_code=True)
+
+
+@pytest.fixture
+def seed_oss_tool_parser(seed_oss_tokenizer):
+    return SeedOssToolParser(seed_oss_tokenizer)
+
+
+@pytest.fixture
+def sample_tools():
+    return [
+        ChatCompletionToolsParam(
+            type="function",
+            function={
+                "name": "get_weather",
+                "description": "Get current temperature for a given location.",
+                "parameters": {
+                    "type": "object",
+                    "properties": {
+                        "location": {
+                            "type": "string",
+                            "description":
+                            "City and country e.g. Bogot√°, Colombia"
+                        },
+                        "unit": {
+                            "type": "string",
+                            "description": "this is the unit of temperature"
+                        }
+                    },
+                    "required": ["location"],
+                    "additionalProperties": False
+                },
+                "returns": {
+                    "type": "object",
+                    "properties": {
+                        "temperature": {
+                            "type": "number",
+                            "description": "temperature in celsius"
+                        }
+                    },
+                    "required": ["temperature"],
+                    "additionalProperties": False
+                },
+                "strict": True
+            }),
+    ]
+
+
+def assert_tool_calls(actual_tool_calls: list[ToolCall],
+                      expected_tool_calls: list[ToolCall]):
+    assert len(actual_tool_calls) == len(expected_tool_calls)
+
+    for actual_tool_call, expected_tool_call in zip(actual_tool_calls,
+                                                    expected_tool_calls):
+        # Seed-OSS tool call will not generate id
+        assert actual_tool_call.type == "function"
+        assert actual_tool_call.function == expected_tool_call.function
+
+        assert actual_tool_call.function.name == expected_tool_call.function.name
+        assert actual_tool_call.function.arguments == expected_tool_call.function.arguments
+
+
+def test_extract_tool_calls_no_tools(seed_oss_tool_parser):
+    model_output = "This is a test response without any tool calls"
+    extracted_tool_calls = seed_oss_tool_parser.extract_tool_calls(
+        model_output, request=None)  # type: ignore[arg-type]
+
+    assert not extracted_tool_calls.tools_called
+    assert extracted_tool_calls.tool_calls == []
+    assert extracted_tool_calls.content == model_output
+
+
+@pytest.mark.parametrize(
+    ids=[
+        "tool_call_0_thinking_budget",
+        "tool_call_512_thinkg_budget",
+        "tool_call_unlimited_thinking_budget",
+    ],
+    argnames=["model_output", "expected_tool_calls", "expected_content"],
+    argvalues=[
+        ("""<seed:think>\n</seed:cot_budget_reflect>\n</seed:cot_budget_reflect>\n"""
+         """The current thinking budget is 0, so I will directly start answering the question.\n</seed:think>\n"""
+         """<seed:tool_call>\n<function=get_weather>\n"""
+         """<parameter=location>Barcelona, Spain</parameter>\n</function>\n</seed:tool_call>""",
+         [
+             ToolCall(function=FunctionCall(
+                 name="get_weather",
+                 arguments=json.dumps({
+                     "location": "Barcelona, Spain",
+                 }, ),
+             ),
+                      type='function')
+         ],
+         """<seed:think>\n</seed:cot_budget_reflect>\n</seed:cot_budget_reflect>\n"""
+         """The current thinking budget is 0, so I will directly start answering the question.\n</seed:think>\n"""
+         ),
+        (
+            """<seed:think>The user\'s current thinking budget is 512.</seed:cot_budget_reflect>\nLet me analyze the """
+            """question. The user wants to know the weather in Barcelona, Spain. Looking at the functions available, """
+            """there\'s a get_weather function that can retrieve the current temperature for a given location. \n\nFirst, """
+            """check the parameters required by get_weather: location is mandatory (needs city and country), and unit is """
+            """optional. The user provided "Barcelona Spain" as the location, which fits the required format (city, """
+            """country). \n<seed:cot_budget_reflect>I have used 131 tokens, and there are 381 tokens remaining for use."""
+            """</seed:cot_budget_reflect>\n Since the unit isn\'t specified, the function will default to Celsius, which """
+            """is fine. \n\nThere\'s no need to ask for more information because the location is clear. So I should call """
+            """the get_weather function with location set to "Barcelona, Spain" (adding a comma for clarity, though the """
+            """user\'s input has a space, but the function might accept either; to be safe, using the standard format """
+            """with a comma).\n<seed:cot_budget_reflect>I have used 257 tokens, and there are 255 tokens remaining for """
+            """use.</seed:cot_budget_reflect>\n The unit parameter can be omitted since it\'s optional.</seed:think>\n"""
+            """<seed:tool_call>\n<function=get_weather>\n<parameter=location>Barcelona, Spain</parameter>\n</function>"""
+            """\n</seed:tool_call>""",
+            [
+                ToolCall(function=FunctionCall(
+                    name="get_weather",
+                    arguments=json.dumps({
+                        "location": "Barcelona, Spain",
+                    }, ),
+                ),
+                         type='function')
+            ],
+            """<seed:think>The user\'s current thinking budget is 512.</seed:cot_budget_reflect>\nLet me analyze the """
+            """question. The user wants to know the weather in Barcelona, Spain. Looking at the functions available, """
+            """there\'s a get_weather function that can retrieve the current temperature for a given location. \n\nFirst, """
+            """check the parameters required by get_weather: location is mandatory (needs city and country), and unit is """
+            """optional. The user provided "Barcelona Spain" as the location, which fits the required format (city, """
+            """country). \n<seed:cot_budget_reflect>I have used 131 tokens, and there are 381 tokens remaining for use."""
+            """</seed:cot_budget_reflect>\n Since the unit isn\'t specified, the function will default to Celsius, which """
+            """is fine. \n\nThere\'s no need to ask for more information because the location is clear. So I should call """
+            """the get_weather function with location set to "Barcelona, Spain" (adding a comma for clarity, though the """
+            """user\'s input has a space, but the function might accept either; to be safe, using the standard format """
+            """with a comma).\n<seed:cot_budget_reflect>I have used 257 tokens, and there are 255 tokens remaining for """
+            """use.</seed:cot_budget_reflect>\n The unit parameter can be omitted since it\'s optional.</seed:think>\n""",
+        ),
+        (
+            """<seed:think>\nGot it, let\'s see. The user asked for the weather in Barcelona, Spain. """
+            """First, I need to remember the function I can use: get_weather. The function requires a """
+            """location (city and country) which is "Barcelona, Spain" here, and unit is optional. Since """
+            """the user didn\'t specify the unit, the default in the function is Celsius, right? Wait, """
+            """let me check the function docstring again. Oh, the function says unit is optional, and """
+            """returns temperature in Celsius. So I should call get_weather with location "Barcelona, """
+            """Spain" and maybe omit unit or set to Celsius. Let me format the function call correctly. """
+            """The format is <seed:tool_call>\n<function=get_weather>\n<parameter=location>Barcelona, """
+            """Spain</parameter>\n<parameter=unit>celsius</parameter>\n</function>\n</seed:tool_call>. """
+            """Wait, but does the unit parameter accept "celsius"? The docstring says unit is the unit """
+            """of temperature, but the return is in Celsius anyway. Maybe even if I don\'t pass unit, """
+            """it\'s okay, but to be explicit, maybe pass "celsius". Let me go with that. So the function """
+            """call should be as above. Then wait for the result to come back and tell the user the """
+            """temperature in Celsius.</seed:think><seed:tool_call>\n<function=get_weather>\n<parameter=location>"""
+            """Barcelona, Spain</parameter>\n<parameter=unit>celsius</parameter>\n</function>\n</seed:tool_call>""",
+            [
+                ToolCall(function=FunctionCall(
+                    name="get_weather",
+                    arguments=json.dumps(
+                        {
+                            "location": "Barcelona, Spain",
+                            "unit": "celsius",
+                        }, ),
+                ),
+                         type='function')
+            ],
+            """<seed:think>\nGot it, let\'s see. The user asked for the weather in Barcelona, Spain. """
+            """First, I need to remember the function I can use: get_weather. The function requires a """
+            """location (city and country) which is "Barcelona, Spain" here, and unit is optional. Since """
+            """the user didn\'t specify the unit, the default in the function is Celsius, right? Wait, """
+            """let me check the function docstring again. Oh, the function says unit is optional, and """
+            """returns temperature in Celsius. So I should call get_weather with location "Barcelona, """
+            """Spain" and maybe omit unit or set to Celsius. Let me format the function call correctly. """
+            """The format is <seed:tool_call>\n<function=get_weather>\n<parameter=location>Barcelona, """
+            """Spain</parameter>\n<parameter=unit>celsius</parameter>\n</function>\n</seed:tool_call>. """
+            """Wait, but does the unit parameter accept "celsius"? The docstring says unit is the unit """
+            """of temperature, but the return is in Celsius anyway. Maybe even if I don\'t pass unit, """
+            """it\'s okay, but to be explicit, maybe pass "celsius". Let me go with that. So the function """
+            """call should be as above. Then wait for the result to come back and tell the user the """
+            """temperature in Celsius.</seed:think>""",
+        ),
+    ],
+)
+def test_extract_tool_calls(seed_oss_tool_parser, sample_tools, model_output,
+                            expected_tool_calls, expected_content):
+    request = ChatCompletionRequest(model=MODEL,
+                                    messages=[],
+                                    tools=sample_tools)
+    extracted_tool_calls = seed_oss_tool_parser.extract_tool_calls(
+        model_output, request=request)  # type: ignore[arg-type]
+    assert extracted_tool_calls.tools_called
+
+    assert_tool_calls(extracted_tool_calls.tool_calls, expected_tool_calls)
+
+    assert extracted_tool_calls.content == expected_content
+
+
+def test_streaming_tool_calls_no_tools(seed_oss_tool_parser):
+    model_output = "This is a test response without any tool calls"
+
+    result = seed_oss_tool_parser.extract_tool_calls_streaming(
+        previous_text="his is a test response",
+        current_text=model_output,
+        delta_text=" without any tool calls.",
+        previous_token_ids=[],
+        current_token_ids=[],
+        delta_token_ids=[],
+        request=None,
+    )
+
+    # Should return the delta text as content
+    assert result is not None
+    assert hasattr(result, 'content')
+    assert result.content == " without any tool calls."
+
+
+def stream_delta_message_generator(
+    seed_oss_tool_parser: SeedOssToolParser,
+    seed_oss_tokenizer: AnyTokenizer,
+    model_output: str,
+    request: Optional[ChatCompletionRequest] = None
+) -> Generator[DeltaMessage, None, None]:
+    all_token_ids = seed_oss_tokenizer.encode(model_output,
+                                              add_special_tokens=False)
+
+    previous_text = ""
+    previous_tokens = None
+    prefix_offset = 0
+    read_offset = 0
+    for i, delta_token in enumerate(all_token_ids):
+        delta_token_ids = [delta_token]
+        previous_token_ids = all_token_ids[:i]
+        current_token_ids = all_token_ids[:i + 1]
+
+        (new_tokens, delta_text, new_prefix_offset,
+         new_read_offset) = detokenize_incrementally(
+             tokenizer=seed_oss_tokenizer,
+             all_input_ids=current_token_ids,
+             prev_tokens=previous_tokens,
+             prefix_offset=prefix_offset,
+             read_offset=read_offset,
+             skip_special_tokens=False,
+             spaces_between_special_tokens=True,
+         )
+
+        current_text = previous_text + delta_text
+
+        delta_message = seed_oss_tool_parser.extract_tool_calls_streaming(
+            previous_text,
+            current_text,
+            delta_text,
+            previous_token_ids,
+            current_token_ids,
+            delta_token_ids,
+            request=request,
+        )
+        if delta_message:
+            yield delta_message
+
+        previous_text = current_text
+        previous_tokens = (previous_tokens +
+                           new_tokens if previous_tokens else new_tokens)
+        prefix_offset = new_prefix_offset
+        read_offset = new_read_offset
+
+
+@pytest.mark.parametrize(
+    ids=[
+        "tool_call_0_thinking_budget",
+        "tool_call_512_thinkg_budget",
+        "tool_call_unlimited_thinking_budget",
+    ],
+    argnames=["model_output", "expected_tool_calls", "expected_content"],
+    argvalues=[
+        ("""<seed:think>\n</seed:cot_budget_reflect>\n</seed:cot_budget_reflect>\n"""
+         """The current thinking budget is 0, so I will directly start answering the question.\n</seed:think>\n"""
+         """<seed:tool_call>\n<function=get_weather>\n"""
+         """<parameter=location>Barcelona, Spain</parameter>\n</function>\n</seed:tool_call>""",
+         [
+             ToolCall(function=FunctionCall(
+                 name="get_weather",
+                 arguments=json.dumps({
+                     "location": "Barcelona, Spain",
+                 }, ),
+             ),
+                      type='function')
+         ],
+         """<seed:think>\n</seed:cot_budget_reflect>\n</seed:cot_budget_reflect>\n"""
+         """The current thinking budget is 0, so I will directly start answering the question.\n</seed:think>\n"""
+         ),
+        (
+            """<seed:think>The user\'s current thinking budget is 512.</seed:cot_budget_reflect>\nLet me analyze the """
+            """question. The user wants to know the weather in Barcelona, Spain. Looking at the functions available, """
+            """there\'s a get_weather function that can retrieve the current temperature for a given location. \n\nFirst, """
+            """check the parameters required by get_weather: location is mandatory (needs city and country), and unit is """
+            """optional. The user provided "Barcelona Spain" as the location, which fits the required format (city, """
+            """country). \n<seed:cot_budget_reflect>I have used 131 tokens, and there are 381 tokens remaining for use."""
+            """</seed:cot_budget_reflect>\n Since the unit isn\'t specified, the function will default to Celsius, which """
+            """is fine. \n\nThere\'s no need to ask for more information because the location is clear. So I should call """
+            """the get_weather function with location set to "Barcelona, Spain" (adding a comma for clarity, though the """
+            """user\'s input has a space, but the function might accept either; to be safe, using the standard format """
+            """with a comma).\n<seed:cot_budget_reflect>I have used 257 tokens, and there are 255 tokens remaining for """
+            """use.</seed:cot_budget_reflect>\n The unit parameter can be omitted since it\'s optional.</seed:think>\n"""
+            """<seed:tool_call>\n<function=get_weather>\n<parameter=location>Barcelona, Spain</parameter>\n</function>"""
+            """\n</seed:tool_call>""",
+            [
+                ToolCall(function=FunctionCall(
+                    name="get_weather",
+                    arguments=json.dumps({
+                        "location": "Barcelona, Spain",
+                    }, ),
+                ),
+                         type='function')
+            ],
+            """<seed:think>The user\'s current thinking budget is 512.</seed:cot_budget_reflect>\nLet me analyze the """
+            """question. The user wants to know the weather in Barcelona, Spain. Looking at the functions available, """
+            """there\'s a get_weather function that can retrieve the current temperature for a given location. \n\nFirst, """
+            """check the parameters required by get_weather: location is mandatory (needs city and country), and unit is """
+            """optional. The user provided "Barcelona Spain" as the location, which fits the required format (city, """
+            """country). \n<seed:cot_budget_reflect>I have used 131 tokens, and there are 381 tokens remaining for use."""
+            """</seed:cot_budget_reflect>\n Since the unit isn\'t specified, the function will default to Celsius, which """
+            """is fine. \n\nThere\'s no need to ask for more information because the location is clear. So I should call """
+            """the get_weather function with location set to "Barcelona, Spain" (adding a comma for clarity, though the """
+            """user\'s input has a space, but the function might accept either; to be safe, using the standard format """
+            """with a comma).\n<seed:cot_budget_reflect>I have used 257 tokens, and there are 255 tokens remaining for """
+            """use.</seed:cot_budget_reflect>\n The unit parameter can be omitted since it\'s optional.</seed:think>\n""",
+        ),
+        (
+            """<seed:think>\nGot it, let\'s see. The user asked for the weather in Barcelona, Spain. """
+            """First, I need to remember the function I can use: get_weather. The function requires a """
+            """location (city and country) which is "Barcelona, Spain" here, and unit is optional. Since """
+            """the user didn\'t specify the unit, the default in the function is Celsius, right? Wait, """
+            """let me check the function docstring again. Oh, the function says unit is optional, and """
+            """returns temperature in Celsius. So I should call get_weather with location "Barcelona, """
+            """Spain" and maybe omit unit or set to Celsius. Let me format the function call correctly. """
+            """The format is <seed:tool_call>\n<function=get_weather>\n<parameter=location>Barcelona, """
+            """Spain</parameter>\n<parameter=unit>celsius</parameter>\n</function>\n</seed:tool_call>. """
+            """Wait, but does the unit parameter accept "celsius"? The docstring says unit is the unit """
+            """of temperature, but the return is in Celsius anyway. Maybe even if I don\'t pass unit, """
+            """it\'s okay, but to be explicit, maybe pass "celsius". Let me go with that. So the function """
+            """call should be as above. Then wait for the result to come back and tell the user the """
+            """temperature in Celsius.</seed:think><seed:tool_call>\n<function=get_weather>\n<parameter=location>"""
+            """Barcelona, Spain</parameter>\n<parameter=unit>celsius</parameter>\n</function>\n</seed:tool_call>""",
+            [
+                ToolCall(function=FunctionCall(
+                    name="get_weather",
+                    arguments=json.dumps(
+                        {
+                            "location": "Barcelona, Spain",
+                            "unit": "celsius",
+                        }, ),
+                ),
+                         type='function')
+            ],
+            """<seed:think>\nGot it, let\'s see. The user asked for the weather in Barcelona, Spain. """
+            """First, I need to remember the function I can use: get_weather. The function requires a """
+            """location (city and country) which is "Barcelona, Spain" here, and unit is optional. Since """
+            """the user didn\'t specify the unit, the default in the function is Celsius, right? Wait, """
+            """let me check the function docstring again. Oh, the function says unit is optional, and """
+            """returns temperature in Celsius. So I should call get_weather with location "Barcelona, """
+            """Spain" and maybe omit unit or set to Celsius. Let me format the function call correctly. """
+            """The format is <seed:tool_call>\n<function=get_weather>\n<parameter=location>Barcelona, """
+            """Spain</parameter>\n<parameter=unit>celsius</parameter>\n</function>\n</seed:tool_call>. """
+            """Wait, but does the unit parameter accept "celsius"? The docstring says unit is the unit """
+            """of temperature, but the return is in Celsius anyway. Maybe even if I don\'t pass unit, """
+            """it\'s okay, but to be explicit, maybe pass "celsius". Let me go with that. So the function """
+            """call should be as above. Then wait for the result to come back and tell the user the """
+            """temperature in Celsius.</seed:think>""",
+        ),
+    ],
+)
+def test_streaming_tool_calls(seed_oss_tool_parser, seed_oss_tokenizer,
+                              sample_tools, model_output, expected_tool_calls,
+                              expected_content):
+    """Test incremental streaming behavior"""
+    request = ChatCompletionRequest(model=MODEL,
+                                    messages=[],
+                                    tools=sample_tools)
+
+    other_content = ''
+    tool_states = {}  # Track state per tool index
+
+    for delta_message in stream_delta_message_generator(
+            seed_oss_tool_parser, seed_oss_tokenizer, model_output, request):
+        # role should never be streamed from tool parser
+        assert not delta_message.role
+
+        if delta_message.content:
+            other_content += delta_message.content
+
+        if delta_message.tool_calls:
+            for tool_call in delta_message.tool_calls:
+                idx = tool_call.index
+
+                # Initialize state for new tool
+                if idx not in tool_states:
+                    tool_states[idx] = {
+                        "id": None,
+                        "name": None,
+                        "arguments": "",
+                        "type": None
+                    }
+
+                # First chunk should have id, name, and type
+                if tool_call.id:
+                    tool_states[idx]["id"] = tool_call.id
+
+                if tool_call.type:
+                    assert tool_call.type == "function"
+                    tool_states[idx]["type"] = tool_call.type
+
+                if tool_call.function:
+                    if tool_call.function.name:
+                        # Should only be set once
+                        assert tool_states[idx]["name"] is None
+                        tool_states[idx]["name"] = tool_call.function.name
+
+                    if tool_call.function.arguments is not None:
+                        # Accumulate arguments incrementally
+                        tool_states[idx][
+                            "arguments"] += tool_call.function.arguments
+
+    # Verify final content
+    assert other_content == expected_content
+
+    # Verify we got all expected tool calls
+    assert len(tool_states) == len(expected_tool_calls)
+
+    # Verify each tool call
+    for idx, expected_tool in enumerate(expected_tool_calls):
+        state = tool_states[idx]
+        assert state["id"] is not None
+        assert state["type"] == "function"
+        assert state["name"] == expected_tool.function.name
+
+        # Parse accumulated arguments
+        arguments_str = state["arguments"]
+        assert arguments_str is not None
+        actual_args = json.loads(arguments_str)
+        expected_args = json.loads(expected_tool.function.arguments)
+        assert actual_args == expected_args
diff --git a/tests/v1/attention/utils.py b/tests/v1/attention/utils.py
index 30cfbdda5..f3ce33fcf 100644
--- a/tests/v1/attention/utils.py
+++ b/tests/v1/attention/utils.py
@@ -93,6 +93,7 @@ def create_common_attn_metadata(
         max_query_len=max_query_len,
         block_table_tensor=block_table_tensor,
         slot_mapping=slot_mapping,
+        causal=True,
     )
 
 
diff --git a/tests/v1/e2e/test_correctness_sliding_window.py b/tests/v1/e2e/test_correctness_sliding_window.py
index 277ea3c83..1624701b0 100644
--- a/tests/v1/e2e/test_correctness_sliding_window.py
+++ b/tests/v1/e2e/test_correctness_sliding_window.py
@@ -18,7 +18,7 @@ class TestConfig:
 
 model_config = {
     "bigcode/starcoder2-3b": TestConfig(4096, (800, 1100)),
-    "google/gemma-3-1b-it": TestConfig(4096, (400, 800)),
+    #"google/gemma-3-1b-it": TestConfig(4096, (400, 800)),
 }
 
 
@@ -26,7 +26,7 @@ model_config = {
     "model",
     [
         "bigcode/starcoder2-3b",  # sliding window only
-        "google/gemma-3-1b-it",  # sliding window + full attention
+        #"google/gemma-3-1b-it",  # sliding window + full attention
     ])
 @pytest.mark.parametrize("batch_size", [5])
 @pytest.mark.parametrize("seed", [1])
@@ -42,7 +42,7 @@ def test_sliding_window_retrieval(monkeypatch, model, batch_size, seed):
 
         test_config = model_config[model]
 
-        llm = LLM(model=model)
+        llm = LLM(model=model, enforce_eager=True, block_size=64)
         sampling_params = SamplingParams(temperature=0.0, max_tokens=100)
 
         prompts, answer, indices = prep_prompts(batch_size,
diff --git a/tests/v1/e2e/test_spec_decode.py b/tests/v1/e2e/test_spec_decode.py
index 2423f966a..f0343cae2 100644
--- a/tests/v1/e2e/test_spec_decode.py
+++ b/tests/v1/e2e/test_spec_decode.py
@@ -68,7 +68,7 @@ def test_ngram_correctness(
     with monkeypatch.context() as m:
         m.setenv("VLLM_USE_V1", "1")
 
-        ref_llm = LLM(model=model_name, max_model_len=1024)
+        ref_llm = LLM(model=model_name, max_model_len=1024, enforce_eager=True, block_size=32, dtype="float16")
         ref_outputs = ref_llm.chat(test_prompts, sampling_config)
         del ref_llm
         torch.cuda.empty_cache()
@@ -83,6 +83,10 @@ def test_ngram_correctness(
                 "num_speculative_tokens": 3,
             },
             max_model_len=1024,
+            enforce_eager=True,
+            block_size=64,
+            dtype="float16",
+            gpu_memory_utilization=0.6,
         )
         spec_outputs = spec_llm.chat(test_prompts, sampling_config)
         matches = 0
@@ -131,7 +135,12 @@ def test_eagle_correctness(
 
         ref_llm = LLM(model=model_name,
                       max_model_len=2048,
-                      tensor_parallel_size=tp_size)
+                      tensor_parallel_size=tp_size,
+                      enforce_eager=True,
+                      block_size=64,
+                      dtype="float16",
+                      gpu_memory_utilization=0.6,
+                      )
         ref_outputs = ref_llm.chat(test_prompts, sampling_config)
         del ref_llm
         torch.cuda.empty_cache()
@@ -147,6 +156,10 @@ def test_eagle_correctness(
                 "num_speculative_tokens": 3,
                 "max_model_len": 2048,
             },
+            enforce_eager=True,
+            block_size=64,
+            dtype="float16",
+            gpu_memory_utilization=0.6,
             max_model_len=2048,
         )
         spec_outputs = spec_llm.chat(test_prompts, sampling_config)
diff --git a/tests/v1/kv_connector/nixl_integration/run_tpu_disagg_accuracy_test.sh b/tests/v1/kv_connector/nixl_integration/run_tpu_disagg_accuracy_test.sh
new file mode 100644
index 000000000..45779d169
--- /dev/null
+++ b/tests/v1/kv_connector/nixl_integration/run_tpu_disagg_accuracy_test.sh
@@ -0,0 +1,162 @@
+#!/bin/bash
+set -xe
+
+# Hosts / ports
+PREFILL_HOST=${PREFILL_HOST:-"localhost"}
+PREFILL_PORT=${PREFILL_PORT:-8100}
+PREFILL_NIXL_SIDE_PORT=${PREFILL_NIXL_SIDE_PORT:-5577}
+DECODE_HOST=${DECODE_HOST:-"localhost"}
+DECODE_PORT=${DECODE_PORT:-8200}
+PROXY_HOST=${PROXY_HOST:-"localhost"}
+PROXY_PORT=${PROXY_PORT:-8192}
+BASELINE_HOST=${BASELINE_HOST:-"localhost"}
+BASELINE_PORT=${BASELINE_PORT:-9290}
+
+
+# Model to run.
+MODEL_NAME=${MODEL_NAME:-"meta-llama/Llama-3.2-3B-Instruct"}
+MAX_MODEL_LEN=${MAX_MODEL_LEN:-1024}
+BLOCK_SIZE=${BLOCK_SIZE:-32}
+
+
+# execution env
+GIT_ROOT=$(git rev-parse --show-toplevel)
+EXP_ROOT="${GIT_ROOT}/tests/v1/kv_connector/nixl_integration"
+CONDA_PATH=${CONDA_PATH:-"/home/${USER}/anaconda3"}
+CONDA_ENV_NAME=${CONDA_ENV_NAME:-"nixl"}
+
+OUTPUT_FILE=${OUTPUT_FILE:-"${EXP_ROOT}/.tpu_accuracy_test_outputs.txt"}
+
+# Trap the SIGINT signal (triggered by Ctrl+C)
+trap 'kill $(jobs -pr)' SIGINT SIGTERM EXIT
+
+
+# Waits for vLLM server to start.
+wait_for_server() {
+  local host=$1
+  local port=$2
+  timeout 1200 bash -c "
+    until curl -s ${host}:${port}/v1/completions > /dev/null; do
+      sleep 1
+    done" && return 0 || return 1
+}
+
+# Cleanup function
+cleanup() {
+    echo "Caught Ctrl+C, cleaning up..."
+    # Cleanup commands
+    pgrep python | xargs kill -9 || true
+    # pkill -f python || true
+    echo "Cleanup complete. Exiting."
+}
+
+launch_baseline() {
+  BASELINE_BASE_CMD="source ${CONDA_PATH}/bin/activate ${CONDA_ENV_NAME};
+  VLLM_LOGGING_LEVEL=DEBUG \
+  VLLM_USE_V1=1 \
+  PJRT_DEVICE=TPU \
+  VLLM_WORKER_MULTIPROC_METHOD=spawn \
+  VLLM_ENABLE_V1_MULTIPROCESSING=0 vllm serve $MODEL_NAME \
+      --host ${BASELINE_HOST} \
+      --port ${BASELINE_PORT} \
+      --max-model-len ${MAX_MODEL_LEN}\
+      --seed 42 \
+      --block-size ${BLOCK_SIZE} \
+      --gpu-memory-utilization 0.5 \
+      --disable-log-requests \
+      --enforce-eager"
+  echo ${BASELINE_BASE_CMD}
+  ssh -tt ${BASELINE_HOST} "${BASELINE_BASE_CMD}" &
+}
+
+launch_pd() {
+  PREFILL_BASE_CMD="source ${CONDA_PATH}/bin/activate ${CONDA_ENV_NAME};
+  UCX_TLS=tcp \
+  VLLM_MULTIPROC_EXECUTE_MODEL_TIMEOUT_S=200 \
+  VLLM_LOGGING_LEVEL=DEBUG \
+  VLLM_USE_V1=1 \
+  VLLM_NIXL_SIDE_CHANNEL_HOST=${PREFILL_HOST} \
+  VLLM_NIXL_SIDE_CHANNEL_PORT=${PREFILL_NIXL_SIDE_PORT} \
+  PJRT_DEVICE=TPU \
+  VLLM_WORKER_MULTIPROC_METHOD=spawn \
+  VLLM_ENABLE_V1_MULTIPROCESSING=0 vllm serve $MODEL_NAME \
+      --host ${PREFILL_HOST} \
+      --port ${PREFILL_PORT} \
+      --max-model-len ${MAX_MODEL_LEN}\
+      --seed 42 \
+      --block-size ${BLOCK_SIZE} \
+      --enforce-eager \
+      --gpu-memory-utilization 0.5 \
+      --disable-log-requests \
+      --kv-transfer-config '{\"kv_connector\":\"NixlConnector\",\"kv_role\":\"kv_both\",\"kv_buffer_device\":\"cpu\"}'"
+
+
+  DECODE_BASE_CMD="source ${CONDA_PATH}/bin/activate ${CONDA_ENV_NAME};
+  UCX_TLS=tcp \
+  VLLM_MULTIPROC_EXECUTE_MODEL_TIMEOUT_S=200 \
+  VLLM_LOGGING_LEVEL=DEBUG \
+  VLLM_USE_V1=1 \
+  PJRT_DEVICE=TPU \
+  VLLM_WORKER_MULTIPROC_METHOD=spawn \
+  VLLM_ENABLE_V1_MULTIPROCESSING=0 vllm serve $MODEL_NAME \
+      --host ${DECODE_HOST} \
+      --port ${DECODE_PORT} \
+      --max-model-len ${MAX_MODEL_LEN}\
+      --seed 42 \
+      --block-size ${BLOCK_SIZE} \
+      --enforce-eager \
+      --gpu-memory-utilization 0.5 \
+      --disable-log-requests \
+      --kv-transfer-config '{\"kv_connector\":\"NixlConnector\",\"kv_role\":\"kv_both\",\"kv_buffer_device\":\"cpu\"}'"
+
+  echo ${PREFILL_BASE_CMD}
+  echo ${DECODE_BASE_CMD}
+  sleep 2
+
+  # execute on hosts
+  ssh -tt ${PREFILL_HOST} "${PREFILL_BASE_CMD}" &
+  ssh -tt ${DECODE_HOST} "${DECODE_BASE_CMD}" &
+  sleep 1
+  wait_for_server ${PREFILL_HOST} ${PREFILL_PORT}
+  sleep 1
+  wait_for_server ${DECODE_HOST} ${DECODE_PORT}
+  sleep 1
+}
+
+launch_pd_proxy(){
+  PROXY_BASE_CMD="source ${CONDA_PATH}/bin/activate ${CONDA_ENV_NAME};
+  python3 ${EXP_ROOT}/toy_proxy_server.py \
+  --prefiller-host ${PREFILL_HOST} --prefiller-port ${PREFILL_PORT} \
+  --decoder-host ${DECODE_HOST} --decoder-port ${DECODE_PORT} \
+  --host=${PROXY_HOST} --port ${PROXY_PORT}"
+  echo ${PROXY_BASE_CMD}
+  ssh -tt ${PROXY_HOST} "${PROXY_BASE_CMD}" &
+}
+
+run_tests(){
+  local service_url=$1
+  local mode=$2
+  python3 ${EXP_ROOT}/test_disagg_accuracy.py --service_url=${service_url} --model_name=${MODEL_NAME} --mode=${mode} --file_name=${OUTPUT_FILE}
+}
+
+
+# run non-disagg. baseline & save outputs
+launch_baseline
+sleep 2
+wait_for_server ${BASELINE_HOST} ${BASELINE_PORT}
+run_tests "http://${BASELINE_HOST}:${BASELINE_PORT}" "baseline"
+cleanup
+sleep 10
+
+
+# run disagg. & do exact-match with the outputs from baseline
+launch_pd
+launch_pd_proxy
+sleep 10
+run_tests "http://${PROXY_HOST}:${PROXY_PORT}" "disagg"
+echo "-----P/D success----"
+
+rm ${OUTPUT_FILE}
+cleanup
+
+exit 0
\ No newline at end of file
diff --git a/tests/v1/kv_connector/nixl_integration/run_tpu_edge_case_test.sh b/tests/v1/kv_connector/nixl_integration/run_tpu_edge_case_test.sh
new file mode 100644
index 000000000..c37c92fdf
--- /dev/null
+++ b/tests/v1/kv_connector/nixl_integration/run_tpu_edge_case_test.sh
@@ -0,0 +1,128 @@
+#!/bin/bash
+set -xe
+
+# Hosts / ports
+PREFILL_HOST=${PREFILL_HOST:-"localhost"}
+PREFILL_PORT=${PREFILL_PORT:-8100}
+PREFILL_NIXL_SIDE_PORT=${PREFILL_NIXL_SIDE_PORT:-5577}
+DECODE_HOST=${DECODE_HOST:-"localhost"}
+DECODE_PORT=${DECODE_PORT:-8200}
+PROXY_HOST=${PROXY_HOST:-"localhost"}
+PROXY_PORT=${PROXY_PORT:-8192}
+BASELINE_HOST=${BASELINE_HOST:-"localhost"}
+BASELINE_PORT=${BASELINE_PORT:-9290}
+
+
+# Model to run.
+MODEL_NAME=${MODEL_NAME:-"meta-llama/Llama-3.2-3B-Instruct"}
+MAX_MODEL_LEN=${MAX_MODEL_LEN:-1024}
+BLOCK_SIZE=${BLOCK_SIZE:-32}
+
+
+# execution env
+GIT_ROOT=$(git rev-parse --show-toplevel)
+EXP_ROOT="${GIT_ROOT}/tests/v1/kv_connector/nixl_integration"
+CONDA_PATH=${CONDA_PATH:-"/home/${USER}/anaconda3"}
+CONDA_ENV_NAME=${CONDA_ENV_NAME:-"nixl"}
+
+OUTPUT_FILE=${OUTPUT_FILE:-"${EXP_ROOT}/.tpu_accuracy_test_outputs.txt"}
+
+# Trap the SIGINT signal (triggered by Ctrl+C)
+trap 'kill $(jobs -pr)' SIGINT SIGTERM EXIT
+
+# Waits for vLLM server to start.
+wait_for_server() {
+  local host=$1
+  local port=$2
+  timeout 1200 bash -c "
+    until curl -s ${host}:${port}/v1/completions > /dev/null; do
+      sleep 1
+    done" && return 0 || return 1
+}
+
+# Cleanup function
+cleanup() {
+    echo "Caught Ctrl+C, cleaning up..."
+    # Cleanup commands
+    pgrep python | xargs kill -9 || true
+    # pkill -f python || true
+    echo "Cleanup complete. Exiting."
+}
+
+
+launch_pd() {
+  PREFILL_BASE_CMD="source ${CONDA_PATH}/bin/activate ${CONDA_ENV_NAME};
+  UCX_TLS=tcp \
+  VLLM_MULTIPROC_EXECUTE_MODEL_TIMEOUT_S=200 \
+  VLLM_LOGGING_LEVEL=DEBUG \
+  VLLM_USE_V1=1 \
+  VLLM_NIXL_SIDE_CHANNEL_HOST=${PREFILL_HOST} \
+  VLLM_NIXL_SIDE_CHANNEL_PORT=${PREFILL_NIXL_SIDE_PORT} \
+  PJRT_DEVICE=TPU \
+  VLLM_WORKER_MULTIPROC_METHOD=spawn \
+  VLLM_ENABLE_V1_MULTIPROCESSING=0 vllm serve $MODEL_NAME \
+      --host ${PREFILL_HOST} \
+      --port ${PREFILL_PORT} \
+      --max-model-len ${MAX_MODEL_LEN}\
+      --seed 42 \
+      --block-size ${BLOCK_SIZE} \
+      --enforce-eager \
+      --gpu-memory-utilization 0.5 \
+      --disable-log-requests \
+      --kv-transfer-config '{\"kv_connector\":\"NixlConnector\",\"kv_role\":\"kv_both\",\"kv_buffer_device\":\"cpu\"}'"
+
+
+  DECODE_BASE_CMD="source ${CONDA_PATH}/bin/activate ${CONDA_ENV_NAME};
+  UCX_TLS=tcp \
+  VLLM_MULTIPROC_EXECUTE_MODEL_TIMEOUT_S=200 \
+  VLLM_LOGGING_LEVEL=DEBUG \
+  VLLM_USE_V1=1 \
+  PJRT_DEVICE=TPU \
+  VLLM_WORKER_MULTIPROC_METHOD=spawn \
+  VLLM_ENABLE_V1_MULTIPROCESSING=0 vllm serve $MODEL_NAME \
+      --host ${DECODE_HOST} \
+      --port ${DECODE_PORT} \
+      --max-model-len ${MAX_MODEL_LEN}\
+      --seed 42 \
+      --block-size ${BLOCK_SIZE} \
+      --enforce-eager \
+      --gpu-memory-utilization 0.5 \
+      --disable-log-requests \
+      --kv-transfer-config '{\"kv_connector\":\"NixlConnector\",\"kv_role\":\"kv_both\",\"kv_buffer_device\":\"cpu\"}'"
+
+  echo ${PREFILL_BASE_CMD}
+  echo ${DECODE_BASE_CMD}
+  sleep 2
+
+  # execute on hosts
+  ssh -tt ${PREFILL_HOST} "${PREFILL_BASE_CMD}" &
+  ssh -tt ${DECODE_HOST} "${DECODE_BASE_CMD}" &
+  sleep 1
+  wait_for_server ${PREFILL_HOST} ${PREFILL_PORT}
+  sleep 1
+  wait_for_server ${DECODE_HOST} ${DECODE_PORT}
+  sleep 1
+}
+
+launch_pd_proxy(){
+  PROXY_BASE_CMD="source ${CONDA_PATH}/bin/activate ${CONDA_ENV_NAME};
+  python3 ${EXP_ROOT}/toy_proxy_server.py \
+  --prefiller-host ${PREFILL_HOST} --prefiller-port ${PREFILL_PORT} \
+  --decoder-host ${DECODE_HOST} --decoder-port ${DECODE_PORT} \
+  --host=${PROXY_HOST} --port ${PROXY_PORT}"
+  echo ${PROXY_BASE_CMD}
+  ssh -tt ${PROXY_HOST} "${PROXY_BASE_CMD}" &
+}
+
+
+# run disagg. & do exact-match with the outputs from baseline
+launch_pd
+launch_pd_proxy
+sleep 10
+
+PREFILL_HOST=${PREFILL_HOST} \
+PREFILL_PORT=${PREFILL_PORT} \
+DECODE_HOST=${DECODE_HOST} \
+DECODE_PORT=${DECODE_PORT} \
+PROXY_HOST=${PROXY_HOST} \
+PROXY_PORT=${PROXY_PORT} python -m pytest -s -v ${GIT_ROOT}/tests/v1/kv_connector/nixl_integration/test_edge_cases.py
\ No newline at end of file
diff --git a/tests/v1/kv_connector/nixl_integration/test_disagg_accuracy.py b/tests/v1/kv_connector/nixl_integration/test_disagg_accuracy.py
new file mode 100644
index 000000000..00e62f351
--- /dev/null
+++ b/tests/v1/kv_connector/nixl_integration/test_disagg_accuracy.py
@@ -0,0 +1,162 @@
+# SPDX-License-Identifier: Apache-2.0
+# SPDX-FileCopyrightText: Copyright contributors to the vLLM project
+import argparse
+import json
+import os
+import time
+
+import openai
+import requests
+
+MAX_OUTPUT_LEN = 30
+
+SAMPLE_PROMPTS = (
+    "Red Hat is the best company in the world to work for because it works on "
+    "open source software, which means that all the contributions are "
+    "delivered to the community. As a result, when working on projects like "
+    "vLLM we are able to meet many amazing people from various organizations "
+    "like AMD, Google, NVIDIA, ",
+    "We hold these truths to be self-evident, that all men are created equal, "
+    "that they are endowed by their Creator with certain unalienable Rights, "
+    "that among these are Life, Liberty and the pursuit of Happiness.--That "
+    "to secure these rights, Governments are instituted among Men, deriving "
+    "their just powers from the consent of the governed, ",
+)
+
+
+def check_vllm_server(url: str, timeout=5, retries=3) -> bool:
+    """
+    Checks if the vLLM server is ready by sending a GET request to the
+    /health endpoint.
+
+    Args:
+        url (str): The base URL of the vLLM server.
+        timeout (int): Timeout in seconds for the request.
+        retries (int): Number of retries if the server is not ready.
+
+    Returns:
+        bool: True if the server is ready, False otherwise.
+    """
+    for attempt in range(retries):
+        try:
+            response = requests.get(url, timeout=timeout)
+            if response.status_code == 200:
+                return True
+            else:
+                print(f"Attempt {attempt + 1}: Server returned status code "
+                      "{response.status_code}")
+        except requests.exceptions.RequestException as e:
+            print(f"Attempt {attempt + 1}: Error connecting to server: {e}")
+        time.sleep(1)  # Wait before retrying
+    return False
+
+
+def run_simple_prompt(base_url: str, model_name: str,
+                      input_prompt: str) -> str:
+    client = openai.OpenAI(api_key="EMPTY", base_url=base_url)
+    completion = client.completions.create(model=model_name,
+                                           prompt=input_prompt,
+                                           max_tokens=MAX_OUTPUT_LEN,
+                                           temperature=0.0,
+                                           seed=42)
+
+    # print("-" * 50)
+    # print(f"Completion results for {model_name}:")
+    # print(completion)
+    # print("-" * 50)
+    return completion.choices[0].text
+
+
+def main():
+    """
+    This script demonstrates how to accept two optional string arguments
+    ("service_url" and "file_name") from the command line, each with a
+    default value of an empty string, using the argparse module.
+    """
+    parser = argparse.ArgumentParser(description="vLLM client script")
+
+    parser.add_argument(
+        "--service_url",  # Name of the first argument
+        type=str,
+        required=True,
+        help="The vLLM service URL.")
+
+    parser.add_argument(
+        "--model_name",  # Name of the first argument
+        type=str,
+        required=True,
+        help="model_name",
+    )
+
+    parser.add_argument(
+        "--mode",  # Name of the second argument
+        type=str,
+        default="baseline",
+        help="mode: baseline==non-disagg, or disagg",
+    )
+
+    parser.add_argument(
+        "--file_name",  # Name of the second argument
+        type=str,
+        default=".vllm_output.txt",
+        help="the file that saves the output tokens ",
+    )
+
+    args = parser.parse_args()
+
+    for arg in vars(args):
+        print(f"{arg}: {getattr(args, arg)}")
+
+    if args.mode == "baseline":
+        # non-disagg
+        health_check_url = f"{args.service_url}/health"
+    else:
+        # disagg proxy
+        health_check_url = f"{args.service_url}/healthcheck"
+        if not os.path.exists(args.file_name):
+            raise ValueError(
+                f"In disagg mode, the output file {args.file_name} from "
+                "non-disagg. baseline does not exist.")
+
+    service_url = f"{args.service_url}/v1"
+
+    if not check_vllm_server(health_check_url):
+        raise RuntimeError(
+            f"vllm server: {args.service_url} is not ready yet!")
+
+    output_strs = dict()
+    for prompt in SAMPLE_PROMPTS:
+        output_str = run_simple_prompt(base_url=service_url,
+                                       model_name=args.model_name,
+                                       input_prompt=prompt)
+        print(f"Prompt: {prompt}, output: {output_str}")
+        output_strs[prompt] = output_str
+
+    if args.mode == "baseline":
+        # baseline: save outputs
+        try:
+            with open(args.file_name, 'w') as json_file:
+                json.dump(output_strs, json_file, indent=4)
+        except OSError as e:
+            print(f"Error writing to file: {e}")
+            raise
+    else:
+        # disagg. verify outputs
+        baseline_outputs = None
+        try:
+            with open(args.file_name) as json_file:
+                baseline_outputs = json.load(json_file)
+        except OSError as e:
+            print(f"Error writing to file: {e}")
+            raise
+        assert isinstance(baseline_outputs, dict)
+        assert len(baseline_outputs) == len(output_strs)
+        for prompt, output in baseline_outputs.items():
+            assert prompt in output_strs, f"{prompt} not included"
+            assert output == output_strs[prompt], (
+                f"baseline_output: {output} != PD output: {output_strs[prompt]}"
+            )
+
+
+if __name__ == "__main__":
+    main()
diff --git a/tests/v1/kv_connector/nixl_integration/test_edge_cases.py b/tests/v1/kv_connector/nixl_integration/test_edge_cases.py
index 95465a25f..8439e30be 100644
--- a/tests/v1/kv_connector/nixl_integration/test_edge_cases.py
+++ b/tests/v1/kv_connector/nixl_integration/test_edge_cases.py
@@ -4,8 +4,11 @@ import os
 
 import openai
 
+PREFILL_HOST = os.getenv("PREFILL_HOST", "localhost")
 PREFILL_PORT = os.getenv("PREFILL_PORT", None)
+DECODE_HOST = os.getenv("DECODE_HOST", "localhost")
 DECODE_PORT = os.getenv("DECODE_PORT", None)
+PROXY_HOST = os.getenv("PROXY_HOST", "localhost")
 PROXY_PORT = os.getenv("PROXY_PORT", None)
 
 if PREFILL_PORT is None or DECODE_PORT is None or PROXY_PORT is None:
@@ -21,15 +24,15 @@ def test_edge_cases():
     # Set the OpenAI API key and base URL
     decode_client = openai.OpenAI(
         api_key="MY_KEY",
-        base_url=f"http://localhost:{DECODE_PORT}/v1",
+        base_url=f"http://{DECODE_HOST}:{DECODE_PORT}/v1",
     )
     prefill_client = openai.OpenAI(
         api_key="MY_KEY",
-        base_url=f"http://localhost:{PREFILL_PORT}/v1",
+        base_url=f"http://{PREFILL_HOST}:{PREFILL_PORT}/v1",
     )
     proxy_client = openai.OpenAI(
         api_key="MY_KEY",
-        base_url=f"http://localhost:{PROXY_PORT}/v1",
+        base_url=f"http://{PROXY_HOST}:{PROXY_PORT}/v1",
     )
 
     # Get the list of models
diff --git a/tests/v1/kv_connector/nixl_integration/toy_proxy_server.py b/tests/v1/kv_connector/nixl_integration/toy_proxy_server.py
index c58cb0286..66e237da0 100644
--- a/tests/v1/kv_connector/nixl_integration/toy_proxy_server.py
+++ b/tests/v1/kv_connector/nixl_integration/toy_proxy_server.py
@@ -3,6 +3,7 @@
 
 import argparse
 import itertools
+import logging
 import os
 import uuid
 from contextlib import asynccontextmanager
@@ -11,9 +12,8 @@ import httpx
 from fastapi import FastAPI, Request
 from fastapi.responses import StreamingResponse
 
-from vllm.logger import init_logger
-
-logger = init_logger(__name__)
+logger = logging.getLogger(__name__)
+logger.setLevel(logging.DEBUG)
 
 
 @asynccontextmanager
diff --git a/tests/v1/test_oracle.py b/tests/v1/test_oracle.py
index b4d4348c7..cc59287a9 100644
--- a/tests/v1/test_oracle.py
+++ b/tests/v1/test_oracle.py
@@ -13,7 +13,6 @@ UNSUPPORTED_MODELS_V1 = [
     "openai/whisper-large-v3",  # transcription
     "facebook/bart-large-cnn",  # encoder decoder
     "state-spaces/mamba-130m-hf",  # mamba1
-    "BAAI/bge-m3",  # embedding
 ]
 
 MODEL = "meta-llama/Llama-3.2-1B-Instruct"
diff --git a/tests/v1/test_utils.py b/tests/v1/test_utils.py
index 0b892bd9d..00d98a873 100644
--- a/tests/v1/test_utils.py
+++ b/tests/v1/test_utils.py
@@ -1,9 +1,8 @@
 # SPDX-License-Identifier: Apache-2.0
 # SPDX-FileCopyrightText: Copyright contributors to the vLLM project
 
-import re
-
 import pytest
+import regex as re
 import requests
 import torch
 
diff --git a/vllm/_custom_ops.py b/vllm/_custom_ops.py
index cf296a3b5..6e60c1ccc 100644
--- a/vllm/_custom_ops.py
+++ b/vllm/_custom_ops.py
@@ -1282,7 +1282,10 @@ def scaled_fp8_quant(
                 output, input.contiguous(), scale, scale_ub)
         else:
             scale = torch.zeros(1, device=input.device, dtype=torch.float32)
-            torch.ops._C.dynamic_scaled_fp8_quant(output, input, scale)
+            if current_platform.is_xpu():
+                torch.ops.torch_ipex.dynamic_scaled_fp8_quant(output, input, scale)
+            else:
+                torch.ops._C.dynamic_scaled_fp8_quant(output, input, scale)
     else:
         assert scale.numel() == 1, f"{scale.shape}"
         torch.ops._C.static_scaled_fp8_quant(output, input, scale)
diff --git a/vllm/_ipex_ops.py b/vllm/_ipex_ops.py
index 7533bf5ef..283b19900 100644
--- a/vllm/_ipex_ops.py
+++ b/vllm/_ipex_ops.py
@@ -6,6 +6,8 @@ from typing import Optional
 import torch
 
 from vllm.logger import init_logger
+from vllm.platforms import current_platform
+from vllm.utils import direct_register_custom_op
 
 logger = init_logger(__name__)
 
@@ -14,6 +16,8 @@ try:
 except ImportError as e:
     logger.warning("Import error msg: %s", e.msg)
 
+import vllm._C.ops
+
 
 class ipex_ops:
 
@@ -77,7 +81,8 @@ class ipex_ops:
         assert kv_cache_dtype == "auto"
         num_heads = out.size(1)
         num_queries_per_tokens = num_heads // num_kv_heads
-        ipex.llm.modules.PagedAttention.single_query_kv_attention(
+        #ipex.llm.modules.PagedAttention.single_query_kv_attention(
+        torch.xpu.paged_attention_v1(
             out,
             query.contiguous(),
             key_cache.view_as(value_cache),
@@ -142,10 +147,12 @@ class ipex_ops:
         cos_sin_cache: torch.Tensor,  # [cos_sin_dim, rot_dim]
         is_neox: bool,
     ) -> None:
-        rot_dim = cos_sin_cache.size(1)
-        ipex.llm.functional.rotary_embedding_batched(positions, query, key,
-                                                     head_size, cos_sin_cache,
-                                                     is_neox, rot_dim)
+        # rot_dim = cos_sin_cache.size(1)
+        # ipex.llm.functional.rotary_embedding_batched(positions, query, key,
+        #                                              head_size, cos_sin_cache,
+        #                                              is_neox, rot_dim)
+        import vllm._C.ops
+        vllm._C.ops.rotary_embedding(positions, query, key, head_size, cos_sin_cache, is_neox)
 
     @staticmethod
     def batched_rotary_embedding(positions: torch.Tensor, query: torch.Tensor,
@@ -161,14 +168,18 @@ class ipex_ops:
     @staticmethod
     def rms_norm(input: torch.Tensor, weight: torch.Tensor,
                  epsilon: float) -> torch.Tensor:
-        return ipex.llm.functional.rms_norm(input, weight, epsilon)
+        # return ipex.llm.functional.rms_norm(input, weight, epsilon)
+        tmp = torch.empty_like(input)
+        vllm._C.ops.rms_norm(tmp, input, weight, epsilon)
+        return tmp
 
     @staticmethod
     def fused_add_rms_norm(input: torch.Tensor, residual: torch.Tensor,
                            weight: torch.Tensor, epsilon: float) -> None:
-        tmp = ipex.llm.functional.add_rms_norm(residual, input, weight, None,
-                                               epsilon, True)
-        input.copy_(tmp)
+        # tmp = ipex.llm.functional.add_rms_norm(residual, input, weight, None,
+        #                                        epsilon, True)
+        # input.copy_(tmp)
+        vllm._C.ops.fused_add_rms_norm(input, residual, weight, epsilon)
 
     @staticmethod
     def varlen_attention(
@@ -348,3 +359,166 @@ class ipex_ops:
     def swap_blocks(src: torch.Tensor, dst: torch.Tensor,
                     block_mapping: torch.Tensor) -> None:
         torch.xpu.swap_blocks(src, dst, block_mapping)  # type: ignore
+
+    @staticmethod
+    def _bgmv_shrink(inputs: torch.Tensor,
+                     lora_a_weights: torch.Tensor,
+                     output_tensor: torch.Tensor,
+                     lora_indices_tensor: torch.Tensor,
+                     scaling: float = 1.0) -> None:
+
+        ipex.llm.functional.bgmv_shrink(inputs, lora_a_weights, output_tensor,
+                                        lora_indices_tensor, scaling)
+
+    def _bgmv_shrink_fake(inputs: torch.Tensor,
+                          lora_a_weights: torch.Tensor,
+                          output_tensor: torch.Tensor,
+                          lora_indices_tensor: torch.Tensor,
+                          scaling: float = 1.0) -> None:
+        pass
+
+    @staticmethod
+    def bgmv_expand(inputs: torch.Tensor,
+                    lora_b_weights: torch.Tensor,
+                    output_tensor: torch.Tensor,
+                    lora_indices_tensor: torch.Tensor,
+                    add_inputs: bool = True) -> None:
+        ipex.llm.functional.bgmv_expand(inputs, lora_b_weights, output_tensor,
+                                        lora_indices_tensor, add_inputs)
+
+    @staticmethod
+    def _bgmv_expand_slice(inputs: torch.Tensor,
+                           lora_b_weights: torch.Tensor,
+                           output_tensor: torch.Tensor,
+                           lora_indices_tensor: torch.Tensor,
+                           slice_offset: int,
+                           slice_size: int,
+                           add_inputs: bool = True) -> None:
+        ipex.llm.functional.bgmv_expand_slice(inputs, lora_b_weights,
+                                              output_tensor,
+                                              lora_indices_tensor,
+                                              slice_offset, slice_size,
+                                              add_inputs)
+
+    def _bgmv_expand_slice_fake(inputs: torch.Tensor,
+                                lora_b_weights: torch.Tensor,
+                                output_tensor: torch.Tensor,
+                                lora_indices_tensor: torch.Tensor,
+                                slice_offset: int,
+                                slice_size: int,
+                                add_inputs: bool = True) -> None:
+        pass
+
+    @staticmethod
+    def sgmv_shrink(inputs: torch.Tensor,
+                    lora_a_weights: torch.Tensor,
+                    output_tensor: torch.Tensor,
+                    b_seq_start_loc: torch.Tensor,
+                    seq_len_tensor: torch.Tensor,
+                    lora_indices_tensor: torch.Tensor,
+                    batches: int,
+                    max_seq_length: int,
+                    token_nums: int,
+                    scaling: float = 1.0) -> None:
+        assert inputs.size(0) == token_nums
+        ipex.llm.functional.sgmv_shrink(inputs, lora_a_weights, output_tensor,
+                                        b_seq_start_loc, seq_len_tensor,
+                                        lora_indices_tensor, batches,
+                                        max_seq_length, scaling)
+
+    @staticmethod
+    def sgmv_expand(inputs: torch.Tensor,
+                    lora_b_weights: torch.Tensor,
+                    output_tensor: torch.Tensor,
+                    b_seq_start_loc: torch.Tensor,
+                    seq_len_tensor: torch.Tensor,
+                    lora_indices_tensor: torch.Tensor,
+                    batches: int,
+                    max_seq_length: int,
+                    token_nums: int,
+                    add_inputs: bool = False) -> None:
+        assert inputs.size(0) == token_nums
+        ipex.llm.functional.sgmv_expand(inputs, lora_b_weights, output_tensor,
+                                        b_seq_start_loc, seq_len_tensor,
+                                        lora_indices_tensor, batches,
+                                        max_seq_length, add_inputs)
+
+    @staticmethod
+    def sgmv_expand_slice(inputs: torch.Tensor,
+                          lora_b_weights: torch.Tensor,
+                          output_tensor: torch.Tensor,
+                          b_seq_start_loc: torch.Tensor,
+                          seq_len_tensor: torch.Tensor,
+                          lora_indices_tensor: torch.Tensor,
+                          batches: int,
+                          max_seq_length: int,
+                          token_nums: int,
+                          slice_offset: int,
+                          slice_size: int,
+                          add_inputs: bool = False) -> None:
+        assert inputs.size(0) == token_nums
+        ipex.llm.functional.sgmv_expand_slice(inputs, lora_b_weights,
+                                              output_tensor, b_seq_start_loc,
+                                              seq_len_tensor,
+                                              lora_indices_tensor, batches,
+                                              max_seq_length, slice_offset,
+                                              slice_size, add_inputs)
+
+    # @staticmethod
+    # def lora_expand(inputs: torch.Tensor,
+    #                 lora_b_weights: List[torch.Tensor],
+    #                 output_tensor: torch.Tensor,
+    #                 token_lora_mapping: torch.Tensor,
+    #                 token_indices_sorted_by_lora_ids: torch.Tensor,
+    #                 num_tokens_per_lora: torch.Tensor,
+    #                 lora_token_start_loc: torch.Tensor,
+    #                 lora_ids: torch.Tensor,
+    #                 offset_start: int = 0,
+    #                 add_inputs: bool = False) -> None:
+    #     ipex.llm.functional.lora_expand(inputs, lora_b_weights,
+    #                                     output_tensor, token_lora_mapping,
+    #                                     token_indices_sorted_by_lora_ids,
+    #                                     num_tokens_per_lora, num_tokens_per_lora, #no_qa
+    #                                     lora_token_start_loc, lora_ids,
+    #                                     offset_start, add_inputs)
+
+    # @staticmethod
+    # def lora_shrink(inputs: torch.Tensor,
+    #                 lora_a_weights: List[torch.Tensor],
+    #                 output_tensor: torch.Tensor,
+    #                 token_lora_mapping: torch.Tensor,
+    #                 token_indices_sorted_by_lora_ids: torch.Tensor,
+    #                 num_tokens_per_lora: torch.Tensor,
+    #                 lora_token_start_loc: torch.Tensor,
+    #                 lora_ids: torch.Tensor,
+    #                 scaling: float) -> None:
+    #     ipex.llm.functional.lora_shrink(inputs, lora_a_weights,
+    #                                     output_tensor, token_lora_mapping,
+    #                                     token_indices_sorted_by_lora_ids,
+    #                                     num_tokens_per_lora, num_tokens_per_lora,
+    #                                     lora_token_start_loc, lora_ids,
+    #                                     scaling)
+
+
+try:
+    direct_register_custom_op(
+        op_name="bgmv_shrink",
+        op_func=ipex_ops._bgmv_shrink,
+        mutates_args=["output_tensor"],
+        fake_impl=ipex_ops._bgmv_shrink_fake,
+        dispatch_key=current_platform.dispatch_key,
+    )
+    bgmv_shrink = torch.ops.vllm.bgmv_shrink
+
+    direct_register_custom_op(
+        op_name="bgmv_expand_slice",
+        op_func=ipex_ops._bgmv_expand_slice,
+        mutates_args=["output_tensor"],
+        fake_impl=ipex_ops._bgmv_expand_slice_fake,
+        dispatch_key=current_platform.dispatch_key,
+    )
+    bgmv_expand_slice = torch.ops.vllm.bgmv_expand_slice
+
+except AttributeError:
+    bgmv_shrink = ipex_ops._bgmv_shrink
+    bgmv_expand_slice = ipex_ops._bgmv_expand_slice
diff --git a/vllm/assets/video.py b/vllm/assets/video.py
index 8ab0e9760..6d09c912a 100644
--- a/vllm/assets/video.py
+++ b/vllm/assets/video.py
@@ -94,6 +94,24 @@ def video_get_metadata(path: str) -> dict[str, Any]:
     return metadata
 
 
+def video_get_metadata(path: str) -> dict[str, Any]:
+    cap = cv2.VideoCapture(path)
+    if not cap.isOpened():
+        raise ValueError(f"Could not open video file {path}")
+
+    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
+    fps = cap.get(cv2.CAP_PROP_FPS)
+    duration = total_frames / fps if fps > 0 else 0
+
+    metadata = {
+        "total_num_frames": total_frames,
+        "fps": fps,
+        "duration": duration,
+        "video_backend": "opencv"
+    }
+    return metadata
+
+
 VideoAssetName = Literal["baby_reading"]
 
 
diff --git a/vllm/attention/layer.py b/vllm/attention/layer.py
index 178453ecd..8b350cd0d 100644
--- a/vllm/attention/layer.py
+++ b/vllm/attention/layer.py
@@ -185,7 +185,7 @@ class Attention(nn.Module):
         # opaque custom op. For other platforms, we directly call them
         # and let torch.compile handle them.
         self.use_direct_call = not current_platform.is_cuda_alike(
-        ) and not current_platform.is_cpu()
+        ) and not current_platform.is_cpu() and not current_platform.is_xpu()
 
         self.use_output = attn_backend.accept_output_buffer
         compilation_config = get_current_vllm_config().compilation_config
@@ -431,6 +431,87 @@ def maybe_save_kv_layer_to_connector(
                             attn_metadata[layer_name])
 
 
+class SelfMultiHeadAttention(nn.Module):
+    """Multi-headed attention without any cache, used for ViT."""
+
+    def __init__(
+        self,
+        num_heads: int,
+        head_size: int,
+        scale: float,
+        num_kv_heads: Optional[int] = None,
+    ):
+        super().__init__()
+        self.num_heads = num_heads
+        self.head_size = head_size
+        self.scale = scale
+        self.num_kv_heads = num_heads if num_kv_heads is None else num_kv_heads
+
+        assert self.num_heads % self.num_kv_heads == 0
+        self.num_queries_per_kv = self.num_heads // self.num_kv_heads
+
+        self.attn_backend =  _Backend.TORCH_SDPA
+
+    def forward(
+        self,
+        query: torch.Tensor,
+        key: torch.Tensor,
+        value: torch.Tensor,
+    ) -> torch.Tensor:
+        """Input shape: batch_size x seq_len x hidden_size"""
+        # TODO(Isotr0py): Use existing backend implementations and support FA3
+        bsz, q_len, _ = query.size()
+        kv_len = key.size(1)
+
+        q = query.view(bsz, q_len, self.num_heads, self.head_size)
+        k = key.view(bsz, kv_len, self.num_kv_heads, self.head_size)
+        v = value.view(bsz, kv_len, self.num_kv_heads, self.head_size)
+        from einops import rearrange
+        q, k, v = (rearrange(x, "b s ... -> (b s) ...") for x in [q, k, v])
+        from vllm._ipex_ops import ipex_ops
+        output = torch.empty(
+                    (q.shape[0], q.shape[1], q.shape[2]),
+                    dtype=q.dtype,
+                    device=q.device)
+        import math
+        head_dim = q.shape[-1]
+        scale = 1 / math.sqrt(head_dim) if self.scale is None else self.scale
+        tmp = [0]
+        tmp.append(q_len)
+        seqlen = torch.tensor(tmp)
+        cu_seqlens = torch.cumsum(seqlen, dim=0).to(device=query.device)
+        max_seqlen = q_len
+        ipex_ops.varlen_attention(q, k, v, output,
+                                cu_seqlens,
+                                cu_seqlens,
+                                None,
+                                max_seqlen,
+                                max_seqlen,
+                                pdropout=0,
+                                softmax_scale=scale,
+                                zero_tensors=False,
+                                is_causal=False,
+                                return_softmax=False,
+                                window_size_left=-1,
+                                window_size_right=-1,
+                                gen_=None,
+                                logits_soft_cap=0
+                                )
+
+        # out = rearrange(output,
+        #                             "(b s) ... -> b s ...",
+        #                             b=batch_size)
+        # query, key, value = (x.transpose(1, 2)
+        #                         for x in (query, key, value))
+        # out = F.scaled_dot_product_attention(query,
+        #                                         key,
+        #                                         value,
+        #                                         scale=self.scale)
+        # out = out.transpose(1, 2)
+
+        return output.reshape(bsz, q_len, -1)
+
+
 def unified_attention(
     query: torch.Tensor,
     key: torch.Tensor,
diff --git a/vllm/compilation/backends.py b/vllm/compilation/backends.py
index 673fb5866..65dfce992 100644
--- a/vllm/compilation/backends.py
+++ b/vllm/compilation/backends.py
@@ -414,7 +414,7 @@ class VllmBackend:
         self.prefix = prefix or model_tag
 
         global global_graph_pool
-        if global_graph_pool is None:
+        if global_graph_pool is None and not current_platform.is_xpu():
             global_graph_pool = current_platform.graph_pool_handle()
 
         # TODO: in the future, if we want to use multiple
diff --git a/vllm/compilation/fix_functionalization.py b/vllm/compilation/fix_functionalization.py
index 286221d32..b15ed7d7a 100644
--- a/vllm/compilation/fix_functionalization.py
+++ b/vllm/compilation/fix_functionalization.py
@@ -9,6 +9,7 @@ import torch
 from torch._higher_order_ops.auto_functionalize import auto_functionalized
 
 from vllm.logger import init_logger
+from vllm.platforms import current_platform
 
 from .fx_utils import is_func
 from .vllm_inductor_pass import VllmInductorPass
@@ -32,6 +33,8 @@ class FixFunctionalizationPass(VllmInductorPass):
         self.nodes_to_remove: list[torch.fx.Node] = []
         count = 0
         for node in graph.nodes:
+            if current_platform.is_xpu():
+                continue
             if not is_func(node, auto_functionalized):
                 continue  # Avoid deep if-elif nesting
 
diff --git a/vllm/compilation/pass_manager.py b/vllm/compilation/pass_manager.py
index 58216a1f0..27e634532 100644
--- a/vllm/compilation/pass_manager.py
+++ b/vllm/compilation/pass_manager.py
@@ -5,12 +5,15 @@ from torch import fx as fx
 
 from vllm.config import VllmConfig
 from vllm.logger import init_logger
+from vllm.platforms import current_platform
+
+if current_platform.is_cuda_alike():
+    from .fusion import FusionPass
+    from .collective_fusion import AllReduceFusionPass, AsyncTPPass
+    from .fusion_attn import AttnFusionPass
 
 from .activation_quant_fusion import ActivationQuantFusionPass
-from .collective_fusion import AllReduceFusionPass, AsyncTPPass
 from .fix_functionalization import FixFunctionalizationPass
-from .fusion import FusionPass
-from .fusion_attn import AttnFusionPass
 from .inductor_pass import CustomGraphPass, InductorPass, get_pass_context
 from .noop_elimination import NoOpEliminationPass
 from .sequence_parallelism import SequenceParallelismPass
@@ -18,6 +21,11 @@ from .vllm_inductor_pass import VllmInductorPass
 
 logger = init_logger(__name__)
 
+try:
+    from .fusion import FusionPass
+except AttributeError:
+    logger.warning("import FusionPass error.")
+
 
 class PostGradPassManager(CustomGraphPass):
     """
@@ -56,7 +64,7 @@ class PostGradPassManager(CustomGraphPass):
             if self.pass_config.enable_async_tp:
                 self.passes += [AsyncTPPass(config)]
 
-        if self.pass_config.enable_fusion:
+        if self.pass_config.enable_fusion and not current_platform.is_xpu():
             self.passes += [FusionPass.instance(config)]
             self.passes += [ActivationQuantFusionPass(config)]
 
diff --git a/vllm/config.py b/vllm/config.py
index f038cdd64..8b79d6728 100644
--- a/vllm/config.py
+++ b/vllm/config.py
@@ -1719,6 +1719,10 @@ class CacheConfig:
             raise ValueError("CPU offload space must be non-negative"
                              f", but got {self.cpu_offload_gb}")
 
+        if self.cpu_offload_gb > 0 and envs.VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT:
+            raise ValueError("CPU offload can't work together with"
+                             "OFFLOAD_WEIGHTS_BEFORE_QUANT")
+
         if self.gpu_memory_utilization > 1.0:
             raise ValueError(
                 "GPU memory utilization must be less than 1.0. Got "
@@ -2730,6 +2734,14 @@ class SpeculativeConfig:
                 "architectures": ["Glm4MoeMTPModel"]
             })
 
+        if hf_config.architectures[0] == "Glm4MoeForCausalLM":
+            hf_config.model_type = "glm4_moe_mtp"
+            n_predict = getattr(hf_config, "num_nextn_predict_layers", None)
+            hf_config.update({
+                "num_hidden_layers": 0,
+                "n_predict": n_predict,
+                "architectures": ["Glm4MoeMTPModel"]
+            })
         return hf_config
 
     def __post_init__(self):
diff --git a/vllm/distributed/device_communicators/xpu_communicator.py b/vllm/distributed/device_communicators/xpu_communicator.py
index dee5ed7a2..9bbfee118 100644
--- a/vllm/distributed/device_communicators/xpu_communicator.py
+++ b/vllm/distributed/device_communicators/xpu_communicator.py
@@ -7,8 +7,13 @@ import torch
 import torch.distributed as dist
 from torch.distributed import ProcessGroup
 
+import vllm.envs as envs
+from vllm.logger import init_logger
+
 from .base_device_communicator import DeviceCommunicatorBase
 
+logger = init_logger(__name__)
+
 
 class XpuCommunicator(DeviceCommunicatorBase):
 
@@ -18,6 +23,14 @@ class XpuCommunicator(DeviceCommunicatorBase):
                  device_group: Optional[ProcessGroup] = None,
                  unique_name: str = ""):
         super().__init__(cpu_group, device, device_group, unique_name)
+        if self.use_all2all:
+            all2all_backend = envs.VLLM_ALL2ALL_BACKEND
+            if all2all_backend == "naive":
+                from .all2all import NaiveAll2AllManager
+                self.all2all_manager = NaiveAll2AllManager(self.cpu_group)
+                logger.info("Using naive all2all manager.")
+            else:
+                raise ValueError(f"Unknown all2all backend: {all2all_backend}")
 
     def all_reduce(self, input_) -> torch.Tensor:
         dist.all_reduce(input_, group=self.device_group)
diff --git a/vllm/distributed/kv_transfer/kv_connector/v1/base.py b/vllm/distributed/kv_transfer/kv_connector/v1/base.py
index e1245775b..8bbdd7e06 100644
--- a/vllm/distributed/kv_transfer/kv_connector/v1/base.py
+++ b/vllm/distributed/kv_transfer/kv_connector/v1/base.py
@@ -32,7 +32,7 @@ The class provides the following primitives:
 
 import enum
 from abc import ABC, abstractmethod
-from typing import TYPE_CHECKING, Any, Optional
+from typing import TYPE_CHECKING, Any, Callable, Literal, Optional
 
 import torch
 
@@ -46,6 +46,12 @@ if TYPE_CHECKING:
     from vllm.v1.core.kv_cache_manager import KVCacheBlocks
     from vllm.v1.request import Request
 
+# s_tensor_list, d_tensor_list, s_indices, d_indices, direction
+CopyBlocksOp = Callable[[
+    dict[str, torch.Tensor], dict[
+        str, torch.Tensor], list[int], list[int], Literal["h2d", "d2h"]
+], None]
+
 logger = init_logger(__name__)
 
 
@@ -127,6 +133,13 @@ class KVConnectorBase_V1(ABC):
         """
         return
 
+    def set_host_xfer_buffer_ops(self, copy_operation: CopyBlocksOp):
+        """
+        Set the xPU-specific ops for copying KV between host and device.
+        Needed when host buffer is used for kv transfer (e.g., in NixlConnector)
+        """
+        return
+
     @abstractmethod
     def start_load_kv(self, forward_context: "ForwardContext",
                       **kwargs) -> None:
diff --git a/vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py b/vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py
index 0c5986bfa..c06cda356 100644
--- a/vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py
+++ b/vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py
@@ -1,6 +1,7 @@
 # SPDX-License-Identifier: Apache-2.0
 # SPDX-FileCopyrightText: Copyright contributors to the vLLM project
 import contextlib
+import logging
 import math
 import queue
 import threading
@@ -20,14 +21,14 @@ from vllm import envs
 from vllm.attention.selector import backend_name_to_enum, get_attn_backend
 from vllm.config import VllmConfig
 from vllm.distributed.kv_transfer.kv_connector.v1.base import (
-    KVConnectorBase_V1, KVConnectorMetadata, KVConnectorRole)
+    CopyBlocksOp, KVConnectorBase_V1, KVConnectorMetadata, KVConnectorRole)
 from vllm.distributed.parallel_state import (
     get_tensor_model_parallel_rank, get_tensor_model_parallel_world_size,
     get_tp_group)
 from vllm.distributed.utils import divide
 from vllm.forward_context import ForwardContext
 from vllm.logger import init_logger
-from vllm.platforms import _Backend
+from vllm.platforms import _Backend, current_platform
 from vllm.utils import make_zmq_path, make_zmq_socket, round_down
 from vllm.v1.core.sched.output import SchedulerOutput
 from vllm.v1.request import RequestStatus
@@ -40,6 +41,7 @@ if TYPE_CHECKING:
 Transfer = tuple[int, float]  # (xfer_handle, start_time)
 EngineId = str
 ReqId = str
+
 GET_META_MSG = b"get_meta_msg"
 
 logger = init_logger(__name__)
@@ -52,6 +54,13 @@ except ImportError:
     logger.warning("NIXL is not available")
     NixlWrapper = None
 
+# Supported xPUs and types of kv transfer buffer.
+# {xPU: tuple of supported kv buffer types}
+_NIXL_SUPPORTED_XPUS = {
+    "cuda": ("cuda", ),
+    "tpu": ("cpu", ),
+}
+
 
 class NixlAgentMetadata(
         msgspec.Struct,
@@ -80,6 +89,7 @@ class NixlConnectorMetadata(KVConnectorMetadata):
 
     def __init__(self):
         self.reqs_to_recv: dict[ReqId, ReqMeta] = {}
+        self.reqs_to_save: dict[ReqId, ReqMeta] = {}
         self.reqs_to_send: dict[ReqId, float] = {}
 
     def add_new_req(
@@ -87,8 +97,12 @@ class NixlConnectorMetadata(KVConnectorMetadata):
         request_id: ReqId,
         local_block_ids: list[int],
         kv_transfer_params: dict[str, Any],
+        load_remote_cache: bool = True,
+        save_to_host: bool = False,
     ):
-        self.reqs_to_recv[request_id] = ReqMeta(
+        # save and load are mutually exclusive
+        assert load_remote_cache ^ save_to_host
+        _req = ReqMeta(
             local_block_ids=local_block_ids,
             remote_block_ids=kv_transfer_params["remote_block_ids"],
             remote_engine_id=kv_transfer_params["remote_engine_id"],
@@ -97,6 +111,10 @@ class NixlConnectorMetadata(KVConnectorMetadata):
             # P workers don't need to receive tp_size from proxy here.
             tp_size=kv_transfer_params.get("tp_size", 1),
         )
+        if save_to_host:
+            self.reqs_to_save[request_id] = _req
+        if load_remote_cache:
+            self.reqs_to_recv[request_id] = _req
 
 
 class NixlConnector(KVConnectorBase_V1):
@@ -155,6 +173,10 @@ class NixlConnector(KVConnectorBase_V1):
         assert self.connector_worker is not None
         self.connector_worker.register_kv_caches(kv_caches)
 
+    def set_host_xfer_buffer_ops(self, copy_operation: CopyBlocksOp):
+        assert self.connector_worker is not None
+        self.connector_worker.set_host_xfer_buffer_ops(copy_operation)
+
     def get_finished(self,
                      finished_req_ids: set[str]) -> tuple[set[str], set[str]]:
         """Get the finished recving and sending requests."""
@@ -177,8 +199,11 @@ class NixlConnector(KVConnectorBase_V1):
         pass
 
     def wait_for_save(self):
-        """NixlConnector does not save explicitly."""
-        pass
+        assert self.connector_worker is not None
+        assert isinstance(self._connector_metadata, NixlConnectorMetadata)
+        if self.connector_worker.use_host_buffer and \
+           self.connector_worker.copy_blocks:
+            self.connector_worker.save_kv_to_host(self._connector_metadata)
 
 
 class NixlConnectorScheduler:
@@ -193,12 +218,15 @@ class NixlConnectorScheduler:
             envs.VLLM_NIXL_SIDE_CHANNEL_PORT +
             vllm_config.parallel_config.data_parallel_rank *
             vllm_config.parallel_config.tensor_parallel_size)
+        self.use_host_buffer = \
+            vllm_config.kv_transfer_config.kv_buffer_device == "cpu"
         logger.info("Initializing NIXL Scheduler %s", engine_id)
 
         # Requests that need to start recv/send.
         # New requests are added by update_state_after_alloc in
         # the scheduler. Used to make metadata passed to Worker.
         self._reqs_need_recv: dict[ReqId, tuple[Request, list[int]]] = {}
+        self._reqs_need_save: dict[ReqId, tuple[Request, list[int]]] = {}
         # Reqs to send and their expiration time
         self._reqs_need_send: dict[ReqId, float] = {}
 
@@ -248,7 +276,25 @@ class NixlConnectorScheduler:
             "num_external_tokens=%s, kv_transfer_params=%s",
             num_external_tokens, params)
 
-        if params is not None and params.get("do_remote_prefill"):
+        if not params:
+            return
+        if self.use_host_buffer and params.get("do_remote_decode"):
+            # NOTE: when accelerator is not directly supported by Nixl,
+            # prefilled blocks need to be saved to host memory before transfer.
+
+            # figure out full computed blocks to save
+            block_ids = blocks.get_block_ids()[0]
+            all_full = request.num_tokens % self.block_size == 0
+            full_block_ids = (block_ids if all_full else block_ids[:-1])
+            # TODO: skip the blocks that are already in the host xfer buffer.
+            # Currently, the host xfer buffer block is 1-to-1 mapped to device
+            # kv blocks, so host blocks won't be flushed as long as its device
+            # block is not overwritten; and it will be safe to skip saving them
+            # to host xfer buffer.
+            if full_block_ids:
+                self._reqs_need_save[request.request_id] = \
+                    (request, full_block_ids)
+        elif params.get("do_remote_prefill"):
             if params.get("remote_block_ids"):
                 if all(p in params for p in ("remote_engine_id", "remote_host",
                                              "remote_port")):
@@ -260,6 +306,7 @@ class NixlConnectorScheduler:
                     # Get unhashed blocks to pull from remote.
                     self._reqs_need_recv[request.request_id] = (
                         request, local_block_ids)
+
                 else:
                     logger.warning(
                         "Got invalid KVTransferParams: %s. This "
@@ -284,10 +331,21 @@ class NixlConnectorScheduler:
                 kv_transfer_params=req.kv_transfer_params,
             )
 
-        # Clear the list once workers start the transfers
-        self._reqs_need_recv.clear()
+        for req_id, (req, block_ids) in self._reqs_need_save.items():
+            assert req.kv_transfer_params is not None
+            meta.add_new_req(
+                request_id=req_id,
+                local_block_ids=block_ids,
+                kv_transfer_params=req.kv_transfer_params,
+                load_remote_cache=False,
+                save_to_host=True,
+            )
 
         meta.reqs_to_send = self._reqs_need_send
+
+        # Clear the list once workers start the transfers
+        self._reqs_need_recv.clear()
+        self._reqs_need_save.clear()
         self._reqs_need_send = {}
 
         return meta
@@ -379,9 +437,36 @@ class NixlConnectorWorker:
         self.tp_rank = get_tensor_model_parallel_rank()
         self.world_size = get_tensor_model_parallel_world_size()
         self.tp_group = get_tp_group()
+        self.num_blocks = 0
 
         # KV Caches and nixl tracking data.
-        self.kv_caches: dict[str, torch.Tensor] = {}
+        self.device_type = current_platform.device_type
+        self.kv_buffer_device: str = \
+            vllm_config.kv_transfer_config.kv_buffer_device
+        if self.device_type not in _NIXL_SUPPORTED_XPUS:
+            raise RuntimeError(f"{self.device_type} is not supported.")
+        elif self.kv_buffer_device not in _NIXL_SUPPORTED_XPUS[
+                self.device_type]:
+            raise RuntimeError(
+                f"{self.device_type} with {self.kv_buffer_device} kv_buffer "
+                "is not supported.")
+        self.device_kv_caches: dict[str, torch.Tensor] = {}
+
+        # cpu kv buffer for xfer
+        # used when xPU memory can not be registered under nixl
+        self.host_xfer_buffers: dict[str, torch.Tensor] = {}
+        self.use_host_buffer = self.kv_buffer_device == "cpu"
+        if self.kv_buffer_device == "cuda":
+            self.nixl_memory_type = "VRAM"
+        elif self.kv_buffer_device == "cpu":
+            self.nixl_memory_type = "DRAM"
+        else:
+            raise RuntimeError(
+                f"{self.device_type} with {self.kv_buffer_device} kv_buffer "
+                "is not supported.")
+
+        # Note: host xfer buffer ops when use_host_buffer is True
+        self.copy_blocks: Optional[CopyBlocksOp] = None
 
         # Map of engine_id -> kv_caches_base_addr. For TP case, each local
         # rank will still only pull from a single remote TP worker.
@@ -404,6 +489,7 @@ class NixlConnectorWorker:
 
         # In progress transfers.
         # [req_id -> list[handle]]
+        self._recving_metadata: dict[ReqId, ReqMeta] = {}
         self._recving_transfers = defaultdict[ReqId, list[Transfer]](list)
         # Track the expiration time of requests that are waiting to be sent.
         self._reqs_to_send: dict[ReqId, float] = {}
@@ -440,6 +526,7 @@ class NixlConnectorWorker:
         self.backend_name = backend.get_name()
         attn_backend = backend_name_to_enum(self.backend_name)
         self._use_flashinfer = attn_backend == _Backend.FLASHINFER_VLLM_V1
+        self._use_pallas_v1 = attn_backend == _Backend.PALLAS_VLLM_V1
         logger.debug("Detected attention backend %s", self.backend_name)
 
         self._tp_size: dict[EngineId, int] = {self.engine_id: self.world_size}
@@ -529,6 +616,31 @@ class NixlConnectorWorker:
         # Remote rank -> agent name.
         return {p_remote_rank: remote_agent_name}
 
+    def initialize_host_xfer_buffer(
+            self, kv_caches: dict[str, torch.Tensor]) -> None:
+        """
+        Initialize transfer buffer in CPU mem for accelerators
+        NOT directly supported by NIXL (e.g., tpu)
+        """
+        xfer_buffers: dict[str, torch.Tensor] = {}
+        try:
+            for layer_name, kv_cache in kv_caches.items():
+                kv_shape = kv_cache.shape
+                kv_dtype = kv_cache.dtype
+                xfer_buffers[layer_name] = torch.empty(kv_shape,
+                                                       dtype=kv_dtype,
+                                                       device="cpu")
+        except MemoryError as e:
+            logger.error("NIXLConnectorWorker gets %s.", e)
+            raise
+
+        self.host_xfer_buffers = xfer_buffers
+
+    def set_host_xfer_buffer_ops(self, copy_operation: CopyBlocksOp):
+        """Assign copy (d2h, h2d) operations when host buffer is used."""
+        assert self.use_host_buffer
+        self.copy_blocks = copy_operation
+
     def _background_nixl_handshake(self, req_id: str,
                                    remote_engine_id: EngineId, meta: ReqMeta):
         # Do NIXL handshake in background and add to _ready_requests when done.
@@ -562,47 +674,76 @@ class NixlConnectorWorker:
         _, first_kv_cache = next(iter(kv_caches.items()))
         kv_elem_size = first_kv_cache.element_size()
 
+        if self.use_host_buffer:
+            self.initialize_host_xfer_buffer(kv_caches=kv_caches)
+            assert len(self.host_xfer_buffers) == len(kv_caches), (
+                f"host_buffer: {len(self.host_xfer_buffers)}, "
+                f"kv_caches: {len(kv_caches)}")
+            xfer_buffers = self.host_xfer_buffers
+        else:
+            xfer_buffers = kv_caches
+            assert not self.host_xfer_buffers, (
+                "host_xfer_buffer should not be initialized when "
+                f"kv_buffer_device is {self.kv_buffer_device}")
+
         # TODO(tms): Find a more robust way to detect and handle MLA
         # NOTE (NickLucche) To move blocks efficiently with NIXL, the expected
         # KV memory layout is HND, as opposed to the default NHD. Note that it
         # will only affects the strides. For MLA instead, we make require no
         # such thing and resort to the standard layout.
         use_mla = len(first_kv_cache.shape) == 3
-        assert use_mla == self.use_mla
-
-        # TODO (NickLucche) not compatible with hybrid allocator. Enforce check
-        # once it goes live, as a single kv layout is expected for xfers.
-        if use_mla:
-            # MLA case.
+        if self.device_type == "tpu":
+            assert not use_mla, f"{self.kv_buffer_device} does not support MLA."
+            assert self._use_pallas_v1, f"attn backend: {self.backend_name}"
+            # tpu (v1) kv shape per layer:
+            # (num_blocks, block_size, num_kv_heads * 2, head_size)
             self.num_blocks = first_kv_cache.shape[0]
-            block_rank = 2  # [block_size, latent_dim]
+            block_rank = 3  # [block_size, kv_heads, head_dim]
             block_shape = first_kv_cache.shape[-block_rank:]
-            block_size, kv_latent_dim = block_shape
-            self.slot_size_bytes = kv_elem_size * kv_latent_dim
-        else:
-            # [2 (k and v), num_blocks, ...]
-            if self._use_flashinfer:
-                # FlashInfer swaps 2<->num_blocks dimensions.
+            block_size, n_kv_heads_x_2, head_dim = block_shape
+            self.slot_size_bytes = kv_elem_size * n_kv_heads_x_2 * head_dim
+        elif self.device_type == "cuda":
+            assert use_mla == self.use_mla
+            # TODO (NickLucche) not compatible with hybrid allocator.
+            # Enforce check once it goes live, as a single kv layout
+            # is expected for xfers.
+            if use_mla:
+                # MLA case.
                 self.num_blocks = first_kv_cache.shape[0]
-                block_rank = 4  # [2, block_size, kv_heads, head_dim]
+                block_rank = 2  # [block_size, latent_dim]
+                block_shape = first_kv_cache.shape[-block_rank:]
+                block_size, kv_latent_dim = block_shape
+                self.slot_size_bytes = kv_elem_size * kv_latent_dim
             else:
-                self.num_blocks = first_kv_cache.shape[1]
-                block_rank = 3  # [block_size, kv_heads, head_dim]
-            block_shape = first_kv_cache.shape[-block_rank:]
-            block_size, n_kv_heads, head_dim = block_shape[-3:]
-            # head size in bytes.
-            self.slot_size_bytes = kv_elem_size * n_kv_heads * head_dim
-        assert block_size == self.block_size
+                # [2 (k and v), num_blocks, ...]
+                if self._use_flashinfer:
+                    # FlashInfer swaps 2<->num_blocks dimensions.
+                    self.num_blocks = first_kv_cache.shape[0]
+                    block_rank = 4  # [2, block_size, kv_heads, head_dim]
+                else:
+                    self.num_blocks = first_kv_cache.shape[1]
+                    block_rank = 3  # [block_size, kv_heads, head_dim]
+                block_shape = first_kv_cache.shape[-block_rank:]
+                block_size, n_kv_heads, head_dim = block_shape[-3:]
+                # head size in bytes.
+                self.slot_size_bytes = kv_elem_size * n_kv_heads * head_dim
+            assert block_size == self.block_size
+        else:
+            raise RuntimeError(
+                f"{self.device_type} ({self.backend_name}) is not supported.")
+
         # TODO(tms): self.block_len needs to be per-layer for sliding window,
         # hybrid attn, etc
         # block size in bytes
         self.block_len = kv_elem_size * math.prod(block_shape)
         logger.info(
-            "Registering KV_Caches: use_mla: %s, num_blocks: %s, "
-            "block_shape: %s, per_layer_kv_cache_shape: %s", use_mla,
-            self.num_blocks, block_shape, first_kv_cache.shape)
+            "Registering KV_Caches. use_mla: %s, kv_buffer_device: %s, "
+            "use_host_buffer: %s, num_blocks: %s, block_shape: %s, "
+            "per_layer_kv_cache_shape: %s", use_mla, self.kv_buffer_device,
+            self.use_host_buffer, self.num_blocks, block_shape,
+            first_kv_cache.shape)
         self.dst_num_blocks[self.engine_id] = self.num_blocks
-        self.kv_caches = kv_caches
+        self.device_kv_caches = kv_caches
         kv_caches_base_addr = []
         caches_data = []
 
@@ -614,19 +755,21 @@ class NixlConnectorWorker:
         # (roughly 8KB vs 5KB).
         # Conversely for FlashInfer, K and V are transferred in the same tensor
         # to better exploit the memory layout (ie num_blocks is the first dim).
-        for cache_or_caches in kv_caches.values():
+        for cache_or_caches in xfer_buffers.values():
             # Normalize to always be a list of caches
-            cache_list = [cache_or_caches] if use_mla or self._use_flashinfer \
-                else cache_or_caches
+            cache_list = [cache_or_caches] if use_mla \
+                         or self._use_pallas_v1 or self._use_flashinfer \
+                         else cache_or_caches
             for cache in cache_list:
                 base_addr = cache.data_ptr()
                 region_len = self.num_blocks * self.block_len
-                caches_data.append(
-                    (base_addr, region_len, cache.device.index, ""))
+                # NOTE: use tp_rank for device_id since multi-node TP
+                # is rarely used.
+                caches_data.append((base_addr, region_len, self.tp_rank, ""))
                 kv_caches_base_addr.append(base_addr)
         self.kv_caches_base_addr[self.engine_id] = kv_caches_base_addr
         self.num_regions = len(caches_data)
-        self.num_layers = len(self.kv_caches.keys())
+        self.num_layers = len(xfer_buffers.keys())
 
         # TODO(mgoin): remove this once we have hybrid memory allocator
         # Optimization for models with local attention (Llama 4)
@@ -648,7 +791,8 @@ class NixlConnectorWorker:
                          self.block_window_per_layer)
             assert len(self.block_window_per_layer) == self.num_layers
 
-        descs = self.nixl_wrapper.get_reg_descs(caches_data, "VRAM")
+        descs = self.nixl_wrapper.get_reg_descs(caches_data,
+                                                self.nixl_memory_type)
         logger.debug("Registering descs: %s", caches_data)
         self.nixl_wrapper.register_memory(descs)
         logger.debug("Done registering descs")
@@ -666,11 +810,13 @@ class NixlConnectorWorker:
                 block_offset = block_id * self.block_len
                 addr = base_addr + block_offset
                 # (addr, len, device id)
+                # TODO: does device_id matter to DRAM?
                 blocks_data.append((addr, self.block_len, self.tp_rank))
         logger.debug("Created %s blocks for src engine %s and rank %s",
                      len(blocks_data), self.engine_id, self.tp_rank)
 
-        descs = self.nixl_wrapper.get_xfer_descs(blocks_data, "VRAM")
+        descs = self.nixl_wrapper.get_xfer_descs(blocks_data,
+                                                 self.nixl_memory_type)
         # NIXL_INIT_AGENT to be used for preparations of local descs.
         self.src_xfer_side_handle = self.nixl_wrapper.prep_xfer_dlist(
             "NIXL_INIT_AGENT", descs)
@@ -755,6 +901,8 @@ class NixlConnectorWorker:
         tp_ratio = divide(self._tp_size[self.engine_id],
                           self._tp_size[engine_id])
         assert tp_ratio > 0, "Decode TP cannot be smaller than prefill TP"
+        assert not self._use_pallas_v1 or tp_ratio == 1, \
+               "TPU (pallas_v1) DOES NOT support heterogeneous TP yet."
 
         # Handle tp_size>num_kv_heads: replicate KV cache.
         total_num_kv_heads = self.model_config.get_total_num_kv_heads()
@@ -813,13 +961,43 @@ class NixlConnectorWorker:
             self.tp_rank)
 
         # Register with NIXL.
-        descs = self.nixl_wrapper.get_xfer_descs(blocks_data, "VRAM")
+        descs = self.nixl_wrapper.get_xfer_descs(blocks_data,
+                                                 self.nixl_memory_type)
         self.dst_xfer_side_handles[
             engine_id] = self.nixl_wrapper.prep_xfer_dlist(
                 remote_agent_name, descs)
 
         return remote_agent_name
 
+    def sync_recved_kv_to_device(self, req_id: str, meta: ReqMeta):
+        """copy recved kv from host buffer to device."""
+        assert self.use_host_buffer
+        assert self.copy_blocks is not None
+
+        local_block_ids = meta.local_block_ids
+        self.copy_blocks(self.host_xfer_buffers, self.device_kv_caches,
+                         local_block_ids, local_block_ids, "h2d")
+        if logger.isEnabledFor(logging.DEBUG):
+            logger.debug(
+                "synced recved kv of request[%s] to device kv buffer,"
+                "local_block_ids: %s. ", req_id,
+                ",".join(map(str, meta.local_block_ids)))
+
+    def save_kv_to_host(self, metadata: NixlConnectorMetadata):
+        """copy kv from device to host buffer."""
+        assert self.use_host_buffer
+        assert self.copy_blocks is not None
+
+        for req_id, meta in metadata.reqs_to_save.items():
+            if logger.isEnabledFor(logging.DEBUG):
+                logger.debug(
+                    "save_load_kv for request[%s] to host xfer buffer."
+                    "local_block_ids: %s. ", req_id,
+                    ",".join(map(str, meta.local_block_ids)))
+            # blocking
+            self.copy_blocks(self.device_kv_caches, self.host_xfer_buffers,
+                             meta.local_block_ids, meta.local_block_ids, "d2h")
+
     def get_finished(self) -> tuple[set[str], set[str]]:
         """
         Get requests that are done sending or recving on this specific worker.
@@ -834,6 +1012,12 @@ class NixlConnectorWorker:
                 "and %s requests done recving", self.tp_rank,
                 len(done_sending), len(done_recving))
 
+        if self.use_host_buffer:
+            for req_id in done_recving:
+                meta = self._recving_metadata.pop(req_id)
+                assert meta, f"{req_id} not found in recving_metadata list"
+                self.sync_recved_kv_to_device(req_id, meta)
+
         # Handle timeout to avoid stranding blocks on remote.
         now = time.perf_counter()
         while self._reqs_to_send:
@@ -904,6 +1088,8 @@ class NixlConnectorWorker:
                 "Num local_block_ids: %s. Num remote_block_ids: %s. ", req_id,
                 remote_engine_id, len(meta.local_block_ids),
                 len(meta.remote_block_ids))
+            if self.use_host_buffer:
+                self._recving_metadata[req_id] = meta
             if remote_engine_id not in self._remote_agents:
                 # Initiate handshake with remote engine to exchange metadata.
                 with self._handshake_lock:
diff --git a/vllm/distributed/parallel_state.py b/vllm/distributed/parallel_state.py
index 1f7a14920..46d20e91d 100644
--- a/vllm/distributed/parallel_state.py
+++ b/vllm/distributed/parallel_state.py
@@ -44,6 +44,7 @@ from vllm.distributed.utils import StatelessProcessGroup
 from vllm.logger import init_logger
 from vllm.utils import (direct_register_custom_op, get_distributed_init_method,
                         resolve_obj_by_qualname, supports_custom_op)
+from vllm.envs import CCL_P2P_CPU
 
 
 @dataclass
@@ -690,6 +691,8 @@ class GroupCoordinator:
                     and tensor.numel() % all_gather_size == 0):
                 tensor = tensor.reshape(all_gather_size, -1)[all_gather_rank]
 
+            if envs.CCL_P2P_CPU:
+                tensor = tensor.cpu()
             if tensor.is_cpu:
                 # use metadata_group for CPU tensors
                 torch.distributed.send(tensor,
@@ -734,9 +737,12 @@ class GroupCoordinator:
         tensor_dict: dict[str, Any] = {}
         for key, value in recv_metadata_list:
             if isinstance(value, TensorMetadata):
+                tensor_device = value.device
+                if envs.CCL_P2P_CPU:
+                    tensor_device = 'cpu'
                 tensor = torch.empty(value.size,
                                      dtype=value.dtype,
-                                     device=value.device)
+                                     device=tensor_device)
                 if tensor.numel() == 0:
                     # Skip broadcasting empty tensors.
                     tensor_dict[key] = tensor
@@ -761,6 +767,8 @@ class GroupCoordinator:
                     torch.distributed.recv(tensor,
                                            src=self.ranks[src],
                                            group=group)
+                if envs.CCL_P2P_CPU:
+                    tensor = tensor.to(value.device)
                 if use_all_gather:
                     # do the allgather
                     tensor = all_gather_group.all_gather(  # type: ignore
diff --git a/vllm/engine/arg_utils.py b/vllm/engine/arg_utils.py
index aec75f826..6b15c9f95 100644
--- a/vllm/engine/arg_utils.py
+++ b/vllm/engine/arg_utils.py
@@ -1651,7 +1651,8 @@ class EngineArgs:
 
         if (self.max_num_seqs is None
                 and usage_context in default_max_num_seqs):
-            self.max_num_seqs = default_max_num_seqs[usage_context]
+            self.max_num_seqs = min(default_max_num_seqs[usage_context],
+                                    self.max_num_batched_tokens or sys.maxsize)
 
             logger.debug("Setting max_num_seqs to %d for %s usage context.",
                          self.max_num_seqs, use_context_value)
diff --git a/vllm/entrypoints/llm.py b/vllm/entrypoints/llm.py
index 2f766a2da..680733966 100644
--- a/vllm/entrypoints/llm.py
+++ b/vllm/entrypoints/llm.py
@@ -239,7 +239,7 @@ class LLM:
                 compilation_config_instance = compilation_config
         else:
             compilation_config_instance = CompilationConfig()
-
+        kwargs.pop("device", None)
         engine_args = EngineArgs(
             model=model,
             task=task,
@@ -1230,6 +1230,44 @@ class LLM:
 
         return [ClassificationRequestOutput.from_base(item) for item in items]
 
+    def reward(
+        self,
+        prompts: Union[PromptType, Sequence[PromptType]],
+        /,
+        *,
+        truncate_prompt_tokens: Optional[int] = None,
+        use_tqdm: Union[bool, Callable[..., tqdm]] = True,
+        pooling_params: Optional[Union[PoolingParams,
+                                       Sequence[PoolingParams]]] = None,
+        lora_request: Optional[Union[list[LoRARequest], LoRARequest]] = None,
+    ) -> list[PoolingRequestOutput]:
+        """
+        Generate rewards for each prompt.
+        Args:
+            prompts: The prompts to the LLM. You may pass a sequence of prompts
+                for batch inference. See [PromptType][vllm.inputs.PromptType]
+                for more details about the format of each prompts.
+            use_tqdm: If `True`, shows a tqdm progress bar.
+                If a callable (e.g., `functools.partial(tqdm, leave=False)`),
+                it is used to create the progress bar.
+                If `False`, no progress bar is created.
+            lora_request: LoRA request to use for generation, if any.
+            pooling_params: The pooling parameters for pooling. If None, we
+                use the default pooling parameters.
+        Returns:
+            A list of `PoolingRequestOutput` objects containing the
+            pooled hidden states in the same order as the input prompts.
+        """
+
+        return self.encode(
+            prompts,
+            use_tqdm=use_tqdm,
+            lora_request=lora_request,
+            pooling_params=pooling_params,
+            truncate_prompt_tokens=truncate_prompt_tokens,
+            pooling_task="encode",
+        )
+
     def _embedding_score(
         self,
         tokenizer: AnyTokenizer,
diff --git a/vllm/entrypoints/openai/tool_parsers/__init__.py b/vllm/entrypoints/openai/tool_parsers/__init__.py
index 88c8aa929..10b9c63e0 100644
--- a/vllm/entrypoints/openai/tool_parsers/__init__.py
+++ b/vllm/entrypoints/openai/tool_parsers/__init__.py
@@ -18,6 +18,8 @@ from .mistral_tool_parser import MistralToolParser
 from .phi4mini_tool_parser import Phi4MiniJsonToolParser
 from .pythonic_tool_parser import PythonicToolParser
 from .qwen3coder_tool_parser import Qwen3CoderToolParser
+from .seed_oss_tool_parser import SeedOssToolParser
+# from .step3_tool_parser import Step3ToolParser
 from .xlam_tool_parser import xLAMToolParser
 
 __all__ = [
@@ -40,4 +42,6 @@ __all__ = [
     "HunyuanA13BToolParser",
     "Glm4MoeModelToolParser",
     "Qwen3CoderToolParser",
+    "SeedOssToolParser",
+    # "Step3ToolParser",
 ]
diff --git a/vllm/entrypoints/openai/tool_parsers/seed_oss_tool_parser.py b/vllm/entrypoints/openai/tool_parsers/seed_oss_tool_parser.py
new file mode 100644
index 000000000..69cf2e68f
--- /dev/null
+++ b/vllm/entrypoints/openai/tool_parsers/seed_oss_tool_parser.py
@@ -0,0 +1,676 @@
+# SPDX-License-Identifier: Apache-2.0
+# SPDX-FileCopyrightText: Copyright contributors to the vLLM project
+# Adapted from qwen3coder xml parser, All rights reserved.
+# ruff: noqa: E501
+
+import ast
+import json
+import uuid
+from collections.abc import Sequence
+from typing import Any, Optional, Union
+
+import regex as re
+
+from vllm.entrypoints.openai.protocol import (ChatCompletionRequest,
+                                              ChatCompletionToolsParam,
+                                              DeltaFunctionCall, DeltaMessage,
+                                              DeltaToolCall,
+                                              ExtractedToolCallInformation,
+                                              FunctionCall, ToolCall)
+from vllm.entrypoints.openai.tool_parsers.abstract_tool_parser import (
+    ToolParser, ToolParserManager)
+from vllm.logger import init_logger
+from vllm.transformers_utils.tokenizer import AnyTokenizer
+
+logger = init_logger(__name__)
+
+
+@ToolParserManager.register_module("seed_oss")
+class SeedOssToolParser(ToolParser):
+    TOOL_CALL_START = "<seed:tool_call>"
+    TOOL_CALL_END = "</seed:tool_call>"
+
+    def __init__(self, tokenizer: AnyTokenizer):
+        super().__init__(tokenizer)
+
+        # --- streaming state ---
+        self._reset_streaming_state()
+        self.prev_tool_call_arr: list[dict] = []
+
+        self.tool_call_start_token: str = self.TOOL_CALL_START
+        self.tool_call_end_token: str = self.TOOL_CALL_END
+        # Sentinel tokens for streaming mode
+        self.tool_call_prefix: str = "<function="
+        self.function_end_token: str = "</function>"
+        self.parameter_prefix: str = "<parameter="
+        self.parameter_end_token: str = "</parameter>"
+        self.think_start_token: str = "<seed:think>"
+        self.think_end_token: str = "</seed:think>"
+        self.is_tool_call_started: bool = False
+        self.is_thinking_end: bool = False
+        self.failed_count: int = 0
+        self._reset_streaming_state()
+
+        self.tool_call_start_token_id = self.vocab.get(
+            self.tool_call_start_token)
+        self.tool_call_end_token_id = self.vocab.get(self.tool_call_end_token)
+        self.think_end_token_id = self.vocab.get(self.think_end_token)
+
+        if (self.tool_call_start_token_id is None
+                or self.tool_call_end_token_id is None):
+            raise RuntimeError(
+                "Seed_Oss XML parser: tokenizer did not include "
+                "<seed:tool_call> or its closing tag.")
+
+        tool_start_re = re.escape(self.tool_call_start_token)
+        tool_end_re = re.escape(self.tool_call_end_token)
+
+        self.tool_call_complete_regex = re.compile(
+            rf"{tool_start_re}(.*?){tool_end_re}", re.DOTALL)
+        self.tool_call_regex = re.compile(
+            rf"{tool_start_re}(.*?){tool_end_re}|{tool_start_re}(.*?)$",
+            re.DOTALL)
+
+        self.tool_call_function_regex = re.compile(
+            r"<function=(.*?)</function>|<function=(.*)$", re.DOTALL)
+        self.tool_call_parameter_regex = re.compile(
+            r"<parameter=(.*?)</parameter>|<parameter=(.*?)$", re.DOTALL)
+
+        logger.info("vLLM Seed-Oss XML tool parser loaded (%s).",
+                    self.__class__.__name__)
+
+    def _generate_tool_call_id(self) -> str:
+        """Generate a unique tool call ID."""
+        return f"call_{uuid.uuid4().hex[:24]}"
+
+    def _reset_streaming_state(self):
+        """Reset all streaming state."""
+        self.current_tool_index = 0
+        self.is_tool_call_started = False
+        self.header_sent = False
+        self.current_tool_id = -1
+        self.current_function_name = None
+        self.current_param_name = None
+        self.current_param_value = ""
+        self.param_count = 0
+        self.in_param = False
+        self.in_function = False
+        self.accumulated_text = ""
+        self.json_started = False
+        self.json_closed = False
+
+    def _parse_xml_function_call(
+            self, function_call_str: str,
+            tools: Optional[list[ChatCompletionToolsParam]]
+    ) -> Optional[ToolCall]:
+
+        def get_arguments_config(func_name: str) -> dict:
+            if tools is None:
+                return {}
+            for config in tools:
+                if not hasattr(config, "type") or not (
+                        hasattr(config, "function")
+                        and hasattr(config.function, "name")):
+                    continue
+                if (config.type == "function"
+                        and config.function.name == func_name):
+                    if not hasattr(config.function, "parameters"):
+                        return {}
+                    params = config.function.parameters
+                    if isinstance(params, dict) and "properties" in params:
+                        return params["properties"]
+                    elif isinstance(params, dict):
+                        return params
+                    else:
+                        return {}
+            logger.warning("Tool '%s' is not defined in the tools list.",
+                           func_name)
+            return {}
+
+        def convert_param_value(param_value: str, param_name: str,
+                                param_config: dict, func_name: str) -> Any:
+            # Handle null value for any type
+            if param_value.lower() == "null":
+                return None
+
+            if param_name not in param_config:
+                if param_config != {}:
+                    logger.warning(
+                        "Parsed parameter '%s' is not defined in "
+                        "the tool parameters for tool '%s', "
+                        "directly returning the string value.", param_name,
+                        func_name)
+                return param_value
+
+            if (isinstance(param_config[param_name], dict)
+                    and "type" in param_config[param_name]):
+                param_type = str(
+                    param_config[param_name]["type"]).strip().lower()
+            else:
+                param_type = "string"
+            if param_type in [
+                    "string", "str", "text", "varchar", "char", "enum"
+            ]:
+                return param_value
+            elif (param_type.startswith("int") or param_type.startswith("uint")
+                  or param_type.startswith("long")
+                  or param_type.startswith("short")
+                  or param_type.startswith("unsigned")):
+                try:
+                    param_value = int(param_value)  # type: ignore
+                except (ValueError, TypeError):
+                    logger.warning(
+                        "Parsed value '%s' of parameter '%s' is not an integer in tool "
+                        "'%s', degenerating to string.", param_value,
+                        param_name, func_name)
+                return param_value
+            elif param_type.startswith("num") or param_type.startswith(
+                    "float"):
+                try:
+                    float_param_value = float(param_value)
+                    param_value = float_param_value if float_param_value - int(
+                        float_param_value) != 0 else int(
+                            float_param_value)  # type: ignore
+                except (ValueError, TypeError):
+                    logger.warning(
+                        "Parsed value '%s' of parameter '%s' is not a float in tool "
+                        "'%s', degenerating to string.", param_value,
+                        param_name, func_name)
+                return param_value
+            elif param_type in ["boolean", "bool", "binary"]:
+                param_value = param_value.lower()
+                if param_value not in ["true", "false"]:
+                    logger.warning(
+                        "Parsed value '%s' of parameter '%s' is not a boolean "
+                        "(`true` of `false`) in tool '%s', degenerating to false.",
+                        param_value, param_name, func_name)
+                return param_value == "true"
+            else:
+                if param_type == "object" or param_type.startswith("dict"):
+                    try:
+                        param_value = json.loads(param_value)
+                        return param_value
+                    except (ValueError, TypeError, json.JSONDecodeError):
+                        logger.warning(
+                            "Parsed value '%s' of parameter '%s' is not a valid JSON "
+                            "object in tool '%s', will try other methods to parse it.",
+                            param_value, param_name, func_name)
+                try:
+                    param_value = ast.literal_eval(param_value)
+                except (ValueError, SyntaxError):
+                    logger.warning(
+                        "Parsed value '%s' of parameter '%s' cannot be converted via "
+                        "Python `ast.literal_eval()` in tool '%s', degenerating to string.",
+                        param_value, param_name, func_name)
+                return param_value
+
+        # Extract function name
+        end_index = function_call_str.index(">")
+        function_name = function_call_str[:end_index]
+        param_config = get_arguments_config(function_name)
+        parameters = function_call_str[end_index + 1:]
+        param_dict = {}
+        for match in self.tool_call_parameter_regex.findall(parameters):
+            match_text = match[0] if match[0] else match[1]
+            idx = match_text.index(">")
+            param_name = match_text[:idx]
+            param_value = str(match_text[idx + 1:])
+            # Remove prefix and trailing \n
+            if param_value.startswith("\n"):
+                param_value = param_value[1:]
+            if param_value.endswith("\n"):
+                param_value = param_value[:-1]
+
+            param_dict[param_name] = convert_param_value(
+                param_value, param_name, param_config, function_name)
+        return ToolCall(
+            type="function",
+            function=FunctionCall(name=function_name,
+                                  arguments=json.dumps(param_dict,
+                                                       ensure_ascii=False)),
+        )
+
+    def _get_function_calls(self, model_output: str) -> list[str]:
+        # Find all tool calls
+        matched_ranges = self.tool_call_regex.findall(model_output)
+        raw_tool_calls = [
+            match[0] if match[0] else match[1] for match in matched_ranges
+        ]
+
+        # Back-off strategy if no tool_call tags found
+        if len(raw_tool_calls) == 0:
+            raw_tool_calls = [model_output]
+
+        raw_function_calls = []
+        for tool_call in raw_tool_calls:
+            raw_function_calls.extend(
+                self.tool_call_function_regex.findall(tool_call))
+
+        function_calls = [
+            match[0] if match[0] else match[1] for match in raw_function_calls
+        ]
+        return function_calls
+
+    def extract_tool_calls(
+        self,
+        model_output: str,
+        request: ChatCompletionRequest,
+    ) -> ExtractedToolCallInformation:
+        # Quick check to avoid unnecessary processing
+        if self.tool_call_prefix not in model_output:
+            return ExtractedToolCallInformation(tools_called=False,
+                                                tool_calls=[],
+                                                content=model_output)
+
+        # Check if both think start and end tokens are present
+        if (self.think_start_token in model_output
+                and self.think_end_token in model_output):
+            # Find the position of think end token
+            think_end_index = model_output.find(self.think_end_token) + len(
+                self.think_end_token)
+            # Extract content after think end token
+            result_content = model_output[think_end_index:]
+            thinking_content = model_output[:think_end_index]
+
+        try:
+            function_calls = self._get_function_calls(result_content)
+            if len(function_calls) == 0:
+                return ExtractedToolCallInformation(tools_called=False,
+                                                    tool_calls=[],
+                                                    content=model_output)
+
+            tool_calls = [
+                self._parse_xml_function_call(function_call_str, request.tools)
+                for function_call_str in function_calls
+            ]
+
+            # Populate prev_tool_call_arr for serving layer to set finish_reason
+            self.prev_tool_call_arr.clear()  # Clear previous calls
+            for tool_call in tool_calls:
+                if tool_call:
+                    self.prev_tool_call_arr.append({
+                        "name":
+                        tool_call.function.name,
+                        "arguments":
+                        tool_call.function.arguments,
+                    })
+
+            # Extract content before tool calls
+            tool_call_start_index = result_content.find(
+                self.tool_call_start_token)
+            tool_call_start_index = (
+                tool_call_start_index if tool_call_start_index >= 0 else
+                result_content.find(self.tool_call_prefix))
+            content = thinking_content + result_content[:tool_call_start_index]
+
+            return ExtractedToolCallInformation(
+                tools_called=(len(tool_calls) > 0),
+                tool_calls=tool_calls,
+                content=content if content else None,
+            )
+
+        except Exception:
+            logger.exception("Error in extracting tool call from response.")
+            return ExtractedToolCallInformation(tools_called=False,
+                                                tool_calls=[],
+                                                content=model_output)
+
+    def extract_tool_calls_streaming(
+        self,
+        previous_text: str,
+        current_text: str,
+        delta_text: str,
+        previous_token_ids: Sequence[int],
+        current_token_ids: Sequence[int],
+        delta_token_ids: Sequence[int],
+        request: ChatCompletionRequest,
+    ) -> Union[DeltaMessage, None]:
+        # If no delta text, return None unless
+        # it's an EOS token after tool calls
+        if not delta_text:
+            # Check if this is an EOS token after all tool calls are complete
+            # We check for tool calls in the text even if is_tool_call_started
+            # is False because it might have been reset after processing all tools
+            if (delta_token_ids
+                    and self.tool_call_end_token_id not in delta_token_ids):
+                # Count complete tool calls
+                complete_calls = len(
+                    self.tool_call_complete_regex.findall(current_text))
+
+                # If we have completed tool calls and populated prev_tool_call_arr
+                if complete_calls > 0 and len(self.prev_tool_call_arr) > 0:
+                    # Check if all tool calls are closed
+                    open_calls = current_text.count(
+                        self.tool_call_start_token) - current_text.count(
+                            self.tool_call_end_token)
+                    if open_calls == 0:
+                        # Return empty delta message to allow finish_reason processing
+                        return DeltaMessage(content="")
+                elif not self.is_tool_call_started and current_text:
+                    # This is a regular content response that's now complete
+                    return DeltaMessage(content="")
+            return None
+
+        # Check if this is the first call (reset state if needed)
+        if not previous_text:
+            self._reset_streaming_state()
+
+        # Update accumulated text
+        self.accumulated_text = current_text
+
+        # Check if we need to advance to next tool
+        if self.json_closed and not self.in_function:
+            # Check if this tool call has ended
+            tool_ends = current_text.count(self.tool_call_end_token)
+            if tool_ends > self.current_tool_index:
+                # This tool has ended, advance to next
+                self.current_tool_index += 1
+                self.header_sent = False
+                self.param_count = 0
+                self.json_started = False
+                self.json_closed = False
+
+                # Check if there are more tool calls
+                if self.current_tool_index >= current_text.count(
+                        self.tool_call_start_token):
+                    # No more tool calls
+                    self.is_tool_call_started = False
+                # Continue processing next tool
+                return None
+
+        # Check if end thinking
+        if (not self.is_thinking_end
+                and (self.think_end_token_id in delta_token_ids
+                     or self.think_end_token in delta_text)):
+            self.is_thinking_end = True
+
+        # If thinking hasn't ended yet, don't process any tool calls
+        if not self.is_thinking_end:
+            return DeltaMessage(content=delta_text)
+
+        # Handle normal content before tool calls
+        if not self.is_tool_call_started:
+            # Check if tool call is starting
+            if (self.tool_call_start_token_id in delta_token_ids
+                    or self.tool_call_start_token in delta_text):
+                self.is_tool_call_started = True
+                # Return any content before the tool call
+                if self.tool_call_start_token in delta_text:
+                    content_before = delta_text[:delta_text.index(
+                        self.tool_call_start_token)]
+                    if content_before:
+                        return DeltaMessage(content=content_before)
+                return None
+            else:
+                # Check if we're between tool calls - skip whitespace
+                if (current_text.rstrip().endswith(self.tool_call_end_token)
+                        and delta_text.strip() == ""):
+                    # We just ended a tool call, skip whitespace
+                    return None
+                # Normal content, no tool call
+                return DeltaMessage(content=delta_text)
+
+        # Check if we're between tool calls (waiting for next one)
+        # Count tool calls we've seen vs processed
+        tool_starts_count = current_text.count(self.tool_call_start_token)
+        if self.current_tool_index >= tool_starts_count:
+            # We're past all tool calls, shouldn't be here
+            return None
+
+        # We're in a tool call, find the current tool call portion
+        # Need to find the correct tool call based on current_tool_index
+        # Only process tool calls after think_end_token
+        think_end_index = current_text.find(self.think_end_token) + len(
+            self.think_end_token
+        ) if self.think_end_token in current_text else 0
+        tool_starts: list[int] = []
+        idx = think_end_index
+        while True:
+            idx = current_text.find(self.tool_call_start_token, idx)
+            if idx == -1:
+                break
+            tool_starts.append(idx)
+            idx += len(self.tool_call_start_token)
+
+        if self.current_tool_index >= len(tool_starts):
+            # No more tool calls to process yet
+            return None
+
+        tool_start_idx = tool_starts[self.current_tool_index]
+        # Find where this tool call ends (or current position if not ended yet)
+        tool_end_idx = current_text.find(self.tool_call_end_token,
+                                         tool_start_idx)
+        if tool_end_idx == -1:
+            tool_text = current_text[tool_start_idx:]
+        else:
+            tool_text = current_text[tool_start_idx:tool_end_idx +
+                                     len(self.tool_call_end_token)]
+
+        # Looking for function header
+        if not self.header_sent:
+            if self.tool_call_prefix in tool_text:
+                func_start = tool_text.find(self.tool_call_prefix) + len(
+                    self.tool_call_prefix)
+                func_end = tool_text.find(">", func_start)
+
+                if func_end != -1:
+                    # Found complete function name
+                    self.current_function_name = tool_text[func_start:func_end]
+                    self.current_tool_id = self._generate_tool_call_id(
+                    )  # type: ignore
+                    self.header_sent = True
+                    self.in_function = True
+
+                    # IMPORTANT: Add to prev_tool_call_arr immediately when we detect a tool call
+                    # This ensures finish_reason="tool_calls" even if parsing isn't complete
+                    already_added = any(
+                        tool.get("name") == self.current_function_name
+                        for tool in self.prev_tool_call_arr)
+                    if not already_added:
+                        self.prev_tool_call_arr.append({
+                            "name": self.current_function_name,
+                            "arguments":
+                            "{}",  # Placeholder, will be updated later
+                        })
+
+                    # Send header with function info
+                    return DeltaMessage(tool_calls=[
+                        DeltaToolCall(
+                            index=self.current_tool_index,
+                            id=self.current_tool_id,
+                            function=DeltaFunctionCall(
+                                name=self.current_function_name, arguments=""),
+                            type="function",
+                        )
+                    ])
+            return None
+
+        # We've sent header, now handle function body
+        if self.in_function:
+            # Send opening brace if not sent yet
+            if (not self.json_started
+                    and self.parameter_prefix not in delta_text):
+                self.json_started = True
+                return DeltaMessage(tool_calls=[
+                    DeltaToolCall(
+                        index=self.current_tool_index,
+                        function=DeltaFunctionCall(arguments="{"),
+                    )
+                ])
+
+            # Make sure json_started is set if we're processing parameters
+            if not self.json_started:
+                self.json_started = True
+
+            # Check for function end in accumulated text
+            if not self.json_closed and self.function_end_token in tool_text:
+                # Close JSON
+                self.json_closed = True
+
+                # Extract the complete tool call to update prev_tool_call_arr with final arguments
+                # Find the function content
+                func_start = tool_text.find(self.tool_call_prefix) + len(
+                    self.tool_call_prefix)
+                func_content_end = tool_text.find(self.function_end_token,
+                                                  func_start)
+                if func_content_end != -1:
+                    func_content = tool_text[func_start:func_content_end]
+                    # Parse to get the complete arguments
+                    try:
+                        parsed_tool = self._parse_xml_function_call(
+                            func_content, request.tools if request else None)
+                        if parsed_tool:
+                            # Update existing entry in prev_tool_call_arr with complete arguments
+                            for i, tool in enumerate(self.prev_tool_call_arr):
+                                if tool.get(
+                                        "name") == parsed_tool.function.name:
+                                    self.prev_tool_call_arr[i]["arguments"] = (
+                                        parsed_tool.function.arguments)
+                                    break
+                    except Exception:
+                        logger.warning(
+                            "Failed to parse tool arguments during streaming.",
+                            exc_info=True)
+
+                result = DeltaMessage(tool_calls=[
+                    DeltaToolCall(
+                        index=self.current_tool_index,
+                        function=DeltaFunctionCall(arguments="}"),
+                    )
+                ])
+
+                # Reset state for next tool
+                self.in_function = False
+                self.json_closed = True
+
+                return result
+
+            # Look for parameters
+            # Count how many complete parameters we have processed
+            complete_params = tool_text.count(self.parameter_end_token)
+
+            # Check if we should start a new parameter
+            if not self.in_param and self.param_count < complete_params:
+                # Find the unprocessed parameter
+                # Count parameter starts
+                param_starts = []
+                idx = 0
+                while True:
+                    idx = tool_text.find(self.parameter_prefix, idx)
+                    if idx == -1:
+                        break
+                    param_starts.append(idx)
+                    idx += len(self.parameter_prefix)
+
+                if len(param_starts) > self.param_count:
+                    # Process the next parameter
+                    param_idx = param_starts[self.param_count]
+                    param_start = param_idx + len(self.parameter_prefix)
+                    remaining = tool_text[param_start:]
+
+                    if ">" in remaining:
+                        # We have the complete parameter name
+                        name_end = remaining.find(">")
+                        self.current_param_name = remaining[:name_end]
+
+                        # Find the parameter value
+                        value_start = param_start + name_end + 1
+                        value_text = tool_text[value_start:]
+                        if value_text.startswith("\n"):
+                            value_text = value_text[1:]
+
+                        # Find where this parameter ends
+                        param_end_idx = value_text.find(
+                            self.parameter_end_token)
+                        if param_end_idx != -1:
+                            # Complete parameter found
+                            param_value = value_text[:param_end_idx]
+                            if param_value.endswith("\n"):
+                                param_value = param_value[:-1]
+
+                            # Build complete JSON fragment for this parameter
+                            if self.param_count == 0:
+                                json_fragment = (
+                                    '"' + self.current_param_name + '": "' +
+                                    json.dumps(param_value)[1:-1] + '"')
+                            else:
+                                json_fragment = (
+                                    ', "' + self.current_param_name + '": "' +
+                                    json.dumps(param_value)[1:-1] + '"')
+
+                            self.param_count += 1
+
+                            return DeltaMessage(tool_calls=[
+                                DeltaToolCall(
+                                    index=self.current_tool_index,
+                                    function=DeltaFunctionCall(
+                                        arguments=json_fragment),
+                                )
+                            ])
+
+            # Continue parameter value
+            if self.in_param:
+                if self.parameter_end_token in delta_text:
+                    # End of parameter
+                    end_idx = delta_text.find(self.parameter_end_token)
+                    value_chunk = delta_text[:end_idx]
+
+                    # Skip past > if at start
+                    if not self.current_param_value and ">" in value_chunk:
+                        gt_idx = value_chunk.find(">")
+                        value_chunk = value_chunk[gt_idx + 1:]
+
+                    if not self.current_param_value and value_chunk.startswith(
+                            "\n"):
+                        value_chunk = value_chunk[1:]
+
+                    # Calculate incremental JSON
+                    full_value = self.current_param_value + value_chunk
+                    prev_escaped = (json.dumps(self.current_param_value)[1:-1]
+                                    if self.current_param_value else "")
+                    full_escaped = json.dumps(full_value)[1:-1]
+                    delta_escaped = full_escaped[len(prev_escaped):]
+
+                    self.in_param = False
+                    self.current_param_value = ""
+
+                    return DeltaMessage(tool_calls=[
+                        DeltaToolCall(
+                            index=self.current_tool_index,
+                            function=DeltaFunctionCall(
+                                arguments=delta_escaped + '"'),
+                        )
+                    ])
+                else:
+                    # Continue accumulating value
+                    value_chunk = delta_text
+
+                    # Handle first chunk after param name
+                    if not self.current_param_value and ">" in value_chunk:
+                        gt_idx = value_chunk.find(">")
+                        value_chunk = value_chunk[gt_idx + 1:]
+
+                    if not self.current_param_value and value_chunk.startswith(
+                            "\n"):
+                        value_chunk = value_chunk[1:]
+
+                    if value_chunk:
+                        # Stream the escaped delta
+                        prev_escaped = (json.dumps(
+                            self.current_param_value)[1:-1]
+                                        if self.current_param_value else "")
+                        self.current_param_value += value_chunk
+                        full_escaped = json.dumps(
+                            self.current_param_value)[1:-1]
+                        delta_escaped = full_escaped[len(prev_escaped):]
+
+                        if delta_escaped:
+                            return DeltaMessage(tool_calls=[
+                                DeltaToolCall(
+                                    index=self.current_tool_index,
+                                    function=DeltaFunctionCall(
+                                        arguments=delta_escaped),
+                                )
+                            ])
+
+        return None
diff --git a/vllm/envs.py b/vllm/envs.py
index 5c414e82d..56a8d7253 100755
--- a/vllm/envs.py
+++ b/vllm/envs.py
@@ -143,6 +143,10 @@ if TYPE_CHECKING:
     VLLM_USE_CUDNN_PREFILL: bool = False
     VLLM_ENABLE_CUDAGRAPH_GC: bool = False
     VLLM_LOOPBACK_IP: str = ""
+    VLLM_XPU_FP8_DTYPE: str = "e5m2"
+    VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT: bool = False
+    CCL_P2P_CPU: bool = False
+    VLLM_QUANTIZE_Q40_LIB: str = "/opt/lib/vllm_int4_for_multi_arc.so"
 
 
 def get_default_cache_root():
@@ -985,6 +989,22 @@ environment_variables: dict[str, Callable[[], Any]] = {
     # Used to force set up loopback IP
     "VLLM_LOOPBACK_IP":
     lambda: os.getenv("VLLM_LOOPBACK_IP", ""),
+
+    # fp8 dtype for XPU platform
+    "VLLM_XPU_FP8_DTYPE":
+    lambda: os.environ.get("VLLM_XPU_FP8_DTYPE", "e5m2"),
+
+    # Offload model weights to cpu before online fp8 quantization
+    "VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT":
+    lambda: os.environ.get("VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT", "0") == "1",
+
+    # Path for finding libs for vLLM sym_int4 quantization support
+    "VLLM_QUANTIZE_Q40_LIB":
+    lambda: os.environ.get("VLLM_QUANTIZE_Q40_LIB", "/opt/lib/vllm_int4_for_multi_arc.so"),
+
+    # Do send/recv on CPU backend
+    "CCL_P2P_CPU":
+    lambda: os.environ.get("CCL_P2P_CPU", "1") == "1"
 }
 
 # --8<-- [end:env-vars-definition]
diff --git a/vllm/executor/ray_distributed_executor.py b/vllm/executor/ray_distributed_executor.py
index e9ad62aeb..acfdcfa96 100644
--- a/vllm/executor/ray_distributed_executor.py
+++ b/vllm/executor/ray_distributed_executor.py
@@ -84,15 +84,6 @@ class RayDistributedExecutor(DistributedExecutorBase):
         # be executed in a remote Ray worker. Currently this requires
         # USE_RAY_COMPILED_DAG=True.
         self.use_ray_spmd_worker = envs.VLLM_USE_RAY_SPMD_WORKER
-        if self.use_ray_compiled_dag:
-            assert self.use_ray_spmd_worker, (
-                "VLLM_USE_RAY_COMPILED_DAG=1 requires "
-                "VLLM_USE_RAY_SPMD_WORKER=1")
-        if self.use_ray_spmd_worker:
-            # TODO: Support SPMD worker for non-DAG Ray executor.
-            assert self.use_ray_compiled_dag, (
-                "VLLM_USE_RAY_SPMD_WORKER=1 requires "
-                "VLLM_USE_RAY_COMPILED_DAG=1")
 
         assert self.uses_ray
         initialize_ray_cluster(self.parallel_config)
@@ -112,7 +103,7 @@ class RayDistributedExecutor(DistributedExecutorBase):
         self.use_v1 = envs.VLLM_USE_V1
 
         self.pp_locks: Optional[List[asyncio.Lock]] = None
-        if not self.use_ray_compiled_dag:
+        if not self.use_ray_spmd_worker:
             self.driver_exec_method = make_async(
                 self.driver_worker.execute_method)
 
diff --git a/vllm/lora/punica_wrapper/punica_gpu.py b/vllm/lora/punica_wrapper/punica_gpu.py
index 2db0e9fee..b25fcdb87 100644
--- a/vllm/lora/punica_wrapper/punica_gpu.py
+++ b/vllm/lora/punica_wrapper/punica_gpu.py
@@ -7,17 +7,36 @@ Punica: Multi-Tenant LoRA Serving.
 https://arxiv.org/abs/2310.18547
 """
 
-from typing import Optional, Union, final
+from typing import Optional, Union, Tuple, final
+import os
 
 import torch
 
 import vllm.envs as envs
 from vllm.lora.layers import LoRAMapping
+from vllm.platforms import current_platform
 from vllm.triton_utils import HAS_TRITON
 
-if HAS_TRITON:
+is_xpu = current_platform.is_xpu()
+xpu_use_triton_kernels = os.getenv("XPU_USE_TRITON_KERNELS", "0") == "1"
+
+if is_xpu and not xpu_use_triton_kernels:
+    from vllm._ipex_ops import bgmv_expand_slice, bgmv_shrink, ipex_ops
+    try:
+        lora_expand = ipex_ops.lora_expand
+        lora_shrink = ipex_ops.lora_shrink
+        XPU_KERNEL_V = 1
+    except AttributeError:
+        bgmv_expand = ipex_ops.bgmv_expand
+        sgmv_expand = ipex_ops.sgmv_expand
+        sgmv_expand_slice = ipex_ops.sgmv_expand_slice
+        sgmv_shrink = ipex_ops.sgmv_shrink
+        XPU_KERNEL_V = 0
+elif HAS_TRITON:
     from vllm.lora.ops.triton_ops import (LoRAKernelMeta, lora_expand,
                                           lora_shrink)
+    if is_xpu:
+        XPU_KERNEL_V = 1
 
 from .punica_base import PunicaWrapperBase
 
@@ -37,9 +56,9 @@ class PunicaWrapperGPU(PunicaWrapperBase):
 
         self.max_loras = kwargs['max_loras']
 
-        self.token_mapping_meta = LoRAKernelMeta.make(self.max_loras,
-                                                      max_num_batched_tokens,
-                                                      device=device)
+        if not (is_xpu and XPU_KERNEL_V == 0):
+            self.token_mapping_meta = LoRAKernelMeta.make(
+                self.max_loras, max_num_batched_tokens, device=device)
 
         # When cudagraph capture size is greater than max_num_seqs (max_batches,
         # here), V0 captures the graph as if max_num_seqs is set to
@@ -47,21 +66,91 @@ class PunicaWrapperGPU(PunicaWrapperBase):
         # V1 doesn't have this problem and always respects max_num_seqs.
         max_num_prompts = (max_batches
                            if envs.VLLM_USE_V1 else max_num_batched_tokens)
-        self.prompt_mapping_meta = LoRAKernelMeta.make(self.max_loras,
-                                                       max_num_prompts,
-                                                       device=device)
+        if not (is_xpu and XPU_KERNEL_V == 0):
+            self.prompt_mapping_meta = LoRAKernelMeta.make(self.max_loras,
+                                                           max_num_prompts,
+                                                           device=device)
 
     def update_metadata(self, mapping: LoRAMapping,
                         lora_index_to_id: list[Optional[int]], max_loras: int,
                         vocab_size: int, extra_vocab_size: int, **kwargs):
 
         self.is_prefill = mapping.is_prefill
-        self._update_base_metadata(mapping, lora_index_to_id, max_loras,
-                                   vocab_size, extra_vocab_size)
+        if is_xpu and XPU_KERNEL_V == 0:
+            PunicaWrapperBase.update_metadata(self, mapping, lora_index_to_id,
+                                              max_loras, vocab_size,
+                                              extra_vocab_size,
+                                              **kwargs)
+        else:
+            self._update_base_metadata(mapping, lora_index_to_id, max_loras,
+                                       vocab_size, extra_vocab_size)
+            # Prepare cuda kernel metadata tensors
+            self.token_mapping_meta.prepare_tensors(self.token_lora_indices)
+            self.prompt_mapping_meta.prepare_tensors(self.sampler_indices)
+
+    def _apply_shrink_prefill(
+        self,
+        y: torch.Tensor,
+        x: torch.Tensor,
+        w_t_all: Tuple[torch.Tensor, ...],
+        scale: float,
+    ):
+        #No LoRA request, so return directly
+        if self.no_lora:
+            return
+        sgmv_shrink(
+            x,
+            w_t_all,
+            y,
+            *self.prefill_metadata,
+            scale,
+        )
+
+    def _get_token_lora_indices(self, x: torch.Tensor) -> torch.IntTensor:
+        return torch.narrow(self._token_lora_indices, 0, 0, x.size(0))
+
+    def _apply_shrink_decode(
+        self,
+        y: torch.Tensor,
+        x: torch.Tensor,
+        w_t_all: torch.Tensor,
+        scale: float,
+    ):
+        bgmv_shrink(x, w_t_all, y, self._get_token_lora_indices(x), scale)
+
+    def _apply_expand_prefill(
+        self,
+        y: torch.Tensor,
+        x: torch.Tensor,
+        w_t_all: Tuple[torch.Tensor, ...],
+        offset_start: int,
+        add_inputs: bool,
+    ):
+        #No LoRA request, so return directly
+        if self.no_lora:
+            return
+
+        sgmv_expand(
+            x,
+            w_t_all,
+            y,
+            *self.prefill_metadata,
+            offset_start=offset_start,
+            add_inputs=add_inputs,
+        )
 
-        # Prepare cuda kernel metadata tensors
-        self.token_mapping_meta.prepare_tensors(self.token_lora_indices)
-        self.prompt_mapping_meta.prepare_tensors(self.sampler_indices)
+    def _apply_expand_decode(
+        self,
+        y: torch.Tensor,
+        x: torch.Tensor,
+        w_t_all: torch.Tensor,
+        y_offset: Optional[int],
+        y_slice_size: Optional[int],
+        add_inputs: bool,
+    ):
+        token_lora_indices = self._get_token_lora_indices(x)
+        bgmv_expand_slice(x, w_t_all, y, token_lora_indices, y_offset,
+                          y_slice_size, add_inputs)
 
     def add_shrink(self, y: torch.Tensor, x: torch.Tensor,
                    lora_a_stacked: tuple[torch.Tensor,
@@ -81,13 +170,20 @@ class PunicaWrapperGPU(PunicaWrapperBase):
         """
 
         x = x.view(-1, x.shape[-1])
-        lora_shrink(
-            x,
-            lora_a_stacked,
-            y,
-            *self.token_mapping_meta.meta_args(x.size(0)),
-            scale,
-        )
+        if is_xpu and XPU_KERNEL_V == 0:
+            for slice_idx in range(len(lora_a_stacked)):
+                self._apply_shrink_decode(y[slice_idx], x,
+                                          lora_a_stacked[slice_idx], scale)
+        else:
+            meta_args = self.token_mapping_meta.meta_args(x.size(0))
+
+            lora_shrink(
+                x,
+                lora_a_stacked,
+                y,
+                *self.token_mapping_meta.meta_args(x.size(0)),
+                scale,
+            )
 
     def add_expand(self,
                    y: torch.Tensor,
@@ -127,17 +223,29 @@ class PunicaWrapperGPU(PunicaWrapperBase):
 
         assert x.ndim == 3
         assert x.size(0) == len(output_slices)
-        num_tokens = x.size(1)  # first dimension is the num slices
-
-        lora_expand(
-            x,
-            lora_b_stacked,
-            y,
-            *self.token_mapping_meta.meta_args(num_tokens),
-            offset_start=offset_start,
-            add_inputs=True,
-        )
 
+        if is_xpu and XPU_KERNEL_V == 0:
+            # TODO fuse these kernels
+            for slice_idx in range(len(lora_b_stacked)):
+                self._apply_expand_decode(
+                    y,
+                    x[slice_idx],
+                    lora_b_stacked[slice_idx],
+                    offset_start,
+                    output_slices[slice_idx],
+                    add_inputs=add_inputs,
+                )
+                offset_start += output_slices[slice_idx]
+        else:
+            num_tokens = x.size(1)  # first dimension is the num slices
+            lora_expand(
+                x,
+                lora_b_stacked,
+                y,
+                *self.token_mapping_meta.meta_args(num_tokens),
+                offset_start=offset_start,
+                add_inputs=True,
+            )
         y = y.view_as(y_org)
 
     def add_lora_embedding(self,
@@ -159,14 +267,18 @@ class PunicaWrapperGPU(PunicaWrapperBase):
             add_inputs (bool): Default to True.
         """
 
-        lora_expand(
-            x.unsqueeze(dim=0),
-            (lora_b_stacked, ),
-            y,
-            *self.token_mapping_meta.meta_args(x.size(0)),
-            offset_start=0,
-            add_inputs=add_inputs,
-        )
+        if is_xpu and XPU_KERNEL_V == 0:
+            bgmv_expand(x, lora_b_stacked, y, self._get_token_lora_indices(x),
+                        add_inputs)
+        else:
+            lora_expand(
+                x.unsqueeze(dim=0),
+                (lora_b_stacked, ),
+                y,
+                *self.token_mapping_meta.meta_args(x.size(0)),
+                offset_start=0,
+                add_inputs=add_inputs,
+            )
 
     def add_lora_linear(self,
                         y: torch.Tensor,
@@ -269,11 +381,19 @@ class PunicaWrapperGPU(PunicaWrapperBase):
                                  dtype=torch.float32,
                                  device=x.device)
 
-        lora_shrink(x, [lora_a_stacked], buffer.unsqueeze(dim=0),
-                    *self.prompt_mapping_meta.meta_args(x.size(0)), scale)
-
-        lora_expand(buffer.unsqueeze(dim=0), [lora_b_stacked],
-                    y,
-                    *self.prompt_mapping_meta.meta_args(buffer.size(0)),
-                    add_inputs=True)
+        if is_xpu and XPU_KERNEL_V == 0:
+            bgmv_shrink(x, lora_a_stacked, buffer, self.sampler_indices, scale)
+            bgmv_expand(buffer,
+                        lora_b_stacked,
+                        y,
+                        self.sampler_indices,
+                        add_inputs=True)
+        else:
+            lora_shrink(x, [lora_a_stacked], buffer.unsqueeze(dim=0),
+                        *self.prompt_mapping_meta.meta_args(x.size(0)), scale)
+
+            lora_expand(buffer.unsqueeze(dim=0), [lora_b_stacked],
+                        y,
+                        *self.prompt_mapping_meta.meta_args(buffer.size(0)),
+                        add_inputs=True)
         y = y.view_as(y_org)
diff --git a/vllm/model_executor/layers/fused_moe/layer.py b/vllm/model_executor/layers/fused_moe/layer.py
index 4a6a3b95e..6785373ae 100644
--- a/vllm/model_executor/layers/fused_moe/layer.py
+++ b/vllm/model_executor/layers/fused_moe/layer.py
@@ -327,7 +327,14 @@ class UnquantizedFusedMoEMethod(FusedMoEMethodBase, CustomOp):
             layer.w13_weight.data = shuffled_w13
             layer.w2_weight.data = shuffled_w2
 
-        if current_platform.is_cpu():
+        if current_platform.is_xpu():
+            import intel_extension_for_pytorch as ipex
+            layer.ipex_fusion = ipex.llm.modules.GatedMLPMOE(
+                layer.w13_weight,
+                layer.w2_weight,
+                use_prepack=True,
+            )
+        elif current_platform.is_cpu():
             if current_platform.get_cpu_architecture() == CpuArchEnum.X86:
                 from vllm.model_executor.layers.fused_moe import cpu_fused_moe
                 dtype = layer.w13_weight.dtype
@@ -501,6 +508,29 @@ class UnquantizedFusedMoEMethod(FusedMoEMethodBase, CustomOp):
             activation,
         )
 
+    def forward_xpu(
+        self,
+        layer: torch.nn.Module,
+        x: torch.Tensor,
+        use_grouped_topk: bool,
+        top_k: int,
+        router_logits: torch.Tensor,
+        renormalize: bool,
+        topk_group: Optional[int] = None,
+        num_expert_group: Optional[int] = None,
+        custom_routing_function: Optional[Callable] = None,
+        **kwargs,
+    ):
+        return layer.ipex_fusion(
+            x,
+            use_grouped_topk,
+            top_k,
+            router_logits,
+            renormalize,
+            topk_group,
+            num_expert_group,
+            custom_routing_function=custom_routing_function)
+
     def forward_tpu(
         self,
         layer: torch.nn.Module,
@@ -544,6 +574,8 @@ class UnquantizedFusedMoEMethod(FusedMoEMethodBase, CustomOp):
         forward_native = forward_tpu
     elif current_platform.is_cpu():
         forward_native = forward_cpu
+    elif current_platform.is_xpu():
+        forward_native = forward_xpu
     else:
         forward_native = forward_cuda
 
diff --git a/vllm/model_executor/layers/quantization/__init__.py b/vllm/model_executor/layers/quantization/__init__.py
index 95aea912a..f9dab92f2 100644
--- a/vllm/model_executor/layers/quantization/__init__.py
+++ b/vllm/model_executor/layers/quantization/__init__.py
@@ -37,6 +37,7 @@ QuantizationMethods = Literal[
     "auto-round",
     "rtn",
     "inc",
+    "sym_int4"
 ]
 QUANTIZATION_METHODS: list[str] = list(get_args(QuantizationMethods))
 
@@ -116,6 +117,7 @@ def get_quantization_config(quantization: str) -> type[QuantizationConfig]:
     from .rtn import RTNConfig
     from .torchao import TorchAOConfig
     from .tpu_int8 import Int8TpuConfig
+    from .sym_int4 import SymInt4Config
 
     method_to_config: dict[str, type[QuantizationConfig]] = {
         "aqlm": AQLMConfig,
@@ -148,6 +150,7 @@ def get_quantization_config(quantization: str) -> type[QuantizationConfig]:
         "auto-round": AutoRoundConfig,
         "rtn": RTNConfig,
         "inc": INCConfig,
+        "sym_int4": SymInt4Config,
     }
     # Update the `method_to_config` with customized quantization methods.
     method_to_config.update(_CUSTOMIZED_METHOD_TO_QUANT_CONFIG)
diff --git a/vllm/model_executor/layers/quantization/fp8.py b/vllm/model_executor/layers/quantization/fp8.py
index 75f8adf34..73f8488dc 100644
--- a/vllm/model_executor/layers/quantization/fp8.py
+++ b/vllm/model_executor/layers/quantization/fp8.py
@@ -197,7 +197,7 @@ class Fp8LinearMethod(LinearMethodBase):
         self.use_marlin = (not current_platform.has_device_capability(89)
                            or envs.VLLM_TEST_FORCE_FP8_MARLIN)
         # Disable marlin for rocm
-        if current_platform.is_rocm():
+        if current_platform.is_rocm() or current_platform.is_xpu():
             self.use_marlin = False
 
         # AITER is only supported on ROCm and only for FP8_FNUZ
@@ -278,10 +278,14 @@ class Fp8LinearMethod(LinearMethodBase):
                         if self.quant_config.is_checkpoint_fp8_serialized else
                         params_dtype)
 
+        # Force offloading weights to cpu if VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT
+        # enabled, otherwise use original device config which can be gpu or cpu
+        # (may happen when cpu_offload_gb > 0)
         weight = ModelWeightParameter(data=torch.empty(
             output_size_per_partition,
             input_size_per_partition,
-            dtype=weight_dtype),
+            dtype=weight_dtype,
+            device="cpu" if envs.VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT else None),
                                       input_dim=1,
                                       output_dim=0,
                                       weight_loader=weight_loader)
@@ -363,12 +367,15 @@ class Fp8LinearMethod(LinearMethodBase):
                                                requires_grad=False)
 
         # If checkpoint not serialized fp8, quantize the weights.
-        elif not self.quant_config.is_checkpoint_fp8_serialized:
+        if not self.quant_config.is_checkpoint_fp8_serialized:
             qweight, weight_scale = ops.scaled_fp8_quant(layer.weight,
                                                          scale=None)
 
             # Update the layer with the new values.
-            layer.weight = Parameter(qweight.t(), requires_grad=False)
+            if current_platform.is_xpu():
+                layer.weight = Parameter(qweight, requires_grad=False)
+            else:
+                layer.weight = Parameter(qweight.t(), requires_grad=False)
             layer.weight_scale = Parameter(weight_scale, requires_grad=False)
             layer.input_scale = None
 
@@ -434,6 +441,14 @@ class Fp8LinearMethod(LinearMethodBase):
               layer: torch.nn.Module,
               x: torch.Tensor,
               bias: Optional[torch.Tensor] = None) -> torch.Tensor:
+        if not hasattr(layer, "weight_scale"):
+            return F.linear(x, layer.weight, bias)
+
+        if current_platform.is_xpu():
+            weight = layer.weight.data
+            scale = layer.weight_scale.data
+            output = torch.ops.torch_ipex.fp8_gemm_w8a16(x, weight, True, scale, bias)
+            return output
 
         if self.use_marlin:
             return apply_fp8_marlin_linear(
@@ -587,8 +602,10 @@ class Fp8MoEMethod(FusedMoEMethodBase):
             num_experts,
             2 * intermediate_size_per_partition,
             hidden_size,
-            dtype=params_dtype),
+            dtype=params_dtype,
+            device="cpu" if envs.VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT else None),
                                         requires_grad=False)
+
         layer.register_parameter("w13_weight", w13_weight)
         set_weight_attrs(w13_weight, extra_weight_attrs)
 
@@ -596,7 +613,8 @@ class Fp8MoEMethod(FusedMoEMethodBase):
             num_experts,
             hidden_size,
             intermediate_size_per_partition,
-            dtype=params_dtype),
+            dtype=params_dtype,
+            device="cpu" if envs.VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT else None),
                                        requires_grad=False)
         layer.register_parameter("w2_weight", w2_weight)
         set_weight_attrs(w2_weight, extra_weight_attrs)
@@ -768,6 +786,23 @@ class Fp8MoEMethod(FusedMoEMethodBase):
                                                       requires_grad=False)
                 layer.w2_weight = torch.nn.Parameter(shuffled_w2,
                                                      requires_grad=False)
+            
+            if current_platform.is_xpu():
+                import intel_extension_for_pytorch as ipex
+                layer.ipex_fusion = ipex.llm.modules.GatedMLPMOE(
+                    layer.w13_weight,
+                    layer.w2_weight,
+                    w1_scale_inv=(layer.w13_weight_scale_inv
+                        if self.block_quant else layer.w13_weight_scale),
+                    w2_scale_inv=(layer.w2_weight_scale_inv
+                        if self.block_quant else layer.w2_weight_scale),
+                    a1_scale_inv=layer.w13_input_scale,
+                    a2_scale_inv=layer.w2_input_scale,
+                    use_prepack=True,
+                )
+
+            return
+
         # If checkpoint is fp8, we need to handle that the
         # MoE kernels require single activation scale and single weight
         # scale for w13 per expert.
@@ -936,6 +971,24 @@ class Fp8MoEMethod(FusedMoEMethodBase):
         logical_to_physical_map: Optional[torch.Tensor] = None,
         logical_replica_count: Optional[torch.Tensor] = None,
     ) -> torch.Tensor:
+        if current_platform.is_xpu():
+            return self.forward_xpu(
+                x=x,
+                layer=layer,
+                router_logits=router_logits,
+                top_k=top_k,
+                renormalize=renormalize,
+                use_grouped_topk=use_grouped_topk,
+                topk_group=topk_group,
+                num_expert_group=num_expert_group,
+                global_num_experts=global_num_experts,
+                expert_map=expert_map,
+                custom_routing_function=custom_routing_function,
+                scoring_func=scoring_func,
+                e_score_correction_bias=e_score_correction_bias,
+                activation=activation,
+                apply_router_weight_on_input=apply_router_weight_on_input)
+
         if enable_eplb:
             assert expert_load_view is not None
             assert logical_to_physical_map is not None
@@ -1042,6 +1095,30 @@ class Fp8MoEMethod(FusedMoEMethodBase):
                 a2_scale=layer.w2_input_scale,
             )
 
+    def forward_xpu(
+            self,
+            layer: torch.nn.Module,
+            x: torch.Tensor,
+            use_grouped_topk: bool,
+            top_k: int,
+            router_logits: torch.Tensor,
+            renormalize: bool,
+            topk_group: Optional[int] = None,
+            num_expert_group: Optional[int] = None,
+            custom_routing_function: Optional[Callable] = None,
+            **kwargs,
+    ):
+
+        return layer.ipex_fusion(
+            x,
+            use_grouped_topk,
+            top_k,
+            router_logits,
+            renormalize,
+            topk_group,
+            num_expert_group,
+            custom_routing_function=custom_routing_function,
+        )
 
 class Fp8KVCacheMethod(BaseKVCacheMethod):
     """
diff --git a/vllm/model_executor/layers/quantization/ipex_quant.py b/vllm/model_executor/layers/quantization/ipex_quant.py
index 428e9b882..b778b4ae0 100644
--- a/vllm/model_executor/layers/quantization/ipex_quant.py
+++ b/vllm/model_executor/layers/quantization/ipex_quant.py
@@ -4,6 +4,7 @@
 from typing import Any, Optional
 
 import torch
+import time
 
 from vllm.model_executor.layers.linear import (LinearBase, LinearMethodBase,
                                                UnquantizedLinearMethod)
@@ -13,11 +14,18 @@ from vllm.model_executor.layers.quantization.awq import (AWQLinearMethod,
 from vllm.model_executor.layers.quantization.base_config import (
     QuantizationConfig)
 from vllm.model_executor.layers.quantization.gptq import GPTQLinearMethod
+from vllm.model_executor.parameter import (ModelWeightParameter,
+                                           PerTensorScaleParameter)
+from vllm.model_executor.utils import set_weight_attrs
 from vllm.platforms import current_platform
 
-MIN_IPEX_VERSION = "2.6.0"
+from vllm.model_executor.layers.quantization import register_quantization_config
+from vllm.model_executor.layers.quantization import get_quantization_config
 
 
+MIN_IPEX_VERSION = "2.7.0"
+ACTIVATION_SCHEMES = ["static", "dynamic"]
+
 class IPEXConfig(QuantizationConfig):
     """INT8 quantization config class using IPEX for the CPU/XPU backend,
     including AWQ, GPTQ.
@@ -36,6 +44,7 @@ class IPEXConfig(QuantizationConfig):
         modules_to_not_convert: Optional[list[str]] = None,
         desc_act: Optional[bool] = None,
         lm_head_quantized: Optional[bool] = None,
+        is_checkpoint_fp8_serialized: bool = False,
     ) -> None:
         super().__init__()
         self.method = method
@@ -45,14 +54,15 @@ class IPEXConfig(QuantizationConfig):
         self.desc_act = desc_act
         self.lm_head_quantized = lm_head_quantized
         self.pack_factor = 32 // self.weight_bits
-
-        if self.weight_bits not in [4]:
-            raise ValueError(f"IPEX quantization supports weight bits [4], "
-                             f"but got {self.weight_bits}.")
-
-        if self.method not in ["awq", "gptq"]:
-            raise ValueError(f"IPEX quantization supports [awq, gptq], "
+        self.is_checkpoint_fp8_serialized = is_checkpoint_fp8_serialized
+        if self.method not in ["awq", "gptq", "auto-round", "fp8"]:
+            raise ValueError(f"IPEX quantization supports [awq, gptq, auto-round, fp8], "
                              f"but got {self.method}.")
+        if is_checkpoint_fp8_serialized:
+            self.quant_method = "fp8"
+            print("Detected fp8 checkpoint. Please note that the "
+                   "format is experimental and subject to change.")
+        self.activation_scheme = "dynamic"
 
     def __repr__(self) -> str:
         return (f"IPEXConfig(method={self.method},"
@@ -94,9 +104,13 @@ class IPEXConfig(QuantizationConfig):
         group_size = cls.get_from_keys(config, ["group_size"])
         lm_head_quantized = cls.get_from_keys_or(config, ["lm_head"],
                                                  default=False)
+        data_type = cls.get_from_keys_or(config, ["data_type"],
+                                      default="int4")
+        is_checkpoint_fp8_serialized = ("fp8" in data_type)
+
         desc_act = cls.get_from_keys_or(config, ["desc_act"], default=False)
         return cls(method, weight_bits, group_size, [], desc_act,
-                   lm_head_quantized)
+                   lm_head_quantized, is_checkpoint_fp8_serialized)
 
     @classmethod
     def override_quantization_method(
@@ -106,7 +120,7 @@ class IPEXConfig(QuantizationConfig):
 
         quant_method = hf_quant_cfg.get("quant_method", "").lower()
 
-        if quant_method in ["awq", "gptq"]:
+        if quant_method in ["awq", "gptq", "auto-round", "fp8"]:
             return cls.get_name()
 
         return None
@@ -120,8 +134,84 @@ class IPEXConfig(QuantizationConfig):
                 return IPEXAWQLinearMethod(self)
             if self.method == "gptq":
                 return IPEXGPTQLinearMethod(self)
+            if self.method == "auto-round" or self.method == "fp8":
+                return IPEXAutoRoundLinearMethod(self)
         return None
 
+class IPEXAutoRoundLinearMethod(LinearMethodBase):
+    def __init__(self, quant_config: IPEXConfig):
+        self.quant_config = quant_config
+        self.out_dtype = torch.get_default_dtype()
+
+    def create_weights(
+            self,
+            layer: torch.nn.Module,
+            input_size_per_partition: int,
+            output_partition_sizes: list[int],
+            input_size: int,
+            output_size: int,
+            params_dtype: torch.dtype,
+            **extra_weight_attrs,
+    ):
+        # maybe_create_device_identity()
+
+        output_size_per_partition = sum(output_partition_sizes)
+        weight_loader = extra_weight_attrs.get("weight_loader")
+        layer.logical_widths = output_partition_sizes
+
+        layer.input_size_per_partition = input_size_per_partition
+        layer.output_size_per_partition = output_size_per_partition
+        layer.orig_dtype = params_dtype
+        # WEIGHT
+        weight_dtype = (torch.float8_e5m2
+                        if self.quant_config.is_checkpoint_fp8_serialized else
+                        params_dtype)
+
+        weight = ModelWeightParameter(data=torch.empty(
+            output_size_per_partition,
+            input_size_per_partition,
+            dtype=weight_dtype),
+            input_dim=1,
+            output_dim=0,
+            weight_loader=weight_loader)
+        layer.register_parameter("weight", weight)
+
+        # If checkpoint is serialized fp8, load them.
+        # Otherwise, wait until process_weights_after_loading.
+        if self.quant_config.is_checkpoint_fp8_serialized:
+            # WEIGHT SCALE
+            scale = PerTensorScaleParameter(
+                data=torch.empty(len(output_partition_sizes),
+                                 dtype=torch.float32),
+                weight_loader=weight_loader,
+            )
+            scale[:] = torch.finfo(torch.float32).min
+            set_weight_attrs(scale, {"scale_type": "weight_scale"})
+            set_weight_attrs(scale, {"needs_scalar_to_array": True})
+            layer.register_parameter("weight_scale", scale)
+            # INPUT ACTIVATION SCALE
+            if self.quant_config.activation_scheme == "static":
+                scale = PerTensorScaleParameter(data=torch.empty(
+                    1, dtype=torch.float32),
+                    weight_loader=weight_loader)
+
+                scale[:] = torch.finfo(torch.float32).min
+                set_weight_attrs(scale, {"scale_type": "input_scale"})
+                layer.register_parameter("input_scale", scale)
+            else:
+                layer.register_parameter("input_scale", None)
+
+    def process_weights_after_loading(self, layer: torch.nn.Module) -> None:
+        pass
+
+    def apply(self,
+              layer: torch.nn.Module,
+              x: torch.Tensor,
+              bias: Optional[torch.Tensor] = None) -> torch.Tensor:
+        weight = layer.weight.data
+        scale = layer.weight_scale.data
+        output = torch.ops.torch_ipex.fp8_gemm_w8a16(x, weight, True, scale, bias)
+        return output
 
 class IPEXGPTQLinearMethod(GPTQLinearMethod):
     """GPTQ linear method using IPEX for the CPU/XPU backend.
diff --git a/vllm/model_executor/layers/quantization/sym_int4.py b/vllm/model_executor/layers/quantization/sym_int4.py
new file mode 100644
index 000000000..10d1d3d56
--- /dev/null
+++ b/vllm/model_executor/layers/quantization/sym_int4.py
@@ -0,0 +1,223 @@
+# SPDX-License-Identifier: Apache-2.0
+
+from typing import Any, Dict, List, Optional, Tuple
+# from vllm.model_executor.layers.quantization import register_quantization_config
+from vllm.model_executor.layers.quantization.base_config import (
+    QuantizationConfig, QuantizeMethodBase)
+from vllm.model_executor.layers.linear import (LinearBase, LinearMethodBase,
+                                               UnquantizedLinearMethod)
+from vllm.model_executor.parameter import (BlockQuantScaleParameter,
+                                           ModelWeightParameter,
+                                           PerTensorScaleParameter)
+
+import torch
+from torch.nn import Module
+from torch.nn.parameter import Parameter
+# from vllm.model_executor.layers.quantization.ipex_quant import MIN_IPEX_VERSION
+
+from vllm.envs import VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT, VLLM_QUANTIZE_Q40_LIB
+import ctypes
+
+MIN_IPEX_VERSION = "2.5.0"
+class SymInt4Config(QuantizationConfig):
+    """SYM_INT4 quantization config class which uses IPEX kernel behind the scene...
+    The weight will be quantized according to GPTQ setups...
+    """
+    def __init__(
+        self,
+    ) -> None:
+        super().__init__()
+
+
+    @classmethod
+    def get_name(cls) -> str:
+        return "sym_int4"
+
+
+    @classmethod
+    def get_supported_act_dtypes(cls) -> List[torch.dtype]:
+        return [torch.half]
+
+
+    @classmethod
+    def get_min_capability(cls) -> int:
+        # TODO: check if this will affect things...
+        # May need to check platform xpu
+        return -1
+
+
+    @classmethod
+    def get_config_filenames(cls) -> List[str]:
+        return []
+
+
+    @classmethod
+    def from_config(cls, config: Dict[str, Any]) -> "SymInt4Config":
+        return cls()
+
+    @classmethod
+    def get_quant_method(self, layer: torch.nn.Module,
+                         prefix: str) -> Optional["QuantizeMethodBase"]:
+        """Get the quantize method to use for the quantized layer.
+
+        Args:
+            layer: The layer for the quant method.
+            prefix: The full name of the layer in the state dict
+        Returns:
+            The quantize method. None if the given layer doesn't support quant
+            method.
+        """
+        modules_to_not_convert=["visual", "vision", "vpm", "resampler"]
+        if any(key in prefix for key in modules_to_not_convert):
+            return UnquantizedLinearMethod()
+        if isinstance(layer, LinearBase):
+            return SymInt4LinearMethod(self)
+        else:
+            return None
+
+
+
+class SymInt4LinearMethod(LinearMethodBase):
+    def __init__(self, quant_config: SymInt4Config):
+        self.quant_config = quant_config
+        # Initialize the quant_config
+        try:
+            self.clib = ctypes.CDLL(VLLM_QUANTIZE_Q40_LIB)
+        except OSError as e:
+            raise RuntimeError(f"Failed to load required quantization lib at {VLLM_QUANTIZE_Q40_LIB}: {e}")
+        self.clib.quantize_q4_0_to_qweight_and_scale.argtypes = [
+            ctypes.POINTER(ctypes.c_float),
+            ctypes.POINTER(ctypes.c_int32),
+            ctypes.POINTER(ctypes.c_uint16),
+            ctypes.c_int,
+            ctypes.c_int,
+        ]
+        self.clib.quantize_q4_0_to_qweight_and_scale.restype = ctypes.c_size_t
+
+    def ggml_quantize_tensor(self, weight: torch.Tensor, out_qweight: torch.Tensor, out_scale:torch.Tensor, out_features: int, in_features: int):
+        # Convert src to float *
+        # Currently, only handles dimension = 2
+        assert(weight.dim()==2)
+        assert out_qweight.shape == (out_features, in_features // 8)
+        assert out_scale.shape == (out_features, in_features // 64)
+
+        assert weight.dtype == torch.float32
+        assert out_qweight.dtype == torch.int32
+        assert out_scale.dtype == torch.float16
+
+        assert(out_qweight.is_contiguous())
+        assert(out_scale.is_contiguous())
+        src = weight.data.data_ptr()
+        src = ctypes.cast(src, ctypes.POINTER(ctypes.c_float))
+
+        qweight = out_qweight.data.data_ptr()
+        qweight = ctypes.cast(qweight, ctypes.POINTER(ctypes.c_int32))
+
+        scale = out_scale.data.data_ptr()
+        scale = ctypes.cast(scale, ctypes.POINTER(ctypes.c_uint16))
+        self.clib.quantize_q4_0_to_qweight_and_scale(src, qweight, scale, out_features, in_features)
+        out_qweight = out_qweight.transpose(0,1).contiguous()
+        out_scale = out_scale.transpose(0,1).contiguous()
+        return out_qweight, out_scale
+
+
+    def create_weights(
+        self,
+        layer: torch.nn.Module,
+        input_size_per_partition: int,
+        output_partition_sizes: List[int],
+        input_size: int,
+        output_size: int,
+        params_dtype: torch.dtype,
+        **extra_weight_attrs,
+    ):
+        output_size_per_partition = sum(output_partition_sizes)
+        weight_loader = extra_weight_attrs.get("weight_loader")
+
+        layer.logical_widths = output_partition_sizes
+
+        layer.input_size_per_partition = input_size_per_partition
+        layer.output_size_per_partition = output_size_per_partition
+        layer.orig_dtype = params_dtype
+
+        weight_dtype = params_dtype
+        weight = ModelWeightParameter(data=torch.empty(
+            output_size_per_partition,
+            input_size_per_partition,
+            dtype=weight_dtype,
+            device="cpu"),
+                                      input_dim=1,
+                                      output_dim=0,
+                                      weight_loader=weight_loader)
+        layer.register_parameter("weight", weight)
+
+
+    def apply(self,
+              layer: torch.nn.Module,
+              x: torch.Tensor,
+              bias: Optional[torch.Tensor] = None) -> torch.Tensor:
+        # The same with the GPTQ's linear method by IPEX
+        reshaped_x = x.reshape(-1, x.shape[-1])
+        out = layer.ipex_qlinear(reshaped_x)
+        if bias is not None:
+            out.add_(bias)
+        return out.reshape(x.shape[:-1] + (layer.ipex_output_size, ))
+
+
+    def process_weights_after_loading(self, layer: Module) -> None:
+        weight = layer.weight.float()
+        out_features = layer.weight.shape[0]
+        in_features = layer.weight.shape[1]
+
+        qweight = torch.zeros((out_features, in_features // 8), dtype=torch.int32, device=layer.weight.device)
+        scale = torch.zeros((out_features, in_features // 64), dtype=torch.float16, device=layer.weight.device)
+        qweight, scale = self.ggml_quantize_tensor(weight, qweight, scale, out_features, in_features)
+        
+        qweight = qweight.to("xpu")
+        scale = scale.to("xpu")
+
+        # Use qweight to replace weight...
+        layer.weight = Parameter(qweight, requires_grad=False)
+        # qweight_scale
+        layer.weight_scale = Parameter(scale, requires_grad=False)
+        # layer.input_scale = None
+        try:
+            import intel_extension_for_pytorch as ipex
+            if ipex.__version__ < MIN_IPEX_VERSION:
+                raise ImportError(
+                    "intel_extension_for_pytorch version is "
+                    "wrong. Please install "
+                    f"intel_extension_for_pytorch>={MIN_IPEX_VERSION}.")
+        except ImportError as err:
+            raise ImportError(
+                "Please install "
+                f"intel_extension_for_pytorch>={MIN_IPEX_VERSION} via "
+                f"`pip install intel_extension_for_pytorch>={MIN_IPEX_VERSION}`"
+                " to use IPEX-AWQ linear method.") from err
+        lowp_mode = ipex.quantization.WoqLowpMode.INT8
+        # The weight will be de-packed from INT4 to INT8.
+        weight_dtype = ipex.quantization.WoqWeightDtype.INT4
+        # The float activation will be quantized (dynamic, per-token) to INT8.
+        act_quant_mode = ipex.quantization.WoqActQuantMode.PER_BATCH_IC_BLOCK
+        qconfig = ipex.quantization.get_weight_only_quant_qconfig_mapping(
+            weight_dtype=weight_dtype,
+            lowp_mode=lowp_mode,
+            act_quant_mode=act_quant_mode,
+            group_size=64,
+        )
+        layer.ipex_output_size = layer.weight.shape[-1]
+        g_idx = None
+        layer.ipex_qlinear = ipex.llm.quantization.woq_linear. \
+            IPEXWeightOnlyQuantizedLinear.from_weight(
+            layer.weight,     # weight should be on xpu...
+            layer.weight_scale,
+            torch.tensor([8], device=layer.weight.device, dtype=torch.int8),
+            layer.weight.size(0),
+            layer.ipex_output_size,
+            qconfig=qconfig,
+            g_idx=g_idx,
+            bias=None,
+            group_size=64,
+            # For GPTQ layout
+            quant_method=0
+        )
diff --git a/vllm/model_executor/layers/rotary_embedding.py b/vllm/model_executor/layers/rotary_embedding.py
index dddd4d6a7..324645157 100644
--- a/vllm/model_executor/layers/rotary_embedding.py
+++ b/vllm/model_executor/layers/rotary_embedding.py
@@ -1003,6 +1003,8 @@ class MRotaryEmbedding(RotaryEmbedding):
         assert positions.ndim == 1 or positions.ndim == 2
         assert key is not None
 
+        return self.forward_xpu(positions, query, key)
+        '''
         num_tokens = positions.shape[-1]
         cos_sin = self.cos_sin_cache[positions]
         cos, sin = cos_sin.chunk(2, dim=-1)
@@ -1034,6 +1036,7 @@ class MRotaryEmbedding(RotaryEmbedding):
         key_rot = _apply_rotary_emb(key_rot, cos, sin, self.is_neox_style)
         key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
         return query, key
+        '''
 
     @classmethod
     def get_input_positions(
diff --git a/vllm/model_executor/model_loader/utils.py b/vllm/model_executor/model_loader/utils.py
index 4b30336f0..5298ed666 100644
--- a/vllm/model_executor/model_loader/utils.py
+++ b/vllm/model_executor/model_loader/utils.py
@@ -16,6 +16,7 @@ from transformers.dynamic_module_utils import get_class_from_dynamic_module
 from vllm.attention import Attention
 from vllm.config import (ModelConfig, ModelImpl, VllmConfig,
                          set_current_vllm_config)
+from vllm.envs import VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT
 from vllm.logger import init_logger
 from vllm.model_executor.layers.linear import QKVCrossParallelLinear
 from vllm.model_executor.layers.quantization.base_config import (
@@ -28,6 +29,7 @@ from vllm.model_executor.models.interfaces import SupportsQuant
 from vllm.model_executor.models.registry import (_PREVIOUSLY_SUPPORTED_MODELS,
                                                  _TRANSFORMERS_MODELS)
 from vllm.utils import is_pin_memory_available
+from vllm.model_executor.layers.quantization.sym_int4 import SymInt4LinearMethod
 
 logger = init_logger(__name__)
 
@@ -99,7 +101,9 @@ def initialize_model(
 
 def process_weights_after_loading(model: nn.Module, model_config: ModelConfig,
                                   target_device: torch.device) -> None:
-    for _, module in model.named_modules():
+    # gc: Any changes here need to be added to SymInt4Config.get_quant_method
+    modules_to_not_convert=["visual", "vision", "vpm", "resampler"]
+    for name, module in model.named_modules():
         if isinstance(module, QKVCrossParallelLinear):
             # NOTE(Isotr0py): special case for cross QKV layer because
             # q and kv proj aren't registered as submodules intentionally
@@ -107,12 +111,18 @@ def process_weights_after_loading(model: nn.Module, model_config: ModelConfig,
             continue
         quant_method = getattr(module, "quant_method", None)
         if isinstance(quant_method, QuantizeMethodBase):
-            # When quant methods need to process weights after loading
-            # (for repacking, quantizing, etc), they expect parameters
-            # to be on the global target device. This scope is for the
-            # case where cpu offloading is used, where we will move the
-            # parameters onto device for processing and back off after.
-            with device_loading_context(module, target_device):
+            # The quantization of SYM_INT4 happens on CPU instead of XPU.
+            # We uses the parameter quantization_on_cpu=isinstance(quant_method, SymInt4LinearMethod)
+            # to skip moving tensors to XPU
+            with device_loading_context(module, target_device, isinstance(quant_method, SymInt4LinearMethod)):
+                # When quant methods need to process weights after loading
+                # (for repacking, quantizing, etc), they expect parameters
+                # to be on the global target device. This scope is for the
+                # case where cpu offloading is used, where we will move the
+                # parameters onto device for processing and back off after.
+                if any(key in name for key in modules_to_not_convert):
+                    continue
+
                 quant_method.process_weights_after_loading(module)
 
     # Currently only used by MLA.
@@ -128,7 +138,8 @@ def process_weights_after_loading(model: nn.Module, model_config: ModelConfig,
 
 @contextmanager
 def device_loading_context(module: torch.nn.Module,
-                           target_device: torch.device):
+                           target_device: torch.device,
+                           quantization_on_cpu: False):
     if target_device.type == "cpu":
         # If target is CPU, no need to move anything
         yield module
@@ -137,36 +148,41 @@ def device_loading_context(module: torch.nn.Module,
     original_device_states: dict[str, torch.device] = {}
 
     # Store original device states and move parameters to GPU if they're on CPU
-    for name, p in module.named_parameters():
-        if p.device.type == "cpu":
-            original_device_states[name] = p.device
-            p.data = p.data.to(target_device)
-        # Parameters already on target device are not touched
+    if not quantization_on_cpu:
+        for name, p in module.named_parameters():
+            if p.device.type == "cpu":
+                original_device_states[name] = p.device
+                p.data = p.data.to(target_device)
+            # Parameters already on target device are not touched
 
     try:
         yield module
 
     finally:
-        # Restore parameters to their original devices, ignoring new parameters
-        pin_memory = is_pin_memory_available()
-        for name, p in module.named_parameters():
-            if name in original_device_states:
-                original_device: torch.device = original_device_states[name]
-                if original_device.type == "cpu":
-                    # `torch.empty_like` does not support `pin_memory` argument
-                    cpu_data = torch.empty_strided(
-                        size=p.data.size(),
-                        stride=p.data.stride(),
-                        dtype=p.data.dtype,
-                        layout=p.data.layout,
-                        device="cpu",
-                        pin_memory=pin_memory,
-                    )
-                    cpu_data.copy_(p.data)
-                    p.data = cpu_data
-                else:
-                    p.data = p.data.to(original_device)
-        # New parameters or parameters already on target device are untouched
+        # If weights were loaded onto the CPU for FP8 online quantization, there
+        # is no need to move them back to the original device.
+        if not VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT:
+            # Restore parameters to their original devices, ignoring new parameters # noqa: E501
+            pin_memory = is_pin_memory_available()
+            for name, p in module.named_parameters():
+                if name in original_device_states:
+                    original_device: torch.device = original_device_states[
+                        name]
+                    if original_device.type == "cpu":
+                        # `torch.empty_like` does not support `pin_memory` argument # noqa: E501
+                        cpu_data = torch.empty_strided(
+                            size=p.data.size(),
+                            stride=p.data.stride(),
+                            dtype=p.data.dtype,
+                            layout=p.data.layout,
+                            device="cpu",
+                            pin_memory=pin_memory,
+                        )
+                        cpu_data.copy_(p.data)
+                        p.data = cpu_data
+                    else:
+                        p.data = p.data.to(original_device)
+            # New parameters or parameters already on target device are untouched # noqa: E501
 
 
 def resolve_transformers_arch(model_config: ModelConfig,
diff --git a/vllm/model_executor/models/bert.py b/vllm/model_executor/models/bert.py
index 9dc6115f8..37c3a79cb 100644
--- a/vllm/model_executor/models/bert.py
+++ b/vllm/model_executor/models/bert.py
@@ -12,7 +12,6 @@ from vllm.attention import Attention, AttentionType
 from vllm.compilation.decorators import support_torch_compile
 from vllm.config import CacheConfig, PoolerConfig, VllmConfig
 from vllm.distributed import get_tensor_model_parallel_world_size
-from vllm.forward_context import get_forward_context
 from vllm.model_executor.layers.activation import get_act_fn
 from vllm.model_executor.layers.linear import (ColumnParallelLinear,
                                                QKVParallelLinear,
@@ -60,7 +59,6 @@ class BertEmbedding(nn.Module):
     def forward(
         self,
         input_ids: torch.Tensor,
-        seq_lens: torch.Tensor,
         position_ids: torch.Tensor,
         token_type_ids: Optional[torch.Tensor] = None,
     ) -> torch.Tensor:
@@ -119,7 +117,6 @@ class BertPooler(Pooler):
         return pooled_output
 
 
-@support_torch_compile
 class BertEncoder(nn.Module):
 
     def __init__(self, vllm_config: VllmConfig, prefix: str = ""):
@@ -337,6 +334,7 @@ class BertOutput(nn.Module):
         return hidden_states
 
 
+@support_torch_compile
 class BertModel(nn.Module, SupportsQuant):
 
     is_pooling_model = True
@@ -368,13 +366,9 @@ class BertModel(nn.Module, SupportsQuant):
         if inputs_embeds is not None:
             hidden_states = inputs_embeds
         else:
-            attn_metadata = get_forward_context().attn_metadata
-            assert hasattr(attn_metadata, "seq_lens_tensor")
-            hidden_states = self.embeddings(
-                input_ids=input_ids,
-                seq_lens=attn_metadata.seq_lens_tensor,
-                position_ids=position_ids,
-                token_type_ids=token_type_ids)
+            hidden_states = self.embeddings(input_ids=input_ids,
+                                            position_ids=position_ids,
+                                            token_type_ids=token_type_ids)
         return self.encoder(hidden_states)
 
     def _load_weights(self, weights: Iterable[tuple[str, torch.Tensor]]):
@@ -447,7 +441,7 @@ class BertPoolingModel(BertModel):
         return loaded_params
 
 
-class BertEmbeddingModel(nn.Module, SupportsV0Only, SupportsQuant):
+class BertEmbeddingModel(nn.Module, SupportsQuant):
     """A model that uses Bert to provide embedding functionalities.
 
     This class encapsulates the BertModel and provides an interface for
@@ -474,11 +468,13 @@ class BertEmbeddingModel(nn.Module, SupportsV0Only, SupportsQuant):
         self,
         input_ids: Optional[torch.Tensor],
         positions: torch.Tensor,
+        token_type_ids: Optional[torch.Tensor] = None,
         intermediate_tensors: Optional[IntermediateTensors] = None,
         inputs_embeds: Optional[torch.Tensor] = None,
     ) -> torch.Tensor:
         return self.model(input_ids=input_ids,
                           position_ids=positions,
+                          token_type_ids=token_type_ids,
                           inputs_embeds=inputs_embeds,
                           intermediate_tensors=intermediate_tensors)
 
diff --git a/vllm/model_executor/models/gemma2.py b/vllm/model_executor/models/gemma2.py
index 8beefb2cd..54f8c0a65 100644
--- a/vllm/model_executor/models/gemma2.py
+++ b/vllm/model_executor/models/gemma2.py
@@ -147,10 +147,7 @@ class Gemma2Attention(nn.Module):
         # reference:
         # https://github.com/huggingface/transformers/blob/54be2d7ae87e873482b984cc956e165ca4dc0ba3/src/transformers/models/gemma2/modeling_gemma2.py#L312 # noqa
         layer_idx = extract_layer_index(prefix)
-        use_sliding_window = (layer_idx % 2 == 0 and getattr(
-            config, "interleaved_sliding_window", None) is not None)
-        sliding_window = config.interleaved_sliding_window if \
-            use_sliding_window else None
+        sliding_window = config.sliding_window if not bool(layer_idx % 2) else None
         self.attn = Attention(self.num_heads,
                               self.head_dim,
                               self.scaling,
diff --git a/vllm/model_executor/models/gemma3_mm.py b/vllm/model_executor/models/gemma3_mm.py
index d14f5fa3d..07ad2bce4 100644
--- a/vllm/model_executor/models/gemma3_mm.py
+++ b/vllm/model_executor/models/gemma3_mm.py
@@ -498,8 +498,13 @@ class Gemma3ForConditionalGeneration(nn.Module, SupportsMultiModal, SupportsPP,
         self.config = config
         self.quant_config = quant_config
         self.multimodal_config = multimodal_config
-        self.sliding_window = getattr(config.text_config,
-                                      "interleaved_sliding_window", None)
+        if hasattr(config, "sliding_window"):
+            self.sliding_window = getattr(config.text_config,
+                                    "sliding_window", None)
+        else:
+            self.sliding_window = getattr(config.text_config,
+                                    "interleaved_sliding_window", None)
+
 
         self.vision_tower = SiglipVisionModel(config.vision_config,
                                               quant_config,
diff --git a/vllm/model_executor/models/glm4_1v.py b/vllm/model_executor/models/glm4_1v.py
index 0996bcf60..c06e9853f 100644
--- a/vllm/model_executor/models/glm4_1v.py
+++ b/vllm/model_executor/models/glm4_1v.py
@@ -259,7 +259,8 @@ class Glm4vVisionAttention(nn.Module):
         )
 
         # Detect attention implementation.
-        self.attn_backend: _Backend = get_vit_attn_backend(support_fa=True)
+        # self.attn_backend: _Backend = get_vit_attn_backend(support_fa=True)
+        self.attn_backend = _Backend.TORCH_SDPA
         if self.attn_backend not in {
                 _Backend.FLASH_ATTN,
                 _Backend.TORCH_SDPA,
@@ -343,22 +344,51 @@ class Glm4vVisionAttention(nn.Module):
                                       b=batch_size)
         elif self.attn_backend == _Backend.TORCH_SDPA:
             # Execute attention entry by entry for speed & less VRAM.
-            outputs = []
-            for i in range(1, len(cu_seqlens)):
-                start_idx = cu_seqlens[i - 1]
-                end_idx = cu_seqlens[i]
-                q_i = q[:, start_idx:end_idx]
-                k_i = k[:, start_idx:end_idx]
-                v_i = v[:, start_idx:end_idx]
-                q_i, k_i, v_i = (rearrange(x, "b s h d -> b h s d")
-                                 for x in [q_i, k_i, v_i])
-                output_i = F.scaled_dot_product_attention(q_i,
-                                                          k_i,
-                                                          v_i,
-                                                          dropout_p=0.0)
-                output_i = rearrange(output_i, "b h s d -> b s h d ")
-                outputs.append(output_i)
-            context_layer = torch.cat(outputs, dim=1)
+            q, k, v = (rearrange(x, "b s ... -> (b s) ...") for x in [q, k, v])
+            from vllm._ipex_ops import ipex_ops
+            output = torch.empty(
+                        (q.shape[0], q.shape[1], q.shape[2]),
+                        dtype=q.dtype,
+                        device=q.device)
+            import math
+            head_dim = q.shape[-1]
+            scale = 1 / math.sqrt(head_dim)
+            ipex_ops.varlen_attention(q, k, v, output,
+                                    cu_seqlens,
+                                    cu_seqlens,
+                                    None,
+                                    max_seqlen,
+                                    max_seqlen,
+                                    pdropout=0,
+                                    softmax_scale=scale,
+                                    zero_tensors=False,
+                                    is_causal=False,
+                                    return_softmax=False,
+                                    window_size_left=-1,
+                                    window_size_right=-1,
+                                    gen_=None,
+                                    logits_soft_cap=0
+                                    )
+
+            context_layer = rearrange(output,
+                                      "(b s) ... -> b s ...",
+                                      b=batch_size)
+            # outputs = []
+            # for i in range(1, len(cu_seqlens)):
+            #     start_idx = cu_seqlens[i - 1]
+            #     end_idx = cu_seqlens[i]
+            #     q_i = q[:, start_idx:end_idx]
+            #     k_i = k[:, start_idx:end_idx]
+            #     v_i = v[:, start_idx:end_idx]
+            #     q_i, k_i, v_i = (rearrange(x, "b s h d -> b h s d")
+            #                      for x in [q_i, k_i, v_i])
+            #     output_i = F.scaled_dot_product_attention(q_i,
+            #                                               k_i,
+            #                                               v_i,
+            #                                               dropout_p=0.0)
+            #     output_i = rearrange(output_i, "b h s d -> b s h d ")
+            #     outputs.append(output_i)
+            # context_layer = torch.cat(outputs, dim=1)
         elif self.attn_backend == _Backend.XFORMERS:
             from xformers import ops as xops
             from xformers.ops.fmha.attn_bias import BlockDiagonalMask
@@ -730,7 +760,7 @@ class Glm4vVisionTransformer(nn.Module):
     ) -> tuple[Optional[int], Optional[list[int]]]:
         max_seqlen, seqlens = None, None
         seqlens = (cu_seqlens[1:] - cu_seqlens[:-1]).tolist()
-        if self.attn_backend == _Backend.FLASH_ATTN:
+        if self.attn_backend == _Backend.FLASH_ATTN or self.attn_backend == _Backend.IPEX:
             max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()
         return max_seqlen, seqlens
 
diff --git a/vllm/model_executor/models/glm4_moe.py b/vllm/model_executor/models/glm4_moe.py
index bdca293d2..8fa5d8b9b 100644
--- a/vllm/model_executor/models/glm4_moe.py
+++ b/vllm/model_executor/models/glm4_moe.py
@@ -53,7 +53,7 @@ from vllm.model_executor.model_loader.weight_utils import (
 from vllm.model_executor.sampling_metadata import SamplingMetadata
 from vllm.sequence import IntermediateTensors
 
-from .interfaces import SupportsPP
+from .interfaces import SupportsLoRA, SupportsPP
 from .utils import (AutoWeightsLoader, PPMissingLayer, is_pp_missing_parameter,
                     make_empty_intermediate_tensors_factory, make_layers,
                     maybe_prefix)
@@ -118,16 +118,17 @@ class Glm4MoE(nn.Module):
         if config.hidden_act != "silu":
             raise ValueError(f"Unsupported activation: {config.hidden_act}. "
                              "Only silu is supported for now.")
-
-        self.gate = ReplicatedLinear(config.hidden_size,
-                                     config.n_routed_experts,
-                                     bias=False,
-                                     quant_config=None,
-                                     prefix=f"{prefix}.gate")
-
-        # noaux_tc is not set in transformers new config now
-        self.gate.e_score_correction_bias = (nn.Parameter(
-            torch.empty(config.n_routed_experts)))
+        # NOTE In the transformers implementation, the gate isn't an nn.Linear,
+        # so we cannot use ReplicatedLinear here.
+        # See: https://github.com/huggingface/transformers/blob/v4.55.1/src/transformers/models/glm4_moe/modeling_glm4_moe.py#L260
+        self.gate = nn.Linear(
+            config.hidden_size,
+            config.n_routed_experts,
+            bias=False,
+            dtype=torch.float32,
+        )
+        self.gate.e_score_correction_bias = nn.Parameter(
+            torch.empty(config.n_routed_experts, dtype=torch.float32))
 
         # Load balancing settings.
         vllm_config = get_current_vllm_config()
@@ -181,7 +182,7 @@ class Glm4MoE(nn.Module):
 
         if self.n_shared_experts is not None:
             shared_output = self.shared_experts(hidden_states)
-        router_logits, _ = self.gate(hidden_states)
+        router_logits = self.gate(hidden_states.to(dtype=torch.float32))
         final_hidden_states = self.experts(
             hidden_states=hidden_states,
             router_logits=router_logits) * self.routed_scaling_factor
@@ -372,7 +373,13 @@ class Glm4MoeDecoderLayer(nn.Module):
         return hidden_states, residual
 
 
-@support_torch_compile
+@support_torch_compile(
+    dynamic_arg_dims={
+        "input_ids": 0,
+        "positions": -1,
+        "intermediate_tensors": 0,
+        "inputs_embeds": 0,
+    })
 class Glm4MoeModel(nn.Module):
 
     def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
@@ -390,7 +397,6 @@ class Glm4MoeModel(nn.Module):
             self.embed_tokens = VocabParallelEmbedding(
                 config.vocab_size,
                 config.hidden_size,
-                quant_config=quant_config,
                 prefix=f"{prefix}.embed_tokens")
         else:
             self.embed_tokens = PPMissingLayer()
@@ -462,6 +468,15 @@ class Glm4MoeModel(nn.Module):
                         device=device),
         })
 
+    def get_expert_mapping(self) -> list[tuple[str, str, int, str]]:
+        # Params for weights, fp8 weight scales, fp8 activation scales
+        # (param_name, weight_name, expert_id, shard_id)
+        return FusedMoE.make_expert_params_mapping(
+            ckpt_gate_proj_name="gate_proj",
+            ckpt_down_proj_name="down_proj",
+            ckpt_up_proj_name="up_proj",
+            num_experts=self.config.n_routed_experts)
+
     def load_weights(self, weights: Iterable[tuple[str,
                                                    torch.Tensor]]) -> set[str]:
         stacked_params_mapping = [
@@ -473,16 +488,9 @@ class Glm4MoeModel(nn.Module):
             ("gate_up_proj", "up_proj", 1),
         ]
 
-        # Params for weights, fp8 weight scales, fp8 activation scales
-        # (param_name, weight_name, expert_id, shard_id)
-        expert_params_mapping = FusedMoE.make_expert_params_mapping(
-            ckpt_gate_proj_name="gate_proj",
-            ckpt_down_proj_name="down_proj",
-            ckpt_up_proj_name="up_proj",
-            num_experts=self.config.n_routed_experts)
-
         params_dict = dict(self.named_parameters())
         loaded_params: set[str] = set()
+        expert_params_mapping = self.get_expert_mapping()
         for name, loaded_weight in weights:
             spec_layer = get_spec_layer_idx_from_weight_name(self.config, name)
             if spec_layer is not None:
@@ -571,7 +579,7 @@ class Glm4MoeModel(nn.Module):
         return loaded_params
 
 
-class Glm4MoeForCausalLM(nn.Module, SupportsPP):
+class Glm4MoeForCausalLM(nn.Module, SupportsPP, SupportsLoRA):
     packed_modules_mapping = {
         "qkv_proj": [
             "q_proj",
@@ -600,8 +608,6 @@ class Glm4MoeForCausalLM(nn.Module, SupportsPP):
                                           quant_config=quant_config)
         else:
             self.lm_head = PPMissingLayer()
-        if self.config.tie_word_embeddings:
-            self.lm_head.weight = self.model.embed_tokens.weight
         self.logits_processor = LogitsProcessor(config.vocab_size)
         self.make_empty_intermediate_tensors = (
             self.model.make_empty_intermediate_tensors)
@@ -613,14 +619,19 @@ class Glm4MoeForCausalLM(nn.Module, SupportsPP):
         self.num_expert_groups = config.n_group
 
         self.moe_layers: list[FusedMoE] = []
+        example_moe = None
         for layer in self.model.layers:
+            if isinstance(layer, PPMissingLayer):
+                continue
             assert isinstance(layer, Glm4MoeDecoderLayer)
             if isinstance(layer.mlp, Glm4MoE):
+                # Pick last one layer since the first ones may be dense layers.
+                example_moe = layer.mlp
                 self.moe_layers.append(layer.mlp.experts)
 
         # Pick last one layer since the first ones may be dense layers.
-        example_moe = typing.cast(
-            Glm4MoE, self.model.layers[config.num_hidden_layers - 1].mlp)
+        if example_moe is None:
+            raise RuntimeError("No Glm4MoE layer found in model.layers.")
         self.num_logical_experts = example_moe.n_logical_experts
         self.num_physical_experts = example_moe.n_physical_experts
         self.num_local_physical_experts = example_moe.n_local_physical_experts
@@ -673,6 +684,10 @@ class Glm4MoeForCausalLM(nn.Module, SupportsPP):
         return loader.load_weights(weights)
 
 
+    def get_expert_mapping(self) -> list[tuple[str, str, int, str]]:
+        return self.model.get_expert_mapping()
+
+
 def get_spec_layer_idx_from_weight_name(config: PretrainedConfig,
                                         weight_name: str) -> Optional[int]:
     if hasattr(config,
diff --git a/vllm/model_executor/models/glm4v.py b/vllm/model_executor/models/glm4v.py
index 7584b5188..79d2700ac 100644
--- a/vllm/model_executor/models/glm4v.py
+++ b/vllm/model_executor/models/glm4v.py
@@ -18,6 +18,7 @@ from transformers.image_utils import ImageInput
 from transformers.tokenization_utils_base import TextInput
 
 from vllm.attention.layer import MultiHeadAttention
+from vllm.attention.layer import SelfMultiHeadAttention
 from vllm.config import VllmConfig
 from vllm.distributed import get_tensor_model_parallel_world_size
 from vllm.model_executor.layers.activation import SiluAndMul, get_act_fn
@@ -112,7 +113,9 @@ class EVA2CLIPAttention(nn.Module):
             prefix=f"{prefix}.dense",
         )
 
-        self.attn = MultiHeadAttention(self.num_heads_per_rank, self.head_dim,
+        # self.attn = MultiHeadAttention(self.num_heads_per_rank, self.head_dim,
+        #                                self.scale)
+        self.attn = SelfMultiHeadAttention(self.num_heads_per_rank, self.head_dim,
                                        self.scale)
         self.output_dropout = torch.nn.Dropout(config.dropout_prob)
 
diff --git a/vllm/model_executor/models/idefics2_vision_model.py b/vllm/model_executor/models/idefics2_vision_model.py
index 9e27200fb..ab8bd737f 100644
--- a/vllm/model_executor/models/idefics2_vision_model.py
+++ b/vllm/model_executor/models/idefics2_vision_model.py
@@ -27,6 +27,7 @@ from transformers.models.idefics2.configuration_idefics2 import (
     Idefics2Config, Idefics2VisionConfig)
 
 from vllm.attention.layer import MultiHeadAttention
+from vllm.attention.layer import SelfMultiHeadAttention
 from vllm.distributed import divide, get_tensor_model_parallel_world_size
 from vllm.model_executor.layers.activation import get_act_fn
 from vllm.model_executor.layers.linear import (ColumnParallelLinear,
@@ -146,8 +147,10 @@ class Idefics2VisionAttention(nn.Module):
         )
         self.tp_size = get_tensor_model_parallel_world_size()
         self.num_heads_per_partition = divide(self.num_heads, self.tp_size)
-        self.attn = MultiHeadAttention(self.num_heads_per_partition,
-                                       self.head_dim, self.scale)
+        # self.attn = MultiHeadAttention(self.num_heads_per_partition,
+        #                                self.head_dim, self.scale)
+        self.attn = SelfMultiHeadAttention(self.num_heads_per_partition, self.head_dim,
+                                       self.scale)
 
     def forward(
         self,
diff --git a/vllm/model_executor/models/interns1.py b/vllm/model_executor/models/interns1.py
new file mode 100644
index 000000000..ab21cbe91
--- /dev/null
+++ b/vllm/model_executor/models/interns1.py
@@ -0,0 +1,832 @@
+# SPDX-License-Identifier: Apache-2.0
+# SPDX-FileCopyrightText: Copyright contributors to the vLLM project
+
+# --------------------------------------------------------
+# InternS1
+# Copyright (c) 2025 Shanghai AI Lab
+# Licensed under The MIT License [see LICENSE for details]
+# --------------------------------------------------------
+from collections.abc import Iterable, Mapping, Sequence
+from typing import Literal, Optional, TypedDict, Union
+
+import regex as re
+import torch
+import torch.nn as nn
+from transformers import BatchFeature, InternVLProcessor, PretrainedConfig
+from transformers.activations import ACT2FN
+from transformers.models.got_ocr2.image_processing_got_ocr2_fast import (
+    GotOcr2ImageProcessorFast)
+
+from vllm.config import VllmConfig
+from vllm.model_executor.layers.quantization import QuantizationConfig
+from vllm.model_executor.models.interns1_vit import InternS1VisionModel
+from vllm.model_executor.models.module_mapping import MultiModelKeys
+from vllm.model_executor.sampling_metadata import SamplingMetadata
+from vllm.multimodal import MULTIMODAL_REGISTRY
+from vllm.multimodal.inputs import (MultiModalDataDict, MultiModalFieldConfig,
+                                    MultiModalKwargs, NestedTensors)
+from vllm.multimodal.parse import (ImageEmbeddingItems, ImageProcessorItems,
+                                   ImageSize, MultiModalDataItems)
+from vllm.multimodal.processing import (BaseMultiModalProcessor,
+                                        BaseProcessingInfo, PromptReplacement,
+                                        PromptUpdate, PromptUpdateDetails)
+from vllm.multimodal.profiling import BaseDummyInputsBuilder
+from vllm.sequence import IntermediateTensors
+
+from .interfaces import (MultiModalEmbeddings, SupportsLoRA,
+                         SupportsMultiModal, SupportsPP)
+from .utils import (AutoWeightsLoader, WeightsMapper, flatten_bn,
+                    init_vllm_registered_model, maybe_prefix,
+                    merge_multimodal_embeddings)
+
+
+class InternS1MultiModalProjector(nn.Module):
+
+    def __init__(self, config):
+        super().__init__()
+        self.layer_norm = nn.LayerNorm(config.vision_config.hidden_size *
+                                       int(1 / config.downsample_ratio)**2)
+        self.linear_1 = nn.Linear(
+            config.vision_config.hidden_size *
+            int(1 / config.downsample_ratio)**2,
+            config.text_config.hidden_size)
+        self.act = ACT2FN[config.projector_hidden_act]
+        self.linear_2 = nn.Linear(config.text_config.hidden_size,
+                                  config.text_config.hidden_size)
+
+    def forward(self, image_features):
+        hidden_states = self.layer_norm(image_features)
+        hidden_states = self.linear_1(hidden_states)
+        hidden_states = self.act(hidden_states)
+        hidden_states = self.linear_2(hidden_states)
+        return hidden_states
+
+
+class InternS1ImagePixelInputs(TypedDict):
+    type: Literal["pixel_values"]
+    pixel_values: torch.Tensor
+    """
+    Shape:
+    `(batch_size * num_images * (1 + num_patches), num_channels, height, width)`
+    """
+
+
+class InternS1ImageEmbeddingInputs(TypedDict):
+    type: Literal["image_embeds"]
+    data: Union[torch.Tensor, list[torch.Tensor]]
+    """
+    A tensor of shape `(num_images, total_image_feature_size, hidden_size)`
+    or a list of tensors of shape `(total_image_feature_size, hidden_size)`
+
+    `hidden_size` must match the hidden size of language model backbone.
+    """
+
+
+InternS1ImageInputs = Union[InternS1ImagePixelInputs,
+                            InternS1ImageEmbeddingInputs]
+
+
+class InternS1VideoPixelInputs(TypedDict):
+    type: Literal["pixel_values_videos"]
+    pixel_values: torch.Tensor
+    """
+    Shape:
+    `(batch_size * num_video * num_frames, num_channels, height, width)`
+    """
+
+    num_patches: torch.Tensor
+    """Shape: `(batch_size * num_images)`"""
+
+
+class InternS1VideoEmbeddingInputs(TypedDict):
+    type: Literal["video_embeds"]
+    data: Union[torch.Tensor, list[torch.Tensor]]
+    """
+    A tensor of shape `(num_videos, total_video_feature_size, hidden_size)`
+    or a list of tensors of shape `(total_video_feature_size, hidden_size)`
+
+    `hidden_size` must match the hidden size of language model backbone.
+    """
+
+
+InternS1VideoInputs = Union[InternS1VideoPixelInputs,
+                            InternS1VideoEmbeddingInputs]
+
+
+def resolve_interns1_min_max_num(
+    min_dynamic_patch: int,
+    max_dynamic_patch: int,
+    dynamic_image_size: bool,
+    use_thumbnail: bool,
+) -> tuple[int, int]:
+    min_dynamic_patch = min_dynamic_patch if dynamic_image_size else 1
+    max_dynamic_patch = max_dynamic_patch if dynamic_image_size else 1
+
+    if use_thumbnail and max_dynamic_patch != 1:
+        max_dynamic_patch += 1
+
+    return min_dynamic_patch, max_dynamic_patch
+
+
+def get_interns1_target_ratios(
+    min_num: int,
+    max_num: int,
+) -> list[tuple[int, int]]:
+    target_ratios = {(i, j)
+                     for n in range(min_num, max_num + 1)
+                     for i in range(1, n + 1)
+                     for j in range(1, n + 1) if min_num <= i * j <= max_num}
+    return sorted(target_ratios, key=lambda x: x[0] * x[1])
+
+
+class InternS1ProcessingInfo(BaseProcessingInfo):
+    """ProcessingInfo for InternS1-style models."""
+
+    def get_hf_processor(self, **kwargs: object) -> InternVLProcessor:
+        return self.ctx.get_hf_processor(InternVLProcessor, **kwargs)
+
+    def get_supported_mm_limits(self) -> Mapping[str, Optional[int]]:
+        return {"image": None, "video": None}
+
+    def get_num_image_tokens(
+        self,
+        *,
+        image_width: int,
+        image_height: int,
+        processor: Optional['GotOcr2ImageProcessorFast'] = None,
+    ) -> int:
+        if processor is None:
+            processor = self.get_hf_processor().image_processor
+
+        if not isinstance(processor, GotOcr2ImageProcessorFast):
+            raise ValueError(f'GotOcr2ImageProcessorFast is expected but got '
+                             f'{type(processor)}')
+        num_image_patches = processor.get_number_of_image_tokens(
+            image_height, image_width, images_kwargs=dict())
+        num_image_tokens = self.get_hf_processor(
+        ).image_seq_length * num_image_patches
+        return num_image_tokens
+
+    def resolve_target_ratios(self, use_thumbnail: Optional[bool] = None):
+        image_processor = self.get_hf_processor().image_processor
+        min_dynamic_patch = image_processor.min_patches
+        max_dynamic_patch = image_processor.max_patches
+        # HF format's InternVL processor uses `crop_to_patches` which is
+        # equivalent to `use_thumbnail` in original format.
+        use_thumbnail = image_processor.crop_to_patches
+        dynamic_image_size = True
+        min_num, max_num = resolve_interns1_min_max_num(
+            min_dynamic_patch,
+            max_dynamic_patch,
+            dynamic_image_size,
+            use_thumbnail=use_thumbnail)
+
+        return get_interns1_target_ratios(min_num, max_num)
+
+    def get_image_size_with_most_features(self) -> ImageSize:
+        processor = self.get_hf_processor()
+
+        hf_config = self.ctx.get_hf_config()
+        base_height, base_width = hf_config.vision_config.image_size
+        target_ratios = self.resolve_target_ratios()
+
+        largest_feature_size, largest_feature_pinpoint = 0, None
+        for wr, hr in target_ratios:
+            width, height = base_width * wr, base_height * hr
+
+            feat_size = self.get_num_image_tokens(
+                image_width=width,
+                image_height=height,
+                processor=processor.image_processor,
+            )
+            if feat_size > largest_feature_size:
+                largest_feature_size = feat_size
+                largest_feature_pinpoint = ImageSize(width=width,
+                                                     height=height)
+
+        assert not (largest_feature_size == 0 or largest_feature_pinpoint
+                    is None), ("Cannot have a largest feature size of 0!")
+
+        return largest_feature_pinpoint
+
+    def get_max_image_tokens(self) -> int:
+        processor = self.get_hf_processor()
+        target_width, target_height = self.get_image_size_with_most_features()
+
+        return self.get_num_image_tokens(
+            image_width=target_width,
+            image_height=target_height,
+            processor=processor.image_processor,
+        )
+
+    def get_num_frames_with_most_features(
+        self,
+        seq_len: int,
+        mm_counts: Mapping[str, int],
+    ) -> int:
+        max_images = mm_counts.get("image", 0)
+        max_videos = mm_counts.get("video", 0)
+
+        processor = self.get_hf_processor()
+
+        max_image_tokens = self.get_max_image_tokens() * max_images
+        max_total_frames = (seq_len -
+                            max_image_tokens) // processor.image_seq_length
+        max_frames_per_video = max_total_frames // max(max_videos, 1)
+
+        return max(max_frames_per_video, 1)
+
+
+class InternS1DummyInputsBuilder(BaseDummyInputsBuilder[InternS1ProcessingInfo]
+                                 ):
+    """DummyInputsBuilder for InternS1-style models."""
+
+    def get_dummy_text(self, mm_counts: Mapping[str, int]) -> str:
+        num_images = mm_counts.get("image", 0)
+        num_videos = mm_counts.get("video", 0)
+        image_token = self.info.get_hf_processor().image_token
+        video_token = self.info.get_hf_processor().video_token
+
+        return image_token * num_images + video_token * num_videos
+
+    def get_dummy_mm_data(
+        self,
+        seq_len: int,
+        mm_counts: Mapping[str, int],
+    ) -> MultiModalDataDict:
+        target_width, target_height = \
+            self.info.get_image_size_with_most_features()
+        target_num_frames = \
+                self.info.get_num_frames_with_most_features(seq_len, mm_counts)
+        num_images = mm_counts.get("image", 0)
+        num_videos = mm_counts.get("video", 0)
+
+        config = self.info.get_hf_config()
+        image_size_h, image_size_w = config.vision_config.image_size
+
+        return {
+            "image":
+            self._get_dummy_images(width=target_width,
+                                   height=target_height,
+                                   num_images=num_images),
+            "video":
+            self._get_dummy_videos(width=image_size_w,
+                                   height=image_size_h,
+                                   num_frames=target_num_frames,
+                                   num_videos=num_videos),
+        }
+
+
+class InternS1MultiModalProcessor(
+        BaseMultiModalProcessor[InternS1ProcessingInfo]):
+    """ Basic image-only MultiModalProcessor for InternS1-style models."""
+
+    def _call_hf_processor(
+        self,
+        prompt: str,
+        mm_data: Mapping[str, object],
+        mm_kwargs: Mapping[str, object],
+        tok_kwargs: Mapping[str, object],
+    ) -> Mapping[str, NestedTensors]:
+        mm_data = dict(mm_data)
+        videos = mm_data.pop("videos", [])
+        images = mm_data.pop("images", [])
+        assert isinstance(videos, list)
+        assert isinstance(images, list)
+
+        hf_processor = self.info.get_hf_processor(**mm_kwargs)
+        tokenizer = hf_processor.tokenizer
+        video_token_id = tokenizer.encode(hf_processor.video_token,
+                                          add_special_tokens=False)
+        assert len(video_token_id) == 1
+        video_token_id = video_token_id[0]
+
+        prompt = re.sub(hf_processor.image_token, "<image_placeholder>",
+                        prompt)
+        prompt = re.sub(hf_processor.video_token, "<video_placeholder>",
+                        prompt)
+
+        image_outputs = {}
+        if images:
+            image_pixel_values = []
+            for image in images:
+                processed_outputs = super()._call_hf_processor(
+                    prompt=hf_processor.image_token,
+                    mm_data={"images": image},
+                    mm_kwargs=mm_kwargs,
+                    tok_kwargs=tok_kwargs,
+                )
+                image_pixel_values.append(
+                    processed_outputs.pop("pixel_values"))
+
+                input_ids = processed_outputs.pop("input_ids")
+                image_placeholder = tokenizer.batch_decode(input_ids)[0]
+                prompt = prompt.replace("<image_placeholder>",
+                                        image_placeholder, 1)
+
+            num_patches = [len(item) for item in image_pixel_values]
+            image_outputs: dict[str, NestedTensors] = {
+                "pixel_values": torch.concat(image_pixel_values),
+                "image_num_patches": torch.tensor(num_patches),
+                "image_token_id": torch.tensor(hf_processor.image_token_id),
+            }
+
+        video_outputs = {}
+        if videos:
+            video_pixel_values = []
+            for video in videos:
+                processed_outputs = super()._call_hf_processor(
+                    prompt=hf_processor.video_token,
+                    mm_data={"videos": video},
+                    mm_kwargs=mm_kwargs,
+                    tok_kwargs=tok_kwargs,
+                )
+                video_pixel_values.append(
+                    processed_outputs.pop("pixel_values"))
+
+                input_ids = processed_outputs.pop("input_ids")
+                input_ids[input_ids ==
+                          hf_processor.image_token_id] = video_token_id
+
+                video_placeholder = tokenizer.batch_decode(input_ids)[0]
+                prompt = prompt.replace("<video_placeholder>",
+                                        video_placeholder, 1)
+
+            num_frames = [len(item) for item in video_pixel_values]
+            video_outputs: dict[str, NestedTensors] = {
+                "pixel_values_videos": torch.concat(video_pixel_values),
+                "video_num_patches": torch.tensor(num_frames),
+                "video_token_id": torch.tensor(video_token_id),
+            }
+
+        prompt = re.sub("<image_placeholder>", hf_processor.image_token,
+                        prompt)
+        prompt = re.sub("<video_placeholder>", hf_processor.video_token,
+                        prompt)
+        text_outputs = tokenizer(prompt, **tok_kwargs, return_tensors="pt")
+
+        combined_outputs = dict(
+            **text_outputs,
+            **image_outputs,
+            **video_outputs,
+        )
+        return BatchFeature(combined_outputs)
+
+    def _get_mm_fields_config(
+        self,
+        hf_inputs: Mapping[str, NestedTensors],
+        hf_processor_mm_kwargs: Mapping[str, object],
+    ) -> Mapping[str, MultiModalFieldConfig]:
+
+        image_num_patches = hf_inputs.get("image_num_patches", torch.empty(0))
+        video_num_patches = hf_inputs.get("video_num_patches", torch.empty(0))
+        num_images = len(image_num_patches)
+        num_videos = len(video_num_patches)
+
+        return dict(
+            pixel_values=MultiModalFieldConfig.flat_from_sizes(
+                "image", image_num_patches),
+            image_num_patches=MultiModalFieldConfig.batched("image"),
+            image_embeds=MultiModalFieldConfig.batched("image"),
+            image_token_id=MultiModalFieldConfig.shared("image", num_images),
+            pixel_values_videos=MultiModalFieldConfig.flat_from_sizes(
+                "video", video_num_patches),
+            video_num_patches=MultiModalFieldConfig.batched("video"),
+            video_token_id=MultiModalFieldConfig.shared("video", num_videos),
+        )
+
+    def _get_prompt_updates(
+        self,
+        mm_items: MultiModalDataItems,
+        hf_processor_mm_kwargs: Mapping[str, object],
+        out_mm_kwargs: MultiModalKwargs,
+    ) -> Sequence[PromptUpdate]:
+        hf_processor = self.info.get_hf_processor(**hf_processor_mm_kwargs)
+        img_context_token = hf_processor.image_token
+        start_image_token = hf_processor.start_image_token
+        end_image_token = hf_processor.end_image_token
+        video_token = hf_processor.video_token
+
+        if "video_num_patches" in out_mm_kwargs:
+            video_num_patches = out_mm_kwargs["video_num_patches"]
+            assert isinstance(video_num_patches, torch.Tensor)
+            video_num_patches = video_num_patches.tolist()
+        else:
+            video_num_patches = []
+
+        if "image_num_patches" in out_mm_kwargs:
+            image_num_patches = out_mm_kwargs["image_num_patches"]
+            assert isinstance(image_num_patches, torch.Tensor)
+            image_num_patches = image_num_patches.tolist()
+        else:
+            image_num_patches = []
+
+        def get_replacement_interns1_image(item_idx: int):
+            images = mm_items.get_items(
+                "image", (ImageEmbeddingItems, ImageProcessorItems))
+
+            if isinstance(images, ImageEmbeddingItems):
+                feature_size = images.get_feature_size(item_idx)
+            else:
+                num_patches = image_num_patches[item_idx]
+                feature_size = num_patches * hf_processor.image_seq_length
+
+            repl_features = img_context_token * feature_size
+            repl_full = start_image_token + repl_features + end_image_token
+            return PromptUpdateDetails.select_text(repl_full,
+                                                   img_context_token)
+
+        def get_replacement_interns1_video(item_idx: int):
+            num_patches = video_num_patches[item_idx]
+            repl_features = video_token * hf_processor.image_seq_length
+            repl_features_with_sep = (start_image_token + repl_features +
+                                      end_image_token)
+            # num_patches is equal to num_frames
+            repl_full = '\n'.join([
+                f'Frame{i+1}: {repl_features_with_sep}'
+                for i in range(num_patches)
+            ])
+
+            return PromptUpdateDetails.select_text(repl_full, video_token)
+
+        return [
+            PromptReplacement(
+                modality="image",
+                target=img_context_token,
+                replacement=get_replacement_interns1_image,
+            ),
+            PromptReplacement(
+                modality="video",
+                target=video_token,
+                replacement=get_replacement_interns1_video,
+            ),
+        ]
+
+
+@MULTIMODAL_REGISTRY.register_processor(
+    InternS1MultiModalProcessor,
+    info=InternS1ProcessingInfo,
+    dummy_inputs=InternS1DummyInputsBuilder)
+class InternS1ForConditionalGeneration(nn.Module, SupportsMultiModal,
+                                       SupportsPP, SupportsLoRA):
+
+    # To ensure correct weight loading and mapping.
+    hf_to_vllm_mapper = WeightsMapper(
+        orig_to_new_prefix={
+            "lm_head.": "language_model.lm_head.",
+            "model.language_model.": "language_model.model.",
+            "model.vision_tower.": "vision_tower.",
+            "model.multi_modal_projector.": "multi_modal_projector.",
+        })
+
+    @classmethod
+    def get_placeholder_str(cls, modality: str, i: int) -> Optional[str]:
+        # transformers InternVLProcessor uses <IMG_CONTEXT> as the seperator
+        # refer to https://github.com/huggingface/transformers/blob/f90de364c2484c7c325bbe05befdcf487bd75b63/src/transformers/models/internvl/processing_internvl.py#L116
+        if modality.startswith("image"):
+            return '<IMG_CONTEXT>'
+        if modality.startswith("video"):
+            return "<video>"
+
+        raise ValueError("Only image or video modality is supported")
+
+    def __init__(self, *, vllm_config: VllmConfig, prefix: str = "") -> None:
+        super().__init__()
+        config = vllm_config.model_config.hf_config
+        quant_config = vllm_config.quant_config
+        multimodal_config = vllm_config.model_config.multimodal_config
+
+        self.config = config
+        self.multimodal_config = multimodal_config
+
+        image_size = config.vision_config.image_size[0]
+        patch_size = config.vision_config.patch_size[0]
+        self.patch_size = patch_size
+        self.num_image_token = int(
+            (image_size // patch_size)**2 * (config.downsample_ratio**2))
+        self.downsample_ratio = config.downsample_ratio
+
+        self.llm_arch_name = config.text_config.architectures[0]
+        self.vision_tower = self._init_vision_model(
+            config,
+            quant_config=quant_config,
+            prefix=maybe_prefix(prefix, "vision_tower"),
+        )
+
+        self.language_model = init_vllm_registered_model(
+            vllm_config=vllm_config,
+            hf_config=config.text_config,
+            prefix=maybe_prefix(prefix, "language_model"),
+        )
+
+        self.multi_modal_projector = self._init_mlp1(config)
+
+        self.img_context_token_id = None
+        self.video_context_token_id = None
+
+        self.visual_token_mask = None
+        self.make_empty_intermediate_tensors = (
+            self.language_model.make_empty_intermediate_tensors)
+
+    def _init_vision_model(
+        self,
+        config: PretrainedConfig,
+        quant_config: Optional[QuantizationConfig],
+        *,
+        prefix: str,
+    ):
+        num_hidden_layers = config.vision_config.num_hidden_layers
+        return InternS1VisionModel(
+            config.vision_config,
+            quant_config=quant_config,
+            num_hidden_layers_override=num_hidden_layers,
+            prefix=prefix,
+        )
+
+    def _init_mlp1(self, config: PretrainedConfig) -> nn.Sequential:
+        return InternS1MultiModalProjector(config)
+
+    def pixel_shuffle(self, x, scale_factor=0.5):
+        n, w, h, c = x.size()
+        # N, W, H, C --> N, W, H * scale, C // scale
+        x = x.view(n, w, int(h * scale_factor), int(c / scale_factor))
+        # N, W, H * scale, C // scale --> N, H * scale, W, C // scale
+        x = x.permute(0, 2, 1, 3).contiguous()
+        x = x.view(n, int(h * scale_factor), int(w * scale_factor),
+                   int(c / (scale_factor * scale_factor)))
+        x = x.permute(0, 2, 1, 3).contiguous()
+        return x
+
+    def extract_feature(self, pixel_values: torch.Tensor) -> torch.Tensor:
+        vit_embeds = self.vision_tower(pixel_values=pixel_values)
+        vit_embeds = vit_embeds[:, 1:, :]
+
+        h = w = int(vit_embeds.shape[1]**0.5)
+        vit_embeds = vit_embeds.reshape(vit_embeds.shape[0], h, w, -1)
+        vit_embeds = self.pixel_shuffle(vit_embeds,
+                                        scale_factor=self.downsample_ratio)
+        vit_embeds = vit_embeds.reshape(vit_embeds.shape[0], -1,
+                                        vit_embeds.shape[-1])
+
+        vit_embeds = self.multi_modal_projector(vit_embeds)
+        return vit_embeds
+
+    def _validate_pixel_values(self, data: torch.Tensor) -> torch.Tensor:
+
+        h, w = self.config.vision_config.image_size
+        expected_dims = (3, h, w)
+
+        def _validate_shape(d: torch.Tensor):
+            actual_dims = tuple(d.shape)
+
+            if actual_dims != expected_dims:
+                expected_expr = str(expected_dims)
+                raise ValueError(
+                    "The expected shape of pixel values per image per batch "
+                    f" per patch is {expected_expr}. "
+                    f"You supplied {tuple(d.shape)}.")
+
+        for d in data:
+            _validate_shape(d)
+
+        return data
+
+    def _parse_and_validate_image_input(
+            self, **kwargs: object) -> Optional[InternS1ImageInputs]:
+        pixel_values = kwargs.pop("pixel_values", None)
+        image_num_patches = kwargs.pop("image_num_patches", None)
+        image_embeds = kwargs.pop("image_embeds", None)
+
+        if pixel_values is None and image_embeds is None:
+            return None
+
+        if image_embeds is not None:
+            if not isinstance(image_embeds, (torch.Tensor, list)):
+                raise ValueError("Incorrect type of image embeddings. "
+                                 f"Got type: {type(image_embeds)}")
+
+            return InternS1ImageEmbeddingInputs(
+                type="image_embeds",
+                data=flatten_bn(image_embeds),
+            )
+
+        image_token_id = kwargs["image_token_id"]
+        assert isinstance(image_token_id, torch.Tensor)
+        self.img_context_token_id = image_token_id.flatten().unique().item()
+
+        if pixel_values is not None:
+            if not isinstance(pixel_values, (torch.Tensor, list)):
+                raise ValueError("Incorrect type of pixel values. "
+                                 f"Got type: {type(pixel_values)}")
+
+            if not isinstance(image_num_patches, (torch.Tensor, list)):
+                raise ValueError("Incorrect type of image_num_patches. "
+                                 f"Got type: {type(image_num_patches)}")
+
+            pixel_values = flatten_bn(pixel_values, concat=True)
+            image_num_patches = flatten_bn(image_num_patches, concat=True)
+
+            return InternS1ImagePixelInputs(
+                type="pixel_values",
+                pixel_values=self._validate_pixel_values(pixel_values),
+                num_patches=image_num_patches,
+            )
+
+        raise AssertionError("This line should be unreachable.")
+
+    def _parse_and_validate_video_input(
+            self, **kwargs: object) -> Optional[InternS1VideoPixelInputs]:
+        pixel_values_flat_video = kwargs.pop("pixel_values_videos", None)
+        video_num_patches = kwargs.pop("video_num_patches", None)
+        video_embeds = kwargs.pop("video_embeds", None)
+
+        if pixel_values_flat_video is None and video_embeds is None:
+            return None
+
+        if video_embeds is not None:
+            if not isinstance(video_embeds, (torch.Tensor, list)):
+                raise ValueError("Incorrect type of video embeddings. "
+                                 f"Got type: {type(video_embeds)}")
+
+            return InternS1ImageEmbeddingInputs(
+                type="video_embeds",
+                data=flatten_bn(video_embeds),
+            )
+
+        video_token_id = kwargs["video_token_id"]
+        assert isinstance(video_token_id, torch.Tensor)
+        self.video_context_token_id = video_token_id.flatten().unique().item()
+
+        if pixel_values_flat_video is not None:
+            if not isinstance(pixel_values_flat_video, (torch.Tensor, list)):
+                raise ValueError("Incorrect type of pixel values. "
+                                 f"Got type: {type(pixel_values_flat_video)}")
+
+            if not isinstance(video_num_patches, (torch.Tensor, list)):
+                raise ValueError("Incorrect type of image_num_patches. "
+                                 f"Got type: {type(video_num_patches)}")
+
+            pixel_values_flat_video = flatten_bn(pixel_values_flat_video,
+                                                 concat=True)
+            video_num_patches = flatten_bn(video_num_patches, concat=True)
+
+            return InternS1VideoPixelInputs(
+                type="pixel_values_videos",
+                pixel_values=self._validate_pixel_values(
+                    pixel_values_flat_video),
+                num_patches=video_num_patches,
+            )
+
+        raise AssertionError("This line should be unreachable.")
+
+    def _process_image_input(
+        self,
+        image_input: Union[InternS1ImageInputs, InternS1VideoPixelInputs],
+    ) -> tuple[torch.Tensor, ...]:
+        if image_input["type"] == "image_embeds":
+            return image_input["data"]
+
+        assert self.vision_tower is not None
+
+        image_embeds = self.extract_feature(image_input["pixel_values"])
+
+        num_patches = image_input["num_patches"]
+
+        # Only one image in the current batch
+        if len(num_patches) == 1:
+            return (image_embeds.view(-1,
+                                      self.config.text_config.hidden_size), )
+
+        # NOTE: Image embeddings are split into separate tensors for each image
+        # by the size of each embedding.
+        feature_size = image_embeds.shape[1]
+        image_embeds = image_embeds.view(-1,
+                                         self.config.text_config.hidden_size)
+        image_feature_sizes = [
+            num_patches * feature_size for num_patches in num_patches
+        ]
+        return image_embeds.split(image_feature_sizes)
+
+    def _parse_and_validate_multimodal_inputs(self, **kwargs: object) -> dict:
+        modalities = {}
+
+        # Preserve the order of modalities if there are multiple of them
+        # from the order of kwargs.
+        for input_key in kwargs:
+            if input_key in ("pixel_values",
+                             "image_embeds") and "images" not in modalities:
+                modalities["images"] = self._parse_and_validate_image_input(
+                    **kwargs)
+            if input_key in (
+                    "pixel_values_videos", ) and "videos" not in modalities:
+                modalities["videos"] = self._parse_and_validate_video_input(
+                    **kwargs)
+
+        return modalities
+
+    def _set_visual_token_mask(self, input_ids: torch.Tensor) -> None:
+        self.visual_token_mask = None
+
+    def get_language_model(self) -> torch.nn.Module:
+        return self.language_model
+
+    def get_multimodal_embeddings(self,
+                                  **kwargs: object) -> MultiModalEmbeddings:
+
+        modalities = self._parse_and_validate_multimodal_inputs(**kwargs)
+        if not modalities:
+            return []
+
+        # The result multimodal_embeddings is tuple of tensors, with each
+        # tensor correspoending to a multimodal data item (image or video).
+        multimodal_embeddings: tuple[torch.Tensor, ...] = ()
+
+        # NOTE: It is important to iterate over the keys in this dictionary
+        # to preserve the order of the modalities.
+        for modality in modalities:
+            if modality == "images":
+                image_input = modalities["images"]
+                vision_embeddings = self._process_image_input(image_input)
+                multimodal_embeddings += vision_embeddings
+            if modality == "videos":
+                video_input = modalities["videos"]
+                video_embeddings = self._process_image_input(video_input)
+                multimodal_embeddings += video_embeddings
+
+        return multimodal_embeddings
+
+    def get_input_embeddings(
+        self,
+        input_ids: torch.Tensor,
+        multimodal_embeddings: Optional[MultiModalEmbeddings] = None,
+    ) -> torch.Tensor:
+        inputs_embeds = self.language_model.get_input_embeddings(input_ids)
+        if multimodal_embeddings is not None \
+            and len(multimodal_embeddings) != 0:
+            context_token_ids = [
+                token_id for token_id in (self.img_context_token_id,
+                                          self.video_context_token_id)
+                if token_id is not None
+            ]
+            assert len(context_token_ids) >= 1
+            self._set_visual_token_mask(input_ids)
+            inputs_embeds = merge_multimodal_embeddings(
+                input_ids,
+                inputs_embeds,
+                multimodal_embeddings,
+                context_token_ids,
+            )
+        return inputs_embeds
+
+    def forward(
+        self,
+        input_ids: torch.Tensor,
+        positions: torch.Tensor,
+        intermediate_tensors: Optional[IntermediateTensors] = None,
+        inputs_embeds: Optional[torch.Tensor] = None,
+        **kwargs: object,
+    ) -> IntermediateTensors:
+
+        if intermediate_tensors is not None:
+            input_ids = None
+            inputs_embeds = None
+
+        # NOTE: In v1, inputs_embeds is always generated at model runner, this
+        # condition is for v0 compatibility.
+        elif inputs_embeds is None:
+            vision_embeddings = self.get_multimodal_embeddings(**kwargs)
+            inputs_embeds = self.get_input_embeddings(input_ids,
+                                                      vision_embeddings)
+            input_ids = None
+
+        forward_kwargs = {
+            "input_ids": input_ids,
+            "positions": positions,
+            "intermediate_tensors": intermediate_tensors,
+            "inputs_embeds": inputs_embeds,
+        }
+
+        hidden_states = self.language_model.model(**forward_kwargs)
+        return hidden_states
+
+    def compute_logits(
+        self,
+        hidden_states: torch.Tensor,
+        sampling_metadata: SamplingMetadata,
+    ) -> Optional[torch.Tensor]:
+        return self.language_model.compute_logits(hidden_states,
+                                                  sampling_metadata)
+
+    def load_weights(self, weights: Iterable[tuple[str,
+                                                   torch.Tensor]]) -> set[str]:
+        loader = AutoWeightsLoader(self)
+        return loader.load_weights(weights, mapper=self.hf_to_vllm_mapper)
+
+    def get_mm_mapping(self) -> MultiModelKeys:
+        """
+        Get the module prefix in multimodal models
+        """
+        return MultiModelKeys.from_string_field(
+            language_model="language_model",
+            connector="multi_modal_projector",
+            tower_model="vision_tower")
diff --git a/vllm/model_executor/models/internvl.py b/vllm/model_executor/models/internvl.py
index f8b9ea2c5..2a9366d87 100644
--- a/vllm/model_executor/models/internvl.py
+++ b/vllm/model_executor/models/internvl.py
@@ -9,7 +9,7 @@
 # --------------------------------------------------------
 from abc import ABC, abstractmethod
 from collections.abc import Iterable, Mapping, Sequence
-from typing import Any, Literal, Optional, TypedDict, TypeVar, Union
+from typing import Annotated, Any, Literal, Optional, TypeVar, Union
 
 import numpy.typing as npt
 import torch
@@ -37,6 +37,7 @@ from vllm.multimodal.processing import (BaseMultiModalProcessor,
 from vllm.multimodal.profiling import BaseDummyInputsBuilder
 from vllm.sequence import IntermediateTensors
 from vllm.transformers_utils.tokenizer import AnyTokenizer
+from vllm.utils.tensor_schema import TensorSchema, TensorShape
 
 from .interfaces import (MultiModalEmbeddings, SupportsLoRA,
                          SupportsMultiModal, SupportsPP)
@@ -51,54 +52,60 @@ IMAGENET_MEAN = (0.485, 0.456, 0.406)
 IMAGENET_STD = (0.229, 0.224, 0.225)
 
 
-class InternVLImagePixelInputs(TypedDict):
-    type: Literal["pixel_values"]
-    pixel_values_flat: torch.Tensor
+class InternVLImagePixelInputs(TensorSchema):
     """
-    Shape:
-    `(batch_size * num_images * (1 + num_patches), num_channels, height, width)`
+    Dimensions:
+        - bn: Batch size * number of images
+        - bnp: Batch size * number of images * (1 + num_patches)
+        - c: Number of channels (3)
+        - h: Height of each image patch
+        - w: Width of each image patch
     """
+    type: Literal["pixel_values"]
+    pixel_values_flat: Annotated[torch.Tensor, TensorShape("bnp", 3, "h", "w")]
+    num_patches: Annotated[torch.Tensor, TensorShape("bn")]
 
-    num_patches: torch.Tensor
-    """Shape: `(batch_size * num_images)`"""
-
-
-class InternVLImageEmbeddingInputs(TypedDict):
-    type: Literal["image_embeds"]
-    data: Union[torch.Tensor, list[torch.Tensor]]
-    """ 
-    A tensor of shape `(num_images, total_image_feature_size, hidden_size)`
-    or a list of tensors of shape `(total_image_feature_size, hidden_size)`
 
-    `hidden_size` must match the hidden size of language model backbone.
+class InternVLImageEmbeddingInputs(TensorSchema):
     """
+    Dimensions:
+        - n: Number of images
+        - f: Total image feature size
+        - h: Hidden size (must match the hidden size of language model backbone)
+    """
+    type: Literal["image_embeds"]
+    data: Annotated[Union[torch.Tensor, list[torch.Tensor]],
+                    TensorShape("n", "f", "h")]
 
 
 InternVLImageInputs = Union[InternVLImagePixelInputs,
                             InternVLImageEmbeddingInputs]
 
 
-class InternVLVideoPixelInputs(TypedDict):
-    type: Literal["pixel_values_videos"]
-    pixel_values_flat: torch.Tensor
+class InternVLVideoPixelInputs(TensorSchema):
     """
-    Shape:
-    `(batch_size * num_video * num_frames, num_channels, height, width)`
+    Dimensions:
+        - bvf: Batch size * number of videos * num_frames
+        - bn: Batch size * number of images
+        - c: Number of channels (3)
+        - h: Height of each video frame
+        - w: Width of each video frame
     """
+    type: Literal["pixel_values_videos"]
+    pixel_values_flat: Annotated[torch.Tensor, TensorShape("bvf", 3, "h", "w")]
+    num_patches: Annotated[torch.Tensor, TensorShape("bn")]
 
-    num_patches: torch.Tensor
-    """Shape: `(batch_size * num_images)`"""
-
-
-class InternVLVideoEmbeddingInputs(TypedDict):
-    type: Literal["video_embeds"]
-    data: Union[torch.Tensor, list[torch.Tensor]]
-    """ 
-    A tensor of shape `(num_videos, total_video_feature_size, hidden_size)`
-    or a list of tensors of shape `(total_video_feature_size, hidden_size)`
 
-    `hidden_size` must match the hidden size of language model backbone.
+class InternVLVideoEmbeddingInputs(TensorSchema):
+    """
+    Dimensions:
+        - n: Number of videos
+        - f: Total video feature size
+        - h: Hidden size (must match the hidden size of language model backbone)
     """
+    type: Literal["video_embeds"]
+    data: Annotated[Union[torch.Tensor, list[torch.Tensor]],
+                    TensorShape("n", "f", "h")]
 
 
 InternVLVideoInputs = Union[InternVLVideoPixelInputs,
@@ -658,14 +665,7 @@ class BaseInternVLProcessingInfo(BaseProcessingInfo):
     """Basic image-only ProcessingInfo for InternVL-style models."""
 
     @abstractmethod
-    def get_hf_processor(
-        self,
-        *,
-        min_dynamic_patch: Optional[int] = None,
-        max_dynamic_patch: Optional[int] = None,
-        dynamic_image_size: Optional[bool] = None,
-        **kwargs: object,
-    ) -> BaseInternVLProcessor:
+    def get_hf_processor(self, **kwargs: object) -> BaseInternVLProcessor:
         raise NotImplementedError
 
     def get_supported_mm_limits(self) -> Mapping[str, Optional[int]]:
@@ -854,9 +854,13 @@ class InternVLProcessingInfo(BaseInternVLProcessingInfo):
 
     def get_video_token(self) -> Optional[str]:
         text_model_type = self.get_hf_config().get_text_config().model_type
-        if text_model_type == "qwen2":
-            return "<|video_pad|>"
-        return None
+        video_token_map = {
+            "qwen2": "<|video_pad|>",
+            "qwen3": "<|video_pad|>",
+            "qwen3_moe": "<|video_pad|>",
+            "gpt_oss": "<|reserved_200000|>",
+        }
+        return video_token_map.get(text_model_type)
 
     def get_num_frames_with_most_features(
         self,
@@ -875,27 +879,12 @@ class InternVLProcessingInfo(BaseInternVLProcessingInfo):
 
         return max(max_frames_per_video, 1)
 
-    def get_hf_processor(
-        self,
-        *,
-        min_dynamic_patch: Optional[int] = None,
-        max_dynamic_patch: Optional[int] = None,
-        dynamic_image_size: Optional[bool] = None,
-        **kwargs: object,
-    ) -> InternVLProcessor:
-        if min_dynamic_patch is not None:
-            kwargs["min_dynamic_patch"] = min_dynamic_patch
-        if max_dynamic_patch is not None:
-            kwargs["max_dynamic_patch"] = max_dynamic_patch
-        if dynamic_image_size is not None:
-            kwargs["dynamic_image_size"] = dynamic_image_size
-
-        kwargs["video_token"] = self.get_video_token()
-
+    def get_hf_processor(self, **kwargs: object) -> InternVLProcessor:
         return self.ctx.init_processor(
             InternVLProcessor,
             config=self.get_hf_config(),
             tokenizer=self.get_tokenizer(),
+            video_token=self.get_video_token(),
             **kwargs,
         )
 
@@ -1151,26 +1140,6 @@ class InternVLChatModel(nn.Module, SupportsMultiModal, SupportsPP,
         vit_embeds = self.mlp1(vit_embeds)
         return vit_embeds
 
-    def _validate_pixel_values(self, data: torch.Tensor) -> torch.Tensor:
-
-        h = w = self.config.vision_config.image_size
-        expected_dims = (3, h, w)
-
-        def _validate_shape(d: torch.Tensor):
-            actual_dims = tuple(d.shape)
-
-            if actual_dims != expected_dims:
-                expected_expr = str(expected_dims)
-                raise ValueError(
-                    "The expected shape of pixel values per image per batch "
-                    f" per patch is {expected_expr}. "
-                    f"You supplied {tuple(d.shape)}.")
-
-        for d in data:
-            _validate_shape(d)
-
-        return data
-
     def _parse_and_validate_image_input(
             self, **kwargs: object) -> Optional[InternVLImageInputs]:
         pixel_values_flat = kwargs.pop("pixel_values_flat", None)
@@ -1205,12 +1174,14 @@ class InternVLChatModel(nn.Module, SupportsMultiModal, SupportsPP,
 
             pixel_values_flat = flatten_bn(pixel_values_flat, concat=True)
             image_num_patches = flatten_bn(image_num_patches, concat=True)
+            expected_h = expected_w = self.config.vision_config.image_size
+            resolve_bindings = {"h": expected_h, "w": expected_w}
 
             return InternVLImagePixelInputs(
                 type="pixel_values",
-                pixel_values_flat=self._validate_pixel_values(
-                    pixel_values_flat),
+                pixel_values_flat=pixel_values_flat,
                 num_patches=image_num_patches,
+                resolve_bindings=resolve_bindings,
             )
 
         raise AssertionError("This line should be unreachable.")
@@ -1225,11 +1196,7 @@ class InternVLChatModel(nn.Module, SupportsMultiModal, SupportsPP,
             return None
 
         if video_embeds is not None:
-            if not isinstance(video_embeds, (torch.Tensor, list)):
-                raise ValueError("Incorrect type of video embeddings. "
-                                 f"Got type: {type(video_embeds)}")
-
-            return InternVLImageEmbeddingInputs(
+            return InternVLVideoEmbeddingInputs(
                 type="video_embeds",
                 data=flatten_bn(video_embeds),
             )
@@ -1250,12 +1217,14 @@ class InternVLChatModel(nn.Module, SupportsMultiModal, SupportsPP,
             pixel_values_flat_video = flatten_bn(pixel_values_flat_video,
                                                  concat=True)
             video_num_patches = flatten_bn(video_num_patches, concat=True)
+            expected_h = expected_w = self.config.vision_config.image_size
+            resolve_bindings = {"h": expected_h, "w": expected_w}
 
             return InternVLVideoPixelInputs(
                 type="pixel_values_videos",
-                pixel_values_flat=self._validate_pixel_values(
-                    pixel_values_flat_video),
+                pixel_values_flat=pixel_values_flat_video,
                 num_patches=video_num_patches,
+                resolve_bindings=resolve_bindings,
             )
 
         raise AssertionError("This line should be unreachable.")
@@ -1322,7 +1291,6 @@ class InternVLChatModel(nn.Module, SupportsMultiModal, SupportsPP,
         modalities = self._parse_and_validate_multimodal_inputs(**kwargs)
         if not modalities:
             return []
-            return None
 
         # The result multimodal_embeddings is tuple of tensors, with each
         # tensor correspoending to a multimodal data item (image or video).
diff --git a/vllm/model_executor/models/llama.py b/vllm/model_executor/models/llama.py
index 48ec611df..09e6d8af7 100644
--- a/vllm/model_executor/models/llama.py
+++ b/vllm/model_executor/models/llama.py
@@ -437,6 +437,9 @@ class LlamaModel(nn.Module):
             if "scale" in name:
                 # Remapping the name of FP8 kv-scale.
                 name = maybe_remap_kv_scale_name(name, params_dict)
+                # temp fix for unit scale INC model, will can be removed
+                if "proj.scale" in name and not "scales" in name:
+                    name = name.replace("scale", "weight_scale")
                 if name is None:
                     continue
             for param_name, weight_name, shard_id in stacked_params_mapping:
diff --git a/vllm/model_executor/models/minicpmv.py b/vllm/model_executor/models/minicpmv.py
index 70f2d4a64..816ea00d2 100644
--- a/vllm/model_executor/models/minicpmv.py
+++ b/vllm/model_executor/models/minicpmv.py
@@ -27,17 +27,21 @@ import math
 from collections import defaultdict
 from collections.abc import Iterable, Mapping, Sequence
 from functools import partial
+from itertools import chain
 from typing import Any, Callable, Literal, Optional, TypedDict, Union
 
 import numpy as np
 import torch
 import torch.types
 from torch import nn
+from torch.nn.init import trunc_normal_
 from transformers import BatchFeature, PretrainedConfig
 from typing_extensions import TypeVar
 
 from vllm.config import VllmConfig
 from vllm.model_executor.layers.quantization import QuantizationConfig
+from vllm.model_executor.layers.quantization.awq import AWQConfig
+from vllm.model_executor.layers.quantization.awq_marlin import AWQMarlinConfig
 from vllm.model_executor.layers.resampler import (BaseResampler, Resampler2,
                                                   get_2d_sincos_pos_embed)
 from vllm.model_executor.model_loader.utils import set_default_torch_dtype
@@ -45,6 +49,7 @@ from vllm.model_executor.models.llama import LlamaForCausalLM
 from vllm.model_executor.models.minicpm import MiniCPMForCausalLM
 from vllm.model_executor.models.module_mapping import MultiModelKeys
 from vllm.model_executor.models.qwen2 import Qwen2ForCausalLM
+from vllm.model_executor.models.qwen3 import Qwen3ForCausalLM
 from vllm.model_executor.sampling_metadata import SamplingMetadata
 from vllm.multimodal import MULTIMODAL_REGISTRY, MultiModalKwargs
 from vllm.multimodal.inputs import (MultiModalDataDict, MultiModalFieldConfig,
@@ -203,6 +208,187 @@ class Resampler2_5(BaseResampler):
         return x
 
 
+class Resampler4_5(Resampler2_5):
+
+    def __init__(self,
+                 num_queries: int,
+                 embed_dim: int,
+                 num_heads: int,
+                 kv_dim: Optional[int] = None,
+                 norm_layer: Callable[[int], nn.LayerNorm] = DEFAULT_LN,
+                 max_size: tuple[int, int] = (70, 70),
+                 max_temporal_size: int = 36000,
+                 quant_config: Optional[QuantizationConfig] = None,
+                 prefix: str = "") -> None:
+        super().__init__(num_queries,
+                         embed_dim,
+                         num_heads,
+                         kv_dim,
+                         norm_layer,
+                         max_size,
+                         quant_config=quant_config,
+                         prefix=prefix)
+
+        trunc_normal_(self.query, std=.02)
+        self.max_temporal_size = max_temporal_size
+        self._set_temporal_pos_cache(self.max_temporal_size)
+        self.apply(self._init_weights)
+
+    def get_1d_sincos_pos_embed_from_temporal_size(self, embed_dim: int,
+                                                   pos: np.ndarray):
+        """
+        embed_dim: output dimension for each position
+        pos: a list of positions to be encoded: size (M,)
+        out: (M, D)
+        """
+        assert embed_dim % 2 == 0
+        omega = np.arange(embed_dim // 2, dtype=np.float32)
+        omega /= embed_dim / 2.
+        omega = 1. / 10000**omega  # (D/2,)
+
+        pos = pos.reshape(-1)  # (M,)
+        out = np.einsum('m,d->md', pos, omega)  # (M, D/2), outer product
+
+        emb_sin = np.sin(out)  # (M, D/2)
+        emb_cos = np.cos(out)  # (M, D/2)
+
+        emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)
+        return emb
+
+    def _set_temporal_pos_cache(self,
+                                max_temporal_size: int,
+                                device: torch.types.Device = "cpu") -> None:
+        temporal_size = np.arange(max_temporal_size, dtype=np.float32)
+        pos_embed = torch.from_numpy(
+            self.get_1d_sincos_pos_embed_from_temporal_size(
+                self.embed_dim, temporal_size)).float().to(device)
+        self.register_buffer("temporal_pos_embed", pos_embed, persistent=False)
+
+    def _adjust_temporal_pos_cache(self,
+                                   max_temporal_size: int,
+                                   device: torch.types.Device = "cpu"):
+        if max_temporal_size > self.max_temporal_size:
+            self.max_temporal_size = max_temporal_size
+            self._set_temporal_pos_cache(self.max_temporal_size, device)
+
+    def _init_weights(self, m: Union[nn.Linear, nn.LayerNorm]):
+        if isinstance(m, nn.Linear):
+            trunc_normal_(m.weight, std=.02)
+            if isinstance(m, nn.Linear) and m.bias is not None:
+                nn.init.constant_(m.bias, 0)
+        elif isinstance(m, nn.LayerNorm):
+            nn.init.constant_(m.bias, 0)
+            nn.init.constant_(m.weight, 1.0)
+
+    def forward(
+        self,
+        x: torch.Tensor,
+        tgt_sizes: torch.Tensor,
+        # temporal_ids for high refresh rate videos
+        temporal_ids=None
+    ) -> torch.Tensor:
+        assert x.shape[0] == tgt_sizes.shape[0]
+        bs = x.shape[0]
+
+        device = x.device
+        dtype = x.dtype
+
+        patch_len = tgt_sizes[:, 0] * tgt_sizes[:, 1]
+
+        self._adjust_pos_cache(tgt_sizes, device=device)
+
+        temporal_pos_emb = False
+        temporal_ids_flatten = None
+        if temporal_ids is not None:
+            # example: [[-1], [-1], [2, 6, 9]]
+            temporal_ids_flatten = list(chain.from_iterable(temporal_ids))
+            max_temporal_size = max(temporal_ids_flatten, default=0)
+            if max_temporal_size > -1:
+                temporal_pos_emb = True
+            if max_temporal_size > self.max_temporal_size:
+                self._adjust_temporal_pos_cache(max_temporal_size, device)
+
+        max_patch_len = patch_len.max().item()
+        assert isinstance(max_patch_len, int)
+
+        key_padding_mask = torch.zeros((bs, max_patch_len),
+                                       dtype=torch.bool,
+                                       device=device)
+
+        x, _ = self.kv_proj(x)  # B * L * D
+        x = self.ln_kv(x).permute(1, 0, 2)  # L * B * D
+        q = self.ln_q(self.query)  # Q * D
+
+        pos_embed_2d = []
+        pos_embed_temporal = []
+        for i in range(bs):
+            tgt_h, tgt_w = tgt_sizes[i]
+            if temporal_pos_emb:
+                if temporal_ids_flatten[i] == -1:
+                    pos_embed_temporal.append(
+                        torch.zeros(self.embed_dim, dtype=dtype,
+                                    device=device))
+                else:
+                    pos_embed_temporal.append(self.temporal_pos_embed[
+                        temporal_ids_flatten[i]].to(dtype))  # D
+
+            pos_embed_2d.append(self.pos_embed[:tgt_h, :tgt_w, :].reshape(
+                (tgt_h * tgt_w, -1)).to(dtype))  # patches * D
+            key_padding_mask[i, patch_len[i]:] = True
+
+        pos_embed_2d = torch.nn.utils.rnn.pad_sequence(
+            pos_embed_2d, batch_first=True,
+            padding_value=0.0).permute(1, 0, 2)  # BLD => L * B * D
+
+        k = x
+        v = x + pos_embed_2d
+        if pos_embed_temporal:
+            k += torch.stack(pos_embed_temporal, dim=0)
+            bs = len(temporal_ids)
+            merge_k = []
+            merge_v = []
+            merge_key_padding_mask = []
+
+            start = 0
+            for tp in temporal_ids:
+                end = start + len(tp)
+                # L * (end-start) * D -> (end-start) * L * D
+                # -> 1 * L*(end-start) * D
+                merge_k.append(k[:, start:end, :].permute(1, 0, 2).reshape(
+                    -1, self.embed_dim))
+                merge_v.append(v[:, start:end, :].permute(1, 0, 2).reshape(
+                    -1, self.embed_dim))
+                merge_key_padding_mask.append(
+                    key_padding_mask[start:end, :].reshape(-1, 1))
+
+                start = end
+
+            k = torch.nn.utils.rnn.pad_sequence(merge_k,
+                                                batch_first=True,
+                                                padding_value=0.0).permute(
+                                                    1, 0, 2)  # L*(end-start)
+            v = torch.nn.utils.rnn.pad_sequence(merge_v,
+                                                batch_first=True,
+                                                padding_value=0.0).permute(
+                                                    1, 0, 2)  # L*(end-start)
+            key_padding_mask = torch.nn.utils.rnn.pad_sequence(
+                merge_key_padding_mask, batch_first=True,
+                padding_value=True).squeeze(-1)
+
+        out = self.attn(
+            self._repeat(q, bs),  # Q * B * D
+            k,  # L * B * D +  L * B * D
+            v,
+            key_padding_mask=key_padding_mask,
+        )[0]
+        #  out: Q * B * D
+        x = out.permute(1, 0, 2)  # B * Q * D
+
+        x = self.ln_post(x)
+        x = x @ self.proj
+        return x
+
+
 def get_version_by_config(config: PretrainedConfig) -> tuple[int, ...]:
     version_float = getattr(config, "version", None)
 
@@ -331,17 +517,15 @@ class MiniCPMVProcessingInfo(BaseProcessingInfo):
 
         return hf_processor
 
-    def get_image_processor(self):
-        hf_processor = self.get_hf_processor()
-        image_processor = hf_processor.image_processor  # type: ignore
-        return image_processor
+    def get_image_processor(self, **kwargs: object):
+        return self.get_hf_processor(**kwargs).image_processor
 
     def get_model_version(self):
         return get_version_by_config(self.get_hf_config())
 
     def get_supported_mm_limits(self) -> Mapping[str, Optional[int]]:
         mm_limits = {"image": None}
-        if self.get_model_version() == (2, 6):
+        if self.get_model_version() in {(2, 6), (4, 0), (4, 5)}:
             mm_limits["video"] = None
 
         return mm_limits
@@ -622,7 +806,7 @@ class MiniCPMVMultiModalProcessor(BaseMultiModalProcessor[_I]):
         out_keys: set[str],
     ) -> dict[str, NestedTensors]:
         # This processor supports zipping prompt and mm_data together
-        if self.info.get_model_version() == (2, 6):
+        if self.info.get_model_version() in {(2, 6), (4, 0), (4, 5)}:
             inputs = super()._call_hf_processor(
                 prompt=prompts,  # type: ignore
                 mm_data=mm_data,
@@ -681,10 +865,18 @@ class MiniCPMVMultiModalProcessor(BaseMultiModalProcessor[_I]):
         hf_processor_mm_kwargs: Mapping[str, object],
         out_mm_kwargs: MultiModalKwargs,
     ) -> Sequence[PromptUpdate]:
-        placeholder = {
-            "image": self.info.image_pattern,
-            "video": self.info.video_pattern,
-        }
+        placeholders = [("image", self.info.image_pattern),
+                        ("video", self.info.video_pattern)]
+
+        # hard code for inconsistency of encode-decode image_pattern
+        additional_placeholders = []
+        tokenizer = self.info.get_tokenizer()
+        for modality, pattern in placeholders:
+            sub_pattern = tokenizer.decode(
+                tokenizer.encode(pattern, add_special_tokens=False))
+            if sub_pattern != pattern:
+                additional_placeholders.append((modality, sub_pattern))
+        placeholders += additional_placeholders
 
         def get_image_replacement(item_idx: int):
             images = mm_items.get_items(
@@ -716,9 +908,9 @@ class MiniCPMVMultiModalProcessor(BaseMultiModalProcessor[_I]):
 
         return [
             PromptReplacement(modality=modality,
-                              target=placeholder[modality],
+                              target=pattern,
                               replacement=get_replacement[modality])
-            for modality in ("image", "video")
+            for modality, pattern in placeholders
         ]
 
     def _get_mm_fields_config(
@@ -1264,11 +1456,234 @@ class MiniCPMV2_6(MiniCPMVBaseModel, SupportsLoRA):
 
         return self.resampler(vision_embedding, tgt_sizes)
 
+    def load_weights(self, weights: Iterable[tuple[str,
+                                                   torch.Tensor]]) -> set[str]:
+        loader = AutoWeightsLoader(self,
+                                   skip_prefixes=["apm.", "audio", "tts"])
+        return loader.load_weights(weights)
+
+
+class MiniCPMV4_0(MiniCPMVBaseModel, SupportsLoRA):
+    packed_modules_mapping = {
+        "qkv_proj": [
+            "q_proj",
+            "k_proj",
+            "v_proj",
+        ],
+        "gate_up_proj": [
+            "gate_proj",
+            "up_proj",
+        ],
+    }
+
+    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
+        super().__init__(vllm_config=vllm_config, prefix=prefix)
+        assert self.version == (4, 0)
+
+    def _maybe_ignore_quant_config(self, quant_config: QuantizationConfig):
+        if isinstance(quant_config, (AWQConfig, AWQMarlinConfig)):
+            return None
+        return quant_config
+
+    def init_llm(
+        self,
+        vllm_config: VllmConfig,
+        prefix: str = "",
+    ) -> nn.Module:
+        return LlamaForCausalLM(vllm_config=vllm_config, prefix=prefix)
+
+    def init_vision_module(
+        self,
+        config: PretrainedConfig,
+        quant_config: Optional[QuantizationConfig] = None,
+        prefix: str = "",
+    ) -> nn.Module:
+        quant_config = self._maybe_ignore_quant_config(quant_config)
+        model = Idefics2VisionTransformer(config.vision_config,
+                                          quant_config=quant_config,
+                                          prefix=prefix)
+        if self.config.drop_vision_last_layer:
+            model.encoder.layers = model.encoder.layers[:-1]
+        return model
+
+    def init_resampler(
+        self,
+        embed_dim: int,
+        vision_dim: int,
+        quant_config: Optional[QuantizationConfig] = None,
+        prefix: str = "",
+    ) -> nn.Module:
+        quant_config = self._maybe_ignore_quant_config(quant_config)
+        with set_default_torch_dtype(torch.float16):
+            # The resampler in 4.0 remains consistent with the one in 2.5/2.6.
+            resampler = Resampler2_5(num_queries=self.config.query_num,
+                                     embed_dim=embed_dim,
+                                     num_heads=embed_dim // 128,
+                                     kv_dim=vision_dim,
+                                     quant_config=quant_config,
+                                     prefix=prefix)
+
+        return resampler.to(device=current_platform.device_type,
+                            dtype=torch.get_default_dtype())
+
+    def get_vision_hidden_states(
+            self, data: MiniCPMVImagePixelInputs) -> torch.Tensor:
+        pixel_values = data["pixel_values"]
+        tgt_sizes = data["tgt_sizes"]
+
+        B = len(pixel_values)
+        P = pixel_values[0].shape[-2]
+        L = max(item.shape[-1] for item in pixel_values)
+        device = pixel_values[0].device
+        dtype = pixel_values[0].dtype
+
+        all_pixel_values = torch.zeros((B, 3, P, L),
+                                       dtype=dtype,
+                                       device=device)
+        for i, pixel_values_item in enumerate(pixel_values):
+            L_item = pixel_values_item.shape[-1]
+            all_pixel_values[i, ..., :L_item] = pixel_values_item
+
+        num_patches = tgt_sizes.prod(-1)
+        max_patches = num_patches.max().item()
+        assert isinstance(max_patches, int)
+
+        patch_attn_mask = torch.zeros((B, max_patches),
+                                      dtype=torch.bool,
+                                      device=device)
+        for i, num_patches_item in enumerate(num_patches):
+            patch_attn_mask[i, :num_patches_item] = True
+
+        vision_embedding = self.vpm(
+            all_pixel_values,
+            patch_attention_mask=patch_attn_mask.unsqueeze(1),
+            tgt_sizes=tgt_sizes,
+        )
+
+        return self.resampler(vision_embedding, tgt_sizes)
+
+    def load_weights(self, weights: Iterable[tuple[str,
+                                                   torch.Tensor]]) -> set[str]:
+        loader = AutoWeightsLoader(self,
+                                   skip_prefixes=["apm.", "audio", "tts"])
+        return loader.load_weights(weights)
+
+
+class MiniCPMV4_5(MiniCPMVBaseModel, SupportsLoRA):
+    packed_modules_mapping = {
+        "qkv_proj": [
+            "q_proj",
+            "k_proj",
+            "v_proj",
+        ],
+        "gate_up_proj": [
+            "gate_proj",
+            "up_proj",
+        ],
+    }
+
+    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
+        super().__init__(vllm_config=vllm_config, prefix=prefix)
+        assert self.version == (4, 5)
+
+    def _maybe_ignore_quant_config(self, quant_config: QuantizationConfig):
+        if isinstance(quant_config, (AWQConfig, AWQMarlinConfig)):
+            return None
+        return quant_config
+
+    def init_llm(
+        self,
+        vllm_config: VllmConfig,
+        prefix: str = "",
+    ) -> nn.Module:
+        return Qwen3ForCausalLM(vllm_config=vllm_config, prefix=prefix)
+
+    def init_vision_module(
+        self,
+        config: PretrainedConfig,
+        quant_config: Optional[QuantizationConfig] = None,
+        prefix: str = "",
+    ) -> nn.Module:
+        quant_config = self._maybe_ignore_quant_config(quant_config)
+        model = Idefics2VisionTransformer(config.vision_config,
+                                          quant_config=quant_config,
+                                          prefix=prefix)
+        if self.config.drop_vision_last_layer:
+            model.encoder.layers = model.encoder.layers[:-1]
+        return model
+
+    def init_resampler(
+        self,
+        embed_dim: int,
+        vision_dim: int,
+        quant_config: Optional[QuantizationConfig] = None,
+        prefix: str = "",
+    ) -> nn.Module:
+        quant_config = self._maybe_ignore_quant_config(quant_config)
+        with set_default_torch_dtype(torch.float16):
+            # The resampler in 4.0 remains consistent with the one in 2.5/2.6.
+            resampler = Resampler4_5(num_queries=self.config.query_num,
+                                     embed_dim=embed_dim,
+                                     num_heads=embed_dim // 128,
+                                     kv_dim=vision_dim,
+                                     quant_config=quant_config,
+                                     prefix=prefix)
+
+        return resampler.to(device=current_platform.device_type,
+                            dtype=torch.get_default_dtype())
+
+    def get_vision_hidden_states(
+            self, data: MiniCPMVImagePixelInputs) -> torch.Tensor:
+        pixel_values = data["pixel_values"]
+        tgt_sizes = data["tgt_sizes"]
+        temporal_ids = data.get('temporal_ids', None)
+
+        B = len(pixel_values)
+        P = pixel_values[0].shape[-2]
+        L = max(item.shape[-1] for item in pixel_values)
+        device = pixel_values[0].device
+        dtype = pixel_values[0].dtype
+
+        all_pixel_values = torch.zeros((B, 3, P, L),
+                                       dtype=dtype,
+                                       device=device)
+        all_temporal_ids = None if temporal_ids is None else flatten_2d_lists(
+            temporal_ids)
+        for i, pixel_values_item in enumerate(pixel_values):
+            L_item = pixel_values_item.shape[-1]
+            all_pixel_values[i, ..., :L_item] = pixel_values_item
+
+        num_patches = tgt_sizes.prod(-1)
+        max_patches = num_patches.max().item()
+        assert isinstance(max_patches, int)
+
+        patch_attn_mask = torch.zeros((B, max_patches),
+                                      dtype=torch.bool,
+                                      device=device)
+        for i, num_patches_item in enumerate(num_patches):
+            patch_attn_mask[i, :num_patches_item] = True
+
+        vision_embedding = self.vpm(
+            all_pixel_values,
+            patch_attention_mask=patch_attn_mask.unsqueeze(1),
+            tgt_sizes=tgt_sizes,
+        )
+
+        return self.resampler(vision_embedding, tgt_sizes, all_temporal_ids)
+
+    def load_weights(self, weights: Iterable[tuple[str,
+                                                   torch.Tensor]]) -> set[str]:
+        loader = AutoWeightsLoader(self,
+                                   skip_prefixes=["apm.", "audio", "tts"])
+        return loader.load_weights(weights)
+
 
 _SUPPORT_VERSION = {
     (2, 0): MiniCPMV2_0,
     (2, 5): MiniCPMV2_5,
     (2, 6): MiniCPMV2_6,
+    (4, 0): MiniCPMV4_0,
+    (4, 5): MiniCPMV4_5,
 }
 
 
@@ -1296,8 +1711,10 @@ class MiniCPMV(MiniCPMVBaseModel, SupportsMultiModal, SupportsLoRA):
         # Dispatch class based on version
         instance_cls = _SUPPORT_VERSION.get(version)
         if instance_cls is None:
-            raise ValueError(
-                "Currently, MiniCPMV only supports versions 2.0, 2.5, and 2.6")
+            supported_versions = ", ".join(
+                [f"{v[0]}.{v[1]}" for v in sorted(_SUPPORT_VERSION.keys())])
+            raise ValueError(f"Currently, MiniCPMV only supports versions "
+                             f"{supported_versions}. Got version: {version}")
 
         # quant_config references base class members,
         # so update values before init is called
diff --git a/vllm/model_executor/models/phi4mm_audio.py b/vllm/model_executor/models/phi4mm_audio.py
index 0b0d66ae7..ab54409e5 100644
--- a/vllm/model_executor/models/phi4mm_audio.py
+++ b/vllm/model_executor/models/phi4mm_audio.py
@@ -535,9 +535,10 @@ class TransformerEncoderBase(abc.ABC, nn.Module):
                                                   self.chunk_size,
                                                   self.left_chunk)
 
-        if xs_pad.is_cuda:
-            enc_streaming_mask = enc_streaming_mask.cuda()
-            xs_pad = xs_pad.cuda()
+        device = xs_pad.device
+        if device.type != "cpu":
+            enc_streaming_mask = enc_streaming_mask.to(device)
+            xs_pad = xs_pad.to(device)
 
         input_tensor = xs_pad
         input_tensor, masks = self._forward_embeddings_core(
@@ -554,8 +555,8 @@ class TransformerEncoderBase(abc.ABC, nn.Module):
         if chunk_size_nc is not None:
             enc_streaming_mask_nc = self._streaming_mask(
                 seq_len, batch_size, chunk_size_nc, left_chunk_nc)
-            if xs_pad.is_cuda:
-                enc_streaming_mask_nc = enc_streaming_mask_nc.cuda()
+            if device.type != "cpu":
+                enc_streaming_mask_nc = enc_streaming_mask_nc.to(device)
             if masks is not None:
                 hs_mask_nc = masks & enc_streaming_mask_nc
             else:
diff --git a/vllm/model_executor/models/qwen2_5_vl.py b/vllm/model_executor/models/qwen2_5_vl.py
index 8ae096536..472772ce6 100644
--- a/vllm/model_executor/models/qwen2_5_vl.py
+++ b/vllm/model_executor/models/qwen2_5_vl.py
@@ -249,8 +249,9 @@ class Qwen2_5_VisionAttention(nn.Module):
 
         # Detect attention implementation.
         self.attn_backend: _Backend = get_vit_attn_backend(support_fa=True)
+        self.attn_backend = _Backend.TORCH_SDPA
         if self.attn_backend not in {
-                _Backend.FLASH_ATTN, _Backend.TORCH_SDPA, _Backend.XFORMERS
+                _Backend.FLASH_ATTN, _Backend.TORCH_SDPA, _Backend.XFORMERS, _Backend.IPEX
         }:
             raise RuntimeError(
                 f"Qwen2.5-VL does not support {self.attn_backend} backend now."
@@ -321,24 +322,86 @@ class Qwen2_5_VisionAttention(nn.Module):
             context_layer = rearrange(output,
                                       "(b s) ... -> b s ...",
                                       b=batch_size)
+        elif self.attn_backend == _Backend.IPEX:
+            from vllm._ipex_ops import ipex_ops
+
+            q, k, v = (rearrange(x, "b s ... -> (b s) ...") for x in [q, k, v])
+
+            output = torch.empty(
+                q.shape,
+                dtype=q.dtype,
+                device=q.device)
+            ipex_ops.varlen_attention(
+                    q,
+                    k,
+                    v,
+                    output,
+                    cu_seqlens,
+                    cu_seqlens,
+                    None,
+                    max_seqlen,
+                    max_seqlen,
+                    pdropout=0.0,
+                    softmax_scale=1.0/(q.shape[-1] ** 0.5),
+                    zero_tensors=False,
+                    is_causal=True,
+                    return_softmax=False,
+                    gen_=None,
+                    window_size_left=-1,
+                    window_size_right=-1,
+                    logits_soft_cap=-1,
+            )
+            context_layer = rearrange(output,
+                            "(b s) ... -> b s ...",
+                            b=batch_size)
         elif self.attn_backend == _Backend.TORCH_SDPA:
             # Execute attention entry by entry for speed & less VRAM.
-            outputs = []
-            for i in range(1, len(cu_seqlens)):
-                start_idx = cu_seqlens[i - 1]
-                end_idx = cu_seqlens[i]
-                q_i = q[:, start_idx:end_idx]
-                k_i = k[:, start_idx:end_idx]
-                v_i = v[:, start_idx:end_idx]
-                q_i, k_i, v_i = (rearrange(x, "b s h d -> b h s d")
-                                 for x in [q_i, k_i, v_i])
-                output_i = F.scaled_dot_product_attention(q_i,
-                                                          k_i,
-                                                          v_i,
-                                                          dropout_p=0.0)
-                output_i = rearrange(output_i, "b h s d -> b s h d ")
-                outputs.append(output_i)
-            context_layer = torch.cat(outputs, dim=1)
+            # TODO(xiangyu): Maybe add attn_backend xpu?
+            q, k, v = (rearrange(x, "b s ... -> (b s) ...") for x in [q, k, v])
+            from vllm._ipex_ops import ipex_ops
+            output = torch.empty(
+                        (q.shape[0], q.shape[1], q.shape[2]),
+                        dtype=q.dtype,
+                        device=q.device)
+            import math
+            head_dim = q.shape[-1]
+            scale = 1 / math.sqrt(head_dim)
+            ipex_ops.varlen_attention(q, k, v, output,
+                                    cu_seqlens,
+                                    cu_seqlens,
+                                    None,
+                                    max_seqlen,
+                                    max_seqlen,
+                                    pdropout=0,
+                                    softmax_scale=scale,
+                                    zero_tensors=False,
+                                    is_causal=False,
+                                    return_softmax=False,
+                                    window_size_left=-1,
+                                    window_size_right=-1,
+                                    gen_=None,
+                                    logits_soft_cap=0
+                                    )
+
+            context_layer = rearrange(output,
+                                      "(b s) ... -> b s ...",
+                                      b=batch_size)
+            # outputs = []
+            # for i in range(1, len(cu_seqlens)):
+            #     start_idx = cu_seqlens[i - 1]
+            #     end_idx = cu_seqlens[i]
+            #     q_i = q[:, start_idx:end_idx]
+            #     k_i = k[:, start_idx:end_idx]
+            #     v_i = v[:, start_idx:end_idx]
+            #     q_i, k_i, v_i = (rearrange(x, "b s h d -> b h s d")
+            #                      for x in [q_i, k_i, v_i])
+            #     output_i = F.scaled_dot_product_attention(q_i,
+            #                                               k_i,
+            #                                               v_i,
+            #                                               dropout_p=0.0)
+            #     output_i = rearrange(output_i, "b h s d -> b s h d ")
+            #     outputs.append(output_i)
+            # context_layer = torch.cat(outputs, dim=1)
         elif self.attn_backend == _Backend.XFORMERS:
             from xformers import ops as xops
             from xformers.ops.fmha.attn_bias import BlockDiagonalMask
@@ -643,6 +706,8 @@ class Qwen2_5_VisionTransformer(nn.Module):
             max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()
         elif self.attn_backend == _Backend.XFORMERS:
             seqlens = (cu_seqlens[1:] - cu_seqlens[:-1]).tolist()
+        elif self.attn_backend == _Backend.IPEX:
+            max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()
         return max_seqlen, seqlens
 
     def forward(
@@ -1132,10 +1197,10 @@ class Qwen2_5_VLForConditionalGeneration(nn.Module, SupportsMultiModal,
             if image_input is None and video_input is None:
                 inputs_embeds = None
             else:
-                if uses_mrope(self.config):
-                    assert positions.ndim == 2 and positions.size(0) == 3, (
-                        "multimodal section rotary embedding requires "
-                        f"(3, seq_len) positions, but got {positions.size()}")
+                # if uses_mrope(self.config):
+                #     assert positions.ndim == 2 and positions.size(0) == 3, (
+                #         "multimodal section rotary embedding requires "
+                #         f"(3, seq_len) positions, but got {positions.size()}")
                 inputs_embeds = self.get_input_embeddings_v0(
                     input_ids,
                     image_input=image_input,
diff --git a/vllm/model_executor/models/qwen2_vl.py b/vllm/model_executor/models/qwen2_vl.py
index ad63bb4af..2e22799e1 100644
--- a/vllm/model_executor/models/qwen2_vl.py
+++ b/vllm/model_executor/models/qwen2_vl.py
@@ -275,8 +275,9 @@ class Qwen2VisionAttention(nn.Module):
 
         # Detect attention implementation.
         self.attn_backend: _Backend = get_vit_attn_backend(support_fa=True)
+        self.attn_backend = _Backend.TORCH_SDPA
         if self.attn_backend not in {
-                _Backend.FLASH_ATTN, _Backend.TORCH_SDPA, _Backend.XFORMERS
+                _Backend.FLASH_ATTN, _Backend.TORCH_SDPA, _Backend.XFORMERS, _Backend.IPEX
         }:
             raise RuntimeError(
                 f"Qwen2-VL does not support {self.attn_backend} backend now.")
@@ -346,24 +347,69 @@ class Qwen2VisionAttention(nn.Module):
             context_layer = rearrange(output,
                                       "(b s) ... -> b s ...",
                                       b=batch_size)
+        elif self.attn_backend == _Backend.IPEX:
+            from vllm._ipex_ops import ipex_ops
+
+            q, k, v = (rearrange(x, "b s ... -> (b s) ...") for x in [q, k, v])
+
+            output = torch.empty(
+                q.shape,
+                dtype=q.dtype,
+                device=q.device)
+            ipex_ops.varlen_attention(
+                    q,
+                    k,
+                    v,
+                    output,
+                    cu_seqlens,
+                    cu_seqlens,
+                    None,
+                    max_seqlen,
+                    max_seqlen,
+                    pdropout=0.0,
+                    softmax_scale=1.0/(q.shape[-1] ** 0.5),
+                    zero_tensors=False,
+                    is_causal=True,
+                    return_softmax=False,
+                    gen_=None,
+                    window_size_left=-1,
+                    window_size_right=-1,
+                    logits_soft_cap=-1,
+            )
+            context_layer = rearrange(output,
+                            "(b s) ... -> b s ...",
+                            b=batch_size)
         elif self.attn_backend == _Backend.TORCH_SDPA:
             # Execute attention entry by entry for speed & less VRAM.
-            outputs = []
-            for i in range(1, len(cu_seqlens)):
-                start_idx = cu_seqlens[i - 1]
-                end_idx = cu_seqlens[i]
-                q_i = q[:, start_idx:end_idx]
-                k_i = k[:, start_idx:end_idx]
-                v_i = v[:, start_idx:end_idx]
-                q_i, k_i, v_i = (rearrange(x, "b s h d -> b h s d")
-                                 for x in [q_i, k_i, v_i])
-                output_i = F.scaled_dot_product_attention(q_i,
-                                                          k_i,
-                                                          v_i,
-                                                          dropout_p=0.0)
-                output_i = rearrange(output_i, "b h s d -> b s h d ")
-                outputs.append(output_i)
-            context_layer = torch.cat(outputs, dim=1)
+            q, k, v = (rearrange(x, "b s ... -> (b s) ...") for x in [q, k, v])
+            from vllm._ipex_ops import ipex_ops
+            output = torch.empty(
+                        (q.shape[0], q.shape[1], q.shape[2]),
+                        dtype=q.dtype,
+                        device=q.device)
+            import math
+            head_dim = q.shape[-1]
+            scale = 1 / math.sqrt(head_dim)
+            ipex_ops.varlen_attention(q, k, v, output,
+                                    cu_seqlens,
+                                    cu_seqlens,
+                                    None,
+                                    max_seqlen,
+                                    max_seqlen,
+                                    pdropout=0,
+                                    softmax_scale=scale,
+                                    zero_tensors=False,
+                                    is_causal=False,
+                                    return_softmax=False,
+                                    window_size_left=-1,
+                                    window_size_right=-1,
+                                    gen_=None,
+                                    logits_soft_cap=0
+                                    )
+
+            context_layer = rearrange(output,
+                                    "(b s) ... -> b s ...",
+                                    b=batch_size)
         elif self.attn_backend == _Backend.XFORMERS:
             from xformers import ops as xops
             from xformers.ops.fmha.attn_bias import BlockDiagonalMask
@@ -626,6 +672,8 @@ class Qwen2VisionTransformer(nn.Module):
             max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()
         elif self.attn_backend == _Backend.XFORMERS:
             seqlens = (cu_seqlens[1:] - cu_seqlens[:-1]).tolist()
+        elif self.attn_backend == _Backend.IPEX:
+            max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()
         return max_seqlen, seqlens
 
     def forward(
diff --git a/vllm/model_executor/models/qwen3.py b/vllm/model_executor/models/qwen3.py
index 393ce41a9..550c5b273 100644
--- a/vllm/model_executor/models/qwen3.py
+++ b/vllm/model_executor/models/qwen3.py
@@ -135,11 +135,11 @@ class Qwen3Attention(nn.Module):
         # Add qk-norm
         q_by_head = q.view(*q.shape[:-1], q.shape[-1] // self.head_dim,
                            self.head_dim)
-        q_by_head = self.q_norm(q_by_head)
+        q_by_head = self.q_norm.forward(q_by_head.contiguous())
         q = q_by_head.view(q.shape)
         k_by_head = k.view(*k.shape[:-1], k.shape[-1] // self.head_dim,
                            self.head_dim)
-        k_by_head = self.k_norm(k_by_head)
+        k_by_head = self.k_norm.forward(k_by_head.contiguous())
         k = k_by_head.view(k.shape)
         q, k = self.rotary_emb(positions, q, k)
         attn_output = self.attn(q, k, v)
diff --git a/vllm/model_executor/models/qwen3_moe.py b/vllm/model_executor/models/qwen3_moe.py
index 12899c280..951215ee0 100644
--- a/vllm/model_executor/models/qwen3_moe.py
+++ b/vllm/model_executor/models/qwen3_moe.py
@@ -225,12 +225,12 @@ class Qwen3MoeAttention(nn.Module):
         # Add qk-norm
         q_by_head = q.view(*q.shape[:-1], q.shape[-1] // self.head_dim,
                            self.head_dim)
-        q_by_head = self.q_norm(q_by_head)
+        q_by_head = self.q_norm.forward(q_by_head.contiguous())
         q = q_by_head.view(q.shape)
 
         k_by_head = k.view(*k.shape[:-1], k.shape[-1] // self.head_dim,
                            self.head_dim)
-        k_by_head = self.k_norm(k_by_head)
+        k_by_head = self.k_norm.forward(k_by_head.contiguous())
         k = k_by_head.view(k.shape)
         q, k = self.rotary_emb(positions, q, k)
         attn_output = self.attn(q, k, v)
diff --git a/vllm/model_executor/models/registry.py b/vllm/model_executor/models/registry.py
index 2aaac7798..6a832ca27 100644
--- a/vllm/model_executor/models/registry.py
+++ b/vllm/model_executor/models/registry.py
@@ -122,11 +122,13 @@ _TEXT_GENERATION_MODELS = {
     "Qwen3ForCausalLM": ("qwen3", "Qwen3ForCausalLM"),
     "Qwen3MoeForCausalLM": ("qwen3_moe", "Qwen3MoeForCausalLM"),
     "RWForCausalLM": ("falcon", "FalconForCausalLM"),
+    "SeedOssForCausalLM": ("seed_oss", "SeedOssForCausalLM"),
     "StableLMEpochForCausalLM": ("stablelm", "StablelmForCausalLM"),
     "StableLmForCausalLM": ("stablelm", "StablelmForCausalLM"),
     "Starcoder2ForCausalLM": ("starcoder2", "Starcoder2ForCausalLM"),
     "SolarForCausalLM": ("solar", "SolarForCausalLM"),
     "TeleChat2ForCausalLM": ("telechat2", "TeleChat2ForCausalLM"),
+    "TeleChatForCausalLM": ("telechat2", "TeleChat2ForCausalLM"),
     "TeleFLMForCausalLM": ("teleflm", "TeleFLMForCausalLM"),
     "XverseForCausalLM": ("llama", "LlamaForCausalLM"),
     "Zamba2ForCausalLM": ("zamba2", "Zamba2ForCausalLM"),
diff --git a/vllm/model_executor/models/roberta.py b/vllm/model_executor/models/roberta.py
index c6b411644..feb549d44 100644
--- a/vllm/model_executor/models/roberta.py
+++ b/vllm/model_executor/models/roberta.py
@@ -9,6 +9,7 @@ from torch import nn
 from transformers import RobertaConfig
 
 from vllm.config import VllmConfig
+from vllm.forward_context import get_forward_context
 from vllm.model_executor.layers.pooler import (ClassifierPooler, CLSPool,
                                                DispatchPooler, Pooler)
 from vllm.model_executor.layers.vocab_parallel_embedding import (
@@ -19,7 +20,7 @@ from vllm.model_executor.models.utils import (AutoWeightsLoader, WeightsMapper,
 from vllm.sequence import IntermediateTensors
 
 from .bert_with_rope import BertWithRope, JinaRobertaModel
-from .interfaces import SupportsCrossEncoding, SupportsV0Only
+from .interfaces import SupportsCrossEncoding
 
 
 class RobertaEmbedding(nn.Module):
@@ -51,33 +52,12 @@ class RobertaEmbedding(nn.Module):
     def forward(
         self,
         input_ids: torch.Tensor,
-        seq_lens: torch.Tensor,
         position_ids: torch.Tensor,
         token_type_ids: Optional[torch.Tensor] = None,
     ) -> torch.Tensor:
         input_shape = input_ids.size()
         inputs_embeds = self.word_embeddings(input_ids)
 
-        # Replace position ids because in RoBERTa models
-        # they have to start at padding_idx + 1 and ignore
-        # existing padding tokens
-        # References:
-        # - https://github.com/huggingface/transformers/blob/a3d69a8994d673899608a7c17fbf4f953f50474e/src/transformers/models/roberta/modeling_roberta.py#L133
-        # - https://github.com/huggingface/transformers/blob/a3d69a8994d673899608a7c17fbf4f953f50474e/src/transformers/models/roberta/modeling_roberta.py#L1669
-        seq_lens_list = seq_lens.tolist()
-        new_pos_list = []
-        for positions, tokens in zip(position_ids.split(seq_lens_list),
-                                     input_ids.split(seq_lens_list)):
-            # Verify assumption that incoming position are
-            # always a sequence from 0 to N.
-            expected_pos = torch.arange(positions.size()[0],
-                                        dtype=torch.long,
-                                        device=inputs_embeds.device)
-            assert torch.equal(positions, expected_pos)
-            new_pos_list.append(
-                create_position_ids_from_input_ids(tokens, self.padding_idx))
-        position_ids = torch.cat(new_pos_list)
-
         # Position embeddings.
         position_embeddings = self.position_embeddings(position_ids)
         if token_type_ids is None:
@@ -119,6 +99,32 @@ class RobertaEmbeddingModel(BertEmbeddingModel):
        _pooler: An instance of Pooler used for pooling operations.
    """
 
+    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
+        super().__init__(vllm_config=vllm_config, prefix=prefix)
+        self.padding_idx = vllm_config.model_config.hf_config.pad_token_id
+
+    def forward(
+        self,
+        input_ids: Optional[torch.Tensor],
+        positions: torch.Tensor,
+        token_type_ids: Optional[torch.Tensor] = None,
+        intermediate_tensors: Optional[IntermediateTensors] = None,
+        inputs_embeds: Optional[torch.Tensor] = None,
+    ) -> torch.Tensor:
+
+        # Fix Roberta positions here outside of the CUDA graph.
+        # Because we need the to extract the sequences from
+        # input_ids the control flow is data dependent.
+        replace_roberta_positions(input_ids=input_ids,
+                                  position_ids=positions,
+                                  padding_idx=self.padding_idx)
+
+        return self.model(input_ids=input_ids,
+                          position_ids=positions,
+                          token_type_ids=token_type_ids,
+                          inputs_embeds=inputs_embeds,
+                          intermediate_tensors=intermediate_tensors)
+
     def _build_model(self,
                      vllm_config: VllmConfig,
                      prefix: str = "") -> Union[BertModel, BertWithRope]:
@@ -147,8 +153,7 @@ class RobertaEmbeddingModel(BertEmbeddingModel):
         return loader.load_weights(weights_list, mapper=mapper)
 
 
-class RobertaForSequenceClassification(nn.Module, SupportsCrossEncoding,
-                                       SupportsV0Only):
+class RobertaForSequenceClassification(nn.Module, SupportsCrossEncoding):
     """A model that uses Roberta to provide embedding functionalities.
 
    This class encapsulates the BertModel and provides an interface for
@@ -175,6 +180,7 @@ class RobertaForSequenceClassification(nn.Module, SupportsCrossEncoding,
     def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
         super().__init__()
         config = vllm_config.model_config.hf_config
+        self.padding_idx = vllm_config.model_config.hf_config.pad_token_id
 
         self.num_labels = config.num_labels
         self.roberta = BertModel(vllm_config=vllm_config,
@@ -216,6 +222,9 @@ class RobertaForSequenceClassification(nn.Module, SupportsCrossEncoding,
         inputs_embeds: Optional[torch.Tensor] = None,
         token_type_ids: Optional[torch.Tensor] = None,
     ) -> torch.Tensor:
+        replace_roberta_positions(input_ids=input_ids,
+                                  position_ids=positions,
+                                  padding_idx=self.padding_idx)
         return self.roberta(input_ids=input_ids,
                             position_ids=positions,
                             inputs_embeds=inputs_embeds,
@@ -245,3 +254,36 @@ def create_position_ids_from_input_ids(input_ids,
                            past_key_values_length) * mask
 
     return incremental_indices.long() + padding_idx
+
+
+def replace_roberta_positions(input_ids: torch.Tensor,
+                              position_ids: torch.Tensor,
+                              padding_idx: int) -> None:
+
+    seq_lens: Optional[torch.Tensor] = None
+    attn_metadata = get_forward_context().attn_metadata
+    if attn_metadata is not None:  # can be None during warmup
+        if isinstance(attn_metadata, dict):
+            attn_metadata = next(iter(attn_metadata.values()))
+        # TODO: remove "seq_lens_tensor" after V0 is removed
+        seq_lens = getattr(attn_metadata, "seq_lens_tensor",
+                           getattr(attn_metadata, "seq_lens", None))
+
+    if seq_lens is not None:
+        assert isinstance(seq_lens, torch.Tensor)
+
+        # Replace position ids because in RoBERTa models
+        # they have to start at padding_idx + 1 and ignore
+        # existing padding tokens
+        # References:
+        # - https://github.com/huggingface/transformers/blob/a3d69a8994d673899608a7c17fbf4f953f50474e/src/transformers/models/roberta/modeling_roberta.py#L133
+        # - https://github.com/huggingface/transformers/blob/a3d69a8994d673899608a7c17fbf4f953f50474e/src/transformers/models/roberta/modeling_roberta.py#L1669
+        token_list = torch.split(input_ids[:torch.sum(seq_lens)],
+                                 seq_lens.tolist())
+
+        offset = 0
+        for tokens in token_list:
+            length = tokens.shape[0]
+            position_ids[offset:offset+length] = \
+                create_position_ids_from_input_ids(tokens, padding_idx)
+            offset = offset + length
diff --git a/vllm/model_executor/models/seed_oss.py b/vllm/model_executor/models/seed_oss.py
new file mode 100644
index 000000000..34a87a6a6
--- /dev/null
+++ b/vllm/model_executor/models/seed_oss.py
@@ -0,0 +1,487 @@
+# SPDX-License-Identifier: Apache-2.0
+# SPDX-FileCopyrightText: Copyright contributors to the vLLM project
+
+# Copyright 2025 The Seed team.
+# Copyright 2023 The vLLM team.
+# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.
+#
+# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX
+# and OPT implementations in this library. It has been modified from its
+# original forms to accommodate minor architectural differences compared
+# to GPT-NeoX and OPT used by the Meta AI team that trained the model.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+"""Inference-only SeedOss model compatible with HuggingFace weights."""
+from collections.abc import Iterable
+from typing import Optional, Union
+
+import torch
+from torch import nn
+from transformers import PretrainedConfig as SeedOssConfig
+
+from vllm.attention import Attention, AttentionType
+from vllm.compilation.decorators import support_torch_compile
+from vllm.config import CacheConfig, VllmConfig
+from vllm.distributed import get_pp_group, get_tensor_model_parallel_world_size
+from vllm.logger import init_logger
+from vllm.model_executor.layers.activation import SiluAndMul
+from vllm.model_executor.layers.layernorm import RMSNorm
+from vllm.model_executor.layers.linear import (MergedColumnParallelLinear,
+                                               QKVParallelLinear,
+                                               RowParallelLinear)
+from vllm.model_executor.layers.logits_processor import LogitsProcessor
+from vllm.model_executor.layers.quantization import QuantizationConfig
+from vllm.model_executor.layers.rotary_embedding import get_rope
+from vllm.model_executor.layers.vocab_parallel_embedding import (
+    ParallelLMHead, VocabParallelEmbedding)
+from vllm.model_executor.model_loader.weight_utils import (
+    default_weight_loader, maybe_remap_kv_scale_name)
+from vllm.model_executor.sampling_metadata import SamplingMetadata
+from vllm.sequence import IntermediateTensors
+
+from .interfaces import SupportsLoRA, SupportsPP
+from .utils import (AutoWeightsLoader, PPMissingLayer, is_pp_missing_parameter,
+                    make_empty_intermediate_tensors_factory, make_layers,
+                    maybe_prefix)
+
+logger = init_logger(__name__)
+
+
+class SeedOssMLP(nn.Module):
+
+    def __init__(
+        self,
+        hidden_size: int,
+        intermediate_size: int,
+        hidden_act: str,
+        quant_config: Optional[QuantizationConfig] = None,
+        prefix: str = "",
+    ) -> None:
+        super().__init__()
+        self.gate_up_proj = MergedColumnParallelLinear(
+            hidden_size,
+            [intermediate_size] * 2,
+            bias=False,
+            quant_config=quant_config,
+            prefix=f"{prefix}.gate_up_proj",
+        )
+        self.down_proj = RowParallelLinear(
+            intermediate_size,
+            hidden_size,
+            bias=False,
+            quant_config=quant_config,
+            prefix=f"{prefix}.down_proj",
+        )
+        if hidden_act != "silu":
+            raise ValueError(f"Unsupported activation: {hidden_act}. "
+                             "Only silu is supported for now.")
+        self.act_fn = SiluAndMul()
+
+    def forward(self, x):
+        gate_up, _ = self.gate_up_proj(x)
+        x = self.act_fn(gate_up)
+        x, _ = self.down_proj(x)
+        return x
+
+
+class SeedOssAttention(nn.Module):
+
+    def __init__(
+        self,
+        hidden_size: int,
+        num_heads: int,
+        num_kv_heads: int,
+        head_dim: int,
+        max_position: int = 4096 * 32,
+        rope_theta: float = 10000,
+        cache_config: Optional[CacheConfig] = None,
+        quant_config: Optional[QuantizationConfig] = None,
+        rope_scaling: Optional[tuple] = None,
+        prefix: str = "",
+        attn_type: str = AttentionType.DECODER,
+    ) -> None:
+        super().__init__()
+        self.hidden_size = hidden_size
+        tp_size = get_tensor_model_parallel_world_size()
+        self.total_num_heads = num_heads
+        assert self.total_num_heads % tp_size == 0
+        self.num_heads = self.total_num_heads // tp_size
+        self.total_num_kv_heads = num_kv_heads
+        self.head_dim = head_dim
+        if self.total_num_kv_heads >= tp_size:
+            # Number of KV heads is greater than TP size, so we partition
+            # the KV heads across multiple tensor parallel GPUs.
+            assert self.total_num_kv_heads % tp_size == 0
+        else:
+            # Number of KV heads is less than TP size, so we replicate
+            # the KV heads across multiple tensor parallel GPUs.
+            assert tp_size % self.total_num_kv_heads == 0
+        self.num_kv_heads = max(1, self.total_num_kv_heads // tp_size)
+        self.q_size = self.num_heads * self.head_dim
+        self.kv_size = self.num_kv_heads * self.head_dim
+        self.scaling = self.head_dim**-0.5
+        self.rope_theta = rope_theta
+
+        self.qkv_proj = QKVParallelLinear(
+            hidden_size,
+            self.head_dim,
+            self.total_num_heads,
+            self.total_num_kv_heads,
+            bias=True,
+            quant_config=quant_config,
+            prefix=f"{prefix}.qkv_proj",
+        )
+        self.o_proj = RowParallelLinear(
+            self.total_num_heads * self.head_dim,
+            hidden_size,
+            bias=False,
+            quant_config=quant_config,
+            prefix=f"{prefix}.o_proj",
+        )
+
+        self.rotary_emb = get_rope(
+            self.head_dim,
+            rotary_dim=self.head_dim,
+            max_position=max_position,
+            base=self.rope_theta,
+            rope_scaling=rope_scaling,
+        )
+        self.attn = Attention(
+            self.num_heads,
+            self.head_dim,
+            self.scaling,
+            num_kv_heads=self.num_kv_heads,
+            cache_config=cache_config,
+            quant_config=quant_config,
+            attn_type=attn_type,
+            prefix=f"{prefix}.attn",
+        )
+
+    def forward(
+        self,
+        positions: torch.Tensor,
+        hidden_states: torch.Tensor,
+    ) -> torch.Tensor:
+        qkv, _ = self.qkv_proj(hidden_states)
+        q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
+        q, k = self.rotary_emb(positions, q, k)
+        attn_output = self.attn(q, k, v)
+        output, _ = self.o_proj(attn_output)
+        return output
+
+
+class SeedOssDecoderLayer(nn.Module):
+
+    def __init__(
+        self,
+        config: SeedOssConfig,
+        cache_config: Optional[CacheConfig] = None,
+        quant_config: Optional[QuantizationConfig] = None,
+        prefix: str = "",
+    ) -> None:
+        super().__init__()
+        self.hidden_size = config.hidden_size
+        # Requires transformers > 4.32.0
+        rope_theta = getattr(config, "rope_theta", 1000000)
+        rope_scaling = getattr(config, "rope_scaling", None)
+
+        # By default, SeedOss uses causal attention as it is a
+        # decoder-only model.
+        # You can override the HF config with `is_causal=False` to enable
+        # bidirectional attention, which is used in some embedding models
+        if getattr(config, "is_causal", True):
+            attn_type = AttentionType.DECODER
+        else:
+            attn_type = AttentionType.ENCODER_ONLY
+
+        self.self_attn = SeedOssAttention(
+            hidden_size=self.hidden_size,
+            num_heads=config.num_attention_heads,
+            max_position=config.max_position_embeddings,
+            num_kv_heads=config.num_key_value_heads,
+            head_dim=config.head_dim,
+            rope_theta=rope_theta,
+            cache_config=cache_config,
+            quant_config=quant_config,
+            rope_scaling=rope_scaling,
+            prefix=f"{prefix}.self_attn",
+            attn_type=attn_type,
+        )
+        self.mlp = SeedOssMLP(
+            hidden_size=self.hidden_size,
+            intermediate_size=config.intermediate_size,
+            hidden_act=config.hidden_act,
+            quant_config=quant_config,
+            prefix=f"{prefix}.mlp",
+        )
+        self.input_layernorm = RMSNorm(config.hidden_size,
+                                       eps=config.rms_norm_eps)
+        self.post_attention_layernorm = RMSNorm(config.hidden_size,
+                                                eps=config.rms_norm_eps)
+
+    def forward(
+        self,
+        positions: torch.Tensor,
+        hidden_states: torch.Tensor,
+        residual: Optional[torch.Tensor],
+    ) -> tuple[torch.Tensor, torch.Tensor]:
+        # Self Attention
+        if residual is None:
+            residual = hidden_states
+            hidden_states = self.input_layernorm(hidden_states)
+        else:
+            hidden_states, residual = self.input_layernorm(
+                hidden_states, residual)
+        hidden_states = self.self_attn(
+            positions=positions,
+            hidden_states=hidden_states,
+        )
+
+        # Fully Connected
+        hidden_states, residual = self.post_attention_layernorm(
+            hidden_states, residual)
+        hidden_states = self.mlp(hidden_states)
+        return hidden_states, residual
+
+
+@support_torch_compile(
+    dynamic_arg_dims={
+        "input_ids": 0,
+        "positions": -1,
+        "intermediate_tensors": 0,
+        "inputs_embeds": 0,
+    })
+class SeedOssModel(nn.Module):
+
+    def __init__(self,
+                 *,
+                 vllm_config: VllmConfig,
+                 prefix: str = "",
+                 decoder_layer_type: type[nn.Module] = SeedOssDecoderLayer):
+        super().__init__()
+
+        config = vllm_config.model_config.hf_config
+        cache_config = vllm_config.cache_config
+        quant_config = vllm_config.quant_config
+
+        # TODO (@robertgshaw2): see if this can be moved out
+        if (cache_config.sliding_window is not None
+                and hasattr(config, "max_window_layers")):
+            assert config.max_window_layers == config.num_hidden_layers, (
+                "Sliding window for some but all layers is not supported. "
+                "This model uses sliding window but `max_window_layers` = {} "
+                "is less than `num_hidden_layers` = {}. Please open an issue "
+                "to discuss this feature.".format(
+                    config.max_window_layers,
+                    config.num_hidden_layers,
+                ))
+
+        self.config = config
+        self.quant_config = quant_config
+        self.vocab_size = config.vocab_size
+
+        if get_pp_group().is_first_rank or (config.tie_word_embeddings
+                                            and get_pp_group().is_last_rank):
+            self.embed_tokens = VocabParallelEmbedding(
+                config.vocab_size,
+                config.hidden_size,
+                quant_config=quant_config,
+                prefix=f"{prefix}.embed_tokens",
+            )
+        else:
+            self.embed_tokens = PPMissingLayer()
+
+        # Use the provided decoder layer type or default to SeedDecoderLayer
+        decoder_layer_type = decoder_layer_type or SeedOssDecoderLayer
+        self.start_layer, self.end_layer, self.layers = make_layers(
+            config.num_hidden_layers,
+            lambda prefix: decoder_layer_type(config=config,
+                                              cache_config=cache_config,
+                                              quant_config=quant_config,
+                                              prefix=prefix),
+            prefix=f"{prefix}.layers",
+        )
+
+        self.make_empty_intermediate_tensors = (
+            make_empty_intermediate_tensors_factory(
+                ["hidden_states", "residual"], config.hidden_size))
+        if get_pp_group().is_last_rank:
+            self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
+        else:
+            self.norm = PPMissingLayer()
+
+    def get_input_embeddings(self, input_ids: torch.Tensor) -> torch.Tensor:
+        return self.embed_tokens(input_ids)
+
+    def forward(
+        self,
+        input_ids: torch.Tensor,
+        positions: torch.Tensor,
+        intermediate_tensors: Optional[IntermediateTensors] = None,
+        inputs_embeds: Optional[torch.Tensor] = None,
+    ) -> Union[torch.Tensor, IntermediateTensors]:
+        if get_pp_group().is_first_rank:
+            if inputs_embeds is not None:
+                hidden_states = inputs_embeds
+            else:
+                hidden_states = self.get_input_embeddings(input_ids)
+            residual = None
+        else:
+            assert intermediate_tensors is not None
+            hidden_states = intermediate_tensors["hidden_states"]
+            residual = intermediate_tensors["residual"]
+        for layer in self.layers[self.start_layer:self.end_layer]:
+            hidden_states, residual = layer(
+                positions,
+                hidden_states,
+                residual,
+            )
+        if not get_pp_group().is_last_rank:
+            return IntermediateTensors({
+                "hidden_states": hidden_states,
+                "residual": residual
+            })
+        hidden_states, _ = self.norm(hidden_states, residual)
+        return hidden_states
+
+    def load_weights(self, weights: Iterable[tuple[str,
+                                                   torch.Tensor]]) -> set[str]:
+        stacked_params_mapping = [
+            # (param_name, shard_name, shard_id)
+            ("qkv_proj", "q_proj", "q"),
+            ("qkv_proj", "k_proj", "k"),
+            ("qkv_proj", "v_proj", "v"),
+            ("gate_up_proj", "gate_proj", 0),
+            ("gate_up_proj", "up_proj", 1),
+        ]
+        params_dict = dict(self.named_parameters(remove_duplicate=False))
+        loaded_params: set[str] = set()
+        for name, loaded_weight in weights:
+            if "rotary_emb.inv_freq" in name:
+                continue
+            if (self.quant_config is not None and
+                (scale_name := self.quant_config.get_cache_scale(name))):
+                # Loading kv cache quantization scales
+                param = params_dict[scale_name]
+                weight_loader = getattr(param, "weight_loader",
+                                        default_weight_loader)
+                loaded_weight = (loaded_weight if loaded_weight.dim() == 0 else
+                                 loaded_weight[0])
+                weight_loader(param, loaded_weight)
+                loaded_params.add(scale_name)
+                continue
+            for (param_name, weight_name, shard_id) in stacked_params_mapping:
+                if weight_name not in name:
+                    continue
+                name = name.replace(weight_name, param_name)
+                # Skip loading extra bias for GPTQ models.
+                if name.endswith(".bias") and name not in params_dict:
+                    continue
+                if is_pp_missing_parameter(name, self):
+                    continue
+                param = params_dict[name]
+                weight_loader = param.weight_loader
+                weight_loader(param, loaded_weight, shard_id)
+                break
+            else:
+                # Skip loading extra bias for GPTQ models.
+                if name.endswith(".bias") and name not in params_dict:
+                    continue
+                # Remapping the name of FP8 kv-scale.
+                name = maybe_remap_kv_scale_name(name, params_dict)
+                if name is None:
+                    continue
+                if is_pp_missing_parameter(name, self):
+                    continue
+                param = params_dict[name]
+                weight_loader = getattr(param, "weight_loader",
+                                        default_weight_loader)
+                weight_loader(param, loaded_weight)
+            loaded_params.add(name)
+        return loaded_params
+
+
+class SeedOssForCausalLM(nn.Module, SupportsLoRA, SupportsPP):
+    packed_modules_mapping = {
+        "qkv_proj": [
+            "q_proj",
+            "k_proj",
+            "v_proj",
+        ],
+        "gate_up_proj": [
+            "gate_proj",
+            "up_proj",
+        ],
+    }
+
+    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
+        super().__init__()
+        config = vllm_config.model_config.hf_config
+        quant_config = vllm_config.quant_config
+        lora_config = vllm_config.lora_config
+
+        self.config = config
+        self.lora_config = lora_config
+
+        self.quant_config = quant_config
+        self.model = SeedOssModel(vllm_config=vllm_config,
+                                  prefix=maybe_prefix(prefix, "model"))
+
+        if get_pp_group().is_last_rank:
+            if config.tie_word_embeddings:
+                self.lm_head = self.model.embed_tokens
+            else:
+                self.lm_head = ParallelLMHead(config.vocab_size,
+                                              config.hidden_size,
+                                              quant_config=quant_config,
+                                              prefix=maybe_prefix(
+                                                  prefix, "lm_head"))
+        else:
+            self.lm_head = PPMissingLayer()
+
+        self.logits_processor = LogitsProcessor(config.vocab_size)
+
+        self.make_empty_intermediate_tensors = (
+            self.model.make_empty_intermediate_tensors)
+
+    def get_input_embeddings(self, input_ids: torch.Tensor) -> torch.Tensor:
+        return self.model.get_input_embeddings(input_ids)
+
+    def forward(
+        self,
+        input_ids: torch.Tensor,
+        positions: torch.Tensor,
+        intermediate_tensors: Optional[IntermediateTensors] = None,
+        inputs_embeds: Optional[torch.Tensor] = None,
+    ) -> Union[torch.Tensor, IntermediateTensors]:
+        hidden_states = self.model(input_ids, positions, intermediate_tensors,
+                                   inputs_embeds)
+        return hidden_states
+
+    def compute_logits(
+        self,
+        hidden_states: torch.Tensor,
+        sampling_metadata: SamplingMetadata,
+    ) -> Optional[torch.Tensor]:
+        logits = self.logits_processor(self.lm_head, hidden_states,
+                                       sampling_metadata)
+        return logits
+
+    def load_weights(self, weights: Iterable[tuple[str,
+                                                   torch.Tensor]]) -> set[str]:
+        loader = AutoWeightsLoader(
+            self,
+            skip_prefixes=(["lm_head."]
+                           if self.config.tie_word_embeddings else None),
+        )
+        return loader.load_weights(weights)
diff --git a/vllm/model_executor/models/siglip.py b/vllm/model_executor/models/siglip.py
index 3630f59f5..62566d8f6 100644
--- a/vllm/model_executor/models/siglip.py
+++ b/vllm/model_executor/models/siglip.py
@@ -12,6 +12,7 @@ from torch import nn
 from transformers import SiglipVisionConfig
 
 from vllm.attention.layer import MultiHeadAttention
+from vllm.attention.layer import SelfMultiHeadAttention
 from vllm.distributed import divide, get_tensor_model_parallel_world_size
 from vllm.model_executor.layers.activation import get_act_fn
 from vllm.model_executor.layers.linear import (ColumnParallelLinear,
@@ -177,7 +178,9 @@ class SiglipAttention(nn.Module):
         self.tp_size = get_tensor_model_parallel_world_size()
         self.num_heads_per_partition = divide(self.num_heads, self.tp_size)
 
-        self.attn = MultiHeadAttention(self.num_heads_per_partition,
+        # self.attn = MultiHeadAttention(self.num_heads_per_partition,
+        #                                self.head_dim, self.scale)
+        self.attn = SelfMultiHeadAttention(self.num_heads_per_partition,
                                        self.head_dim, self.scale)
 
     def forward(
diff --git a/vllm/model_executor/models/vision.py b/vllm/model_executor/models/vision.py
index ac6a659bb..fa25ee8e1 100644
--- a/vllm/model_executor/models/vision.py
+++ b/vllm/model_executor/models/vision.py
@@ -97,6 +97,8 @@ def get_vit_attn_backend(support_fa: bool = False) -> _Backend:
             else:
                 # For Volta and Turing GPUs, use xformers instead.
                 selected_backend = _Backend.XFORMERS
+        elif current_platform.is_xpu:
+            selected_backend = _Backend.IPEX
         else:
             # Default to torch SDPA for other non-GPU platforms.
             selected_backend = _Backend.TORCH_SDPA
diff --git a/vllm/platforms/xpu.py b/vllm/platforms/xpu.py
index c4530c1df..6a1bb9a78 100644
--- a/vllm/platforms/xpu.py
+++ b/vllm/platforms/xpu.py
@@ -4,6 +4,7 @@
 import os
 from typing import TYPE_CHECKING, Optional
 
+import os
 import torch
 
 import vllm.envs as envs
@@ -20,6 +21,23 @@ else:
 
 logger = init_logger(__name__)
 
+def device_id_to_physical_device_id(device_id: int) -> int:
+    if "ZE_AFFINITY_MASK" in os.environ:
+        device_ids = os.environ["ZE_AFFINITY_MASK"].split(",")
+        if device_ids == [""]:
+            msg = (
+                "ZE_AFFINITY_MASK is set to empty string, which means"
+                " GPU support is disabled. If you are using ray, please unset"
+                " the environment variable `ZE_AFFINITY_MASK` inside the"
+                " worker/actor. "
+                "Check https://github.com/vllm-project/vllm/issues/8402 for"
+                " more information.")
+            raise RuntimeError(msg)
+        physical_device_id = device_ids[device_id]
+        return int(physical_device_id)
+    else:
+        return device_id
+
 
 class XPUPlatform(Platform):
     _enum = PlatformEnum.XPU
@@ -30,7 +48,7 @@ class XPUPlatform(Platform):
     # see https://github.com/ray-project/ray/blob/6a5eb5865eeb9ccf058a79b44f107e327e360673/python/ray/_private/accelerators/intel_gpu.py#L20 # noqa: E501
     ray_device_key: str = "GPU"
     dist_backend: str = "ccl"  # ccl | xccl
-    device_control_env_var: str = "ONEAPI_DEVICE_SELECTOR"
+    device_control_env_var: str = "ZE_AFFINITY_MASK"
 
     @classmethod
     def get_attn_backend_cls(cls, selected_backend: _Backend, head_size: int,
@@ -78,6 +96,10 @@ class XPUPlatform(Platform):
     def is_async_output_supported(cls, enforce_eager: Optional[bool]) -> bool:
         return True
 
+    @classmethod
+    def get_piecewise_backend_cls(cls) -> str:
+        return "vllm.compilation.cuda_piecewise_backend.CUDAPiecewiseBackend"  # noqa
+
     @classmethod
     def inference_mode(cls):
         return torch.no_grad()
@@ -85,6 +107,7 @@ class XPUPlatform(Platform):
     @classmethod
     def check_and_update_config(cls, vllm_config: VllmConfig) -> None:
         cache_config = vllm_config.cache_config
+        vllm_config.compilation_config.use_cudagraph = False
         model_config = vllm_config.model_config
         # in V1(or with ipex chunked prefill) block_size is 64
         if cache_config and cache_config.block_size is None:
@@ -173,6 +196,13 @@ class XPUPlatform(Platform):
                 device_name)
             return True
 
+    @classmethod
+    def fp8_dtype(cls) -> torch.dtype:
+        if envs.VLLM_XPU_FP8_DTYPE == "e4m3":
+            return torch.float8_e4m3fn
+        else:
+            return torch.float8_e5m2
+
     @classmethod
     def is_data_center_gpu(cls) -> bool:
         device_name = cls.get_device_name().lower()
diff --git a/vllm/plugins/__init__.py b/vllm/plugins/__init__.py
index 51c78ddc1..93bf5ec25 100644
--- a/vllm/plugins/__init__.py
+++ b/vllm/plugins/__init__.py
@@ -68,13 +68,6 @@ def load_general_plugins():
         return
     plugins_loaded = True
 
-    # some platform-specific configurations
-    from vllm.platforms import current_platform
-
-    if current_platform.is_xpu():
-        # see https://github.com/pytorch/pytorch/blob/43c5f59/torch/_dynamo/config.py#L158
-        torch._dynamo.config.disable = True
-
     plugins = load_plugins_by_group(group=DEFAULT_PLUGINS_GROUP)
     # general plugins, we only need to execute the loaded functions
     for func in plugins.values():
diff --git a/vllm/transformers_utils/chat_templates/registry.py b/vllm/transformers_utils/chat_templates/registry.py
index e0ef7f099..d09c5fa92 100644
--- a/vllm/transformers_utils/chat_templates/registry.py
+++ b/vllm/transformers_utils/chat_templates/registry.py
@@ -20,6 +20,16 @@ def _get_qwen_chat_template_fallback(
     return CHAT_TEMPLATES_DIR / "template_basic.jinja"
 
 
+def _get_minicpmv_chat_template_fallback(
+        tokenizer_name_or_path: str) -> Optional[Path]:
+    # MiniCPM-V-4.5 version uses a dedicated template
+    if "4.5" in tokenizer_name_or_path or "4_5" in tokenizer_name_or_path:
+        return CHAT_TEMPLATES_DIR / "template_minicpmv45.jinja"
+
+    # Other versions use chatml template
+    return CHAT_TEMPLATES_DIR / "template_chatml.jinja"
+
+
 # yapf: disable
 _MODEL_TYPE_TO_CHAT_TEMPLATE_FALLBACK: dict[str, ChatTemplatePath] = {
     "blip-2": CHAT_TEMPLATES_DIR / "template_blip2.jinja",
@@ -27,6 +37,7 @@ _MODEL_TYPE_TO_CHAT_TEMPLATE_FALLBACK: dict[str, ChatTemplatePath] = {
     "deepseek_vl_v2": CHAT_TEMPLATES_DIR / "template_deepseek_vl2.jinja",
     "florence2": CHAT_TEMPLATES_DIR / "template_basic.jinja",
     "fuyu": CHAT_TEMPLATES_DIR / "template_fuyu.jinja",
+    "minicpmv": _get_minicpmv_chat_template_fallback,
     "paligemma": CHAT_TEMPLATES_DIR / "template_basic.jinja",
     "qwen": _get_qwen_chat_template_fallback,
 }
diff --git a/vllm/transformers_utils/chat_templates/template_minicpmv45.jinja b/vllm/transformers_utils/chat_templates/template_minicpmv45.jinja
new file mode 100644
index 000000000..661ebd1cf
--- /dev/null
+++ b/vllm/transformers_utils/chat_templates/template_minicpmv45.jinja
@@ -0,0 +1,93 @@
+{%- set enable_thinking = enable_thinking | default(false) %}
+{%- if tools %}
+    {{- '<|im_start|>system\n' }}
+    {%- if messages[0].role == 'system' %}
+        {{- messages[0].content + '\n\n' }}
+    {%- endif %}
+    {{- "# Tools\n\nYou may call one or more functions to assist with the user query.\n\nYou are provided with function signatures within <tools></tools> XML tags:\n<tools>" }}
+    {%- for tool in tools %}
+        {{- "\n" }}
+        {{- tool | tojson }}
+    {%- endfor %}
+    {{- "\n</tools>\n\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\n<tool_call>\n{\"name\": <function-name>, \"arguments\": <args-json-object>}\n</tool_call><|im_end|>\n" }}
+{%- else %}
+    {%- if messages[0].role == 'system' %}
+        {{- '<|im_start|>system\n' + messages[0].content + '<|im_end|>\n' }}
+    {%- endif %}
+{%- endif %}
+
+{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}
+{%- for message in messages[::-1] %}
+    {%- set index = (messages|length - 1) - loop.index0 %}
+    {%- if ns.multi_step_tool and message.role == "user" and not(message.content.startswith('<tool_response>') and message.content.endswith('</tool_response>')) %}
+        {%- set ns.multi_step_tool = false %}
+        {%- set ns.last_query_index = index %}
+    {%- endif %}
+{%- endfor %}
+
+{%- for message in messages %}
+    {%- if (message.role == "user") or (message.role == "system" and not loop.first) %}
+        {{- '<|im_start|>' + message.role + '\n' + message.content + '<|im_end|>' + '\n' }}
+    {%- elif message.role == "assistant" %}
+        {%- set content = message.content %}
+        {%- set reasoning_content = '' %}
+        {%- if message.reasoning_content is defined and message.reasoning_content is not none %}
+            {%- set reasoning_content = message.reasoning_content %}
+        {%- else %}
+            {%- if '</think>' in message.content %}
+                {%- set content = message.content.split('</think>')[-1].lstrip('\n') %}
+                {%- set reasoning_content = message.content.split('</think>')[0].rstrip('\n').split('<think>')[-1].lstrip('\n') %}
+            {%- endif %}
+        {%- endif %}
+        {%- if loop.index0 > ns.last_query_index %}
+            {%- if loop.last or (not loop.last and reasoning_content) %}
+                {{- '<|im_start|>' + message.role + '\n<think>\n' + reasoning_content.strip('\n') + '\n</think>\n\n' + content.lstrip('\n') }}
+            {%- else %}
+                {{- '<|im_start|>' + message.role + '\n' + content }}
+            {%- endif %}
+        {%- else %}
+            {{- '<|im_start|>' + message.role + '\n' + content }}
+        {%- endif %}
+
+        {%- if message.tool_calls %}
+            {%- for tool_call in message.tool_calls %}
+                {%- if (loop.first and content) or (not loop.first) %}
+                    {{- '\n' }}
+                {%- endif %}
+                {%- if tool_call.function %}
+                    {%- set tool_call = tool_call.function %}
+                {%- endif %}
+                {{- '<tool_call>\n{"name": "' }}
+                {{- tool_call.name }}
+                {{- '", "arguments": ' }}
+                {%- if tool_call.arguments is string %}
+                    {{- tool_call.arguments }}
+                {%- else %}
+                    {{- tool_call.arguments | tojson }}
+                {%- endif %}
+                {{- '}\n</tool_call>' }}
+            {%- endfor %}
+        {%- endif %}
+        {{- '<|im_end|>\n' }}
+    {%- elif message.role == "tool" %}
+        {%- if loop.first or (messages[loop.index0 - 1].role != "tool") %}
+            {{- '<|im_start|>user' }}
+        {%- endif %}
+        {{- '\n<tool_response>\n' }}
+        {{- message.content }}
+        {{- '\n</tool_response>' }}
+        {%- if loop.last or (messages[loop.index0 + 1].role != "tool") %}
+            {{- '<|im_end|>\n' }}
+        {%- endif %}
+    {%- endif %}
+{%- endfor %}
+
+{%- if add_generation_prompt %}
+    {{- '<|im_start|>assistant\n' }}
+    {%- if enable_thinking is defined and enable_thinking is false %}
+        {{- '<think>\n\n</think>\n\n' }}
+    {%- endif %}
+    {%- if enable_thinking is defined and enable_thinking is true %}
+        {{- '<think>\n' }}
+    {%- endif %}
+{%- endif %}
\ No newline at end of file
diff --git a/vllm/transformers_utils/config.py b/vllm/transformers_utils/config.py
index 8d1f59e6e..0d96bcfef 100644
--- a/vllm/transformers_utils/config.py
+++ b/vllm/transformers_utils/config.py
@@ -264,7 +264,8 @@ def _uses_mrope(config: PretrainedConfig) -> bool:
 
 def uses_mrope(config: PretrainedConfig) -> bool:
     """Detect if the model with this config uses M-ROPE."""
-    return _uses_mrope(config) or thinker_uses_mrope(config)
+    return _uses_mrope(config) or _uses_mrope(
+        config.get_text_config()) or thinker_uses_mrope(config)
 
 
 def thinker_uses_mrope(config: PretrainedConfig) -> bool:
diff --git a/vllm/utils/tensor_schema.py b/vllm/utils/tensor_schema.py
new file mode 100644
index 000000000..0959d4a00
--- /dev/null
+++ b/vllm/utils/tensor_schema.py
@@ -0,0 +1,236 @@
+# SPDX-License-Identifier: Apache-2.0
+# SPDX-FileCopyrightText: Copyright contributors to the vLLM project
+from typing import (Annotated, Any, Optional, Union, get_args, get_origin,
+                    get_type_hints)
+
+import torch
+
+from vllm.logger import init_logger
+
+logger = init_logger(__name__)
+
+
+class TensorShape:
+
+    def __init__(
+        self,
+        *dims: Union[int, str],
+        dynamic_dims: Optional[set[str]] = None,
+    ) -> None:
+        super().__init__()
+
+        self.dims = dims
+        self.dynamic_dims = dynamic_dims if dynamic_dims else set()
+
+    def resolve(self, **bindings: dict[str,
+                                       int]) -> tuple[Union[int, str], ...]:
+        resolved = []
+        for dim in self.dims:
+            if isinstance(dim, str) and dim in bindings:
+                resolved.append(bindings[dim])
+            else:
+                resolved.append(dim)
+        return tuple(resolved)
+
+    def __str__(self) -> str:
+        """Return a string representation of the tensor shape."""
+        dim_strs = []
+        for dim in self.dims:
+            if isinstance(dim, str):
+                if dim in self.dynamic_dims:
+                    dim_strs.append(
+                        f"{dim}*")  # Mark dynamic dimensions with *
+                else:
+                    dim_strs.append(dim)
+            else:
+                dim_strs.append(str(dim))
+        return f"({', '.join(dim_strs)})"
+
+
+class TensorSchema:
+
+    def __init__(
+        self,
+        *,
+        validate: bool = True,
+        resolve_bindings: Optional[dict[str, int]] = None,
+        **kwargs: Any,
+    ) -> None:
+        super().__init__()
+
+        self._resolve_bindings = resolve_bindings if resolve_bindings else {}
+
+        for key, value in kwargs.items():
+            setattr(self, key, value)
+
+        if validate:
+            self.validate()
+
+    def __getitem__(self, key: str) -> Any:
+        return getattr(self, key)
+
+    def get(self, key: str, default: Any = None) -> Any:
+        return getattr(self, key, default)
+
+    def _match_shape_with_dynamic(
+        self,
+        actual: tuple[int, ...],
+        reference: tuple[int, ...],
+        expected_shape: tuple[Union[int, str], ...],
+        dynamic_dims: set[str],
+    ) -> bool:
+        if len(actual) != len(reference) or len(actual) > len(expected_shape):
+            return False
+
+        for i, (a, r) in enumerate(zip(actual, reference)):
+            # When validating list inputs, we match shape suffixes only
+            # (e.g. "p", 3, "h", "w"), assuming the list length corresponds
+            # to the leading symbolic dim (e.g. "bn"). This allows comparing
+            # only the trailing dimensions of each element in the list.
+            dim = expected_shape[-len(actual) + i]
+            # Skip this dimension if it's marked dynamic
+            if dim in dynamic_dims:
+                continue
+            if a != r:
+                return False
+        return True
+
+    def _validate_nested_tensors(
+        self,
+        value: Union[list[torch.Tensor], tuple[torch.Tensor, ...]],
+        field_name: str,
+        expected_shape: tuple[Union[int, str], ...],
+        dynamic_dims: set[str],
+    ) -> tuple[int, ...]:
+        """Validate a list/tuple of tensors and return the actual shape."""
+        # Ensure all tensors in the list have the same
+        # shape, besides dynamic dimensions
+        first = value[0]
+        for i, v in enumerate(value):
+            if not isinstance(v, torch.Tensor):
+                raise ValueError(f"{field_name}[{i}] is not a "
+                                 f"torch.Tensor")
+            if not self._match_shape_with_dynamic(
+                    v.shape,
+                    first.shape,
+                    expected_shape,
+                    dynamic_dims,
+            ):
+                raise ValueError(f"{field_name} contains inconsistent "
+                                 f"shapes: {first.shape} vs {v.shape} "
+                                 f"at index {i}")
+
+        # Treat the list as a stacked tensor:
+        # shape = (len(list), *tensor.shape)
+        return (len(value), ) + first.shape
+
+    def _validate_tensor_shape_expected(
+        self,
+        actual_shape: tuple[int, ...],
+        expected_shape: tuple[Union[int, str], ...],
+        field_name: str,
+        shape_env: dict[str, int],
+        dynamic_dims: set[str],
+    ) -> None:
+        """Validate that the actual tensor shape matches the expected shape."""
+
+        if len(actual_shape) != len(expected_shape):
+            raise ValueError(f"{field_name} has rank {len(actual_shape)} "
+                             f"but expected {len(expected_shape)}")
+
+        for i, dim in enumerate(expected_shape):
+            if dim in dynamic_dims:
+                continue
+            elif isinstance(dim, int):
+                if actual_shape[i] != dim:
+                    raise ValueError(f"{field_name} dim[{i}] expected "
+                                     f"{dim}, got {actual_shape[i]}")
+            elif isinstance(dim, str):
+                if dim in shape_env:
+                    if actual_shape[i] != shape_env[dim]:
+                        raise ValueError(f"{field_name} dim[{i}] expected "
+                                         f"'{dim}'={shape_env[dim]}, got "
+                                         f"{actual_shape[i]}")
+                else:
+                    shape_env[dim] = actual_shape[i]
+            else:
+                raise TypeError(f"{field_name} dim[{i}] has unsupported "
+                                f"type: {type(dim)}")
+
+    def validate(self) -> None:
+        type_hints = get_type_hints(self.__class__, include_extras=True)
+        shape_env = {}
+
+        for field_name, field_type in type_hints.items():
+            # Check if field is missing
+            if (not hasattr(self, field_name)
+                    or getattr(self, field_name) is None):
+                # Check if field is marked as optional
+                actual_type = field_type
+                if get_origin(field_type) is Annotated:
+                    args = get_args(field_type)
+                    actual_type = args[0]
+
+                # Check arg was provided as Union
+                if get_origin(actual_type) is Union:
+                    args = get_args(actual_type)
+                    # Skip validation when Union contains None
+                    if type(None) in args:
+                        continue
+                # Otherwise field is required, raise error
+                raise ValueError(f"Required field '{field_name}' is missing")
+
+            # Field exists, proceed with validation
+            value = getattr(self, field_name)
+            if get_origin(field_type) is not None:
+                args = get_args(field_type)
+
+                for arg in args:
+                    if isinstance(arg, TensorShape):
+                        expected_shape = arg.resolve(**self._resolve_bindings)
+                        if isinstance(value, (list, tuple)):
+                            # list/tuple of Tensors ‚Üí shape = (len(value), ...)
+                            if value and isinstance(value[0], torch.Tensor):
+                                actual_shape = self._validate_nested_tensors(
+                                    value, field_name, expected_shape,
+                                    arg.dynamic_dims)
+                            elif value:
+                                # list/tuple of scalars ‚Üí shape = (len(value),)
+                                actual_shape = (len(value), )
+                            else:
+                                raise ValueError(
+                                    f"{field_name} is an empty list")
+
+                        # Tensor ‚Üí shape = tensor.shape
+                        elif isinstance(value, torch.Tensor):
+                            actual_shape = value.shape
+
+                        # Otherwise, it's an unsupported type
+                        else:
+                            type_names = []
+                            for arg in args:
+                                if hasattr(arg, "__name__"):
+                                    type_names.append(str(arg.__name__))
+                                else:
+                                    type_names.append(str(arg))
+
+                            expected_types = ", ".join(type_names)
+                            raise ValueError(
+                                f"{field_name} is not one of the expected "
+                                f"types: {expected_types}")
+
+                        self._validate_tensor_shape_expected(
+                            actual_shape, expected_shape, field_name,
+                            shape_env, arg.dynamic_dims)
+
+    def print_shapes(self) -> None:
+        """Print TensorShape annotations for debugging."""
+        logger.debug("Shapes in %s:", self.__class__.__name__)
+        type_hints = get_type_hints(self.__class__, include_extras=True)
+
+        for field_name, field_type in type_hints.items():
+            if get_origin(field_type) is not None:
+                args = get_args(field_type)
+                for arg in args:
+                    if isinstance(arg, TensorShape):
+                        logger.debug("  %s: %s", field_name, str(arg))
\ No newline at end of file
diff --git a/vllm/v1/attention/backends/flash_attn.py b/vllm/v1/attention/backends/flash_attn.py
index 5fe274f2c..4a8657ee5 100755
--- a/vllm/v1/attention/backends/flash_attn.py
+++ b/vllm/v1/attention/backends/flash_attn.py
@@ -24,12 +24,15 @@ if is_flash_attn_varlen_func_available():
 
 from vllm.config import VllmConfig, get_layers_from_vllm_config
 from vllm.logger import init_logger
+from vllm.platforms import current_platform
 from vllm.utils import cdiv
 from vllm.v1.attention.backends.utils import (AttentionMetadataBuilder,
                                               CommonAttentionMetadata,
                                               get_kv_cache_layout)
 from vllm.v1.kv_cache_interface import AttentionSpec
 
+if current_platform.is_xpu():
+    from vllm._ipex_ops import ipex_ops
 logger = init_logger(__name__)
 
 # NOTE(woosuk): This is an arbitrary number. Tune it if needed.
@@ -46,7 +49,7 @@ class FlashAttentionBackend(AttentionBackend):
 
     @classmethod
     def get_supported_head_sizes(cls) -> list[int]:
-        return [32, 64, 96, 128, 160, 192, 224, 256]
+        return [32, 64, 80, 96, 128, 160, 192, 224, 256]
 
     @classmethod
     def validate_head_size(cls, head_size: int) -> None:
@@ -125,11 +128,16 @@ class FlashAttentionMetadata:
     prefix_kv_lens: Optional[torch.Tensor]
     suffix_kv_lens: Optional[torch.Tensor]
 
+    # For XPU.
+    seq_start_loc: Optional[torch.Tensor]
+
     # Optional aot scheduling
     scheduler_metadata: Optional[torch.Tensor] = None
     prefix_scheduler_metadata: Optional[torch.Tensor] = None
     max_num_splits: int = 0
 
+    causal: bool = True
+
 
 def _get_sliding_window_configs(
         vllm_config: VllmConfig) -> set[Optional[tuple[int, int]]]:
@@ -209,10 +217,13 @@ class FlashAttentionMetadataBuilder(
         max_query_len = common_attn_metadata.max_query_len
         max_seq_len = int(common_attn_metadata.seq_lens_cpu.max())
         query_start_loc = common_attn_metadata.query_start_loc
+        seq_start_loc = common_attn_metadata.seq_start_loc \
+            if current_platform.is_xpu() else None
         seq_lens = common_attn_metadata.seq_lens
         seq_lens_cpu = common_attn_metadata.seq_lens_cpu
         block_table_tensor = common_attn_metadata.block_table_tensor
         slot_mapping = common_attn_metadata.slot_mapping
+        causal = common_attn_metadata.causal
 
         # the overhead of the aot schedule is not worth it for spec-decode
         aot_schedule = self.aot_schedule and not fast_build
@@ -288,7 +299,7 @@ class FlashAttentionMetadataBuilder(
                                           max_query_len=max_query_len,
                                           seqlens=seq_lens,
                                           max_seq_len=max_seq_len,
-                                          causal=True)
+                                          causal=causal)
 
         if self.use_full_cuda_graph:
             assert scheduler_metadata is not None
@@ -314,6 +325,7 @@ class FlashAttentionMetadataBuilder(
             num_actual_tokens=num_actual_tokens,
             max_query_len=max_query_len,
             query_start_loc=query_start_loc,
+            seq_start_loc=seq_start_loc,
             max_seq_len=max_seq_len,
             seq_lens=seq_lens,
             block_table=block_table_tensor,
@@ -326,7 +338,7 @@ class FlashAttentionMetadataBuilder(
             suffix_kv_lens=suffix_kv_lens,
             prefix_scheduler_metadata=prefix_scheduler_metadata,
             max_num_splits=max_num_splits,
-        )
+            causal=causal)
         return attn_metadata
 
     def can_run_in_cudagraph(
@@ -375,11 +387,14 @@ class FlashAttentionImpl(AttentionImpl):
 
         FlashAttentionBackend.validate_head_size(head_size)
 
-        if attn_type != AttentionType.DECODER:
-            raise NotImplementedError("Encoder self-attention and "
-                                      "encoder/decoder cross-attention "
-                                      "are not implemented for "
+        if attn_type not in [
+                AttentionType.DECODER, AttentionType.ENCODER_ONLY
+        ]:
+            raise NotImplementedError("Encoder/decoder cross-attention "
+                                      "is not implemented for "
                                       "FlashAttentionImpl")
+
+        self.attn_type = attn_type
         self.vllm_flash_attn_version = get_flash_attn_version()
         if is_quantized_kv_cache(self.kv_cache_dtype) \
             and not flash_attn_supports_fp8():
@@ -420,7 +435,9 @@ class FlashAttentionImpl(AttentionImpl):
 
         if attn_metadata is None:
             # Profiling run.
-            return output
+            return output.uniform_()
+
+        attn_type = self.attn_type
 
         # IMPORTANT!
         # NOTE(woosuk): With piece-wise CUDA graphs, this method is executed in
@@ -432,6 +449,18 @@ class FlashAttentionImpl(AttentionImpl):
         # performance to make sure it does not introduce any overhead.
 
         num_actual_tokens = attn_metadata.num_actual_tokens
+
+        # Handle encoder attention differently - no KV cache needed
+        if attn_type in (AttentionType.ENCODER_ONLY, ):
+            # For encoder attention,
+            # we use direct Q, K, V tensors without caching
+            return self._forward_encoder_attention(query[:num_actual_tokens],
+                                                   key[:num_actual_tokens],
+                                                   value[:num_actual_tokens],
+                                                   output[:num_actual_tokens],
+                                                   attn_metadata, layer)
+
+        # For decoder and cross-attention, use KV cache as before
         key_cache, value_cache = kv_cache.unbind(0)
 
         if self.kv_sharing_target_layer_name is None:
@@ -472,6 +501,8 @@ class FlashAttentionImpl(AttentionImpl):
             scheduler_metadata = attn_metadata.scheduler_metadata
 
             descale_shape = (cu_seqlens_q.shape[0] - 1, key.shape[1])
+            cu_seqlens_k = attn_metadata.seq_start_loc if \
+                current_platform.is_xpu() else None
 
             flash_attn_varlen_func(
                 q=query[:num_actual_tokens],
@@ -480,10 +511,11 @@ class FlashAttentionImpl(AttentionImpl):
                 out=output[:num_actual_tokens],
                 cu_seqlens_q=cu_seqlens_q,
                 max_seqlen_q=max_seqlen_q,
+                cu_seqlens_k=cu_seqlens_k,
                 seqused_k=seqused_k,
                 max_seqlen_k=max_seqlen_k,
                 softmax_scale=self.scale,
-                causal=True,
+                causal=attn_metadata.causal,
                 alibi_slopes=self.alibi_slopes,
                 window_size=self.sliding_window,
                 block_table=block_table,
@@ -524,6 +556,86 @@ class FlashAttentionImpl(AttentionImpl):
         )
         return output
 
+    def _forward_encoder_attention(
+        self,
+        query: torch.Tensor,
+        key: torch.Tensor,
+        value: torch.Tensor,
+        output: torch.Tensor,
+        attn_metadata: FlashAttentionMetadata,
+        layer: torch.nn.Module,
+    ) -> torch.Tensor:
+        """Forward pass for encoder attention without KV cache.
+
+        Args:
+            query: shape = [num_encoder_tokens, num_heads, head_size]
+            key: shape = [num_encoder_tokens, num_kv_heads, head_size]
+            value: shape = [num_encoder_tokens, num_kv_heads, head_size]
+            output: shape = [num_encoder_tokens, num_heads, head_size]
+            attn_metadata: Encoder attention metadata
+            layer: The attention layer
+        """
+        # For encoder attention, process FP8 quantization if needed
+        if self.kv_cache_dtype.startswith("fp8"):
+            raise NotImplementedError(
+                "quantization is not supported for encoder attention")
+
+        # Use encoder-specific metadata for sequence information
+        cu_seqlens_q = attn_metadata.query_start_loc
+        cu_seqlens_k = attn_metadata.query_start_loc
+        max_seqlen_q = attn_metadata.max_query_len
+        max_seqlen_k = attn_metadata.max_query_len
+
+        descale_shape = (
+            cu_seqlens_q.shape[0] - 1,  # type: ignore[union-attr]
+            self.num_kv_heads)
+
+        if current_platform.is_xpu():
+            ipex_ops.varlen_attention(
+                    query=query,
+                    key=key,
+                    value=value,
+                    out=output,
+                    seqlen_q=cu_seqlens_q,
+                    seqlen_k=cu_seqlens_k,
+                    max_seqlen_q=max_seqlen_q,
+                    max_seqlen_k=max_seqlen_k,
+                    softmax_scale=self.scale,
+                    is_causal=False,  # Encoder attention is bidirectional
+                    alibi_slopes=self.alibi_slopes,
+                    pdropout=0.0,
+                    zero_tensors=False,
+                    return_softmax=False,
+                    gen_=None,
+                    window_size_left=-1,
+                    window_size_right=-1,
+                    logits_soft_cap=self.logits_soft_cap,
+                    )
+            return output
+
+        # Call flash attention directly on Q, K, V tensors
+        flash_attn_varlen_func(
+            q=query,
+            k=key,
+            v=value,
+            out=output,
+            cu_seqlens_q=cu_seqlens_q,
+            cu_seqlens_k=cu_seqlens_k,
+            max_seqlen_q=max_seqlen_q,
+            max_seqlen_k=max_seqlen_k,
+            softmax_scale=self.scale,
+            causal=False,  # Encoder attention is bidirectional
+            alibi_slopes=self.alibi_slopes,
+            window_size=self.sliding_window,
+            softcap=self.logits_soft_cap,
+            fa_version=self.vllm_flash_attn_version,
+            q_descale=layer._q_scale.expand(descale_shape),
+            k_descale=layer._k_scale.expand(descale_shape),
+            v_descale=layer._v_scale.expand(descale_shape),
+        )
+
+        return output
+
 
 def use_cascade_attention(
     common_prefix_len: int,
diff --git a/vllm/v1/attention/backends/utils.py b/vllm/v1/attention/backends/utils.py
index fc8649d58..b7d1b84cc 100644
--- a/vllm/v1/attention/backends/utils.py
+++ b/vllm/v1/attention/backends/utils.py
@@ -41,6 +41,9 @@ class CommonAttentionMetadata:
     query_start_loc_cpu: torch.Tensor
     """(batch_size + 1,), the start location of each request in query Tensor"""
 
+    seq_start_loc: torch.Tensor
+    seq_start_loc_cpu: torch.Tensor
+
     seq_lens: torch.Tensor
     seq_lens_cpu: torch.Tensor
     """(batch_size,), the length of each request including both computed tokens
@@ -59,6 +62,8 @@ class CommonAttentionMetadata:
     block_table_tensor: torch.Tensor
     slot_mapping: torch.Tensor
 
+    causal: bool = True
+
 
 M = TypeVar("M")
 
@@ -387,6 +392,8 @@ def make_local_attention_virtual_batches(
         query_start_loc_cpu=query_start_loc_cpu,
         query_start_loc=query_start_loc_cpu.to(device=device,
                                                non_blocking=True),
+        seq_start_loc=None,
+        seq_start_loc_cpu=None,
         seq_lens_cpu=seq_lens_cpu,
         seq_lens=seq_lens_cpu.to(device=device, non_blocking=True),
         num_computed_tokens_cpu=torch.from_numpy(num_computed_tokens_local),
@@ -395,6 +402,7 @@ def make_local_attention_virtual_batches(
         max_query_len=seqlens_q_local.max(),
         block_table_tensor=block_table_local,
         slot_mapping=common_attn_metadata.slot_mapping,
+        causal=True,
     )
 
 
diff --git a/vllm/v1/engine/core.py b/vllm/v1/engine/core.py
index 7779b559c..346638da3 100644
--- a/vllm/v1/engine/core.py
+++ b/vllm/v1/engine/core.py
@@ -109,6 +109,12 @@ class EngineCore:
                 "compatibility may not be maintained.",
                 vllm_config.scheduler_config.scheduler_cls)
 
+        if len(kv_cache_config.kv_cache_groups) == 0:
+            # Encoder models without KV cache don't support
+            # chunked prefill. But do SSM models?
+            logger.info("Disabling chunked prefill for model without KVCache")
+            vllm_config.scheduler_config.chunked_prefill_enabled = False
+
         self.scheduler: SchedulerInterface = Scheduler(
             vllm_config=vllm_config,
             kv_cache_config=kv_cache_config,
diff --git a/vllm/v1/executor/abstract.py b/vllm/v1/executor/abstract.py
index 50b9634a4..1649b8ff7 100644
--- a/vllm/v1/executor/abstract.py
+++ b/vllm/v1/executor/abstract.py
@@ -30,6 +30,7 @@ class Executor(ExecutorBase):
         parallel_config = vllm_config.parallel_config
         distributed_executor_backend = (
             parallel_config.distributed_executor_backend)
+        data_parallel_size = parallel_config.data_parallel_size
         # distributed_executor_backend must be set in VllmConfig.__post_init__
         if isinstance(distributed_executor_backend, type):
             if not issubclass(distributed_executor_backend, ExecutorBase):
diff --git a/vllm/v1/executor/ray_distributed_executor.py b/vllm/v1/executor/ray_distributed_executor.py
index b86ac048f..277c16ff8 100644
--- a/vllm/v1/executor/ray_distributed_executor.py
+++ b/vllm/v1/executor/ray_distributed_executor.py
@@ -11,6 +11,8 @@ from vllm.logger import init_logger
 from vllm.v1.engine import ReconfigureDistributedRequest, ReconfigureRankType
 from vllm.v1.executor.abstract import Executor
 from vllm.v1.outputs import ModelRunnerOutput
+import ray
+import cloudpickle
 
 logger = init_logger(__name__)
 
@@ -103,4 +105,4 @@ class RayDistributedExecutor(RayDistributedExecutorV0, Executor):
         if reconfig_request.new_data_parallel_rank == \
         ReconfigureRankType.SHUTDOWN_CURRENT_RANK:
             self.shutdown()
-        return
\ No newline at end of file
+        return
diff --git a/vllm/v1/spec_decode/eagle.py b/vllm/v1/spec_decode/eagle.py
index 967847c02..80c103829 100644
--- a/vllm/v1/spec_decode/eagle.py
+++ b/vllm/v1/spec_decode/eagle.py
@@ -322,6 +322,8 @@ class EagleProposer:
                                                        non_blocking=True),
             seq_lens=new_seq_lens_cpu.to(device, non_blocking=True),
             query_start_loc_cpu=new_query_start_loc_cpu,
+            seq_start_loc=None,
+            seq_start_loc_cpu=None,
             seq_lens_cpu=new_seq_lens_cpu,
             num_computed_tokens_cpu=common_attn_metadata.
             num_computed_tokens_cpu,
@@ -330,6 +332,7 @@ class EagleProposer:
             max_query_len=new_query_len_per_req.max().item(),
             block_table_tensor=common_attn_metadata.block_table_tensor,
             slot_mapping=common_attn_metadata.slot_mapping[token_indices],
+            causal=True,
         )
 
         return spec_common_attn_metadata, token_indices
diff --git a/vllm/v1/worker/cpu_model_runner.py b/vllm/v1/worker/cpu_model_runner.py
index ca94ac8c6..6b2b50a57 100644
--- a/vllm/v1/worker/cpu_model_runner.py
+++ b/vllm/v1/worker/cpu_model_runner.py
@@ -4,6 +4,7 @@ from contextlib import contextmanager
 from typing import Any
 
 import torch
+import torch.nn as nn
 
 from vllm.config import VllmConfig
 from vllm.logger import init_logger
@@ -59,6 +60,9 @@ class CPUModelRunner(GPUModelRunner):
                                               self.scheduler_config,
                                               self.lora_config, self.device)
 
+    def get_model(self) -> nn.Module:
+        return self.model
+
     def warming_up_model(self) -> None:
         logger.info("Warming up model for the compilation...")
         # Only generate graph for the generic shape
diff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py
index a5bf197ba..f17c2fa1b 100644
--- a/vllm/v1/worker/gpu_model_runner.py
+++ b/vllm/v1/worker/gpu_model_runner.py
@@ -1,7 +1,6 @@
 # SPDX-License-Identifier: Apache-2.0
 # SPDX-FileCopyrightText: Copyright contributors to the vLLM project
 
-import copy
 import gc
 import time
 from contextlib import contextmanager
@@ -23,12 +22,10 @@ from vllm.config import (CompilationLevel, VllmConfig,
 from vllm.distributed.eplb.eplb_state import EplbState
 from vllm.distributed.kv_transfer import (get_kv_transfer_group,
                                           has_kv_transfer_group)
-from vllm.distributed.kv_transfer.kv_connector.v1 import KVConnectorBase_V1
 from vllm.distributed.parallel_state import (
     get_pp_group, get_tp_group, graph_capture, is_global_first_rank,
     prepare_communication_buffer_for_model)
-from vllm.forward_context import (DPMetadata, get_forward_context,
-                                  set_forward_context)
+from vllm.forward_context import DPMetadata, set_forward_context
 from vllm.logger import init_logger
 from vllm.model_executor.layers.mamba.mamba_mixer2 import MambaBase
 from vllm.model_executor.layers.rotary_embedding import MRotaryEmbedding
@@ -66,6 +63,8 @@ from vllm.v1.spec_decode.medusa import MedusaProposer
 from vllm.v1.spec_decode.metadata import SpecDecodeMetadata
 from vllm.v1.spec_decode.ngram_proposer import NgramProposer
 from vllm.v1.worker.gpu_input_batch import CachedRequestState, InputBatch
+from vllm.v1.worker.kv_connector_model_runner_mixin import (
+    KVConnectorModelRunnerMixin)
 from vllm.v1.worker.lora_model_runner_mixin import LoRAModelRunnerMixin
 
 from ..sample.logits_processor import LogitsProcessorManager
@@ -88,7 +87,7 @@ else:
 logger = init_logger(__name__)
 
 
-class GPUModelRunner(LoRAModelRunnerMixin):
+class GPUModelRunner(LoRAModelRunnerMixin, KVConnectorModelRunnerMixin):
 
     def __init__(
         self,
@@ -125,6 +124,7 @@ class GPUModelRunner(LoRAModelRunnerMixin):
 
         self.is_multimodal_model = model_config.is_multimodal_model
         self.is_pooling_model = model_config.pooler_config is not None
+        self.is_encoder_only_model = False
         self.model_supports_multimodal_raw_input = (
             model_config.model_supports_multimodal_raw_input)
         self.max_model_len = model_config.max_model_len
@@ -177,6 +177,9 @@ class GPUModelRunner(LoRAModelRunnerMixin):
         # NOTE(Jiayi): currently we put the entire draft model on
         # the last PP rank. This is not ideal if there are many
         # layers in the draft model.
+        self.use_spec_decode = False
+        if self.speculative_config:
+            self.use_spec_decode = True
         if self.speculative_config and get_pp_group().is_last_rank:
             if self.speculative_config.method == "ngram":
                 self.drafter = NgramProposer(self.vllm_config)
@@ -313,6 +316,16 @@ class GPUModelRunner(LoRAModelRunnerMixin):
                                         pin_memory=self.pin_memory)
         self.seq_lens_np = self.seq_lens_cpu.numpy()
 
+        # this is XPU specific
+        self.seq_start_loc = torch.zeros(self.max_num_reqs + 1,
+                                         dtype=torch.int32,
+                                         device=self.device)
+        self.seq_start_loc_cpu = torch.zeros(self.max_num_reqs + 1,
+                                             dtype=torch.int32,
+                                             device="cpu",
+                                             pin_memory=self.pin_memory)
+        self.seq_start_loc_np = self.seq_start_loc_cpu.numpy()
+
         # Layer pairings for cross-layer KV sharing.
         # If an Attention layer `layer_name` is in the keys of this dict, it
         # means this layer will perform attention using the keys and values
@@ -703,6 +716,14 @@ class GPUModelRunner(LoRAModelRunnerMixin):
             self.input_batch.num_computed_tokens_cpu[:num_reqs] +
             num_scheduled_tokens)
 
+        # for xpu
+        seq_lens = (self.input_batch.num_computed_tokens_cpu[:num_reqs] +
+                    num_scheduled_tokens)
+        self.seq_start_loc_np[0] = 0
+        np.cumsum(seq_lens, out=self.seq_start_loc_np[1:num_reqs + 1])
+        self.seq_start_loc[:num_reqs + 1].copy_(
+            self.seq_start_loc_cpu[:num_reqs + 1], non_blocking=True)
+
         # Copy the tensors to the GPU.
         self.input_ids[:total_num_scheduled_tokens].copy_(
             self.input_ids_cpu[:total_num_scheduled_tokens], non_blocking=True)
@@ -734,6 +755,21 @@ class GPUModelRunner(LoRAModelRunnerMixin):
         spec_decode_common_attn_metadata = None
 
         attn_metadata: dict[str, Any] = {}
+
+        # Prepare encoder attention metadata separately
+        # (encoder layers are not in KV cache groups)
+        if self.is_encoder_only_model:
+            common_attn_metadata, encoder_attn_metadata = \
+                self._build_encoder_only_attn_metadata(
+                scheduler_output)
+
+            # Add encoder attention metadata for all encoder layers
+            attention_layers = get_layers_from_vllm_config(
+                self.vllm_config, Attention)
+            for layer_name, attn_module in attention_layers.items():
+                if attn_module.attn_type == AttentionType.ENCODER_ONLY:
+                    attn_metadata[layer_name] = encoder_attn_metadata
+
         # Prepare the attention metadata for each KV cache group and make layers
         # in the same group share the same metadata.
         for kv_cache_group_id, kv_cache_group_spec in enumerate(
@@ -750,6 +786,8 @@ class GPUModelRunner(LoRAModelRunnerMixin):
             common_attn_metadata = CommonAttentionMetadata(
                 query_start_loc=self.query_start_loc[:num_reqs + 1],
                 query_start_loc_cpu=self.query_start_loc_cpu[:num_reqs + 1],
+                seq_start_loc=self.seq_start_loc[:num_reqs + 1],
+                seq_start_loc_cpu=self.seq_start_loc_cpu[:num_reqs + 1],
                 seq_lens=self.seq_lens[:num_reqs],
                 seq_lens_cpu=self.seq_lens_cpu[:num_reqs],
                 num_computed_tokens_cpu=self.input_batch.
@@ -759,6 +797,7 @@ class GPUModelRunner(LoRAModelRunnerMixin):
                 max_query_len=max_num_scheduled_tokens,
                 block_table_tensor=blk_table_tensor,
                 slot_mapping=slot_mapping,
+                causal=True,
             )
 
             if self.speculative_config and \
@@ -1357,7 +1396,8 @@ class GPUModelRunner(LoRAModelRunnerMixin):
                 # Return empty ModelRunnerOutput if there's no work to do.
                 return EMPTY_MODEL_RUNNER_OUTPUT
 
-            return self.kv_connector_no_forward(scheduler_output)
+            return self.kv_connector_no_forward(scheduler_output,
+                                                self.vllm_config)
 
         # Prepare the decoder inputs.
         (attn_metadata, attention_cuda_graphs, logits_indices,
@@ -1745,52 +1785,6 @@ class GPUModelRunner(LoRAModelRunnerMixin):
             spec_token_ids = draft_token_ids.tolist()
         return spec_token_ids
 
-    @staticmethod
-    def maybe_setup_kv_connector(scheduler_output: "SchedulerOutput"):
-        # Update KVConnector with the KVConnector metadata forward().
-        if has_kv_transfer_group():
-            kv_connector = get_kv_transfer_group()
-            assert isinstance(kv_connector, KVConnectorBase_V1)
-            assert scheduler_output.kv_connector_metadata is not None
-            kv_connector.bind_connector_metadata(
-                scheduler_output.kv_connector_metadata)
-
-            # Background KV cache transfers happen here.
-            # These transfers are designed to be async and the requests
-            # involved may be disjoint from the running requests.
-            # Do this here to save a collective_rpc.
-            kv_connector.start_load_kv(get_forward_context())
-
-    @staticmethod
-    def maybe_wait_for_kv_save() -> None:
-        if has_kv_transfer_group():
-            get_kv_transfer_group().wait_for_save()
-
-    @staticmethod
-    def get_finished_kv_transfers(
-        scheduler_output: "SchedulerOutput",
-    ) -> tuple[Optional[set[str]], Optional[set[str]]]:
-        if has_kv_transfer_group():
-            return get_kv_transfer_group().get_finished(
-                scheduler_output.finished_req_ids)
-        return None, None
-
-    def kv_connector_no_forward(
-            self, scheduler_output: "SchedulerOutput") -> ModelRunnerOutput:
-        # KV send/recv even if no work to do.
-        with set_forward_context(None, self.vllm_config):
-            self.maybe_setup_kv_connector(scheduler_output)
-            finished_sending, finished_recving = (
-                self.get_finished_kv_transfers(scheduler_output))
-
-        if not finished_sending and not finished_recving:
-            return EMPTY_MODEL_RUNNER_OUTPUT
-
-        output = copy.copy(EMPTY_MODEL_RUNNER_OUTPUT)
-        output.finished_sending = finished_sending
-        output.finished_recving = finished_recving
-        return output
-
     def propose_ngram_draft_token_ids(
         self,
         sampled_token_ids: list[list[int]],
@@ -2111,6 +2105,8 @@ class GPUModelRunner(LoRAModelRunnerMixin):
                     query_start_loc=self.query_start_loc[:num_reqs + 1],
                     query_start_loc_cpu=self.query_start_loc_cpu[:num_reqs +
                                                                  1],
+                    seq_start_loc=self.seq_start_loc[:num_reqs + 1],
+                    seq_start_loc_cpu=self.seq_start_loc_cpu[:num_reqs + 1],
                     seq_lens=self.seq_lens[:num_reqs],
                     seq_lens_cpu=self.seq_lens_cpu[:num_reqs],
                     num_computed_tokens_cpu=self.input_batch.
@@ -2121,7 +2117,8 @@ class GPUModelRunner(LoRAModelRunnerMixin):
                     block_table_tensor=self.input_batch.block_table[
                         kv_cache_group_id].get_device_tensor()[:num_reqs],
                     slot_mapping=self.input_batch.
-                    block_table[kv_cache_group_id].slot_mapping[:num_tokens])
+                    block_table[kv_cache_group_id].slot_mapping[:num_tokens],
+                    causal=True)
 
                 attn_metadata_i = self.attn_metadata_builders[
                     kv_cache_group_id].build_for_cudagraph_capture(
@@ -2410,11 +2407,10 @@ class GPUModelRunner(LoRAModelRunnerMixin):
 
             # Cache the dummy encoder outputs.
             self.encoder_cache["tmp"] = dict(enumerate(dummy_encoder_outputs))
-
         # Add `is_profile` here to pre-allocate communication buffers
         hidden_states, last_hidden_states \
             = self._dummy_run(self.max_num_tokens, is_profile=True)
-        if get_pp_group().is_last_rank:
+        if get_pp_group().is_last_rank and not self.use_spec_decode:
             if self.is_pooling_model:
                 output = self._dummy_pooler_run(hidden_states)
             else:
@@ -2485,6 +2481,49 @@ class GPUModelRunner(LoRAModelRunnerMixin):
         logger.info("Graph capturing finished in %.0f secs, took %.2f GiB",
                     elapsed_time, cuda_graph_size / (1 << 30))
 
+    def _initialize_single_attn_backend(
+        self, kv_cache_spec: KVCacheSpec
+    ) -> tuple[AttentionBackend, AttentionMetadataBuilder]:
+        if isinstance(kv_cache_spec, AttentionSpec):
+            attn_backend_i = get_attn_backend(
+                kv_cache_spec.head_size,
+                self.dtype,
+                kv_cache_spec.dtype,
+                kv_cache_spec.block_size,
+                self.model_config.is_attention_free,
+                use_mla=kv_cache_spec.use_mla,
+            )
+            if attn_backend_i is None:
+                error_msg = (f"Error with get_attn_backend: "
+                             f"{kv_cache_spec.head_size=}, "
+                             f"{self.dtype=}, {kv_cache_spec.dtype=}, "
+                             f"{kv_cache_spec.block_size=}, "
+                             f"{self.model_config.is_attention_free=}, "
+                             f"{kv_cache_spec.use_mla=}")
+                logger.error(error_msg)
+                raise NotImplementedError(
+                    "Non-Attention backend is not supported by V1 "
+                    "GPUModelRunner.")
+        elif isinstance(kv_cache_spec, MambaSpec):
+            attn_backend_i = Mamba2AttentionBackend
+        else:
+            raise ValueError(
+                f"Unknown KV cache spec type: {type(kv_cache_spec)}")
+
+        attn_metadata_builder_i = attn_backend_i.get_builder_cls()(
+            kv_cache_spec,
+            self.vllm_config,
+            self.device,
+        )
+
+        if (self.full_cuda_graph
+                and not attn_metadata_builder_i.full_cudagraph_supported):
+            raise ValueError(
+                f"Full CUDAGraph not supported for "
+                f"{attn_backend_i.__name__}. Turn off CompilationConfig."
+                f"full_cuda_graph or use a different attention backend.")
+        return attn_backend_i, attn_metadata_builder_i
+
     def initialize_attn_backend(self, kv_cache_config: KVCacheConfig) -> None:
         """
         Initialize the attention backends and attention metadata builders.
@@ -2495,48 +2534,45 @@ class GPUModelRunner(LoRAModelRunnerMixin):
         for i, kv_cache_group_spec in enumerate(
                 kv_cache_config.kv_cache_groups):
             kv_cache_spec = kv_cache_group_spec.kv_cache_spec
-            if isinstance(kv_cache_spec, AttentionSpec):
-                attn_backend_i = get_attn_backend(
-                    kv_cache_spec.head_size,
-                    self.dtype,
-                    kv_cache_spec.dtype,
-                    kv_cache_spec.block_size,
-                    self.model_config.is_attention_free,
-                    use_mla=kv_cache_spec.use_mla,
-                )
-                if attn_backend_i is None:
-                    error_msg = (f"Error with get_attn_backend: "
-                                 f"{kv_cache_spec.head_size=}, "
-                                 f"{self.dtype=}, {kv_cache_spec.dtype=}, "
-                                 f"{kv_cache_spec.block_size=}, "
-                                 f"{self.model_config.is_attention_free=}, "
-                                 f"{kv_cache_spec.use_mla=}")
-                    logger.error(error_msg)
-                    raise NotImplementedError(
-                        "Non-Attention backend is not supported by V1 "
-                        "GPUModelRunner.")
-            elif isinstance(kv_cache_spec, MambaSpec):
-                attn_backend_i = Mamba2AttentionBackend
-            else:
-                raise ValueError(
-                    f"Unknown KV cache spec type: {type(kv_cache_spec)}")
-
-            attn_metadata_builder_i = attn_backend_i.get_builder_cls()(
-                kv_cache_spec,
-                self.vllm_config,
-                self.device,
-            )
-
-            if (self.full_cuda_graph
-                    and not attn_metadata_builder_i.full_cudagraph_supported):
-                raise ValueError(
-                    f"Full CUDAGraph not supported for "
-                    f"{attn_backend_i.__name__}. Turn off CompilationConfig."
-                    f"full_cuda_graph or use a different attention backend.")
 
+            attn_backend_i, attn_metadata_builder_i = \
+                self._initialize_single_attn_backend(kv_cache_spec)
             self.attn_backends.append(attn_backend_i)
             self.attn_metadata_builders.append(attn_metadata_builder_i)
 
+        if len(self.attn_backends) > 0:
+            return
+
+        # Check if model is encoder-only
+        block_size = self.vllm_config.cache_config.block_size
+        use_mla = self.vllm_config.model_config.use_mla
+        attn_specs = list[AttentionSpec]()
+        attn_layers = get_layers_from_vllm_config(self.vllm_config, Attention)
+        for attn_module in attn_layers.values():
+
+            if attn_module.attn_type == AttentionType.ENCODER_ONLY:
+                assert attn_module.sliding_window is None, "Sliding "
+                "window attention is not supported for encoder-only models"
+
+                attn_specs.append(
+                    FullAttentionSpec(block_size=block_size,
+                                      num_kv_heads=attn_module.num_kv_heads,
+                                      head_size=attn_module.head_size,
+                                      dtype=self.kv_cache_dtype,
+                                      use_mla=use_mla))
+            else:
+                raise ValueError("Expected only encoder-only layers")
+
+        if len(attn_specs) > 0:
+            assert len(attn_specs) == len(attn_layers), \
+                "All or none of the layers are expected to be encoder-only"
+
+            attn_backend, attn_metadata_builder = \
+                self._initialize_single_attn_backend(attn_specs[0])
+            self.attn_backends.append(attn_backend)
+            self.attn_metadata_builders.append(attn_metadata_builder)
+            self.is_encoder_only_model = True
+
     def may_reinitialize_input_batch(self,
                                      kv_cache_config: KVCacheConfig) -> None:
         """
@@ -2852,3 +2888,55 @@ class GPUModelRunner(LoRAModelRunnerMixin):
                     page_size_padded=page_size_padded)
 
         return kv_cache_spec
+
+    def _build_encoder_only_attn_metadata(
+            self, scheduler_output: "SchedulerOutput") -> \
+                tuple[CommonAttentionMetadata, Any]:
+        """Prepare encoder attention metadata for encoder-only models.
+
+        Args:
+            scheduler_output: Scheduler output
+
+        Returns:
+            dict[str, Any]: Encoder attention metadata
+        """
+        num_reqs = self.input_batch.num_reqs
+        total_num_scheduled_tokens = scheduler_output.total_num_scheduled_tokens
+
+        # Get the number of scheduled tokens for each request.
+        req_ids = self.input_batch.req_ids
+        tokens = [scheduler_output.num_scheduled_tokens[i] for i in req_ids]
+        max_num_scheduled_tokens = max(tokens)
+
+        # Use the first attention metadata builder
+        # to create encoder attention metadata
+        builder = self.attn_metadata_builders[0]
+
+        dummy_block_table = torch.zeros((num_reqs, 1),
+                                        dtype=torch.int32,
+                                        device=self.device)
+        dummy_slot_mapping = torch.zeros((total_num_scheduled_tokens, ),
+                                         dtype=torch.int32,
+                                         device=self.device)
+
+        common_metadata = CommonAttentionMetadata(
+            query_start_loc=self.query_start_loc[:num_reqs + 1],
+            query_start_loc_cpu=self.query_start_loc_cpu[:num_reqs + 1],
+            seq_start_loc=self.seq_start_loc[:num_reqs + 1],
+            seq_start_loc_cpu=self.seq_start_loc_cpu[:num_reqs + 1],
+            seq_lens=self.seq_lens[:num_reqs],
+            seq_lens_cpu=self.seq_lens_cpu[:num_reqs],
+            num_computed_tokens_cpu=self.input_batch.
+            num_computed_tokens_cpu_tensor[:num_reqs],
+            num_reqs=num_reqs,
+            num_actual_tokens=total_num_scheduled_tokens,
+            max_query_len=max_num_scheduled_tokens,
+            block_table_tensor=dummy_block_table,
+            slot_mapping=dummy_slot_mapping,
+            causal=False,
+        )
+
+        return common_metadata, builder.build(
+            common_prefix_len=0,  # No cascade for encoder
+            common_attn_metadata=common_metadata,
+        )
diff --git a/vllm/v1/worker/gpu_worker.py b/vllm/v1/worker/gpu_worker.py
index 522946351..472eab749 100644
--- a/vllm/v1/worker/gpu_worker.py
+++ b/vllm/v1/worker/gpu_worker.py
@@ -297,7 +297,7 @@ class Worker(WorkerBase):
         # fragmentation issue.
         # NOTE: This is called after `capture_model` on purpose to prevent
         # memory buffers from being cleared by `torch.cuda.empty_cache`.
-        if get_pp_group().is_last_rank:
+        if get_pp_group().is_last_rank and get_pp_group().world_size > 1:
             max_num_reqs = min(self.scheduler_config.max_num_seqs,
                                self.scheduler_config.max_num_batched_tokens)
 
@@ -309,7 +309,7 @@ class Worker(WorkerBase):
                 )
             if self.model_runner.is_pooling_model:
                 self.model_runner._dummy_pooler_run(hidden_states)
-            else:
+            elif not self.model_runner.use_spec_decode:
                 self.model_runner._dummy_sampler_run(
                     hidden_states=last_hidden_states)
 
diff --git a/vllm/v1/worker/kv_connector_model_runner_mixin.py b/vllm/v1/worker/kv_connector_model_runner_mixin.py
new file mode 100644
index 000000000..5a3186058
--- /dev/null
+++ b/vllm/v1/worker/kv_connector_model_runner_mixin.py
@@ -0,0 +1,70 @@
+# SPDX-License-Identifier: Apache-2.0
+# SPDX-FileCopyrightText: Copyright contributors to the vLLM project
+"""
+Define KV connector functionality mixin for model runners.
+"""
+import copy
+from typing import TYPE_CHECKING, Optional
+
+from vllm.config import VllmConfig
+from vllm.distributed.kv_transfer import (get_kv_transfer_group,
+                                          has_kv_transfer_group)
+from vllm.distributed.kv_transfer.kv_connector.v1 import KVConnectorBase_V1
+from vllm.forward_context import get_forward_context, set_forward_context
+from vllm.logger import init_logger
+from vllm.v1.outputs import EMPTY_MODEL_RUNNER_OUTPUT, ModelRunnerOutput
+
+if TYPE_CHECKING:
+    from vllm.v1.core.sched.output import SchedulerOutput
+
+logger = init_logger(__name__)
+
+
+# Defined as a kv connector functionality mixin for ModelRunner (GPU, TPU)
+class KVConnectorModelRunnerMixin:
+
+    @staticmethod
+    def maybe_setup_kv_connector(scheduler_output: "SchedulerOutput"):
+        # Update KVConnector with the KVConnector metadata forward().
+        if has_kv_transfer_group():
+            kv_connector = get_kv_transfer_group()
+            assert isinstance(kv_connector, KVConnectorBase_V1)
+            assert scheduler_output.kv_connector_metadata is not None
+            kv_connector.bind_connector_metadata(
+                scheduler_output.kv_connector_metadata)
+
+            # Background KV cache transfers happen here.
+            # These transfers are designed to be async and the requests
+            # involved may be disjoint from the running requests.
+            # Do this here to save a collective_rpc.
+            kv_connector.start_load_kv(get_forward_context())
+
+    @staticmethod
+    def maybe_wait_for_kv_save() -> None:
+        if has_kv_transfer_group():
+            get_kv_transfer_group().wait_for_save()
+
+    @staticmethod
+    def get_finished_kv_transfers(
+        scheduler_output: "SchedulerOutput",
+    ) -> tuple[Optional[set[str]], Optional[set[str]]]:
+        if has_kv_transfer_group():
+            return get_kv_transfer_group().get_finished(
+                scheduler_output.finished_req_ids)
+        return None, None
+
+    def kv_connector_no_forward(self, scheduler_output: "SchedulerOutput",
+                                vllm_config: VllmConfig) -> ModelRunnerOutput:
+        # KV send/recv even if no work to do.
+        with set_forward_context(None, vllm_config):
+            self.maybe_setup_kv_connector(scheduler_output)
+            finished_sending, finished_recving = (
+                self.get_finished_kv_transfers(scheduler_output))
+
+        if not finished_sending and not finished_recving:
+            return EMPTY_MODEL_RUNNER_OUTPUT
+
+        output = copy.copy(EMPTY_MODEL_RUNNER_OUTPUT)
+        output.finished_sending = finished_sending
+        output.finished_recving = finished_recving
+        return output
diff --git a/vllm/v1/worker/tpu_model_runner.py b/vllm/v1/worker/tpu_model_runner.py
index 3bb033f14..e8c800845 100644
--- a/vllm/v1/worker/tpu_model_runner.py
+++ b/vllm/v1/worker/tpu_model_runner.py
@@ -3,7 +3,7 @@
 import bisect
 import gc
 import time
-from typing import TYPE_CHECKING, Any, Optional, cast
+from typing import TYPE_CHECKING, Any, Literal, Optional, Union, cast
 from unittest.mock import patch
 
 import numpy as np
@@ -20,6 +20,8 @@ from vllm.attention.layer import Attention
 from vllm.compilation.wrapper import TorchCompileWrapperWithCustomDispatcher
 from vllm.config import (ParallelConfig, VllmConfig,
                          get_layers_from_vllm_config, update_config)
+from vllm.distributed.kv_transfer import (get_kv_transfer_group,
+                                          has_kv_transfer_group)
 from vllm.forward_context import set_forward_context
 from vllm.logger import init_logger
 from vllm.lora.layers import BaseLayerWithLoRA
@@ -46,6 +48,8 @@ from vllm.v1.outputs import (EMPTY_MODEL_RUNNER_OUTPUT, LogprobsLists,
                              LogprobsTensors, ModelRunnerOutput)
 from vllm.v1.sample.tpu.metadata import TPUSupportedSamplingMetadata
 from vllm.v1.sample.tpu.sampler import Sampler as TPUSampler
+from vllm.v1.worker.kv_connector_model_runner_mixin import (
+    KVConnectorModelRunnerMixin)
 from vllm.v1.worker.lora_model_runner_mixin import LoRAModelRunnerMixin
 from vllm.v1.worker.tpu_input_batch import CachedRequestState, InputBatch
 
@@ -97,7 +101,7 @@ MIN_NUM_SEQS = 8
 # The dummy_run should be comprehensive, ensuring all potential input shapes and
 # branch predictions are included as subgraph inputs to facilitate
 # pre-compilation.
-class TPUModelRunner(LoRAModelRunnerMixin):
+class TPUModelRunner(LoRAModelRunnerMixin, KVConnectorModelRunnerMixin):
 
     def __init__(
         self,
@@ -971,8 +975,12 @@ class TPUModelRunner(LoRAModelRunnerMixin):
         # Update cached state
         self._update_states(scheduler_output)
         if not scheduler_output.total_num_scheduled_tokens:
-            # Return empty ModelRunnerOutput if there's no work to do.
-            return EMPTY_MODEL_RUNNER_OUTPUT
+            if not has_kv_transfer_group():
+                # Return empty ModelRunnerOutput if there's no work to do.
+                return EMPTY_MODEL_RUNNER_OUTPUT
+
+            return self.kv_connector_no_forward(scheduler_output,
+                                                self.vllm_config)
 
         if self.is_multimodal_model:
             # Run the multimodal encoder if any.
@@ -986,6 +994,12 @@ class TPUModelRunner(LoRAModelRunnerMixin):
         start_index = 0
         combined_selected_tokens: list[torch.Tensor] = []
         combined_logprobs: list[LogprobsLists] = []
+
+        # NOTE: setup current batch's metadata for kv connector.
+        # Currently, only verified with NixlConnector
+        with set_forward_context(None, self.vllm_config):
+            self.maybe_setup_kv_connector(scheduler_output)
+
         while start_index < self.input_batch.num_reqs:
             attn_metadata, logits_indices, padded_num_reqs, num_reqs,\
                 end_index = self._prepare_inputs(scheduler_output, start_index)
@@ -1032,6 +1046,14 @@ class TPUModelRunner(LoRAModelRunnerMixin):
 
             start_index = end_index
 
+        # NOTE: current kv load and save get h2d/d2h copies involved.
+        # Those copies are blocking. Once they become async., kv_save
+        # should be called right after each single forward pass,
+        # instead of the forwards of the entire input batch.
+        self.maybe_wait_for_kv_save()
+        finished_sending, finished_recving = (
+            self.get_finished_kv_transfers(scheduler_output))
+
         selected_token_ids = torch.cat(combined_selected_tokens, dim=0)
         if tpu_sampling_metadata.logprobs:
 
@@ -1126,6 +1148,8 @@ class TPUModelRunner(LoRAModelRunnerMixin):
             logprobs=logprobs_lists,
             prompt_logprobs_dict=prompt_logprobs_dict,
             pooler_output=[],
+            finished_sending=finished_sending,
+            finished_recving=finished_recving,
         )
 
         # Check there are no new graphs compiled - all the graphs should be
@@ -1637,6 +1661,10 @@ class TPUModelRunner(LoRAModelRunnerMixin):
             for cache in self.kv_caches:
                 xs.mark_sharding(cache, self.mesh, (None, 'x', None, None))
 
+        if has_kv_transfer_group():
+            get_kv_transfer_group().register_kv_caches(kv_caches)
+            get_kv_transfer_group().set_host_xfer_buffer_ops(copy_kv_blocks)
+
     def reset_dynamo_cache(self):
         if self.is_multimodal_model:
             compiled_model = self.model.get_language_model().model
@@ -1851,6 +1879,75 @@ def _get_padded_token_len(paddings: list[int], x: int) -> int:
     return paddings[index]
 
 
+def _make_src_and_dst_indices(
+    src_block_ids: list[int],
+    dst_block_ids: list[int],
+    src_device: Union[torch.device, str],
+    dst_device: Union[torch.device, str],
+) -> tuple[torch.Tensor, torch.Tensor]:
+    src_indices = torch.tensor(src_block_ids,
+                               device=src_device,
+                               dtype=torch.int64)
+    dst_indices = torch.tensor(dst_block_ids,
+                               device=dst_device,
+                               dtype=torch.int64)
+    return src_indices, dst_indices
+
+
+@torch.compile(backend="openxla")
+def _insert_blocks_to_tpu(
+    cpu_cache: torch.Tensor,
+    tpu_cache: torch.Tensor,
+    cpu_block_indices: torch.Tensor,
+    tpu_block_indices: torch.Tensor,
+) -> None:
+    torch.ops.xla.dynamo_set_buffer_donor_(tpu_cache, True)
+    tpu_cache[tpu_block_indices] = cpu_cache[cpu_block_indices].to(
+        tpu_cache.device)
+
+
+@torch.compile(backend="openxla")
+def _swap_out_tpu_blocks(
+    tpu_cache: torch.Tensor,
+    cpu_cache: torch.Tensor,
+    tpu_block_indices: torch.Tensor,
+    cpu_block_indices: torch.Tensor,
+) -> None:
+    """ tpu blocks to cpu blocks"""
+    torch.ops.xla.dynamo_set_buffer_donor_(tpu_cache, True)
+    cpu_cache[cpu_block_indices] = tpu_cache[tpu_block_indices].cpu()
+
+
+def copy_kv_blocks(
+    src_kv_caches: dict[str, torch.Tensor],
+    dst_kv_caches: dict[str, torch.Tensor],
+    src_block_ids: list[int],
+    dst_block_ids: list[int],
+    direction: Literal["h2d", "d2h"],
+) -> None:
+    """Copy kv blocks between different buffers."""
+    if not src_kv_caches or not dst_kv_caches or \
+       not src_block_ids or not dst_block_ids or \
+       len(src_block_ids) != len(dst_block_ids):
+        return
+
+    src_device = next(iter(src_kv_caches.values())).device
+    dst_device = next(iter(dst_kv_caches.values())).device
+
+    src_indices, dst_indices = _make_src_and_dst_indices(
+        src_block_ids=src_block_ids,
+        dst_block_ids=dst_block_ids,
+        src_device=src_device,
+        dst_device=dst_device)
+
+    _copy_fn = _insert_blocks_to_tpu if direction == "h2d" else \
+               _swap_out_tpu_blocks
+    for layer_name in src_kv_caches:
+        src_tensor = src_kv_caches[layer_name]
+        dst_tensor = dst_kv_caches[layer_name]
+        _copy_fn(src_tensor, dst_tensor, src_indices, dst_indices)
+
+
 def _get_padded_num_kv_cache_update_slices(
         num_tokens: int, max_num_reqs: int, page_size: int,
         num_slices_per_kv_cache_update_block: int) -> int:
diff --git a/vllm/v1/worker/tpu_worker.py b/vllm/v1/worker/tpu_worker.py
index 648d9c319..254b058d2 100644
--- a/vllm/v1/worker/tpu_worker.py
+++ b/vllm/v1/worker/tpu_worker.py
@@ -12,9 +12,11 @@ import torch_xla.debug.profiler as xp
 import torch_xla.runtime as xr
 
 import vllm.envs as envs
-from vllm.config import ParallelConfig, VllmConfig
+from vllm.config import VllmConfig
 from vllm.distributed import (ensure_model_parallel_initialized,
                               init_distributed_environment)
+from vllm.distributed.kv_transfer import (ensure_kv_transfer_initialized,
+                                          has_kv_transfer_group)
 from vllm.logger import init_logger
 from vllm.lora.request import LoRARequest
 from vllm.model_executor import set_random_seed
@@ -118,7 +120,7 @@ class TPUWorker:
 
         # Initialize the distributed environment.
         self._init_tpu_worker_distributed_environment(
-            self.parallel_config, self.rank, self.distributed_init_method,
+            self.vllm_config, self.rank, self.distributed_init_method,
             self.local_rank)
 
         # Device initialization should happen after initializing
@@ -242,7 +244,9 @@ class TPUWorker:
         scheduler_output: "SchedulerOutput",
     ) -> Optional[ModelRunnerOutput]:
         output = self.model_runner.execute_model(scheduler_output)
-        return output if self.is_driver_worker else None
+        # every worker's output is needed when kv_transfer_group is setup
+        return output if self.is_driver_worker or has_kv_transfer_group(
+        ) else None
 
     def profile(self, is_start: bool = True):
         if self.rank < 1:
@@ -294,7 +298,7 @@ class TPUWorker:
 
     def _init_tpu_worker_distributed_environment(
         self,
-        parallel_config: ParallelConfig,
+        vllm_config: VllmConfig,
         rank: int,
         distributed_init_method: Optional[str] = None,
         local_rank: int = -1,
@@ -306,6 +310,7 @@ class TPUWorker:
         # the input objects on CPU. The all-reduce and all-gather ops on TPU
         # are invoked by `xm.all_reduce` and `xm.all_gather` which use their
         # own context.
+        parallel_config = vllm_config.parallel_config
         init_distributed_environment(
             world_size=parallel_config.world_size,
             rank=rank,
@@ -317,6 +322,8 @@ class TPUWorker:
             parallel_config.tensor_parallel_size,
             parallel_config.pipeline_parallel_size)
 
+        ensure_kv_transfer_initialized(vllm_config)
+
 
 try:
     from tpu_commons.worker import TPUWorker as TPUCommonsWorker
diff --git a/vllm/v1/worker/xpu_worker.py b/vllm/v1/worker/xpu_worker.py
index c7885694f..85b302609 100644
--- a/vllm/v1/worker/xpu_worker.py
+++ b/vllm/v1/worker/xpu_worker.py
@@ -72,9 +72,11 @@ class XPUWorker(Worker):
     def determine_available_memory(self) -> int:
         """Profiles the peak memory usage of the model to determine how many
         KV blocks may be allocated without OOMs.
+
         The engine will first conduct a profiling of the existing memory usage.
         Then, it calculate the maximum possible number of GPU and CPU blocks
         that can be allocated with the remaining free memory.
+
         .. tip::
             You may limit the usage of GPU memory
             by adjusting the `gpu_memory_utilization` parameter.
@@ -82,51 +84,36 @@ class XPUWorker(Worker):
         # Profile the memory usage of the model and get the maximum number of
         # cache blocks that can be allocated with the remaining free memory.
         torch.xpu.empty_cache()
-        torch.xpu.reset_peak_memory_stats()
-
-        free_gpu_memory, total_gpu_memory = torch.xpu.mem_get_info()
-        current_allocated_bytes = torch.xpu.memory_allocated()
-        msg = ("Before memory profiling run, "
-               f"total GPU memory: {total_gpu_memory / 1024**2:.2f} MB, "
-               f"model load takes {current_allocated_bytes / 1024**2:.2f} MB, "
-               f"free gpu memory is {free_gpu_memory / 1024**2:.2f} MB.")
-        logger.info(msg)
+
         # Execute a forward pass with dummy inputs to profile the memory usage
         # of the model.
         self.model_runner.profile_run()
 
-        free_gpu_memory, _ = self.xpu_get_mem_info()
+        # Calculate the number of blocks that can be allocated with the
+        # profiled peak memory.
+        torch.xpu.synchronize()
+        used_memory = torch.xpu.memory_reserved()
+        total_gpu_memory = torch.xpu.get_device_properties(
+            self.local_rank).total_memory
+        free_gpu_memory = total_gpu_memory - used_memory
+
         # NOTE(woosuk): Here we assume that the other processes using the same
         # GPU did not change their memory usage during the profiling.
-        assert self.init_gpu_memory > free_gpu_memory, (
+        peak_memory = self.init_gpu_memory - free_gpu_memory
+        assert peak_memory > 0, (
             "Error in memory profiling. "
             f"Initial free memory {self.init_gpu_memory}, current free memory"
             f" {free_gpu_memory}. This happens when the GPU memory was "
             "not properly cleaned up before initializing the vLLM instance.")
 
-        # Get the peak memory allocation recorded by torch
-        peak_memory = torch.xpu.memory_stats()["allocated_bytes.all.peak"]
-
         torch.xpu.empty_cache()
-        torch_allocated_bytes = torch.xpu.memory_stats(
-        )["allocated_bytes.all.current"]
-        total_allocated_bytes = self.xpu_get_mem_info(
-        )[1] - self.xpu_get_mem_info()[0]
-
-        non_torch_allocations = total_allocated_bytes - torch_allocated_bytes
-        if non_torch_allocations > 0:
-            peak_memory += non_torch_allocations
+
         available_kv_cache_memory = (
             total_gpu_memory * self.cache_config.gpu_memory_utilization -
             peak_memory)
 
-        msg = ("After memory profiling run, "
-               f"peak memory usage is {peak_memory / 1024**2:.2f} MB,"
-               f"torch mem is {torch_allocated_bytes / 1024**2:.2f} MB, "
-               f"non-torch mem is {non_torch_allocations / 1024**2:.2f} MB, "
-               f"free gpu memory is {free_gpu_memory / 1024**2:.2f} MB.")
-        logger.info(msg)
-
+        # self.cache_config.threshold_mem = reserved_memory + available_kv_cache_memory
+        self.cache_config.threshold_mem = total_gpu_memory * 0.97
         return int(available_kv_cache_memory)
 
     def init_device(self):
@@ -141,11 +128,9 @@ class XPUWorker(Worker):
             raise RuntimeError(
                 f"Not support device type: {self.device_config.device}")
 
-        ENV_CCL_ZE_IPC_EXCHANGE = os.getenv("CCL_ZE_IPC_EXCHANGE", "drmfd")
         ENV_CCL_ATL_TRANSPORT = os.getenv("CCL_ATL_TRANSPORT", "ofi")
         ENV_LOCAL_WORLD_SIZE = os.getenv("LOCAL_WORLD_SIZE",
                                          str(self.parallel_config.world_size))
-        os.environ["CCL_ZE_IPC_EXCHANGE"] = ENV_CCL_ZE_IPC_EXCHANGE
         os.environ["CCL_ATL_TRANSPORT"] = ENV_CCL_ATL_TRANSPORT
         os.environ["LOCAL_WORLD_SIZE"] = ENV_LOCAL_WORLD_SIZE
         os.environ["LOCAL_RANK"] = str(self.local_rank)
