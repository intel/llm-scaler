diff --git a/.buildkite/lm-eval-harness/run-lm-eval-gsm-vllm-baseline.sh b/.buildkite/lm-eval-harness/run-lm-eval-gsm-vllm-baseline.sh
index 65be3c5d9..7e4808151 100644
--- a/.buildkite/lm-eval-harness/run-lm-eval-gsm-vllm-baseline.sh
+++ b/.buildkite/lm-eval-harness/run-lm-eval-gsm-vllm-baseline.sh
@@ -46,6 +46,6 @@ while getopts "m:b:l:f:t:" OPT; do
 done
 
 lm_eval --model vllm \
-  --model_args "pretrained=$MODEL,tensor_parallel_size=$TP_SIZE,distributed_executor_backend=ray,trust_remote_code=true,max_model_len=4096" \
+  --model_args "pretrained=$MODEL,tensor_parallel_size=$TP_SIZE,distributed_executor_backend=mp,trust_remote_code=true,max_model_len=4096,enforce_eager=true,max_num_batched_tokens=4096" \
   --tasks gsm8k --num_fewshot "$FEWSHOT" --limit "$LIMIT" \
   --batch_size "$BATCH_SIZE"
diff --git a/.buildkite/scripts/hardware_ci/run-xpu-test.sh b/.buildkite/scripts/hardware_ci/run-xpu-test.sh
index f54010c42..827649bfc 100644
--- a/.buildkite/scripts/hardware_ci/run-xpu-test.sh
+++ b/.buildkite/scripts/hardware_ci/run-xpu-test.sh
@@ -28,4 +28,5 @@ docker run \
     sh -c '
     VLLM_USE_V1=0 python3 examples/offline_inference/basic/generate.py --model facebook/opt-125m
     VLLM_USE_V1=0 python3 examples/offline_inference/basic/generate.py --model facebook/opt-125m -tp 2
+    VLLM_USE_V1=1 python3 examples/offline_inference/basic/generate.py --model facebook/opt-125m --block-size 64 --enforce-eager
 '
diff --git a/CMakeLists.txt b/CMakeLists.txt
index 3c5856fc5..d875cb4a0 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -92,6 +92,10 @@ if (NOT VLLM_TARGET_DEVICE STREQUAL "cuda" AND
     NOT VLLM_TARGET_DEVICE STREQUAL "rocm")
     if (VLLM_TARGET_DEVICE STREQUAL "cpu")
         include(${CMAKE_CURRENT_LIST_DIR}/cmake/cpu_extension.cmake)
+    elseif(VLLM_TARGET_DEVICE STREQUAL "xpu")
+        message(STATUS "Building XPU")
+        set(VLLM_GPU_LANG "SYCL")
+        include(${CMAKE_CURRENT_LIST_DIR}/cmake/xpu_extension.cmake)
     else()
         return()
     endif()
diff --git a/benchmarks/backend_request_func.py b/benchmarks/backend_request_func.py
index 88616e110..1b248db3c 100644
--- a/benchmarks/backend_request_func.py
+++ b/benchmarks/backend_request_func.py
@@ -17,7 +17,7 @@ from transformers import AutoTokenizer, PreTrainedTokenizer, PreTrainedTokenizer
 # NOTE(simon): do not import vLLM here so the benchmark script
 # can run without vLLM installed.
 
-AIOHTTP_TIMEOUT = aiohttp.ClientTimeout(total=6 * 60 * 60)
+AIOHTTP_TIMEOUT = aiohttp.ClientTimeout(total=2 * 6 * 60 * 60)
 
 
 @dataclass
diff --git a/benchmarks/benchmark_serving.py b/benchmarks/benchmark_serving.py
index a887e7150..6b197fb23 100644
--- a/benchmarks/benchmark_serving.py
+++ b/benchmarks/benchmark_serving.py
@@ -295,10 +295,11 @@ async def benchmark(
         raise ValueError(f"Unknown backend: {backend}")
 
     print("Starting initial single prompt test run...")
+    # set test_output_len=10 to avoid long prompt test run
     test_prompt, test_prompt_len, test_output_len, test_mm_content = (
         input_requests[0].prompt,
         input_requests[0].prompt_len,
-        input_requests[0].expected_output_len,
+        10,
         input_requests[0].multi_modal_data,
     )
 
diff --git a/benchmarks/benchmark_throughput.py b/benchmarks/benchmark_throughput.py
index 7a13babda..5be342a1b 100644
--- a/benchmarks/benchmark_throughput.py
+++ b/benchmarks/benchmark_throughput.py
@@ -43,6 +43,7 @@ def run_vllm(
     n: int,
     engine_args: EngineArgs,
     disable_detokenize: bool = False,
+    do_profile: bool = False,
 ) -> tuple[float, Optional[list[RequestOutput]]]:
     from vllm import LLM, SamplingParams
 
@@ -88,10 +89,14 @@ def run_vllm(
     outputs = None
     if not use_beam_search:
         start = time.perf_counter()
+        if do_profile:
+            llm.start_profile()
         outputs = llm.generate(
             prompts, sampling_params, lora_request=lora_requests, use_tqdm=True
         )
         end = time.perf_counter()
+        if do_profile:
+            llm.stop_profile()
     else:
         assert lora_requests is None, "BeamSearch API does not support LoRA"
         prompts = [request.prompt for request in requests]
@@ -408,6 +413,7 @@ def main(args: argparse.Namespace):
                 args.n,
                 EngineArgs.from_cli_args(args),
                 args.disable_detokenize,
+                args.profile
             )
     elif args.backend == "hf":
         assert args.tensor_parallel_size == 1
@@ -640,6 +646,10 @@ if __name__ == "__main__":
     parser.add_argument(
         "--num-prompts", type=int, default=1000, help="Number of prompts to process."
     )
+    parser.add_argument("--profile",
+                        action='store_true',
+                        default=False,
+                        help="whether run with profiler.")
     parser.add_argument(
         "--hf-max-batch-size",
         type=int,
diff --git a/cmake/utils.cmake b/cmake/utils.cmake
index 12e4e3902..c145ab9a2 100644
--- a/cmake/utils.cmake
+++ b/cmake/utils.cmake
@@ -449,7 +449,7 @@ function (define_gpu_extension_target GPU_MOD_NAME)
     GPU
     "WITH_SOABI"
     "DESTINATION;LANGUAGE;USE_SABI"
-    "SOURCES;ARCHITECTURES;COMPILE_FLAGS;INCLUDE_DIRECTORIES;LIBRARIES")
+    "SOURCES;ARCHITECTURES;COMPILE_FLAGS;INCLUDE_DIRECTORIES;LIBRARIES;LINK_FLAGS")
 
   # Add hipify preprocessing step when building with HIP/ROCm.
   if (GPU_LANGUAGE STREQUAL "HIP")
@@ -491,6 +491,11 @@ function (define_gpu_extension_target GPU_MOD_NAME)
 
   target_link_libraries(${GPU_MOD_NAME} PRIVATE torch ${GPU_LIBRARIES})
 
+  if (GPU_LANGUAGE STREQUAL "SYCL")
+    target_compile_options(${GPU_MOD_NAME} PRIVATE ${GPU_COMPILE_FLAGS})
+    target_link_options(${GPU_MOD_NAME} PRIVATE ${GPU_LINK_FLAGS})
+  endif()
+
   # Don't use `TORCH_LIBRARIES` for CUDA since it pulls in a bunch of
   # dependencies that are not necessary and may not be installed.
   if (GPU_LANGUAGE STREQUAL "CUDA")
diff --git a/cmake/xpu_extension.cmake b/cmake/xpu_extension.cmake
new file mode 100644
index 000000000..fd671a6bf
--- /dev/null
+++ b/cmake/xpu_extension.cmake
@@ -0,0 +1,62 @@
+set(CMAKE_EXPORT_COMPILE_COMMANDS ON)
+
+#
+# Define environment variables for special configurations
+#
+# TODO: detect Intel GPU Architecture(PVC or Arc) to add AOT flag.
+
+#
+# Check the compile flags
+#
+# append_cmake_prefix_path("intel_extension_for_pytorch" "intel_extension_for_pytorch.cmake_prefix_path")
+# find_package(IPEX REQUIRED)
+# IPEX will overwrite TORCH_LIBRARIES, so re-add torch_python lib.
+append_torchlib_if_found(torch_python)
+# include_directories(${IPEX_INCLUDE_DIRS})
+set(CMPLR_ROOT $ENV{CMPLR_ROOT})
+set(CMAKE_CXX_COMPILER icpx)
+set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -Wno-narrowing")
+set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -O3")
+set(VLLM_EXTRA_INCLUDE_DIRECTORIES ${CMPLR_ROOT}/include/sycl)
+
+list(APPEND VLLM_GPU_FLAGS "-fsycl" "-fsycl-targets=spir64")
+list(APPEND VLLM_GPU_LINK_FLAGS "-fsycl" "-fsycl-targets=spir64")
+list(APPEND VLLM_LINK_LIBRARIES "sycl" "OpenCL" "pthread" "m" "dl" "dnnl" )
+
+#
+# Define extension targets
+#
+
+#
+# _C extension
+#
+set(VLLM_EXT_SRC
+    "csrc/xpu/activation_xpu.cpp"
+    "csrc/xpu/attention_xpu.cpp"
+    "csrc/xpu/attention_xpu_fp8.cpp"
+    "csrc/xpu/cache_ops_xpu.cpp"
+    "csrc/xpu/cache_ops_xpu_fp8.cpp"
+    "csrc/xpu/gemm_kernels_xpu.cpp"
+    "csrc/xpu/layernorm_xpu.cpp"
+    "csrc/xpu/pos_encoding_xpu.cpp"
+    "csrc/xpu/utils.cpp"
+    "csrc/xpu/fused_moe.cpp"
+    "csrc/xpu/pybind.cpp")
+
+define_gpu_extension_target(
+    _C
+    DESTINATION vllm
+    LANGUAGE ${VLLM_GPU_LANG}
+    SOURCES ${VLLM_EXT_SRC}
+    COMPILE_FLAGS ${VLLM_GPU_FLAGS}
+    LINK_FLAGS ${VLLM_GPU_LINK_FLAGS}
+    ARCHITECTURES ${VLLM_GPU_ARCHES}
+    INCLUDE_DIRECTORIES ${VLLM_EXTRA_INCLUDE_DIRECTORIES}
+    LIBRARIES ${VLLM_LINK_LIBRARIES}
+    WITH_SOABI
+)
+
+add_custom_target(default_xpu)
+message(STATUS "Enabling C extension.")
+add_dependencies(default_xpu _C)
+
diff --git a/csrc/xpu/activation_xpu.cpp b/csrc/xpu/activation_xpu.cpp
new file mode 100644
index 000000000..6f98ddbb3
--- /dev/null
+++ b/csrc/xpu/activation_xpu.cpp
@@ -0,0 +1,278 @@
+// clang-format off
+#ifdef VLLM_DEV
+#undef __SYCL_DEVICE_ONLY__
+#endif
+#include <sycl/sycl.hpp>
+// clang-format on
+#include "xpu_types.h"
+
+#include <torch/extension.h>
+#include "utils.h"
+
+template <typename T>
+__inline__ T silu_xpu(const T& x) {
+  // x * sigmoid(x)
+  return (T)(((float)x) / (1.0f + sycl::exp((float)-x)));
+}
+
+template<typename T>
+__inline__ T gelu_xpu(const T& x) {
+  // Equivalent to PyTorch GELU with 'none' approximation.
+  // Refer to:
+  // https://github.com/pytorch/pytorch/blob/8ac9b20d4b090c213799e81acf48a55ea8d437d6/aten/src/ATen/native/cuda/ActivationGeluKernel.cu#L38
+  const float f = (float) x;
+  constexpr float ALPHA = M_SQRT1_2;
+  return (T) (f * 0.5f * (1.0f + sycl::erf(f * ALPHA)));
+}
+
+template<typename T>
+__inline__ T gelu_tanh_xpu(const T& x) {
+  const float f = (float) x;
+  constexpr float BETA = M_SQRT2 * M_2_SQRTPI * 0.5f;
+  constexpr float KAPPA = 0.044715;
+  float x_cube = f * f * f;
+  float inner = BETA * (f + KAPPA * x_cube);
+  return (T) (0.5f * f * (1.0f + ::tanhf(inner)));
+}
+
+template <typename scalar_t>
+void silu_and_mul_kernel(
+    scalar_t* __restrict__ out, // [..., d]
+    const scalar_t* __restrict__ input, // [..., 2, d]
+    const int d,
+    const sycl::nd_item<3>& item_ct1) {
+  const int64_t token_idx = item_ct1.get_group(2);
+  for (int64_t idx = item_ct1.get_local_id(2); idx < d;
+       idx += item_ct1.get_local_range(2)) {
+    const scalar_t x = input[token_idx * 2 * d + idx];
+    const scalar_t y = input[token_idx * 2 * d + d + idx];
+    out[token_idx * d + idx] = silu_xpu(x) * y;
+  }
+}
+
+template <typename scalar_t>
+void gelu_and_mul_kernel(
+    scalar_t* __restrict__ out, // [..., d]
+    const scalar_t* __restrict__ input, // [..., 2, d]
+    const int d,
+    const sycl::nd_item<3>& item_ct1) {
+  const int64_t token_idx = item_ct1.get_group(2);
+  for (int64_t idx = item_ct1.get_local_id(2); idx < d;
+       idx += item_ct1.get_local_range(2)) {
+    const scalar_t x = input[token_idx * 2 * d + idx];
+    const scalar_t y = input[token_idx * 2 * d + d + idx];
+    out[token_idx * d + idx] = gelu_xpu(x) * y;
+  }
+}
+
+template <typename scalar_t>
+void gelu_tanh_and_mul_kernel(
+    scalar_t* __restrict__ out, // [..., d]
+    const scalar_t* __restrict__ input, // [..., 2, d]
+    const int d,
+    const sycl::nd_item<3>& item_ct1) {
+  const int64_t token_idx = item_ct1.get_group(2);
+  for (int64_t idx = item_ct1.get_local_id(2); idx < d;
+       idx += item_ct1.get_local_range(2)) {
+    const scalar_t x = input[token_idx * 2 * d + idx];
+    const scalar_t y = input[token_idx * 2 * d + d + idx];
+    out[token_idx * d + idx] = gelu_tanh_xpu(x) * y;
+  }
+}
+
+
+template <typename scalar_t>
+void call_silu_and_mul_kernel(
+    int num_tokens,
+    int d,
+    const scalar_t* __restrict__ input,
+    scalar_t* __restrict__ output) {
+  using sycl_t = vllm::xpu::SyclTypeTrait<scalar_t>::Type;
+  sycl::range<3> grid(1, 1, num_tokens);
+  sycl::range<3> block(1, 1, std::min(d, 1024));
+  auto& queue = vllm::xpu::vllmGetQueue();
+  queue.submit([&](sycl::handler& cgh) {
+    cgh.parallel_for(
+        sycl::nd_range<3>(grid * block, block), [=](sycl::nd_item<3> item_ct1) {
+          silu_and_mul_kernel<sycl_t>(
+              (sycl_t*)output, (const sycl_t*)input, d, item_ct1);
+        });
+  });
+}
+
+template <typename scalar_t>
+void call_gelu_and_mul_kernel(
+    int num_tokens,
+    int d,
+    const scalar_t* __restrict__ input,
+    scalar_t* __restrict__ output) {
+  using sycl_t = vllm::xpu::SyclTypeTrait<scalar_t>::Type;
+  sycl::range<3> grid(1, 1, num_tokens);
+  sycl::range<3> block(1, 1, std::min(d, 1024));
+  auto& queue = vllm::xpu::vllmGetQueue();
+  queue.submit([&](sycl::handler& cgh) {
+    cgh.parallel_for(
+        sycl::nd_range<3>(grid * block, block), [=](sycl::nd_item<3> item_ct1) {
+          gelu_and_mul_kernel<sycl_t>(
+              (sycl_t*)output, (const sycl_t*)input, d, item_ct1);
+        });
+  });
+}
+
+template <typename scalar_t>
+void call_gelu_tanh_and_mul_kernel(
+    int num_tokens,
+    int d,
+    const scalar_t* __restrict__ input,
+    scalar_t* __restrict__ output) {
+  using sycl_t = vllm::xpu::SyclTypeTrait<scalar_t>::Type;
+  sycl::range<3> grid(1, 1, num_tokens);
+  sycl::range<3> block(1, 1, std::min(d, 1024));
+  auto& queue = vllm::xpu::vllmGetQueue();
+  queue.submit([&](sycl::handler& cgh) {
+    cgh.parallel_for(
+        sycl::nd_range<3>(grid * block, block), [=](sycl::nd_item<3> item_ct1) {
+          gelu_tanh_and_mul_kernel<sycl_t>(
+              (sycl_t*)output, (const sycl_t*)input, d, item_ct1);
+        });
+  });
+}
+
+void silu_and_mul(torch::Tensor& out, torch::Tensor& input) {
+  int num_tokens = input.numel() / input.size(-1);
+  int d = input.size(-1) / 2;
+
+  VLLM_XPU_DISPATCH_FLOATING_TYPES(
+      input.scalar_type(), "call_silu_and_mul_kernel", [&] {
+        call_silu_and_mul_kernel(
+            num_tokens,
+            d,
+            input.data_ptr<scalar_t>(),
+            out.data_ptr<scalar_t>());
+      });
+}
+
+// Element-wise activation kernel template.
+template <typename scalar_t, scalar_t (*ACT_FN)(const scalar_t&)>
+void activation_kernel(
+    scalar_t* __restrict__ out, // [..., d]
+    const scalar_t* __restrict__ input, // [..., d]
+    const int d,
+    const sycl::nd_item<3>& item_ct1) {
+  const int64_t token_idx = item_ct1.get_group(2);
+  for (int64_t idx = item_ct1.get_local_id(2); idx < d;
+       idx += item_ct1.get_local_range(2)) {
+    const scalar_t x = VLLM_LDG(&input[token_idx * d + idx]);
+    out[token_idx * d + idx] = ACT_FN(x);
+  }
+}
+
+template <typename T>
+__inline__ T gelu_new_kernel(const T& x) {
+  const float x3 = (float)(x * x * x);
+  const T t = (T)tanhf((T)(0.79788456f * (float)(x + (T)(0.044715f * x3))));
+  return ((T)0.5) * x * (((T)1.0) + t);
+}
+
+template <typename T>
+__inline__ T gelu_fast_kernel(const T& x) {
+  const float f = (float)x;
+  const T t =
+      (T)tanhf(((T)(f * 0.79788456f)) * (((T)1.0) + (T)(0.044715f * f) * x));
+  return ((T)0.5) * x * (((T)1.0) + t);
+}
+
+template <typename scalar_t>
+void call_gelu_new_activation_kernel(torch::Tensor& out, torch::Tensor& input) {
+  using sycl_t = vllm::xpu::SyclTypeTrait<scalar_t>::Type;
+  int d = input.size(-1);
+  int64_t num_tokens = input.numel() / d;
+  auto out_ptr = out.data_ptr<scalar_t>();
+  auto input_ptr = input.data_ptr<scalar_t>();
+  sycl::range<3> grid(1, 1, num_tokens);
+  sycl::range<3> block(1, 1, std::min(d, 1024));
+  auto& queue = vllm::xpu::vllmGetQueue();
+  queue.submit([&](sycl::handler& cgh) {
+    cgh.parallel_for(
+        sycl::nd_range<3>(grid * block, block), [=](sycl::nd_item<3> item_ct1) {
+          activation_kernel<sycl_t, gelu_new_kernel>(
+              (sycl_t* __restrict__)out_ptr,
+              (const sycl_t* __restrict__)input_ptr,
+              d,
+              item_ct1);
+        });
+  });
+}
+
+template <typename scalar_t>
+void call_gelu_fast_activation_kernel(
+    torch::Tensor& out,
+    torch::Tensor& input) {
+  using sycl_t = vllm::xpu::SyclTypeTrait<scalar_t>::Type;
+  int d = input.size(-1);
+  int64_t num_tokens = input.numel() / d;
+  auto out_ptr = out.data_ptr<scalar_t>();
+  auto input_ptr = input.data_ptr<scalar_t>();
+  sycl::range<3> grid(1, 1, num_tokens);
+  sycl::range<3> block(1, 1, std::min(d, 1024));
+  auto& queue = vllm::xpu::vllmGetQueue();
+  queue.submit([&](sycl::handler& cgh) {
+    cgh.parallel_for(
+        sycl::nd_range<3>(grid * block, block), [=](sycl::nd_item<3> item_ct1) {
+          activation_kernel<sycl_t, gelu_fast_kernel>(
+              (sycl_t* __restrict__)out_ptr,
+              (const sycl_t* __restrict__)input_ptr,
+              d,
+              item_ct1);
+        });
+  });
+}
+
+void gelu_new(torch::Tensor& out, torch::Tensor& input) {
+  VLLM_XPU_DISPATCH_FLOATING_TYPES(
+      out.scalar_type(), "call_gelu_new_activation_kernel", [&] {
+        call_gelu_new_activation_kernel<scalar_t>(out, input);
+      });
+}
+
+void gelu_fast(torch::Tensor& out, torch::Tensor& input) {
+  VLLM_XPU_DISPATCH_FLOATING_TYPES(
+      out.scalar_type(), "call_gelu_fast_activation_kernel", [&] {
+        call_gelu_fast_activation_kernel<scalar_t>(
+            out, input);
+      });
+}
+
+void gelu_and_mul(
+  torch::Tensor& out,      // [..., d]
+  torch::Tensor& input)    // [..., 2 * d]
+{
+    int num_tokens = input.numel() / input.size(-1);
+  int d = input.size(-1) / 2;
+
+  VLLM_XPU_DISPATCH_FLOATING_TYPES(
+      input.scalar_type(), "call_gelu_and_mul_kernel", [&] {
+        call_gelu_and_mul_kernel(
+            num_tokens,
+            d,
+            input.data_ptr<scalar_t>(),
+            out.data_ptr<scalar_t>());
+      });
+}
+
+void gelu_tanh_and_mul(
+  torch::Tensor& out,      // [..., d]
+  torch::Tensor& input)    // [..., 2 * d]
+{
+    int num_tokens = input.numel() / input.size(-1);
+  int d = input.size(-1) / 2;
+
+  VLLM_XPU_DISPATCH_FLOATING_TYPES(
+      input.scalar_type(), "call_gelu_tanh_and_mul_kernel", [&] {
+        call_gelu_tanh_and_mul_kernel(
+            num_tokens,
+            d,
+            input.data_ptr<scalar_t>(),
+            out.data_ptr<scalar_t>());
+      });
+}
\ No newline at end of file
diff --git a/csrc/xpu/attention_generic.h b/csrc/xpu/attention_generic.h
new file mode 100644
index 000000000..ab3688c82
--- /dev/null
+++ b/csrc/xpu/attention_generic.h
@@ -0,0 +1,64 @@
+/*
+ * Copyright (c) 2023, The vLLM team.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+#pragma once
+
+#include <dpct/dpct.hpp>
+#include <stdint.h>
+#include <sycl/sycl.hpp>
+
+namespace vllm {
+
+// A vector type to store Q, K, V elements.
+template <typename T, int VEC_SIZE>
+struct Vec {};
+
+// A vector type to store FP32 accumulators.
+template <typename T>
+struct FloatVec {};
+
+// Template vector operations.
+template <typename Acc, typename A, typename B>
+inline Acc mul(A a, B b);
+
+template <typename T>
+inline float sum(T v);
+
+template <typename T>
+inline float dot(T a, T b) {
+  return sum(mul<T, T, T>(a, b));
+}
+
+template <typename A, typename T>
+inline float dot(T a, T b) {
+  return sum(mul<A, T, T>(a, b));
+}
+
+template <typename T>
+inline void zero(T& dst) {
+  constexpr int WORDS = (sizeof(T) / 4) == 0 ? 1 : (sizeof(T) / 4);
+  union {
+    T raw;
+    uint32_t words[WORDS];
+  } tmp;
+
+#pragma unroll
+  for (int ii = 0; ii < WORDS; ++ii) {
+    tmp.words[ii] = 0u;
+  }
+  dst = tmp.raw;
+}
+
+} // namespace vllm
\ No newline at end of file
diff --git a/csrc/xpu/attention_xpu.cpp b/csrc/xpu/attention_xpu.cpp
new file mode 100644
index 000000000..97d5c0c21
--- /dev/null
+++ b/csrc/xpu/attention_xpu.cpp
@@ -0,0 +1,3031 @@
+// clang-format off
+#ifdef VLLM_DEV
+#undef __SYCL_DEVICE_ONLY__
+#endif
+#include <sycl/sycl.hpp>
+#include <dpct/dpct.hpp>
+#include <ext/intel/esimd.hpp>
+
+// clang-format on
+#include <float.h>
+#include <torch/extension.h>
+#include <stdexcept>
+#include "utils.h"
+#include "xpu_types.h"
+// #include "dtype_bfloat16.dp.hpp"
+#include "dtype_float16.h"
+#include "dtype_float32.h"
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+#include <c10/xpu/XPUStream.h>
+#endif
+
+#include <functional>
+// #include <ipex.h>
+
+#define WARP_SIZE 32
+#define MAX(a, b) ((a) > (b) ? (a) : (b))
+#define MIN(a, b) ((a) < (b) ? (a) : (b))
+#define DIVIDE_ROUND_UP(a, b) (((a) + (b)-1) / (b))
+using namespace sycl::ext::intel::esimd;
+
+template<typename T>
+static inline T attn_softcapping(T qk, float attn_logit_softcapping) {
+    qk = qk / attn_logit_softcapping;
+    qk = (sycl::exp(qk) - sycl::exp(-qk)) / (sycl::exp(qk) + sycl::exp(-qk));
+    qk = qk * attn_logit_softcapping;
+    return qk;
+}
+
+template <typename T>
+struct Float_Trait {
+  using Type = T;
+};
+
+template <>
+struct Float_Trait<c10::Half> {
+  using Type = uint16_t;
+};
+
+template <>
+struct Float_Trait<c10::BFloat16> {
+  using Type = sycl::ext::oneapi::bfloat16;
+};
+
+namespace vllm {
+
+// Q*K^T operation.
+template <int THREAD_GROUP_SIZE, typename Vec, int N>
+inline float qk_dot_(
+    const Vec* q,
+    const Vec* k,
+    const sycl::nd_item<3>& item_ct1) {
+  using A_vec = typename FloatVec<Vec>::Type;
+  // Compute the parallel products for Q*K^T (treat vector lanes separately).
+  A_vec qk_vec = mul<A_vec, Vec, Vec>(q[0], k[0]);
+#pragma unroll
+  for (int ii = 1; ii < N; ++ii) {
+    qk_vec = fma(q[ii], k[ii], qk_vec);
+  }
+
+  // Finalize the reduction across lanes.
+  float qk = sum(qk_vec);
+#pragma unroll
+  for (int mask = THREAD_GROUP_SIZE / 2; mask >= 1; mask /= 2) {
+    
+    qk += dpct::permute_sub_group_by_xor(
+        item_ct1.get_sub_group(), qk, mask);
+  }
+  return qk;
+}
+
+template <typename T, int THREAD_GROUP_SIZE>
+struct Qk_dot {
+  template <typename Vec, int N>
+  static inline float dot(
+      const Vec* q,
+      const Vec* k,
+      const sycl::nd_item<3>& item_ct1) {
+    return qk_dot_<THREAD_GROUP_SIZE, Vec, N>(q, k, item_ct1);
+  }
+};
+
+template <int NUM_WARPS>
+inline float block_sum(
+    float* red_smem,
+    float sum,
+    const sycl::nd_item<3>& item_ct1) {
+  // Decompose the thread index into warp / lane.
+  int warp = item_ct1.get_local_id(2) / WARP_SIZE;
+  int lane = item_ct1.get_local_id(2) % WARP_SIZE;
+
+  // Compute the sum per warp.
+#pragma unroll
+  for (int mask = WARP_SIZE / 2; mask >= 1; mask /= 2) {
+    
+    /*
+    DPCT1096:42: The right-most dimension of the work-group used in the SYCL
+    kernel that calls this function may be less than "32". The function
+    "dpct::permute_sub_group_by_xor" may return an unexpected result on the CPU
+    device. Modify the size of the work-group to ensure that the value of the
+    right-most dimension is a multiple of "32".
+    */
+    sum += dpct::permute_sub_group_by_xor(
+        item_ct1.get_sub_group(), sum, mask);
+  }
+
+  // Warp leaders store the data to shared memory.
+  if (lane == 0) {
+    red_smem[warp] = sum;
+  }
+
+  // Make sure the data is in shared memory.
+  
+  item_ct1.barrier(sycl::access::fence_space::local_space);
+
+  // The warps compute the final sums.
+  if (lane < NUM_WARPS) {
+    sum = red_smem[lane];
+  }
+
+  // Parallel reduction inside the warp.
+#pragma unroll
+  for (int mask = NUM_WARPS / 2; mask >= 1; mask /= 2) {
+    
+    /*
+    DPCT1096:43: The right-most dimension of the work-group used in the SYCL
+    kernel that calls this function may be less than "32". The function
+    "dpct::permute_sub_group_by_xor" may return an unexpected result on the CPU
+    device. Modify the size of the work-group to ensure that the value of the
+    right-most dimension is a multiple of "32".
+    */
+    sum += dpct::permute_sub_group_by_xor(
+        item_ct1.get_sub_group(), sum, mask);
+  }
+
+  // Broadcast to other threads.
+  
+  /*
+  DPCT1096:44: The right-most dimension of the work-group used in the SYCL
+  kernel that calls this function may be less than "32". The function
+  "dpct::select_from_sub_group" may return an unexpected result on the CPU
+  device. Modify the size of the work-group to ensure that the value of the
+  right-most dimension is a multiple of "32".
+  */
+  return dpct::select_from_sub_group(
+        item_ct1.get_sub_group(), sum, 0);
+}
+
+template <typename scalar_t, int GS, int HD>
+void context_attention_kernel_v1_reshaped(
+    void* query, void* key, void* value, const void* block_tables,
+    const float scale, const void* query_start_loc, const void* seq_lens,
+    const void* context_lens, const int block_size,
+    // const int x,  // x in kv_cache
+    void* out,    // output
+    const int block_table_stride_batch, const int block_table_stride_seq,
+    const int query_stride_bs, const int query_stride_head,
+    const int query_stride_dim, const int k_cache_stride_tokens,
+    const int k_cache_stride_head, const int k_cache_stride_block_size,
+    const int k_cache_stride_dim,
+    const int v_cache_stride_tokens, const int v_cache_stride_head,
+    const int v_cache_stride_block_size, const int v_cache_stride_dim,
+    const int out_stride_tokens, const int out_stride_head,
+    const int num_queries_per_kv, const int max_input_length,
+    const int batch_size, const int num_heads) {
+  static_assert(GS * HD * sizeof(scalar_t) * 2 < 64 * 1024);
+
+  const size_t key_slm_offset = 0;
+  const size_t value_slm_offset = GS * HD * sizeof(scalar_t);
+  sycl::queue& queue = vllm::xpu::vllmGetQueue();
+
+  // Get the maximum seq_lens
+  sycl::range<3> global_size(batch_size, num_heads,
+                             (max_input_length + GS - 1) / GS * GS);
+  sycl::range<3> local_size(1, 1, GS);
+
+  auto cgf = [&](sycl::handler& handle) {
+    handle.parallel_for(
+        sycl::nd_range<3>(global_size, local_size),
+        [=](sycl::nd_item<3> item) SYCL_ESIMD_KERNEL {
+          slm_init<GS * HD * sizeof(scalar_t) * 2>();
+
+          const size_t bsz_idx = item.get_global_id(0);
+          const size_t head_idx = item.get_global_id(1);
+          // Assuming we have 32 query head and 8 kv_heads. Then
+          // num_queries_per_group should be 4 For head_idx 13, then
+          // kv_head_idx = 13 / 4 = 3, which is correct
+          const size_t kv_head_idx = head_idx / num_queries_per_kv;
+          const int32_t seq_idx = item.get_global_id(2);
+          const size_t gid = item.get_group(2);
+          const size_t tid = item.get_local_id(2);
+
+          // const int64_t * seq_len = (const int64_t *) seq_lens;
+          const int32_t* seq_len = (const int32_t*)seq_lens;
+          int32_t seq_bound = seq_len[bsz_idx];
+
+          const int32_t* query_loc = (const int32_t*)query_start_loc;
+          // There is a possibility that the current token index pass
+          // over the seq_len, therefore: token_idx is the position in
+          // the query
+          int32_t token_idx =
+              query_loc[bsz_idx] + std::min(seq_idx, seq_bound - 1);
+
+          const int32_t* context_len_pointer = (const int32_t*)context_lens;
+
+          const int* block_tables_ptr = (const int*)block_tables;
+          const int* block_table =
+              block_tables_ptr + bsz_idx * block_table_stride_batch;
+          // I guess this context_len should be 0...
+          const int32_t context_len = context_len_pointer[bsz_idx];
+
+          // Position in the sequence
+          // context + seq_idx
+          // const int32_t token_position =
+          //     context_len + std::min(seq_idx, seq_bound - 1);
+          const int32_t token_position = context_len + seq_idx;
+
+          const scalar_t* query_head = (const scalar_t*)query +
+                                       token_idx * query_stride_bs +
+                                       head_idx * query_stride_head;
+          // Target output
+          scalar_t* out_head =
+              (scalar_t*)out +
+              (query_loc[bsz_idx] + seq_idx) * out_stride_tokens +
+              head_idx * out_stride_head;
+
+          int32_t context_groups = context_len / GS;
+
+          // Each token load its query_row
+          simd<scalar_t, HD> query_row =
+              block_load<scalar_t, HD>(query_head) * scale;
+          simd<scalar_t, HD> accv = 0;
+          simd<scalar_t, GS> softmaxv = 0;
+          scalar_t max_attn = -sycl::detail::max_v<scalar_t>();
+
+          // ################# Handle n * GS context part ######################
+          int32_t n = context_len / GS;
+          int32_t context_offset = context_len % GS;
+
+          for (int32_t group = 0; group < n; ++group) {
+            size_t target_key_position = group * GS + tid;
+            int which_block = target_key_position / block_size;
+            int which_slot = target_key_position % block_size;
+
+            int physical_block_number = block_table[which_block];
+            // Now key shape is [num_blocks, num_heads, block_size, head_dim]
+            const scalar_t* key_head =
+                (const scalar_t*)key +
+                physical_block_number * k_cache_stride_tokens +
+                kv_head_idx * k_cache_stride_head +
+                which_slot * k_cache_stride_block_size;
+            simd<scalar_t, HD> key_row = block_load<scalar_t, HD>(key_head);
+            slm_block_store(key_slm_offset + tid * HD * sizeof(scalar_t), key_row);
+
+            const scalar_t* value_head =
+                (const scalar_t*)value +
+                physical_block_number * v_cache_stride_tokens +
+                kv_head_idx * v_cache_stride_head + which_slot * v_cache_stride_block_size;
+            simd<scalar_t, HD> value_row = block_load<scalar_t, HD>(value_head);
+            slm_block_store(value_slm_offset + tid * HD * sizeof(scalar_t),
+                            value_row);
+            barrier();
+
+            // Calculate QK^T for this group...
+            simd<scalar_t, GS> attnv;
+#pragma unroll
+            for (size_t r = 0; r < GS; ++r) {
+              simd<scalar_t, HD> key_row = slm_block_load<scalar_t, HD>(
+                  key_slm_offset + r * HD * sizeof(scalar_t));
+              scalar_t attn =
+                  sycl::ext::intel::esimd::detail::sum<scalar_t, scalar_t, HD>(
+                      query_row * key_row);
+              attnv[r] = attn;
+            }
+            scalar_t new_max_attn =
+                std::max(hmax<scalar_t, scalar_t, GS>(attnv), max_attn);
+            scalar_t attn_exp = exp(max_attn - new_max_attn);
+            accv = accv * attn_exp;
+            softmaxv = softmaxv * attn_exp;
+            max_attn = new_max_attn;
+            const simd<scalar_t, GS> attn_expv = exp(attnv - max_attn);
+#pragma unorll
+            for (size_t r = 0; r < GS; ++r) {
+              simd<scalar_t, HD> value_row = slm_block_load<scalar_t, HD>(
+                  value_slm_offset + r * HD * sizeof(scalar_t));
+              accv += value_row * attn_expv[r];
+            }
+            softmaxv += attn_expv;
+            barrier();
+          }
+
+          // ########## End for handling context n * GS part ###########
+
+          // ########## Handle n * GS ################
+          for (size_t group = 0; group < gid; ++group) {
+            // 1. begins to load each position's key and value
+            size_t target_key_position = context_len + group * GS + tid;
+            int which_block = target_key_position / block_size;
+            int which_slot = target_key_position % block_size;
+
+            int physical_block_number = block_table[which_block];
+            const scalar_t* key_head =
+                (const scalar_t*)key +
+                physical_block_number * k_cache_stride_tokens +
+                kv_head_idx * k_cache_stride_head +
+                which_slot * k_cache_stride_block_size;
+            simd<scalar_t, HD> key_row = block_load<scalar_t, HD>(key_head);
+            slm_block_store(key_slm_offset + tid * HD * sizeof(scalar_t),
+                            key_row);
+            const scalar_t* value_head =
+                (const scalar_t*)value +
+                physical_block_number * v_cache_stride_tokens +
+                kv_head_idx * v_cache_stride_head + which_slot * v_cache_stride_block_size;
+            simd<scalar_t, HD> value_row = block_load<scalar_t, HD>(value_head);
+            slm_block_store(value_slm_offset + tid * HD * sizeof(scalar_t),
+                            value_row);
+            barrier();
+            simd<scalar_t, GS> attnv;
+#pragma unroll
+            for (size_t r = 0; r < GS; ++r) {
+              simd<scalar_t, HD> key_row = slm_block_load<scalar_t, HD>(
+                  key_slm_offset + r * HD * sizeof(scalar_t));
+              scalar_t attn =
+                  sycl::ext::intel::esimd::detail::sum<scalar_t, scalar_t, HD>(
+                      query_row * key_row);
+              attnv[r] = attn;
+            }
+
+            scalar_t new_max_attn =
+                std::max(hmax<scalar_t, scalar_t, GS>(attnv), max_attn);
+            scalar_t attn_exp = exp(max_attn - new_max_attn);
+            accv = accv * attn_exp;
+
+            softmaxv = softmaxv * attn_exp;
+            max_attn = new_max_attn;
+            const simd<scalar_t, GS> attn_expv = exp(attnv - max_attn);
+#pragma unroll
+            for (size_t r = 0; r < GS; ++r) {
+              simd<scalar_t, HD> value_row = slm_block_load<scalar_t, HD>(
+                  value_slm_offset + r * HD * sizeof(scalar_t));
+              accv += value_row * attn_expv[r];
+            }
+            softmaxv += attn_expv;
+            barrier();
+          }
+
+          // ######### End of handle n * GS part ##########
+
+          // ################ Handle offset part ####################
+          scalar_t softmax =
+              sycl::ext::intel::esimd::detail::sum<scalar_t, scalar_t, GS>(
+                  softmaxv);
+
+          // ########### handle context offset ############
+          if (tid < context_offset) {
+            size_t target_key_position = n * GS + tid;
+            int which_block = target_key_position / block_size;
+            int which_slot = target_key_position % block_size;
+
+            int physical_block_number = block_table[which_block];
+            const scalar_t* key_head =
+                (const scalar_t*)key +
+                physical_block_number * k_cache_stride_tokens +
+                kv_head_idx * k_cache_stride_head +
+                which_slot * k_cache_stride_block_size;
+            simd<scalar_t, HD> key_row = block_load<scalar_t, HD>(key_head);
+            slm_block_store(key_slm_offset + tid * HD * sizeof(scalar_t),
+                            key_row);
+
+            const scalar_t* value_head =
+                (const scalar_t*)value +
+                physical_block_number * v_cache_stride_tokens +
+                kv_head_idx * v_cache_stride_head +
+                which_slot * v_cache_stride_block_size;
+            simd<scalar_t, HD> value_row = block_load<scalar_t, HD>(value_head);
+            slm_block_store(value_slm_offset + tid * HD * sizeof(scalar_t),
+                            value_row);
+          }
+
+          barrier();
+
+          if (token_position < seq_bound) {
+#pragma unroll
+            for (size_t r = 0; r < context_offset; ++r) {
+              simd<scalar_t, HD> key_row = slm_block_load<scalar_t, HD>(
+                  key_slm_offset + r * HD * sizeof(scalar_t));
+              simd<scalar_t, HD> value_row = slm_block_load<scalar_t, HD>(
+                  value_slm_offset + r * HD * sizeof(scalar_t));
+              scalar_t attn =
+                  sycl::ext::intel::esimd::detail::sum<scalar_t, scalar_t, HD>(
+                      query_row * key_row);
+              if (attn <= max_attn) {
+                scalar_t attn_exp =
+                    sycl::ext::intel::esimd::exp(attn - max_attn);
+                accv += value_row * attn_exp;
+                softmax += attn_exp;
+              } else {
+                scalar_t attn_exp =
+                    sycl::ext::intel::esimd::exp(max_attn - attn);
+                accv = accv * attn_exp + value_row;
+                softmax = softmax * attn_exp + 1;
+                max_attn = attn;
+              }
+            }
+          }
+          barrier();
+
+          // ############## handle seq offset #################
+          if (token_position < seq_bound) {
+            const int64_t which_block =
+                static_cast<int64_t>(token_position / block_size);
+            const int64_t which_slot =
+                static_cast<int64_t>(token_position % block_size);
+
+            const int64_t physical_block_number =
+                static_cast<int64_t>(block_table[which_block]);
+
+            const scalar_t* key_head =
+                (const scalar_t*)key +
+                physical_block_number * k_cache_stride_tokens +
+                kv_head_idx * k_cache_stride_head +
+                which_slot * k_cache_stride_block_size;
+            simd<scalar_t, HD> key_row = block_load<scalar_t, HD>(key_head);
+            slm_block_store(key_slm_offset + tid * HD * sizeof(scalar_t),
+                            key_row);
+
+            // [num_blocks, num_kv_heads, head_size, block_size]
+            const scalar_t* value_head =
+                (const scalar_t*)value +
+                physical_block_number * v_cache_stride_tokens +
+                kv_head_idx * v_cache_stride_head +
+                which_slot * v_cache_stride_block_size;
+            simd<scalar_t, HD> value_row = block_load<scalar_t, HD>(value_head);
+            slm_block_store(value_slm_offset + tid * HD * sizeof(scalar_t),
+                            value_row);
+          }
+          barrier();
+
+          if (token_position < seq_bound) {
+            for (size_t r = 0; r <= tid; ++r) {
+              simd<scalar_t, HD> key_row = slm_block_load<scalar_t, HD>(
+                  key_slm_offset + r * HD * sizeof(scalar_t));
+              simd<scalar_t, HD> value_row = slm_block_load<scalar_t, HD>(
+                  value_slm_offset + r * HD * sizeof(scalar_t));
+              scalar_t attn =
+                  sycl::ext::intel::esimd::detail::sum<scalar_t, scalar_t, HD>(
+                      query_row * key_row);
+              if (attn <= max_attn) {
+                scalar_t attn_exp =
+                    sycl::ext::intel::esimd::exp(attn - max_attn);
+                accv += value_row * attn_exp;
+                softmax += attn_exp;
+              } else {
+                scalar_t attn_exp =
+                    sycl::ext::intel::esimd::exp(max_attn - attn);
+                accv = accv * attn_exp + value_row;
+                softmax = softmax * attn_exp + 1;
+                max_attn = attn;
+              }
+            }
+
+            if (softmax > 0) {
+              simd<scalar_t, HD> result = accv / softmax;
+              block_store(out_head, result);
+            } else {
+              simd<scalar_t, HD> result = 0;
+              block_store(out_head, result);
+            }
+          }
+          // ######## Ending of handling seq offset ##########
+        });
+  };
+  queue.submit(cgf);
+}
+
+// How about implement a first edition that can be used with non-chunked
+// prefill requests, so that we can make sure the reference for heads is
+// correct
+template <typename scalar_t, int GS, int HD>
+void context_attention_kernel_v1(
+    void* query, void* key, void* value, const void* block_tables,
+    const float scale, const void* query_start_loc, const void* seq_lens,
+    const void* context_lens, const int block_size,
+    const int x,  // x in kv_cache
+    void* out,    // output
+    const int block_table_stride_batch, const int block_table_stride_seq,
+    const int query_stride_bs, const int query_stride_head,
+    const int query_stride_dim, const int k_cache_stride_tokens,
+    const int k_cache_stride_head, const int k_cache_stride_dim,
+    const int k_cache_stride_block_size, const int k_cache_stride_x,
+    const int v_cache_stride_tokens, const int v_cache_stride_head,
+    const int v_cache_stride_dim, const int v_cache_stride_block_size,
+    const int out_stride_tokens, const int out_stride_head,
+    const int num_queries_per_kv, const int max_input_length,
+    const int batch_size, const int num_heads) {
+  static_assert(GS * HD * sizeof(scalar_t) * 2 < 64 * 1024);
+
+  const size_t key_slm_offset = 0;
+  const size_t value_slm_offset = GS * HD * sizeof(scalar_t);
+  sycl::queue& queue = vllm::xpu::vllmGetQueue();
+
+  // Get the maximum seq_lens
+  sycl::range<3> global_size(batch_size, num_heads,
+                             (max_input_length + GS - 1) / GS * GS);
+  sycl::range<3> local_size(1, 1, GS);
+
+  auto cgf = [&](sycl::handler& handle) {
+    handle.parallel_for(
+        sycl::nd_range<3>(global_size, local_size),
+        [=](sycl::nd_item<3> item) SYCL_ESIMD_KERNEL {
+          slm_init<GS * HD * sizeof(scalar_t) * 2>();
+
+          const size_t bsz_idx = item.get_global_id(0);
+          const size_t head_idx = item.get_global_id(1);
+          // Assuming we have 32 query head and 8 kv_heads. Then
+          // num_queries_per_group should be 4 For head_idx 13, then
+          // kv_head_idx = 13 / 4 = 3, which is correct
+          const size_t kv_head_idx = head_idx / num_queries_per_kv;
+          const int32_t seq_idx = item.get_global_id(2);
+          const size_t gid = item.get_group(2);
+          const size_t tid = item.get_local_id(2);
+
+          // const int64_t * seq_len = (const int64_t *) seq_lens;
+          const int32_t* seq_len = (const int32_t*)seq_lens;
+          int32_t seq_bound = seq_len[bsz_idx];
+
+          const int32_t* query_loc = (const int32_t*)query_start_loc;
+          // There is a possibility that the current token index pass
+          // over the seq_len, therefore: token_idx is the position in
+          // the query
+          int32_t token_idx =
+              query_loc[bsz_idx] + std::min(seq_idx, seq_bound - 1);
+
+          const int32_t* context_len_pointer = (const int32_t*)context_lens;
+
+          const int* block_tables_ptr = (const int*)block_tables;
+          const int* block_table =
+              block_tables_ptr + bsz_idx * block_table_stride_batch;
+          // I guess this context_len should be 0...
+          const int32_t context_len = context_len_pointer[bsz_idx];
+
+          // Position in the sequence
+          // context + seq_idx
+          // const int32_t token_position =
+          //     context_len + std::min(seq_idx, seq_bound - 1);
+          const int32_t token_position = context_len + seq_idx;
+
+          // static const CONSTANT char FMT[] =
+          //     "Invoke target function...\n ";
+
+          // sycl::ext::oneapi::experimental::printf(FMT);
+          // static const CONSTANT char FMT[] =
+          //     "GroupID = %6d bsz_idx = %6d seq_len = %6d seq_idx =
+          //     %6d" "local_id = "
+          //     "%6d "
+          //     "token_idx = %6d "
+          //     "context_len = %6d "
+          //     "v_cache_stride_head_dim = %6d "
+          //     "token_position = %6d\n";
+          // sycl::ext::oneapi::experimental::printf(
+          //     FMT, gid, bsz_idx, seq_bound, seq_idx, tid,
+          //     token_idx, context_len, v_cache_stride_dim,
+          //     token_position);
+
+          const scalar_t* query_head = (const scalar_t*)query +
+                                       token_idx * query_stride_bs +
+                                       head_idx * query_stride_head;
+          // Target output
+          scalar_t* out_head =
+              (scalar_t*)out +
+              (query_loc[bsz_idx] + seq_idx) * out_stride_tokens +
+              head_idx * out_stride_head;
+
+          int32_t context_groups = context_len / GS;
+
+          // Each token load its query_row
+          simd<scalar_t, HD> query_row =
+              block_load<scalar_t, HD>(query_head) * scale;
+          simd<scalar_t, HD> accv = 0;
+          simd<scalar_t, GS> softmaxv = 0;
+          scalar_t max_attn = -sycl::detail::max_v<scalar_t>();
+
+          // ################# Handle n * GS context part ######################
+          int32_t n = context_len / GS;
+          int32_t context_offset = context_len % GS;
+
+          for (int32_t group = 0; group < n; ++group) {
+            size_t target_key_position = group * GS + tid;
+            int which_block = target_key_position / block_size;
+            int which_slot = target_key_position % block_size;
+
+            int physical_block_number = block_table[which_block];
+            const scalar_t* key_head =
+                (const scalar_t*)key +
+                physical_block_number * k_cache_stride_tokens +
+                kv_head_idx * k_cache_stride_head +
+                which_slot * k_cache_stride_block_size;
+            for (int i = 0; i < HD / x; i++) {
+              // Load 8 elements, decided by x
+              simd<scalar_t, 8> key_row =
+                  block_load<scalar_t, 8>(key_head + i * k_cache_stride_dim);
+              slm_block_store(key_slm_offset + tid * HD * sizeof(scalar_t) +
+                                  8 * i * sizeof(scalar_t),
+                              key_row);
+            }
+
+            const scalar_t* value_head =
+                (const scalar_t*)value +
+                physical_block_number * v_cache_stride_tokens +
+                kv_head_idx * v_cache_stride_head + which_slot;
+            for (int i = 0; i < HD; i++) {
+              scalar_t temp_value = value_head[i * v_cache_stride_dim];
+              slm_scalar_store<scalar_t>(value_slm_offset +
+                                             tid * HD * sizeof(scalar_t) +
+                                             i * sizeof(scalar_t),
+                                         temp_value);
+            }
+            barrier();
+
+            // Calculate QK^T for this group...
+            simd<scalar_t, GS> attnv;
+#pragma unroll
+            for (size_t r = 0; r < GS; ++r) {
+              simd<scalar_t, HD> key_row = slm_block_load<scalar_t, HD>(
+                  key_slm_offset + r * HD * sizeof(scalar_t));
+              scalar_t attn =
+                  sycl::ext::intel::esimd::detail::sum<scalar_t, scalar_t, HD>(
+                      query_row * key_row);
+              attnv[r] = attn;
+            }
+            scalar_t new_max_attn =
+                std::max(hmax<scalar_t, scalar_t, GS>(attnv), max_attn);
+            scalar_t attn_exp = exp(max_attn - new_max_attn);
+            accv = accv * attn_exp;
+            softmaxv = softmaxv * attn_exp;
+            max_attn = new_max_attn;
+            const simd<scalar_t, GS> attn_expv = exp(attnv - max_attn);
+#pragma unorll
+            for (size_t r = 0; r < GS; ++r) {
+              simd<scalar_t, HD> value_row = slm_block_load<scalar_t, HD>(
+                  value_slm_offset + r * HD * sizeof(scalar_t));
+              accv += value_row * attn_expv[r];
+            }
+            softmaxv += attn_expv;
+            barrier();
+          }
+
+          // ########## End for handling context n * GS part ###########
+
+          // ########## Handle n * GS ################
+          for (size_t group = 0; group < gid; ++group) {
+            // 1. begins to load each position's key and value
+            size_t target_key_position = context_len + group * GS + tid;
+            int which_block = target_key_position / block_size;
+            int which_slot = target_key_position % block_size;
+
+            int physical_block_number = block_table[which_block];
+            const scalar_t* key_head =
+                (const scalar_t*)key +
+                physical_block_number * k_cache_stride_tokens +
+                kv_head_idx * k_cache_stride_head +
+                which_slot * k_cache_stride_block_size;
+            for (int i = 0; i < HD / x; i++) {
+              // Load 8 elements
+              simd<scalar_t, 8> key_row =
+                  block_load<scalar_t, 8>(key_head + i * k_cache_stride_dim);
+              slm_block_store(key_slm_offset + tid * HD * sizeof(scalar_t) +
+                                  8 * i * sizeof(scalar_t),
+                              key_row);
+            }
+
+            const scalar_t* value_head =
+                (const scalar_t*)value +
+                physical_block_number * v_cache_stride_tokens +
+                kv_head_idx * v_cache_stride_head + which_slot;
+            for (int i = 0; i < HD; i++) {
+              scalar_t temp_value = value_head[i * v_cache_stride_dim];
+              slm_scalar_store<scalar_t>(value_slm_offset +
+                                             tid * HD * sizeof(scalar_t) +
+                                             i * sizeof(scalar_t),
+                                         temp_value);
+            }
+            barrier();
+            simd<scalar_t, GS> attnv;
+#pragma unroll
+            for (size_t r = 0; r < GS; ++r) {
+              simd<scalar_t, HD> key_row = slm_block_load<scalar_t, HD>(
+                  key_slm_offset + r * HD * sizeof(scalar_t));
+              scalar_t attn =
+                  sycl::ext::intel::esimd::detail::sum<scalar_t, scalar_t, HD>(
+                      query_row * key_row);
+              attnv[r] = attn;
+            }
+
+            scalar_t new_max_attn =
+                std::max(hmax<scalar_t, scalar_t, GS>(attnv), max_attn);
+            scalar_t attn_exp = exp(max_attn - new_max_attn);
+            accv = accv * attn_exp;
+
+            softmaxv = softmaxv * attn_exp;
+            max_attn = new_max_attn;
+            const simd<scalar_t, GS> attn_expv = exp(attnv - max_attn);
+#pragma unroll
+            for (size_t r = 0; r < GS; ++r) {
+              simd<scalar_t, HD> value_row = slm_block_load<scalar_t, HD>(
+                  value_slm_offset + r * HD * sizeof(scalar_t));
+              accv += value_row * attn_expv[r];
+            }
+            softmaxv += attn_expv;
+            barrier();
+          }
+
+          // ######### End of handle n * GS part ##########
+
+          // ################ Handle offset part ####################
+          scalar_t softmax =
+              sycl::ext::intel::esimd::detail::sum<scalar_t, scalar_t, GS>(
+                  softmaxv);
+
+          // ########### handle context offset ############
+          if (tid < context_offset) {
+            size_t target_key_position = n * GS + tid;
+            int which_block = target_key_position / block_size;
+            int which_slot = target_key_position % block_size;
+
+            int physical_block_number = block_table[which_block];
+            const scalar_t* key_head =
+                (const scalar_t*)key +
+                physical_block_number * k_cache_stride_tokens +
+                kv_head_idx * k_cache_stride_head +
+                which_slot * k_cache_stride_block_size;
+            for (int i = 0; i < HD / x; i++) {
+              // Load 8 elements
+              simd<scalar_t, 8> key_row =
+                  block_load<scalar_t, 8>(key_head + i * k_cache_stride_dim);
+              slm_block_store(key_slm_offset + tid * HD * sizeof(scalar_t) +
+                                  8 * i * sizeof(scalar_t),
+                              key_row);
+            }
+
+            const scalar_t* value_head =
+                (const scalar_t*)value +
+                physical_block_number * v_cache_stride_tokens +
+                kv_head_idx * v_cache_stride_head + which_slot;
+            for (int i = 0; i < HD; i++) {
+              // Seems to have an error here
+              scalar_t temp_value = value_head[i * v_cache_stride_dim];
+              slm_scalar_store<scalar_t>(value_slm_offset +
+                                             tid * HD * sizeof(scalar_t) +
+                                             i * sizeof(scalar_t),
+                                         temp_value);
+            }
+          }
+
+          barrier();
+
+          if (token_position < seq_bound) {
+#pragma unroll
+            for (size_t r = 0; r < context_offset; ++r) {
+              simd<scalar_t, HD> key_row = slm_block_load<scalar_t, HD>(
+                  key_slm_offset + r * HD * sizeof(scalar_t));
+              simd<scalar_t, HD> value_row = slm_block_load<scalar_t, HD>(
+                  value_slm_offset + r * HD * sizeof(scalar_t));
+              scalar_t attn =
+                  sycl::ext::intel::esimd::detail::sum<scalar_t, scalar_t, HD>(
+                      query_row * key_row);
+              if (attn <= max_attn) {
+                scalar_t attn_exp =
+                    sycl::ext::intel::esimd::exp(attn - max_attn);
+                accv += value_row * attn_exp;
+                softmax += attn_exp;
+              } else {
+                scalar_t attn_exp =
+                    sycl::ext::intel::esimd::exp(max_attn - attn);
+                accv = accv * attn_exp + value_row;
+                softmax = softmax * attn_exp + 1;
+                max_attn = attn;
+              }
+            }
+          }
+          barrier();
+
+          // ############## handle seq offset #################
+          if (token_position < seq_bound) {
+            const int64_t which_block =
+                static_cast<int64_t>(token_position / block_size);
+            const int64_t which_slot =
+                static_cast<int64_t>(token_position % block_size);
+
+            const int64_t physical_block_number =
+                static_cast<int64_t>(block_table[which_block]);
+
+            const scalar_t* key_head =
+                (const scalar_t*)key +
+                physical_block_number * k_cache_stride_tokens +
+                kv_head_idx * k_cache_stride_head +
+                which_slot * k_cache_stride_block_size;
+
+            for (int i = 0; i < HD / x; i++) {
+              // Load 8 elements
+              simd<scalar_t, 8> key_row =
+                  block_load<scalar_t, 8>(key_head + i * k_cache_stride_dim);
+              slm_block_store(key_slm_offset + tid * HD * sizeof(scalar_t) +
+                                  8 * i * sizeof(scalar_t),
+                              key_row);
+            }
+
+            // [num_blocks, num_kv_heads, head_size, block_size]
+            const scalar_t* value_head =
+                (const scalar_t*)value +
+                physical_block_number * v_cache_stride_tokens +
+                kv_head_idx * v_cache_stride_head + which_slot;
+            for (int i = 0; i < HD; i++) {
+              scalar_t temp_value = value_head[i * v_cache_stride_dim];
+              slm_scalar_store<scalar_t>(value_slm_offset +
+                                             tid * HD * sizeof(scalar_t) +
+                                             i * sizeof(scalar_t),
+                                         temp_value);
+            }
+          }
+          barrier();
+
+          if (token_position < seq_bound) {
+            for (size_t r = 0; r <= tid; ++r) {
+              simd<scalar_t, HD> key_row = slm_block_load<scalar_t, HD>(
+                  key_slm_offset + r * HD * sizeof(scalar_t));
+              simd<scalar_t, HD> value_row = slm_block_load<scalar_t, HD>(
+                  value_slm_offset + r * HD * sizeof(scalar_t));
+              scalar_t attn =
+                  sycl::ext::intel::esimd::detail::sum<scalar_t, scalar_t, HD>(
+                      query_row * key_row);
+              if (attn <= max_attn) {
+                scalar_t attn_exp =
+                    sycl::ext::intel::esimd::exp(attn - max_attn);
+                accv += value_row * attn_exp;
+                softmax += attn_exp;
+              } else {
+                scalar_t attn_exp =
+                    sycl::ext::intel::esimd::exp(max_attn - attn);
+                accv = accv * attn_exp + value_row;
+                softmax = softmax * attn_exp + 1;
+                max_attn = attn;
+              }
+            }
+
+            if (softmax > 0) {
+              simd<scalar_t, HD> result = accv / softmax;
+              block_store(out_head, result);
+            } else {
+              simd<scalar_t, HD> result = 0;
+              block_store(out_head, result);
+            }
+          }
+          // ######## Ending of handling seq offset ##########
+        });
+  };
+  queue.submit(cgf);
+}
+
+template <typename T, int GS, int HD>
+void context_attention_kernel_v2(
+    void* query, void* key, void* value, const void* block_tables,
+    const float scale, const void* query_start_loc, const void* seq_lens,
+    const void* context_lens, const int block_size,
+    const int x,  // x in kv_cache
+    void* out,    // output
+    const int block_table_stride_batch, const int block_table_stride_seq,
+    const int query_stride_bs, const int query_stride_head,
+    const int query_stride_dim, const int k_cache_stride_tokens,
+    const int k_cache_stride_head, const int k_cache_stride_dim,
+    const int k_cache_stride_block_size, const int k_cache_stride_x,
+    const int v_cache_stride_tokens, const int v_cache_stride_head,
+    const int v_cache_stride_dim, const int v_cache_stride_block_size,
+    const int out_stride_tokens, const int out_stride_head,
+    const int num_queries_per_kv, const int max_input_length,
+    const int batch_size, const int num_heads, const int num_tokens,
+    const int max_context_len, const int max_q_len) {
+  constexpr int BLOCK_SIZE = 8;
+  constexpr int NUM_THREADS = 128;
+  // Each wrap handles one context block, therefore, each thread_group_size is
+  // this.
+  constexpr int THREAD_GROUP_SIZE = MAX(WARP_SIZE / BLOCK_SIZE, 1);
+  // Each query, and key thread_group loads 16 bytes
+  // Assume TGS=4 then 16 / 4 / sizeof(half) = 2
+  constexpr int VEC_SIZE = MAX(16 / (THREAD_GROUP_SIZE * sizeof(T)), 1);
+  using sycl_t = vllm::xpu::SyclTypeTrait<T>::Type;
+  using Q_Vec = typename Vec<sycl_t, VEC_SIZE>::Type;
+
+  // Assuming HD = 128, TGS = 2, then 128 / 2 / 2 = 32
+  int num_vecs_per_thread = HD / THREAD_GROUP_SIZE / VEC_SIZE;
+  sycl_t* out_p = reinterpret_cast<sycl_t*>(out);
+  sycl_t* query_ptr = reinterpret_cast<sycl_t*>(query);
+  sycl_t* key_cache_ptr = reinterpret_cast<sycl_t*>(key);
+  sycl_t* value_cache_ptr = reinterpret_cast<sycl_t*>(value);
+  const int* query_loc_ptr = reinterpret_cast<const int*>(query_start_loc);
+  const int* block_tables_ptr = reinterpret_cast<const int*>(block_tables);
+  const int* context_lens_ptr = reinterpret_cast<const int*>(context_lens);
+  const int* seq_lens_ptr = reinterpret_cast<const int*>(seq_lens);
+
+  constexpr int NUM_WARPS = NUM_THREADS / WARP_SIZE;
+  int padded_max_context_len =
+      DIVIDE_ROUND_UP(max_context_len + 1 + max_q_len, BLOCK_SIZE) * BLOCK_SIZE;
+  int logits_size = padded_max_context_len * sizeof(float);
+  int outputs_size = (NUM_WARPS / 2) * HD * sizeof(float);
+  // Python-side check in
+  // vllm.worker.worker._check_if_can_support_max_seq_len Keep that in
+  // sync with the logic here!
+  int shared_mem_size = std::max(logits_size, outputs_size);
+  // WARN: we have changed this...
+  sycl::range<3> grid(batch_size, num_heads, max_q_len);
+  // One work-group that is executing on the device
+  sycl::range<3> block(1, 1, NUM_THREADS);
+  sycl::queue& queue = vllm::xpu::vllmGetQueue();
+
+  auto cgf = [&](sycl::handler& handle) {
+    sycl::local_accessor<uint8_t, 1> dpct_local_acc_ct1(
+        sycl::range<1>(shared_mem_size), handle);
+    sycl::local_accessor<Q_Vec, 1> q_vecs_acc_ct1(
+        sycl::range<1>(THREAD_GROUP_SIZE * num_vecs_per_thread), handle);
+    sycl::local_accessor<float, 1> red_smem_acc_ct1(
+        sycl::range<1>(2 * NUM_WARPS), handle);
+
+    handle.parallel_for(
+        sycl::nd_range<3>(grid * block, block),
+        [=](sycl::nd_item<3> item_ct1) [[intel::reqd_sub_group_size(32)]] {
+          const int bsz_idx = item_ct1.get_group(0);
+          const int seq_idx = item_ct1.get_group(2);
+          constexpr bool USE_PARTITIONING = false;
+          int context_len = context_lens_ptr[bsz_idx] + seq_idx;
+          const int seq_len = seq_lens_ptr[bsz_idx];
+          uint8_t* dpct_local = dpct_local_acc_ct1.get_pointer();
+          Q_Vec* q_vecs = q_vecs_acc_ct1.get_pointer();
+          float* red_smem = red_smem_acc_ct1.get_pointer();
+
+          // output_stream << "Original context_len: " <<
+          // context_lens_ptr[bsz_idx] << sycl::endl; output_stream <<
+          // "Batch_idx: " << bsz_idx << " Seq_idx: " << seq_idx
+          //     << " Context_len: " << context_len << " Original context_len: "
+          //     << context_lens_ptr[bsz_idx] << " Seq_len: " << seq_len
+          //     << " Max input length: " << max_input_length
+          //     << sycl::endl;
+          if (context_len >= seq_len) {
+            return;
+          }
+
+          context_len = context_len + 1;
+
+          const int num_context_blocks =
+              DIVIDE_ROUND_UP(context_len, BLOCK_SIZE);
+          const int num_blocks_per_partition = num_context_blocks;
+
+          const int start_block_idx = 0;
+          const int end_block_idx =
+              MIN(start_block_idx + num_context_blocks, num_context_blocks);
+
+          const int num_blocks = end_block_idx - start_block_idx;
+          const int start_token_idx = start_block_idx * BLOCK_SIZE;
+          const int end_token_idx =
+              MIN(start_token_idx + num_blocks * BLOCK_SIZE, context_len);
+          const int num_tokens = end_token_idx - start_token_idx;
+          constexpr int THREAD_GROUP_SIZE = MAX(WARP_SIZE / BLOCK_SIZE, 1);
+          constexpr int NUM_THREAD_GROUPS =
+              NUM_THREADS /
+              THREAD_GROUP_SIZE;  // Note: This assumes THREAD_GROUP_SIZE
+          constexpr int NUM_TOKENS_PER_THREAD_GROUP =
+              DIVIDE_ROUND_UP(BLOCK_SIZE, WARP_SIZE);
+          constexpr int NUM_WARPS = NUM_THREADS / WARP_SIZE;
+          const int thread_idx = item_ct1.get_local_id(2);
+          const int warp_idx = thread_idx / WARP_SIZE;
+          const int lane = thread_idx % WARP_SIZE;
+          const int head_idx = item_ct1.get_group(1);
+          const int num_heads = item_ct1.get_group_range(1);
+          const int kv_head_idx = head_idx / num_queries_per_kv;
+          // TODO: consider alibi_slope later
+          constexpr int NUM_ELEMS_PER_THREAD = HD / THREAD_GROUP_SIZE;
+          constexpr int NUM_VECS_PER_THREAD = NUM_ELEMS_PER_THREAD / VEC_SIZE;
+          const int thread_group_idx = thread_idx / THREAD_GROUP_SIZE;
+          const int thread_group_offset = thread_idx % THREAD_GROUP_SIZE;
+          const sycl_t* q_ptr =
+              query_ptr + (query_loc_ptr[bsz_idx] + seq_idx) * query_stride_bs +
+              head_idx * HD;
+
+#pragma unroll
+          for (int i = thread_group_idx; i < NUM_VECS_PER_THREAD;
+               i += NUM_THREAD_GROUPS) {
+            const int vec_idx = thread_group_offset + i * THREAD_GROUP_SIZE;
+            q_vecs[thread_group_offset * NUM_VECS_PER_THREAD + i] =
+                *reinterpret_cast<const Q_Vec*>(q_ptr + vec_idx * VEC_SIZE);
+          }
+          // Loaded q_vecs
+          item_ct1.barrier(sycl::access::fence_space::local_space);
+          auto shared_mem = (char*)dpct_local;
+          float* logits = reinterpret_cast<float*>(shared_mem);
+          constexpr int x = 16 / sizeof(sycl_t);
+          float qk_max = -FLT_MAX;
+          const int* block_table =
+              block_tables_ptr + bsz_idx * block_table_stride_batch;
+
+          // Loading key
+          for (int block_idx = start_block_idx + warp_idx;
+               block_idx < end_block_idx; block_idx += NUM_WARPS) {
+            const int64_t physical_block_number =
+                static_cast<int64_t>(block_table[block_idx]);
+            for (int i = 0; i < NUM_TOKENS_PER_THREAD_GROUP; i++) {
+              const int physical_block_offset =
+                  (thread_group_idx + i * WARP_SIZE) % BLOCK_SIZE;
+              const int token_idx =
+                  block_idx * BLOCK_SIZE + physical_block_offset;
+
+              Q_Vec k_vecs[NUM_VECS_PER_THREAD];
+
+#pragma unroll
+              for (int j = 0; j < NUM_VECS_PER_THREAD; j++) {
+                const sycl_t* k_ptr =
+                    key_cache_ptr +
+                    physical_block_number * k_cache_stride_tokens +
+                    kv_head_idx * k_cache_stride_head +
+                    physical_block_offset * x;
+
+                const int vec_idx = thread_group_offset + j * THREAD_GROUP_SIZE;
+                const int offset1 = (vec_idx * VEC_SIZE) / x;
+                const int offset2 = (vec_idx * VEC_SIZE) % x;
+                k_vecs[j] = *reinterpret_cast<const Q_Vec*>(
+                    k_ptr + offset1 * BLOCK_SIZE * x + offset2);
+              }
+
+              // Compute dot product.
+              // This includes a reduction across the threads in the
+              // same thread group. Q_Vec_t
+              // q_vec_[NUM_VECS_PER_THREAD] = q_vecs +
+              // thread_group_offset * THREAD_GROUP_SIZE;
+              float qk = scale *
+                         Qk_dot<sycl_t, THREAD_GROUP_SIZE>::template dot<
+                             Q_Vec, NUM_VECS_PER_THREAD>(
+                             q_vecs + thread_group_offset * NUM_VECS_PER_THREAD,
+                             k_vecs, item_ct1);
+
+              if (thread_group_offset == 0) {
+                // Store the partial reductions to shared memory.
+                // NOTE(woosuk): It is required to zero out the
+                // masked logits.
+                const bool mask = token_idx > context_len;
+                logits[token_idx - start_token_idx] = mask ? 0.f : qk;
+                qk_max = mask ? qk_max : sycl::fmax(qk_max, qk);
+              }
+            }
+          }
+#pragma unroll
+          for (int mask = WARP_SIZE / 2; mask >= THREAD_GROUP_SIZE; mask /= 2) {
+            /*
+            DPCT1096:38: The right-most dimension of the work-group used
+            in the SYCL kernel that calls this function may be less than
+            "32". The function "dpct::permute_sub_group_by_xor" may
+            return an unexpected result on the CPU device. Modify the
+            size of the work-group to ensure that the value of the
+            right-most dimension is a multiple of "32".
+            */
+            qk_max =
+                sycl::fmax(qk_max, dpct::permute_sub_group_by_xor(
+                                       item_ct1.get_sub_group(), qk_max, mask));
+          }
+          if (lane == 0) {
+            red_smem[warp_idx] = qk_max;
+          }
+          item_ct1.barrier(sycl::access::fence_space::local_space);
+          // TODO(woosuk): Refactor this part.
+          // Get the max qk value for the sequence.
+          qk_max = lane < NUM_WARPS ? red_smem[lane] : -FLT_MAX;
+#pragma unroll
+          for (int mask = NUM_WARPS / 2; mask >= 1; mask /= 2) {
+            /*
+            DPCT1096:39: The right-most dimension of the work-group used
+            in the SYCL kernel that calls this function may be less than
+            "32". The function "dpct::permute_sub_group_by_xor" may
+            return an unexpected result on the CPU device. Modify the
+            size of the work-group to ensure that the value of the
+            right-most dimension is a multiple of "32".
+            */
+            qk_max =
+                sycl::fmax(qk_max, dpct::permute_sub_group_by_xor(
+                                       item_ct1.get_sub_group(), qk_max, mask));
+          }
+          qk_max =
+              dpct::select_from_sub_group(item_ct1.get_sub_group(), qk_max, 0);
+
+          // Get the sum of the exp values.
+          float exp_sum = 0.f;
+          for (int i = thread_idx; i < num_tokens; i += NUM_THREADS) {
+            float val = sycl::exp(logits[i] - qk_max);
+            logits[i] = val;
+            exp_sum += val;
+          }
+          exp_sum =
+              block_sum<NUM_WARPS>(&red_smem[NUM_WARPS], exp_sum, item_ct1);
+          // Compute softmax.
+          const float inv_sum = 1.f / (exp_sum + 1e-6f);
+#pragma unroll
+          for (int i = thread_idx; i < num_tokens; i += NUM_THREADS) {
+            logits[i] *= inv_sum;
+          }
+
+          item_ct1.barrier(sycl::access::fence_space::local_space);
+          constexpr int V_VEC_SIZE = MIN(16 / sizeof(sycl_t), BLOCK_SIZE);
+          using V_vec = typename Vec<sycl_t, V_VEC_SIZE>::Type;
+          using L_vec = typename Vec<sycl_t, V_VEC_SIZE>::Type;
+          using Float_L_vec = typename FloatVec<L_vec>::Type;
+          constexpr int NUM_V_VECS_PER_ROW = BLOCK_SIZE / V_VEC_SIZE;
+          constexpr int NUM_ROWS_PER_ITER = WARP_SIZE / NUM_V_VECS_PER_ROW;
+          constexpr int NUM_ROWS_PER_THREAD =
+              DIVIDE_ROUND_UP(HD, NUM_ROWS_PER_ITER);
+          // NOTE(woosuk): We use FP32 for the accumulator for better
+          // accuracy.
+          float accs[NUM_ROWS_PER_THREAD];
+#pragma unroll
+          for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
+            accs[i] = 0.f;
+          }
+
+          sycl_t zero_value;
+          zero(zero_value);
+          for (int block_idx = start_block_idx + warp_idx;
+               block_idx < end_block_idx; block_idx += NUM_WARPS) {
+            // NOTE(woosuk): The block number is stored in int32.
+            // However, we cast it to int64 because int32 can lead to
+            // overflow when this variable is multiplied by large
+            // numbers (e.g., kv_block_stride).
+            const int64_t physical_block_number =
+                static_cast<int64_t>(block_table[block_idx]);
+            const int physical_block_offset =
+                (lane % NUM_V_VECS_PER_ROW) * V_VEC_SIZE;
+            const int token_idx =
+                block_idx * BLOCK_SIZE + physical_block_offset;
+            L_vec logits_vec;
+            vllm::from_float(
+                logits_vec, *reinterpret_cast<Float_L_vec*>(logits + token_idx -
+                                                            start_token_idx));
+
+            const sycl_t* v_ptr =
+                value_cache_ptr +
+                physical_block_number * v_cache_stride_tokens +
+                kv_head_idx * v_cache_stride_head;
+#pragma unroll
+            for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
+              const int row_idx =
+                  lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;
+              if (row_idx < HD) {
+                const int offset = row_idx * BLOCK_SIZE + physical_block_offset;
+                V_vec v_vec = *reinterpret_cast<const V_vec*>(v_ptr + offset);
+                if (block_idx == num_context_blocks - 1) {
+                  // NOTE(woosuk): When v_vec contains the tokens
+                  // that are out of the context, we should
+                  // explicitly zero out the values since they may
+                  // contain NaNs. See
+                  // https://github.com/vllm-project/vllm/issues/641#issuecomment-1682544472
+                  sycl_t* v_vec_ptr = reinterpret_cast<sycl_t*>(&v_vec);
+#pragma unroll
+                  for (int j = 0; j < V_VEC_SIZE; j++) {
+                    v_vec_ptr[j] =
+                        token_idx + j < context_len ? v_vec_ptr[j] : zero_value;
+                  }
+                }
+                accs[i] += vllm::dot(logits_vec, v_vec);
+              }
+            }
+          }
+      // Perform reduction within each warp.
+#pragma unroll
+          for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
+            float acc = accs[i];
+#pragma unroll
+            for (int mask = NUM_V_VECS_PER_ROW / 2; mask >= 1; mask /= 2) {
+              /*
+              DPCT1096:41: The right-most dimension of the work-group
+              used in the SYCL kernel that calls this function may be
+              less than "32". The function
+              "dpct::permute_sub_group_by_xor" may return an
+              unexpected result on the CPU device. Modify the size of
+              the work-group to ensure that the value of the
+              right-most dimension is a multiple of "32".
+              */
+              acc += dpct::permute_sub_group_by_xor(item_ct1.get_sub_group(),
+                                                    acc, mask);
+            }
+            accs[i] = acc;
+          }
+
+          // NOTE(woosuk): A barrier is required because the shared memory
+          // space for logits is reused for the output.
+
+          item_ct1.barrier(sycl::access::fence_space::local_space);
+
+          // Perform reduction across warps.
+          float* out_smem = reinterpret_cast<float*>(shared_mem);
+#pragma unroll
+          for (int i = NUM_WARPS; i > 1; i /= 2) {
+            int mid = i / 2;
+            // Upper warps write to shared memory.
+            if (warp_idx >= mid && warp_idx < i) {
+              float* dst = &out_smem[(warp_idx - mid) * HD];
+#pragma unroll
+              for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
+                const int row_idx =
+                    lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;
+                if (row_idx < HD && lane % NUM_V_VECS_PER_ROW == 0) {
+                  dst[row_idx] = accs[i];
+                }
+              }
+            }
+
+            item_ct1.barrier(sycl::access::fence_space::local_space);
+
+            // Lower warps update the output.
+            if (warp_idx < mid) {
+              const float* src = &out_smem[warp_idx * HD];
+#pragma unroll
+              for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
+                const int row_idx =
+                    lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;
+                if (row_idx < HD && lane % NUM_V_VECS_PER_ROW == 0) {
+                  accs[i] += src[row_idx];
+                }
+              }
+            }
+
+            item_ct1.barrier(sycl::access::fence_space::local_space);
+          }
+
+          // Write the final output.
+          if (warp_idx == 0) {
+            sycl_t* out_ptr =
+                out_p + (query_loc_ptr[bsz_idx] + seq_idx) * out_stride_tokens +
+                head_idx * out_stride_head;
+
+#pragma unroll
+            for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
+              const int row_idx =
+                  lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;
+              if (row_idx < HD && lane % NUM_V_VECS_PER_ROW == 0) {
+                vllm::from_float(*(out_ptr + row_idx), accs[i]);
+              }
+            }
+          }
+        });
+    // Each thread_group handles one token
+  };
+  queue.submit(cgf);
+}
+
+template <
+    typename scalar_t,
+    typename Q_Vec_t,
+    int HEAD_SIZE,
+    int BLOCK_SIZE,
+    int NUM_THREADS,
+    int VEC_SIZE,
+    int PARTITION_SIZE = 0> // Zero means no partitioning.
+void paged_attention_kernel(
+    float* __restrict__ exp_sums, // [num_seqs, num_heads, max_num_partitions]
+    float* __restrict__ max_logits, // [num_seqs, num_heads, max_num_partitions]
+    scalar_t* __restrict__ out, // [num_seqs, num_heads, max_num_partitions,
+                                // head_size]
+    const scalar_t* __restrict__ q, // [num_seqs, num_heads, head_size]
+    const scalar_t* __restrict__ k_cache, // [num_blocks, num_kv_heads,
+                                          // head_size/x, block_size, x]
+    const scalar_t* __restrict__ v_cache, // [num_blocks, num_kv_heads,
+                                          // head_size, block_size]
+    const int num_kv_heads, // [num_heads]
+    const float scale,
+    const int* __restrict__ block_tables, // [num_seqs, max_num_blocks_per_seq]
+    const int* __restrict__ context_lens, // [num_seqs]
+    const int max_num_blocks_per_seq,
+    const float* __restrict__ alibi_slopes, // [num_heads]
+    const int q_stride,
+    const int kv_block_stride,
+    const int kv_head_stride,
+    const float attn_logit_softcapping,
+    const sycl::nd_item<3>& item_ct1,
+    uint8_t* dpct_local,
+    Q_Vec_t* q_vecs,
+    float* red_smem) {
+  const int seq_idx = item_ct1.get_group(1);
+  const int partition_idx = item_ct1.get_group(0);
+  const int max_num_partitions = item_ct1.get_group_range(0);
+  constexpr bool USE_PARTITIONING = PARTITION_SIZE > 0;
+  const int context_len = context_lens[seq_idx];
+  if (USE_PARTITIONING && partition_idx * PARTITION_SIZE >= context_len) {
+    // No work to do. Terminate the thread block.
+    return;
+  }
+
+  const int num_context_blocks = DIVIDE_ROUND_UP(context_len, BLOCK_SIZE);
+  const int num_blocks_per_partition =
+      USE_PARTITIONING ? PARTITION_SIZE / BLOCK_SIZE : num_context_blocks;
+
+  // [start_block_idx, end_block_idx) is the range of blocks to process.
+  const int start_block_idx =
+      USE_PARTITIONING ? partition_idx * num_blocks_per_partition : 0;
+  const int end_block_idx =
+      MIN(start_block_idx + num_blocks_per_partition, num_context_blocks);
+  const int num_blocks = end_block_idx - start_block_idx;
+
+  // [start_token_idx, end_token_idx) is the range of tokens to process.
+  const int start_token_idx = start_block_idx * BLOCK_SIZE;
+  const int end_token_idx =
+      MIN(start_token_idx + num_blocks * BLOCK_SIZE, context_len);
+  const int num_tokens = end_token_idx - start_token_idx;
+
+  constexpr int THREAD_GROUP_SIZE = MAX(WARP_SIZE / BLOCK_SIZE, 1);
+  constexpr int NUM_THREAD_GROUPS =
+      NUM_THREADS / THREAD_GROUP_SIZE; // Note: This assumes THREAD_GROUP_SIZE
+                                       // divides NUM_THREADS
+  assert(NUM_THREADS % THREAD_GROUP_SIZE == 0);
+  constexpr int NUM_TOKENS_PER_THREAD_GROUP =
+      DIVIDE_ROUND_UP(BLOCK_SIZE, WARP_SIZE);
+  constexpr int NUM_WARPS = NUM_THREADS / WARP_SIZE;
+  const int thread_idx = item_ct1.get_local_id(2);
+  const int warp_idx = thread_idx / WARP_SIZE;
+  const int lane = thread_idx % WARP_SIZE;
+
+  const int head_idx = item_ct1.get_group(2);
+  const int num_heads = item_ct1.get_group_range(2);
+  const int num_queries_per_kv = num_heads / num_kv_heads;
+
+  const int kv_head_idx = head_idx / num_queries_per_kv;
+  ;
+  const float alibi_slope =
+      alibi_slopes == nullptr ? 0.f : alibi_slopes[head_idx];
+
+  // A vector type to store a part of a key or a query.
+  // The vector size is configured in such a way that the threads in a thread
+  // group fetch or compute 16 bytes at a time. For example, if the size of a
+  // thread group is 4 and the data type is half, then the vector size is 16 /
+  // (4 * sizeof(half)) == 2.
+
+  // constexpr int VEC_SIZE = MAX(16 / (THREAD_GROUP_SIZE * sizeof(scalar_t)),
+  // 1);
+
+  constexpr int NUM_ELEMS_PER_THREAD = HEAD_SIZE / THREAD_GROUP_SIZE;
+  constexpr int NUM_VECS_PER_THREAD = NUM_ELEMS_PER_THREAD / VEC_SIZE;
+
+  const int thread_group_idx = thread_idx / THREAD_GROUP_SIZE;
+  const int thread_group_offset = thread_idx % THREAD_GROUP_SIZE;
+
+  // Load the query to registers.
+  // Each thread in a thread group has a different part of the query.
+  // For example, if the the thread group size is 4, then the first thread in
+  // the group has 0, 4, 8, ... th vectors of the query, and the second thread
+  // has 1, 5, 9, ... th vectors of the query, and so on. NOTE(woosuk): Because
+  // q is split from a qkv tensor, it may not be contiguous.
+  const scalar_t* q_ptr = q + seq_idx * q_stride + head_idx * HEAD_SIZE;
+
+#pragma unroll
+  for (int i = thread_group_idx; i < NUM_VECS_PER_THREAD;
+       i += NUM_THREAD_GROUPS) {
+    const int vec_idx = thread_group_offset + i * THREAD_GROUP_SIZE;
+    q_vecs[thread_group_offset * NUM_VECS_PER_THREAD + i] =
+        *reinterpret_cast<const Q_Vec_t*>(q_ptr + vec_idx * VEC_SIZE);
+  }
+  /*
+  DPCT1065:5: Consider replacing sycl::nd_item::barrier() with
+  sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better
+  performance if there is no access to global memory.
+  */
+  item_ct1.barrier(sycl::access::fence_space::local_space); // TODO(naed90): possible speedup if this is replaced with
+                      // a memory wall right before we use q_vecs
+
+  // Memory planning.
+  auto shared_mem = (char*)dpct_local;
+  // NOTE(woosuk): We use FP32 for the softmax logits for better accuracy.
+  float* logits = reinterpret_cast<float*>(shared_mem);
+  // Workspace for reduction.
+
+  // x == THREAD_GROUP_SIZE * VEC_SIZE
+  // Each thread group fetches x elements from the key at a time.
+  constexpr int x = 16 / sizeof(scalar_t);
+  float qk_max = -FLT_MAX;
+
+  // Iterate over the key blocks.
+  // Each warp fetches a block of keys for each iteration.
+  // Each thread group in a warp fetches a key from the block, and computes
+  // dot product with the query.
+  const int* block_table = block_tables + seq_idx * max_num_blocks_per_seq;
+
+  for (int block_idx = start_block_idx + warp_idx; block_idx < end_block_idx;
+       block_idx += NUM_WARPS) {
+    // NOTE(woosuk): The block number is stored in int32. However, we cast it to
+    // int64 because int32 can lead to overflow when this variable is multiplied
+    // by large numbers (e.g., kv_block_stride).
+    const int64_t physical_block_number =
+        static_cast<int64_t>(block_table[block_idx]);
+
+    // Load a key to registers.
+    // Each thread in a thread group has a different part of the key.
+    // For example, if the the thread group size is 4, then the first thread in
+    // the group has 0, 4, 8, ... th vectors of the key, and the second thread
+    // has 1, 5, 9, ... th vectors of the key, and so on.
+
+    for (int i = 0; i < NUM_TOKENS_PER_THREAD_GROUP; i++) {
+      const int physical_block_offset =
+          (thread_group_idx + i * WARP_SIZE) % BLOCK_SIZE;
+      const int token_idx = block_idx * BLOCK_SIZE + physical_block_offset;
+
+      Q_Vec_t k_vecs[NUM_VECS_PER_THREAD];
+
+#pragma unroll
+      for (int j = 0; j < NUM_VECS_PER_THREAD; j++) {
+        const scalar_t* k_ptr = k_cache +
+            physical_block_number * kv_block_stride +
+            kv_head_idx * kv_head_stride + physical_block_offset * x;
+
+        const int vec_idx = thread_group_offset + j * THREAD_GROUP_SIZE;
+        const int offset1 = (vec_idx * VEC_SIZE) / x;
+        const int offset2 = (vec_idx * VEC_SIZE) % x;
+        k_vecs[j] = *reinterpret_cast<const Q_Vec_t*>(
+            k_ptr + offset1 * BLOCK_SIZE * x + offset2);
+      }
+
+      // Compute dot product.
+      // This includes a reduction across the threads in the same thread group.
+      // Q_Vec_t q_vec_[NUM_VECS_PER_THREAD] = q_vecs + thread_group_offset *
+      // THREAD_GROUP_SIZE;
+      float qk = scale *
+          Qk_dot<scalar_t, THREAD_GROUP_SIZE>::
+              template dot<Q_Vec_t, NUM_VECS_PER_THREAD>(
+                     q_vecs + thread_group_offset * NUM_VECS_PER_THREAD,
+                     k_vecs,
+                     item_ct1);
+      // Add the ALiBi bias if slopes are given.
+      qk +=
+          (alibi_slope != 0) ? alibi_slope * (token_idx - context_len + 1) : 0;
+
+      // Add the attn_logit_softcapp if given.
+      if (attn_logit_softcapping != 0.0) {
+          qk = attn_softcapping(qk, attn_logit_softcapping);
+      }
+      if (thread_group_offset == 0) {
+        // Store the partial reductions to shared memory.
+        // NOTE(woosuk): It is required to zero out the masked logits.
+        const bool mask = token_idx >= context_len;
+        logits[token_idx - start_token_idx] = mask ? 0.f : qk;
+        // Update the max value.
+        qk_max = mask ? qk_max : sycl::fmax(qk_max, qk);
+      }
+    }
+  }
+
+  // Perform reduction across the threads in the same warp to get the
+  // max qk value for each "warp" (not across the thread block yet).
+  // The 0-th thread of each thread group already has its max qk value.
+#pragma unroll
+  for (int mask = WARP_SIZE / 2; mask >= THREAD_GROUP_SIZE; mask /= 2) {
+  
+    /*
+    DPCT1096:38: The right-most dimension of the work-group used in the SYCL
+    kernel that calls this function may be less than "32". The function
+    "dpct::permute_sub_group_by_xor" may return an unexpected result on the CPU
+    device. Modify the size of the work-group to ensure that the value of the
+    right-most dimension is a multiple of "32".
+    */
+    qk_max = sycl::fmax(
+        qk_max,
+        dpct::permute_sub_group_by_xor(
+            item_ct1.get_sub_group(), qk_max, mask));
+  }
+  if (lane == 0) {
+    red_smem[warp_idx] = qk_max;
+  }
+  
+  item_ct1.barrier(sycl::access::fence_space::local_space);
+
+  // TODO(woosuk): Refactor this part.
+  // Get the max qk value for the sequence.
+  qk_max = lane < NUM_WARPS ? red_smem[lane] : -FLT_MAX;
+#pragma unroll
+  for (int mask = NUM_WARPS / 2; mask >= 1; mask /= 2) {
+    
+    /*
+    DPCT1096:39: The right-most dimension of the work-group used in the SYCL
+    kernel that calls this function may be less than "32". The function
+    "dpct::permute_sub_group_by_xor" may return an unexpected result on the CPU
+    device. Modify the size of the work-group to ensure that the value of the
+    right-most dimension is a multiple of "32".
+    */
+    qk_max = sycl::fmax(
+        qk_max,
+        dpct::permute_sub_group_by_xor(
+            item_ct1.get_sub_group(), qk_max, mask));
+  }
+  // Broadcast the max qk value to all threads.
+  
+  /*
+  DPCT1096:40: The right-most dimension of the work-group used in the SYCL
+  kernel that calls this function may be less than "32". The function
+  "dpct::select_from_sub_group" may return an unexpected result on the CPU
+  device. Modify the size of the work-group to ensure that the value of the
+  right-most dimension is a multiple of "32".
+  */
+  qk_max = dpct::select_from_sub_group(
+          item_ct1.get_sub_group(), qk_max, 0);
+
+  // Get the sum of the exp values.
+  float exp_sum = 0.f;
+  for (int i = thread_idx; i < num_tokens; i += NUM_THREADS) {
+    float val = sycl::exp(logits[i] - qk_max);
+    logits[i] = val;
+    exp_sum += val;
+  }
+  exp_sum = block_sum<NUM_WARPS>(&red_smem[NUM_WARPS], exp_sum, item_ct1);
+
+  // Compute softmax.
+  const float inv_sum = 1.f / (exp_sum + 1e-6f);
+  for (int i = thread_idx; i < num_tokens; i += NUM_THREADS) {
+    logits[i] *= inv_sum;
+  }
+  
+  item_ct1.barrier(sycl::access::fence_space::local_space);
+
+  // If partitioning is enabled, store the max logit and exp_sum.
+  if (USE_PARTITIONING && thread_idx == 0) {
+    float* max_logits_ptr = max_logits +
+        seq_idx * num_heads * max_num_partitions +
+        head_idx * max_num_partitions + partition_idx;
+    *max_logits_ptr = qk_max;
+    float* exp_sums_ptr = exp_sums + seq_idx * num_heads * max_num_partitions +
+        head_idx * max_num_partitions + partition_idx;
+    *exp_sums_ptr = exp_sum;
+  }
+
+  // Each thread will fetch 16 bytes from the value cache at a time.
+  constexpr int V_VEC_SIZE = MIN(16 / sizeof(scalar_t), BLOCK_SIZE);
+  using V_vec = typename Vec<scalar_t, V_VEC_SIZE>::Type;
+  using L_vec = typename Vec<scalar_t, V_VEC_SIZE>::Type;
+  using Float_L_vec = typename FloatVec<L_vec>::Type;
+
+  constexpr int NUM_V_VECS_PER_ROW = BLOCK_SIZE / V_VEC_SIZE;
+  constexpr int NUM_ROWS_PER_ITER = WARP_SIZE / NUM_V_VECS_PER_ROW;
+  constexpr int NUM_ROWS_PER_THREAD =
+      DIVIDE_ROUND_UP(HEAD_SIZE, NUM_ROWS_PER_ITER);
+
+  // NOTE(woosuk): We use FP32 for the accumulator for better accuracy.
+  float accs[NUM_ROWS_PER_THREAD];
+#pragma unroll
+  for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
+    accs[i] = 0.f;
+  }
+
+  scalar_t zero_value;
+  zero(zero_value);
+  for (int block_idx = start_block_idx + warp_idx; block_idx < end_block_idx;
+       block_idx += NUM_WARPS) {
+    // NOTE(woosuk): The block number is stored in int32. However, we cast it to
+    // int64 because int32 can lead to overflow when this variable is multiplied
+    // by large numbers (e.g., kv_block_stride).
+    const int64_t physical_block_number =
+        static_cast<int64_t>(block_table[block_idx]);
+    const int physical_block_offset = (lane % NUM_V_VECS_PER_ROW) * V_VEC_SIZE;
+    const int token_idx = block_idx * BLOCK_SIZE + physical_block_offset;
+    L_vec logits_vec;
+    vllm::from_float(
+        logits_vec,
+        *reinterpret_cast<Float_L_vec*>(logits + token_idx - start_token_idx));
+
+    const scalar_t* v_ptr = v_cache + physical_block_number * kv_block_stride +
+        kv_head_idx * kv_head_stride;
+#pragma unroll
+    for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
+      const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;
+      if (row_idx < HEAD_SIZE) {
+        const int offset = row_idx * BLOCK_SIZE + physical_block_offset;
+        V_vec v_vec = *reinterpret_cast<const V_vec*>(v_ptr + offset);
+        if (block_idx == num_context_blocks - 1) {
+          // NOTE(woosuk): When v_vec contains the tokens that are out of the
+          // context, we should explicitly zero out the values since they may
+          // contain NaNs. See
+          // https://github.com/vllm-project/vllm/issues/641#issuecomment-1682544472
+          scalar_t* v_vec_ptr = reinterpret_cast<scalar_t*>(&v_vec);
+#pragma unroll
+          for (int j = 0; j < V_VEC_SIZE; j++) {
+            v_vec_ptr[j] =
+                token_idx + j < context_len ? v_vec_ptr[j] : zero_value;
+          }
+        }
+        accs[i] += vllm::dot(logits_vec, v_vec);
+      }
+    }
+  }
+
+  // Perform reduction within each warp.
+#pragma unroll
+  for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
+    float acc = accs[i];
+#pragma unroll
+    for (int mask = NUM_V_VECS_PER_ROW / 2; mask >= 1; mask /= 2) {
+     
+      /*
+      DPCT1096:41: The right-most dimension of the work-group used in the SYCL
+      kernel that calls this function may be less than "32". The function
+      "dpct::permute_sub_group_by_xor" may return an unexpected result on the
+      CPU device. Modify the size of the work-group to ensure that the value of
+      the right-most dimension is a multiple of "32".
+      */
+      acc += dpct::permute_sub_group_by_xor(
+          item_ct1.get_sub_group(), acc, mask);
+    }
+    accs[i] = acc;
+  }
+
+  // NOTE(woosuk): A barrier is required because the shared memory space for
+  // logits is reused for the output.
+
+  item_ct1.barrier(sycl::access::fence_space::local_space);
+
+  // Perform reduction across warps.
+  float* out_smem = reinterpret_cast<float*>(shared_mem);
+#pragma unroll
+  for (int i = NUM_WARPS; i > 1; i /= 2) {
+    int mid = i / 2;
+    // Upper warps write to shared memory.
+    if (warp_idx >= mid && warp_idx < i) {
+      float* dst = &out_smem[(warp_idx - mid) * HEAD_SIZE];
+#pragma unroll
+      for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
+        const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;
+        if (row_idx < HEAD_SIZE && lane % NUM_V_VECS_PER_ROW == 0) {
+          dst[row_idx] = accs[i];
+        }
+      }
+    }
+    
+    item_ct1.barrier(sycl::access::fence_space::local_space);
+
+    // Lower warps update the output.
+    if (warp_idx < mid) {
+      const float* src = &out_smem[warp_idx * HEAD_SIZE];
+#pragma unroll
+      for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
+        const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;
+        if (row_idx < HEAD_SIZE && lane % NUM_V_VECS_PER_ROW == 0) {
+          accs[i] += src[row_idx];
+        }
+      }
+    }
+    
+    item_ct1.barrier(sycl::access::fence_space::local_space);
+  }
+
+  // Write the final output.
+  if (warp_idx == 0) {
+    scalar_t* out_ptr = out +
+        seq_idx * num_heads * max_num_partitions * HEAD_SIZE +
+        head_idx * max_num_partitions * HEAD_SIZE + partition_idx * HEAD_SIZE;
+#pragma unroll
+    for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
+      const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;
+      if (row_idx < HEAD_SIZE && lane % NUM_V_VECS_PER_ROW == 0) {
+        vllm::from_float(*(out_ptr + row_idx), accs[i]);
+      }
+    }
+  }
+}
+
+// Grid: (num_heads, num_seqs, 1).
+template <
+    typename scalar_t,
+    typename Q_Vec_t,
+    int HEAD_SIZE,
+    int BLOCK_SIZE,
+    int NUM_THREADS,
+    int VEC_SIZE>
+void paged_attention_v1_kernel(
+    scalar_t* __restrict__ out, // [num_seqs, num_heads, head_size]
+    const scalar_t* __restrict__ q, // [num_seqs, num_heads, head_size]
+    const scalar_t* __restrict__ k_cache, // [num_blocks, num_kv_heads,
+                                          // head_size/x, block_size, x]
+    const scalar_t* __restrict__ v_cache, // [num_blocks, num_kv_heads,
+                                          // head_size, block_size]
+    const int num_kv_heads, // [num_heads]
+    const float scale,
+    const int* __restrict__ block_tables, // [num_seqs, max_num_blocks_per_seq]
+    const int* __restrict__ context_lens, // [num_seqs]
+    const int max_num_blocks_per_seq,
+    const float* __restrict__ alibi_slopes, // [num_heads]
+    const int q_stride,
+    const int kv_block_stride,
+    const int kv_head_stride,
+    const float attn_logit_softcapping,
+    const sycl::nd_item<3>& item_ct1,
+    uint8_t* dpct_local,
+    Q_Vec_t* q_vecs,
+    float* red_smem) {
+  paged_attention_kernel<
+      scalar_t,
+      Q_Vec_t,
+      HEAD_SIZE,
+      BLOCK_SIZE,
+      NUM_THREADS,
+      VEC_SIZE>(
+      /* exp_sums */ nullptr,
+      /* max_logits */ nullptr,
+      out,
+      q,
+      k_cache,
+      v_cache,
+      num_kv_heads,
+      scale,
+      block_tables,
+      context_lens,
+      max_num_blocks_per_seq,
+      alibi_slopes,
+      q_stride,
+      kv_block_stride,
+      kv_head_stride,
+      attn_logit_softcapping,
+      item_ct1,
+      dpct_local,
+      q_vecs,
+      red_smem);
+}
+
+#define LAUNCH_ATTENTION_KERNEL(T, HEAD_SIZE, BLOCK_SIZE)      \
+  paged_attention_xpu_v1_impl<T, HEAD_SIZE, BLOCK_SIZE>::call( \
+      out_ptr,                                                 \
+      query_ptr,                                               \
+      key_cache_ptr,                                           \
+      value_cache_ptr,                                         \
+      num_kv_heads,                                            \
+      scale,                                                   \
+      block_tables_ptr,                                        \
+      context_lens_ptr,                                        \
+      max_num_blocks_per_seq,                                  \
+      alibi_slopes_ptr,                                        \
+      q_stride,                                                \
+      kv_block_stride,                                         \
+      kv_head_stride,                                          \
+      num_seqs,                                                \
+      num_heads,                                               \
+      num_blocks);
+
+#define LAUNCH_PAGED_ATTENTION_V1(HEAD_SIZE)                                \
+  event = queue.submit([&](sycl::handler& cgh) {                            \
+    sycl::local_accessor<uint8_t, 1> dpct_local_acc_ct1(                    \
+        sycl::range<1>(shared_mem_size), cgh);                              \
+    sycl::local_accessor<Q_Vec, 1> q_vecs_acc_ct1(                          \
+        sycl::range<1>(THREAD_GROUP_SIZE * num_vecs_per_thread), cgh);      \
+    sycl::local_accessor<float, 1> red_smem_acc_ct1(                        \
+        sycl::range<1>(2 * NUM_WARPS), cgh);                                \
+                                                                            \
+    auto out_ptr_ct0 = out_ptr;                                             \
+    auto query_ptr_ct1 = query_ptr;                                         \
+    auto key_cache_ptr_ct2 = key_cache_ptr;                                 \
+    auto value_cache_ptr_ct3 = value_cache_ptr;                             \
+    auto scale_ct5 = scale;                                                 \
+    auto block_tables_ptr_ct6 = block_tables_ptr;                           \
+    auto context_lens_ptr_ct7 = context_lens_ptr;                           \
+    auto max_num_blocks_per_seq_ct8 = max_num_blocks_per_seq;               \
+    auto alibi_slopes_ptr_ct9 = alibi_slopes_ptr;                           \
+    auto q_stride_ct10 = q_stride;                                          \
+    auto kv_block_stride_ct11 = kv_block_stride;                            \
+    auto kv_head_stride_ct12 = kv_head_stride;                              \
+    auto attn_logit_softcapping_ct13 = attn_logit_softcapping;              \
+                                                                            \
+    cgh.parallel_for(                                                       \
+        sycl::nd_range<3>(grid * block, block),                             \
+        [=](sycl::nd_item<3> item_ct1) [[intel::reqd_sub_group_size(32)]] { \
+          paged_attention_v1_kernel<                                        \
+              sycl_t,                                                       \
+              Q_Vec,                                                        \
+              HEAD_SIZE,                                                    \
+              BLOCK_SIZE,                                                   \
+              NUM_THREADS,                                                  \
+              VEC_SIZE>(                                                    \
+              out_ptr_ct0,                                                  \
+              query_ptr_ct1,                                                \
+              key_cache_ptr_ct2,                                            \
+              value_cache_ptr_ct3,                                          \
+              num_kv_heads,                                                 \
+              scale_ct5,                                                    \
+              block_tables_ptr_ct6,                                         \
+              context_lens_ptr_ct7,                                         \
+              max_num_blocks_per_seq_ct8,                                   \
+              alibi_slopes_ptr_ct9,                                         \
+              q_stride_ct10,                                                \
+              kv_block_stride_ct11,                                         \
+              kv_head_stride_ct12,                                          \
+              attn_logit_softcapping_ct13,                                  \
+              item_ct1,                                                     \
+              dpct_local_acc_ct1.get_pointer(),                             \
+              q_vecs_acc_ct1.get_pointer(),                                 \
+              red_smem_acc_ct1.get_pointer());                              \
+        });                                                                 \
+  });
+
+template <typename T, int BLOCK_SIZE, int NUM_THREADS = 512>
+void paged_attention_xpu_v1_impl_launcher(
+    torch::Tensor& out,
+    torch::Tensor& query,
+    torch::Tensor& key_cache,
+    torch::Tensor& value_cache,
+    int num_kv_heads,
+    float scale,
+    torch::Tensor& block_tables,
+    torch::Tensor& context_lens,
+    int max_context_len,
+    const c10::optional<torch::Tensor>& alibi_slopes,
+    const float attn_logit_softcapping) {
+  int num_seqs = query.size(0);
+  int num_heads = query.size(1);
+  int head_size = query.size(2);
+  int max_num_blocks_per_seq = block_tables.size(1);
+  int q_stride = query.stride(0);
+  int kv_block_stride = key_cache.stride(0);
+  int kv_head_stride = key_cache.stride(1);
+
+  constexpr int THREAD_GROUP_SIZE = MAX(WARP_SIZE / BLOCK_SIZE, 1);
+  constexpr int VEC_SIZE = MAX(16 / (THREAD_GROUP_SIZE * sizeof(T)), 1);
+  using sycl_t = vllm::xpu::SyclTypeTrait<T>::Type;
+  using Q_Vec = typename Vec<sycl_t, VEC_SIZE>::Type;
+
+  int num_vecs_per_thread = head_size / THREAD_GROUP_SIZE / VEC_SIZE;
+  assert(head_size % THREAD_GROUP_SIZE == 0);
+
+  // NOTE: alibi_slopes is optional.
+  const float* alibi_slopes_ptr = alibi_slopes
+      ? reinterpret_cast<const float*>(alibi_slopes.value().data_ptr())
+      : nullptr;
+
+  sycl_t* out_ptr = reinterpret_cast<sycl_t*>(out.data_ptr());
+  sycl_t* query_ptr = reinterpret_cast<sycl_t*>(query.data_ptr());
+  sycl_t* key_cache_ptr = reinterpret_cast<sycl_t*>(key_cache.data_ptr());
+  sycl_t* value_cache_ptr = reinterpret_cast<sycl_t*>(value_cache.data_ptr());
+  int* block_tables_ptr = block_tables.data_ptr<int>();
+  int* context_lens_ptr = context_lens.data_ptr<int>();
+
+  constexpr int NUM_WARPS = NUM_THREADS / WARP_SIZE;
+  int padded_max_context_len =
+      DIVIDE_ROUND_UP(max_context_len, BLOCK_SIZE) * BLOCK_SIZE;
+  
+  int logits_size = padded_max_context_len * sizeof(float);
+  int outputs_size = (NUM_WARPS / 2) * head_size * sizeof(float);
+  // Python-side check in vllm.worker.worker._check_if_can_support_max_seq_len
+  // Keep that in sync with the logic here!
+  int shared_mem_size = std::max(logits_size, outputs_size);
+
+  sycl::range<3> grid(1, num_seqs, num_heads);
+  sycl::range<3> block(1, 1, NUM_THREADS);
+  sycl::queue& queue = vllm::xpu::vllmGetQueue();
+  sycl::event event;
+
+  switch (head_size) {
+    // NOTE(woosuk): To reduce the compilation time, we only compile for the
+    // head sizes that we use in the model. However, we can easily extend this
+    // to support any head size which is a multiple of 16.
+    case 64:
+      LAUNCH_PAGED_ATTENTION_V1(64);
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+    // xpu::profiler_record(event_desc, event);  // Uncomment when needed
+#else
+    ::xpu::profiler_record("paged attn v1", event);
+#endif
+      break;
+    case 80:
+      LAUNCH_PAGED_ATTENTION_V1(80);
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+    // xpu::profiler_record(event_desc, event);  // Uncomment when needed
+#else
+    ::xpu::profiler_record("paged attn v1", event);
+#endif
+      break;
+    case 96:
+      LAUNCH_PAGED_ATTENTION_V1(96);
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+    // xpu::profiler_record(event_desc, event);  // Uncomment when needed
+#else
+    ::xpu::profiler_record("paged attn v1", event);
+#endif
+      break;
+    case 112:
+      LAUNCH_PAGED_ATTENTION_V1(112);
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+    // xpu::profiler_record(event_desc, event);  // Uncomment when needed
+#else
+    ::xpu::profiler_record("paged attn v1", event);
+#endif
+      break;
+    case 128:
+      LAUNCH_PAGED_ATTENTION_V1(128);
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+    // xpu::profiler_record(event_desc, event);  // Uncomment when needed
+#else
+    ::xpu::profiler_record("paged attn v1", event);
+#endif
+      break;
+    case 256:
+      LAUNCH_PAGED_ATTENTION_V1(256);
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+    // xpu::profiler_record(event_desc, event);  // Uncomment when needed
+#else
+    ::xpu::profiler_record("paged attn v1", event);
+#endif
+      break;
+    default:
+      TORCH_CHECK(false, "Unsupported head size: ", head_size);
+      break;
+  }
+  // queue.wait();
+}
+
+#define CALL_KERNEL_LAUNCHER(T, BLOCK_SIZE)                  \
+  vllm::paged_attention_xpu_v1_impl_launcher<T, BLOCK_SIZE>( \
+      out,                                                   \
+      query,                                                 \
+      key_cache,                                             \
+      value_cache,                                           \
+      num_kv_heads,                                          \
+      scale,                                                 \
+      block_tables,                                          \
+      context_lens,                                          \
+      max_context_len,                                       \
+      alibi_slopes,                                          \
+      attn_logit_softcapping);
+
+#define CALL_KERNEL_LAUNCHER_BLOCK_SIZE(T)                        \
+  switch (block_size) {                                           \
+    case 8:                                                      \
+      CALL_KERNEL_LAUNCHER(T, 8);                                \
+      break;                                                      \
+    case 16:                                                      \
+      CALL_KERNEL_LAUNCHER(T, 16);                                \
+      break;                                                      \
+    case 32:                                                      \
+      CALL_KERNEL_LAUNCHER(T, 32);                                \
+      break;                                                      \
+    case 64:                                                      \
+      CALL_KERNEL_LAUNCHER(T, 64);                                \
+      break;                                                      \
+    default:                                                      \
+      TORCH_CHECK(false, "Unsupported block size: ", block_size); \
+      break;                                                      \
+  }
+
+// Grid: (num_heads, num_seqs).
+template <
+    typename scalar_t,
+    int HEAD_SIZE,
+    int NUM_THREADS,
+    int PARTITION_SIZE>
+void paged_attention_v2_reduce_kernel(
+    scalar_t* __restrict__ out, // [num_seqs, num_heads, head_size]
+    const float* __restrict__ exp_sums, // [num_seqs, num_heads,
+                                        // max_num_partitions]
+    const float* __restrict__ max_logits, // [num_seqs, num_heads,
+                                          // max_num_partitions]
+    const scalar_t* __restrict__ tmp_out, // [num_seqs, num_heads,
+                                          // max_num_partitions, head_size]
+    const int* __restrict__ context_lens, // [num_seqs]
+    const int max_num_partitions,
+    const sycl::nd_item<3>& item_ct1,
+    uint8_t* dpct_local,
+    float* red_smem) {
+  const int num_heads = item_ct1.get_group_range(2);
+  const int head_idx = item_ct1.get_group(2);
+  const int seq_idx = item_ct1.get_group(1);
+  const int context_len = context_lens[seq_idx];
+  const int num_partitions = DIVIDE_ROUND_UP(context_len, PARTITION_SIZE);
+  if (num_partitions == 1) {
+    // No need to reduce. Only copy tmp_out to out.
+    scalar_t* out_ptr =
+        out + seq_idx * num_heads * HEAD_SIZE + head_idx * HEAD_SIZE;
+    const scalar_t* tmp_out_ptr = tmp_out +
+        seq_idx * num_heads * max_num_partitions * HEAD_SIZE +
+        head_idx * max_num_partitions * HEAD_SIZE;
+    for (int i = item_ct1.get_local_id(2); i < HEAD_SIZE;
+         i += item_ct1.get_local_range(2)) {
+      out_ptr[i] = tmp_out_ptr[i];
+    }
+    // Terminate the thread block.
+    return;
+  }
+
+  constexpr int NUM_WARPS = NUM_THREADS / WARP_SIZE;
+  const int warp_idx = item_ct1.get_local_id(2) / WARP_SIZE;
+  const int lane = item_ct1.get_local_id(2) % WARP_SIZE;
+
+  // Size: 2 * num_partitions.
+  auto shared_mem = (char*)dpct_local;
+  // Workspace for reduction.
+
+  // Load max logits to shared memory.
+  float* shared_max_logits = reinterpret_cast<float*>(shared_mem);
+  const float* max_logits_ptr = max_logits +
+      seq_idx * num_heads * max_num_partitions + head_idx * max_num_partitions;
+  float max_logit = -FLT_MAX;
+  for (int i = item_ct1.get_local_id(2); i < num_partitions;
+       i += item_ct1.get_local_range(2)) {
+    const float l = max_logits_ptr[i];
+    shared_max_logits[i] = l;
+    max_logit = sycl::fmax(max_logit, (float)l);
+  }
+  
+  item_ct1.barrier(sycl::access::fence_space::local_space);
+
+  // Get the global max logit.
+  // Reduce within the warp.
+#pragma unroll
+  for (int mask = WARP_SIZE / 2; mask >= 1; mask /= 2) {
+    
+    /*
+    DPCT1096:45: The right-most dimension of the work-group used in the SYCL
+    kernel that calls this function may be less than "32". The function
+    "dpct::permute_sub_group_by_xor" may return an unexpected result on the CPU
+    device. Modify the size of the work-group to ensure that the value of the
+    right-most dimension is a multiple of "32".
+    */
+    max_logit = sycl::fmax(
+        max_logit,
+        dpct::permute_sub_group_by_xor(
+            item_ct1.get_sub_group(), max_logit, mask));
+  }
+  if (lane == 0) {
+    red_smem[warp_idx] = max_logit;
+  }
+  
+  item_ct1.barrier(sycl::access::fence_space::local_space);
+  // Reduce across warps.
+  max_logit = lane < NUM_WARPS ? red_smem[lane] : -FLT_MAX;
+#pragma unroll
+  for (int mask = NUM_WARPS / 2; mask >= 1; mask /= 2) {
+    
+    /*
+    DPCT1096:46: The right-most dimension of the work-group used in the SYCL
+    kernel that calls this function may be less than "32". The function
+    "dpct::permute_sub_group_by_xor" may return an unexpected result on the CPU
+    device. Modify the size of the work-group to ensure that the value of the
+    right-most dimension is a multiple of "32".
+    */
+    max_logit = sycl::fmax(
+        max_logit,
+        dpct::permute_sub_group_by_xor(
+            item_ct1.get_sub_group(), max_logit, mask));
+  }
+  // Broadcast the max value to all threads.
+  
+  /*
+  DPCT1096:47: The right-most dimension of the work-group used in the SYCL
+  kernel that calls this function may be less than "32". The function
+  "dpct::select_from_sub_group" may return an unexpected result on the CPU
+  device. Modify the size of the work-group to ensure that the value of the
+  right-most dimension is a multiple of "32".
+  */
+  max_logit = dpct::select_from_sub_group(
+      item_ct1.get_sub_group(), max_logit, 0);
+
+  // Load rescaled exp sums to shared memory.
+  float* shared_exp_sums =
+      reinterpret_cast<float*>(shared_mem + sizeof(float) * num_partitions);
+  const float* exp_sums_ptr = exp_sums +
+      seq_idx * num_heads * max_num_partitions + head_idx * max_num_partitions;
+  float global_exp_sum = 0.0f;
+  for (int i = item_ct1.get_local_id(2); i < num_partitions;
+       i += item_ct1.get_local_range(2)) {
+    float l = shared_max_logits[i];
+    float rescaled_exp_sum = exp_sums_ptr[i] * sycl::exp(l - max_logit);
+    global_exp_sum += rescaled_exp_sum;
+    shared_exp_sums[i] = rescaled_exp_sum;
+  }
+  
+  item_ct1.barrier(sycl::access::fence_space::local_space);
+  global_exp_sum =
+      block_sum<NUM_WARPS>(&red_smem[NUM_WARPS], global_exp_sum, item_ct1);
+  const float inv_global_exp_sum = 1.0f / (global_exp_sum + 1e-6f);
+
+  // Aggregate tmp_out to out.
+  const scalar_t* tmp_out_ptr = tmp_out +
+      seq_idx * num_heads * max_num_partitions * HEAD_SIZE +
+      head_idx * max_num_partitions * HEAD_SIZE;
+  scalar_t* out_ptr =
+      out + seq_idx * num_heads * HEAD_SIZE + head_idx * HEAD_SIZE;
+#pragma unroll
+  for (int i = item_ct1.get_local_id(2); i < HEAD_SIZE; i += NUM_THREADS) {
+    float acc = 0.0f;
+    for (int j = 0; j < num_partitions; ++j) {
+      acc += to_float(tmp_out_ptr[j * HEAD_SIZE + i]) * shared_exp_sums[j] *
+          inv_global_exp_sum;
+    }
+    from_float(out_ptr[i], acc);
+  }
+}
+
+// Grid: (num_heads, num_seqs, max_num_partitions).
+template <
+    typename scalar_t,
+    typename Q_Vec_t,
+    int HEAD_SIZE,
+    int BLOCK_SIZE,
+    int NUM_THREADS,
+    int VEC_SIZE,
+    int PARTITION_SIZE>
+void paged_attention_v2_kernel(
+    float* __restrict__ exp_sums, // [num_seqs, num_heads, max_num_partitions]
+    float* __restrict__ max_logits, // [num_seqs, num_heads, max_num_partitions]
+    scalar_t* __restrict__ tmp_out, // [num_seqs, num_heads, max_num_partitions,
+                                    // head_size]
+    const scalar_t* __restrict__ q, // [num_seqs, num_heads, head_size]
+    const scalar_t* __restrict__ k_cache, // [num_blocks, num_kv_heads,
+                                          // head_size/x, block_size, x]
+    const scalar_t* __restrict__ v_cache, // [num_blocks, num_kv_heads,
+                                          // head_size, block_size]
+    const int num_kv_heads, // [num_heads]
+    const float scale,
+    const int* __restrict__ block_tables, // [num_seqs, max_num_blocks_per_seq]
+    const int* __restrict__ context_lens, // [num_seqs]
+    const int max_num_blocks_per_seq,
+    const float* __restrict__ alibi_slopes, // [num_heads]
+    const int q_stride,
+    const int kv_block_stride,
+    const int kv_head_stride,
+    const float attn_logit_softcapping,
+    const sycl::nd_item<3>& item_ct1,
+    uint8_t* dpct_local,
+    Q_Vec_t* q_vecs,
+    float* red_smem) {
+  paged_attention_kernel<
+      scalar_t,
+      Q_Vec_t,
+      HEAD_SIZE,
+      BLOCK_SIZE,
+      NUM_THREADS,
+      VEC_SIZE,
+      PARTITION_SIZE>(
+      exp_sums,
+      max_logits,
+      tmp_out,
+      q,
+      k_cache,
+      v_cache,
+      num_kv_heads,
+      scale,
+      block_tables,
+      context_lens,
+      max_num_blocks_per_seq,
+      alibi_slopes,
+      q_stride,
+      kv_block_stride,
+      kv_head_stride,
+      attn_logit_softcapping,
+      item_ct1,
+      dpct_local,
+      q_vecs,
+      red_smem);
+}
+
+#define LAUNCH_PAGED_ATTENTION_V2_FIRST_HALF(HEAD_SIZE)                     \
+  event = queue.submit([&](sycl::handler& cgh) {                            \
+    sycl::local_accessor<uint8_t, 1> dpct_local_acc_ct1(                    \
+        sycl::range<1>(shared_mem_size), cgh);                              \
+    sycl::local_accessor<Q_Vec, 1> q_vecs_acc_ct1(                          \
+        sycl::range<1>(THREAD_GROUP_SIZE * num_vecs_per_thread), cgh);      \
+    sycl::local_accessor<float, 1> red_smem_acc_ct1(                        \
+        sycl::range<1>(2 * NUM_WARPS), cgh);                                \
+                                                                            \
+    auto exp_sums_ptr_ct0 = exp_sums_ptr;                                   \
+    auto max_logits_ptr_ct1 = max_logits_ptr;                               \
+    auto tmp_out_ptr_ct2 = tmp_out_ptr;                                     \
+    auto query_ptr_ct3 = query_ptr;                                         \
+    auto key_cache_ptr_ct4 = key_cache_ptr;                                 \
+    auto value_cache_ptr_ct5 = value_cache_ptr;                             \
+    auto scale_ct7 = scale;                                                 \
+    auto block_tables_ptr_ct8 = block_tables_ptr;                           \
+    auto context_lens_ptr_ct9 = context_lens_ptr;                           \
+    auto max_num_blocks_per_seq_ct10 = max_num_blocks_per_seq;              \
+    auto alibi_slopes_ptr_ct11 = alibi_slopes_ptr;                          \
+    auto q_stride_ct12 = q_stride;                                          \
+    auto kv_block_stride_ct13 = kv_block_stride;                            \
+    auto kv_head_stride_ct14 = kv_head_stride;                              \
+    auto attn_logit_softcapping_ct15 = attn_logit_softcapping;              \
+                                                                            \
+    cgh.parallel_for(                                                       \
+        sycl::nd_range<3>(grid * block, block),                             \
+        [=](sycl::nd_item<3> item_ct1) [[intel::reqd_sub_group_size(32)]] { \
+          vllm::paged_attention_v2_kernel<                                  \
+              sycl_t,                                                       \
+              Q_Vec,                                                        \
+              HEAD_SIZE,                                                    \
+              BLOCK_SIZE,                                                   \
+              NUM_THREADS,                                                  \
+              VEC_SIZE,                                                     \
+              PARTITION_SIZE>(                                              \
+              exp_sums_ptr_ct0,                                             \
+              max_logits_ptr_ct1,                                           \
+              tmp_out_ptr_ct2,                                              \
+              query_ptr_ct3,                                                \
+              key_cache_ptr_ct4,                                            \
+              value_cache_ptr_ct5,                                          \
+              num_kv_heads,                                                 \
+              scale_ct7,                                                    \
+              block_tables_ptr_ct8,                                         \
+              context_lens_ptr_ct9,                                         \
+              max_num_blocks_per_seq_ct10,                                  \
+              alibi_slopes_ptr_ct11,                                        \
+              q_stride_ct12,                                                \
+              kv_block_stride_ct13,                                         \
+              kv_head_stride_ct14,                                          \
+              attn_logit_softcapping_ct15,                                  \
+              item_ct1,                                                     \
+              dpct_local_acc_ct1.get_pointer(),                             \
+              q_vecs_acc_ct1.get_pointer(),                                 \
+              red_smem_acc_ct1.get_pointer());                              \
+        });                                                                 \
+  });
+
+#define LAUNCH_PAGED_ATTENTION_V2_SECOND_HALF(HEAD_SIZE)                    \
+  event2 = queue.submit([&](sycl::handler& cgh) {                           \
+    sycl::local_accessor<uint8_t, 1> dpct_local_acc_ct1(                    \
+        sycl::range<1>(reduce_shared_mem_size), cgh);                       \
+    sycl::local_accessor<float, 1> red_smem_acc_ct1(                        \
+        sycl::range<1>(2 * NUM_WARPS), cgh);                                \
+                                                                            \
+    auto out_ptr_ct0 = out_ptr;                                             \
+    auto exp_sums_ptr_ct1 = exp_sums_ptr;                                   \
+    auto max_logits_ptr_ct2 = max_logits_ptr;                               \
+    auto tmp_out_ptr_ct3 = tmp_out_ptr;                                     \
+    auto context_lens_ptr_ct4 = context_lens_ptr;                           \
+    auto max_num_partitions_ct5 = max_num_partitions;                       \
+                                                                            \
+    cgh.parallel_for(                                                       \
+        sycl::nd_range<3>(reduce_grid * block, block),                      \
+        [=](sycl::nd_item<3> item_ct1) [[intel::reqd_sub_group_size(32)]] { \
+          vllm::paged_attention_v2_reduce_kernel<                           \
+              sycl_t,                                                       \
+              HEAD_SIZE,                                                    \
+              NUM_THREADS,                                                  \
+              PARTITION_SIZE>(                                              \
+              out_ptr_ct0,                                                  \
+              exp_sums_ptr_ct1,                                             \
+              max_logits_ptr_ct2,                                           \
+              tmp_out_ptr_ct3,                                              \
+              context_lens_ptr_ct4,                                         \
+              max_num_partitions_ct5,                                       \
+              item_ct1,                                                     \
+              dpct_local_acc_ct1.get_pointer(),                             \
+              red_smem_acc_ct1.get_pointer());                              \
+        });                                                                 \
+  });
+
+template <
+    typename T,
+    int BLOCK_SIZE,
+    int NUM_THREADS = 512,
+    int PARTITION_SIZE = 512>
+void paged_attention_v2_launcher(
+    torch::Tensor& out,
+    torch::Tensor& exp_sums,
+    torch::Tensor& max_logits,
+    torch::Tensor& tmp_out,
+    torch::Tensor& query,
+    torch::Tensor& key_cache,
+    torch::Tensor& value_cache,
+    int num_kv_heads,
+    float scale,
+    torch::Tensor& block_tables,
+    torch::Tensor& context_lens,
+    int max_context_len,
+    const c10::optional<torch::Tensor>& alibi_slopes,
+    const float attn_logit_softcapping) {
+  int num_seqs = query.size(0);
+  int num_heads = query.size(1);
+  int head_size = query.size(2);
+  int max_num_blocks_per_seq = block_tables.size(1);
+  int q_stride = query.stride(0);
+  int kv_block_stride = key_cache.stride(0);
+  int kv_head_stride = key_cache.stride(1);
+
+  constexpr int THREAD_GROUP_SIZE = MAX(WARP_SIZE / BLOCK_SIZE, 1);
+  assert(head_size % THREAD_GROUP_SIZE == 0);
+  constexpr int VEC_SIZE = MAX(16 / (THREAD_GROUP_SIZE * sizeof(T)), 1);
+  using sycl_t = vllm::xpu::SyclTypeTrait<T>::Type;
+  using Q_Vec = typename Vec<sycl_t, VEC_SIZE>::Type;
+
+  int num_vecs_per_thread = head_size / THREAD_GROUP_SIZE / VEC_SIZE;
+  assert(head_size % THREAD_GROUP_SIZE == 0);
+
+  // NOTE: alibi_slopes is optional.
+  const float* alibi_slopes_ptr = alibi_slopes
+      ? reinterpret_cast<const float*>(alibi_slopes.value().data_ptr())
+      : nullptr;
+
+  sycl_t* out_ptr = reinterpret_cast<sycl_t*>(out.data_ptr());
+  float* exp_sums_ptr = reinterpret_cast<float*>(exp_sums.data_ptr());
+  float* max_logits_ptr = reinterpret_cast<float*>(max_logits.data_ptr());
+  sycl_t* tmp_out_ptr = reinterpret_cast<sycl_t*>(tmp_out.data_ptr());
+  sycl_t* query_ptr = reinterpret_cast<sycl_t*>(query.data_ptr());
+  sycl_t* key_cache_ptr = reinterpret_cast<sycl_t*>(key_cache.data_ptr());
+  sycl_t* value_cache_ptr = reinterpret_cast<sycl_t*>(value_cache.data_ptr());
+  int* block_tables_ptr = block_tables.data_ptr<int>();
+  int* context_lens_ptr = context_lens.data_ptr<int>();
+
+  constexpr int NUM_WARPS = NUM_THREADS / WARP_SIZE;
+  int max_num_partitions = DIVIDE_ROUND_UP(max_context_len, PARTITION_SIZE);
+  
+  int logits_size = PARTITION_SIZE * sizeof(float);
+  int outputs_size = (NUM_WARPS / 2) * head_size * sizeof(float);
+
+  // For paged attention v2 kernel.
+  sycl::range<3> grid(max_num_partitions, num_seqs, num_heads);
+  int shared_mem_size = std::max(logits_size, outputs_size);
+  // For paged attention v2 reduce kernel.
+  sycl::range<3> reduce_grid(1, num_seqs, num_heads);
+  
+  int reduce_shared_mem_size = 2 * max_num_partitions * sizeof(float);
+
+  sycl::range<3> block(1, 1, NUM_THREADS);
+  sycl::queue& queue = vllm::xpu::vllmGetQueue();
+  sycl::event event;
+  sycl::event event2;
+  switch (head_size) {
+    // NOTE(woosuk): To reduce the compilation time, we only compile for the
+    // head sizes that we use in the model. However, we can easily extend this
+    // to support any head size which is a multiple of 16.
+    case 64:
+      LAUNCH_PAGED_ATTENTION_V2_FIRST_HALF(64);
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+    // xpu::profiler_record(event_desc, event);  // Uncomment when needed
+#else
+    ::xpu::profiler_record("paged attn v2", event);
+#endif
+      LAUNCH_PAGED_ATTENTION_V2_SECOND_HALF(64);
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+    // xpu::profiler_record(event_desc, event);  // Uncomment when needed
+#else
+    ::xpu::profiler_record("paged attn v2", event2);
+#endif
+      break;
+    case 80:
+      LAUNCH_PAGED_ATTENTION_V2_FIRST_HALF(80);
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+    // xpu::profiler_record(event_desc, event);  // Uncomment when needed
+#else
+    ::xpu::profiler_record("paged attn v2", event);
+#endif
+      LAUNCH_PAGED_ATTENTION_V2_SECOND_HALF(80);
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+    // xpu::profiler_record(event_desc, event);  // Uncomment when needed
+#else
+    ::xpu::profiler_record("paged attn v2", event2);
+#endif
+      break;
+    case 96:
+      LAUNCH_PAGED_ATTENTION_V2_FIRST_HALF(96);
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+    // xpu::profiler_record(event_desc, event);  // Uncomment when needed
+#else
+    ::xpu::profiler_record("paged attn v2", event);
+#endif
+      LAUNCH_PAGED_ATTENTION_V2_SECOND_HALF(96);
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+    // xpu::profiler_record(event_desc, event);  // Uncomment when needed
+#else
+    ::xpu::profiler_record("paged attn v2", event2);
+#endif
+      break;
+    case 112:
+      LAUNCH_PAGED_ATTENTION_V2_FIRST_HALF(112);
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+    // xpu::profiler_record(event_desc, event);  // Uncomment when needed
+#else
+    ::xpu::profiler_record("paged attn v2", event);
+#endif
+      LAUNCH_PAGED_ATTENTION_V2_SECOND_HALF(112);
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+    // xpu::profiler_record(event_desc, event);  // Uncomment when needed
+#else
+    ::xpu::profiler_record("paged attn v2", event2);
+#endif
+      break;
+    case 128:
+      LAUNCH_PAGED_ATTENTION_V2_FIRST_HALF(128);
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+    // xpu::profiler_record(event_desc, event);  // Uncomment when needed
+#else
+    ::xpu::profiler_record("paged attn v2", event);
+#endif
+      LAUNCH_PAGED_ATTENTION_V2_SECOND_HALF(128);
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+    // xpu::profiler_record(event_desc, event);  // Uncomment when needed
+#else
+    ::xpu::profiler_record("paged attn v2", event2);
+#endif
+      break;
+    case 256:
+      LAUNCH_PAGED_ATTENTION_V2_FIRST_HALF(256);
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+    // xpu::profiler_record(event_desc, event);  // Uncomment when needed
+#else
+    ::xpu::profiler_record("paged attn v2", event);
+#endif
+      LAUNCH_PAGED_ATTENTION_V2_SECOND_HALF(256);
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+    // xpu::profiler_record(event_desc, event);  // Uncomment when needed
+#else
+    ::xpu::profiler_record("paged attn v2", event2);
+#endif
+      break;
+    default:
+      TORCH_CHECK(false, "Unsupported head size: ", head_size);
+      break;
+  }
+}
+
+#define CALL_V2_LAUNCHER(T, BLOCK_SIZE)             \
+  vllm::paged_attention_v2_launcher<T, BLOCK_SIZE>( \
+      out,                                          \
+      exp_sums,                                     \
+      max_logits,                                   \
+      tmp_out,                                      \
+      query,                                        \
+      key_cache,                                    \
+      value_cache,                                  \
+      num_kv_heads,                                 \
+      scale,                                        \
+      block_tables,                                 \
+      context_lens,                                 \
+      max_context_len,                              \
+      alibi_slopes,                                 \
+      attn_logit_softcapping);
+
+#define CALL_V2_LAUNCHER_BLOCK_SIZE(T)                            \
+  switch (block_size) {                                           \
+    case 8:                                                       \
+      CALL_V2_LAUNCHER(T, 8);                                     \
+      break;                                                      \
+    case 16:                                                      \
+      CALL_V2_LAUNCHER(T, 16);                                    \
+      break;                                                      \
+    case 32:                                                      \
+      CALL_V2_LAUNCHER(T, 32);                                    \
+      break;                                                      \
+    case 64:                                                      \
+      CALL_V2_LAUNCHER(T, 64);                                    \
+      break;                                                      \
+    default:                                                      \
+      TORCH_CHECK(false, "Unsupported block size: ", block_size); \
+      break;                                                      \
+  }
+
+} // namespace vllm
+
+void paged_attention_v1(
+    torch::Tensor& out,
+    torch::Tensor& query,
+    torch::Tensor& key_cache,
+    torch::Tensor& value_cache,
+    int num_kv_heads,
+    float scale,
+    torch::Tensor& block_tables,
+    torch::Tensor& context_lens,
+    int block_size,
+    int max_context_len,
+    const c10::optional<torch::Tensor>& alibi_slopes,
+    const std::string& kv_cache_dtype,
+    const float kv_scale,
+    const float attn_logit_softcapping) {
+  VLLM_XPU_DISPATCH_FLOATING_TYPES_FLOAT_ONLY(
+      query.scalar_type(), "paged_attention_xpu_v1_impl", [&] {
+        CALL_KERNEL_LAUNCHER_BLOCK_SIZE(scalar_t);
+      });
+}
+
+void paged_attention_v2(
+    torch::Tensor& out,
+    torch::Tensor& exp_sums,
+    torch::Tensor& max_logits,
+    torch::Tensor& tmp_out,
+    torch::Tensor& query,
+    torch::Tensor& key_cache,
+    torch::Tensor& value_cache,
+    int num_kv_heads,
+    float scale,
+    torch::Tensor& block_tables,
+    torch::Tensor& context_lens,
+    int block_size,
+    int max_context_len,
+    const c10::optional<torch::Tensor>& alibi_slopes,
+    const std::string& kv_cache_dtype,
+    const float kv_scale,
+    const float attn_logit_softcapping) {
+  VLLM_XPU_DISPATCH_FLOATING_TYPES_FLOAT_ONLY(
+      query.scalar_type(), "paged_attention_xpu_v2_impl", [&] {
+        CALL_V2_LAUNCHER_BLOCK_SIZE(scalar_t);
+      });
+}
+
+torch::Tensor context_attention_forward_v2(
+    torch::Tensor query,  // [num_tokens, num_kv_head, head_dim]
+    torch::Tensor key,    // [num_tokens, num_kv_heads * head_size]
+    torch::Tensor value,  // [num_tokens, num_kv_heads * head_size]
+    torch::Tensor block_tables, torch::Tensor query_start_loc,
+    torch::Tensor seq_lens, torch::Tensor context_lens, int max_input_length,
+    int max_context_length, int max_q_length) {
+  // Currently, only support fp16 here
+  int64_t num_tokens = query.size(0);
+  int64_t num_heads = query.size(1);
+  int64_t head_dim = query.size(2);
+  int64_t batch_size = seq_lens.size(0);
+  int num_kv_heads = value.size(1);
+
+  int key_dimension = key.dim();
+  auto output = at::empty({query.size(0), query.size(1), query.size(2)},
+                          at::device(query.device()).dtype(query.dtype()));
+
+  assert(key_dimension == 5);
+  assert(query.scalar_type() == key.scalar_type() &&
+         query.scalar_type() == value.scalar_type());
+  assert(head_dim == 128);
+  assert(query.scalar_type() == at::ScalarType::Half);
+
+  int query_stride_token = query.stride(0);
+  int query_stride_head = query.stride(1);
+  int query_stride_dim = query.stride(2);
+  const float attn_scale = 1 / std::sqrt((float)head_dim);
+
+  assert(num_heads % num_kv_heads == 0);
+  int num_queries_per_kv = num_heads / num_kv_heads;
+
+
+  // key: num_blocks, num_kv_heads, head_size // x, num_blocks, x)
+  // value: [num_blocks, num_kv_heads, head_size, block_dim]
+  int block_size = value.size(3);
+  // Currently, only block_size 16 is supported...
+  assert(block_size == 16);
+  int x = key.size(4);
+  int block_table_stride_bsz = block_tables.stride(0);
+  int block_table_stride_seq = block_tables.stride(1);
+  int k_cache_stride_token = key.stride(0);
+  int k_cache_stride_head = key.stride(1);
+  int k_cache_stride_head_dim = key.stride(2);
+  int k_cache_stride_block = key.stride(3);
+  int k_cache_stride_x = key.stride(4);
+
+  int v_cache_stride_token = value.stride(0);
+  int v_cache_stride_head = value.stride(1);
+  int v_cache_stride_head_dim = value.stride(2);
+  int v_cache_stride_block = value.stride(3);
+  switch(head_dim) {
+    case 128:
+      vllm::context_attention_kernel_v2<sycl::half, 32, 128>(
+        query.data_ptr(), key.data_ptr(), value.data_ptr(),
+        block_tables.data_ptr(), attn_scale, query_start_loc.data_ptr(),
+        seq_lens.data_ptr(), context_lens.data_ptr(), block_size, x,
+        output.data_ptr(), block_table_stride_bsz, block_table_stride_seq,
+        query_stride_token, query_stride_head, query_stride_dim,
+        k_cache_stride_token, k_cache_stride_head, k_cache_stride_head_dim,
+        k_cache_stride_block, k_cache_stride_x, v_cache_stride_token,
+        v_cache_stride_head, v_cache_stride_head_dim, v_cache_stride_block,
+        output.stride(0), output.stride(1), num_queries_per_kv,
+        max_input_length, batch_size, num_heads, query.size(0),
+        max_context_length, max_q_length);
+      break;
+    case 64:
+      vllm::context_attention_kernel_v2<sycl::half, 32, 64>(
+        query.data_ptr(), key.data_ptr(), value.data_ptr(),
+        block_tables.data_ptr(), attn_scale, query_start_loc.data_ptr(),
+        seq_lens.data_ptr(), context_lens.data_ptr(), block_size, x,
+        output.data_ptr(), block_table_stride_bsz, block_table_stride_seq,
+        query_stride_token, query_stride_head, query_stride_dim,
+        k_cache_stride_token, k_cache_stride_head, k_cache_stride_head_dim,
+        k_cache_stride_block, k_cache_stride_x, v_cache_stride_token,
+        v_cache_stride_head, v_cache_stride_head_dim, v_cache_stride_block,
+        output.stride(0), output.stride(1), num_queries_per_kv,
+        max_input_length, batch_size, num_heads, query.size(0),
+        max_context_length, max_q_length);
+      break;
+    case 80:
+      vllm::context_attention_kernel_v2<sycl::half, 32, 80>(
+        query.data_ptr(), key.data_ptr(), value.data_ptr(),
+        block_tables.data_ptr(), attn_scale, query_start_loc.data_ptr(),
+        seq_lens.data_ptr(), context_lens.data_ptr(), block_size, x,
+        output.data_ptr(), block_table_stride_bsz, block_table_stride_seq,
+        query_stride_token, query_stride_head, query_stride_dim,
+        k_cache_stride_token, k_cache_stride_head, k_cache_stride_head_dim,
+        k_cache_stride_block, k_cache_stride_x, v_cache_stride_token,
+        v_cache_stride_head, v_cache_stride_head_dim, v_cache_stride_block,
+        output.stride(0), output.stride(1), num_queries_per_kv,
+        max_input_length, batch_size, num_heads, query.size(0),
+        max_context_length, max_q_length);
+      break;
+    case 96:
+      vllm::context_attention_kernel_v2<sycl::half, 32, 96>(
+        query.data_ptr(), key.data_ptr(), value.data_ptr(),
+        block_tables.data_ptr(), attn_scale, query_start_loc.data_ptr(),
+        seq_lens.data_ptr(), context_lens.data_ptr(), block_size, x,
+        output.data_ptr(), block_table_stride_bsz, block_table_stride_seq,
+        query_stride_token, query_stride_head, query_stride_dim,
+        k_cache_stride_token, k_cache_stride_head, k_cache_stride_head_dim,
+        k_cache_stride_block, k_cache_stride_x, v_cache_stride_token,
+        v_cache_stride_head, v_cache_stride_head_dim, v_cache_stride_block,
+        output.stride(0), output.stride(1), num_queries_per_kv,
+        max_input_length, batch_size, num_heads, query.size(0),
+        max_context_length, max_q_length);
+      break;
+    default: throw std::runtime_error("unsupported head_dim");
+  }
+    return output;
+}
+
+torch::Tensor context_attention_forward_v1(
+    torch::Tensor query,  // [num_tokens, num_kv_head, head_dim]
+    torch::Tensor key,    // [num_tokens, num_kv_heads * head_size]
+    torch::Tensor value,  // [num_tokens, num_kv_heads * head_size]
+    torch::Tensor block_tables, torch::Tensor query_start_loc,
+    torch::Tensor seq_lens, torch::Tensor context_lens, int max_input_length,
+    int max_context_length) {
+  // Currently, only support fp16
+  int64_t num_tokens = query.size(0);
+  int64_t num_heads = query.size(1);
+  int64_t head_dim = query.size(2);
+  int64_t batch_size = seq_lens.size(0);
+  int num_kv_heads = value.size(1);
+
+  int key_dimension = key.dim();
+  auto output = at::empty({query.size(0), query.size(1), query.size(2)},
+                          at::device(query.device()).dtype(query.dtype()));
+
+  // key should be in shape:
+  // 1. [num_blocks, num_heads, block_size, head_dim]
+  // 2. [num_blocks, num_heads, head_dim / x, block_size, x]
+  assert(key_dimension == 4 or key_dimension == 5);
+  assert(query.scalar_type() == key.scalar_type() &&
+         query.scalar_type() == value.scalar_type());
+  assert(query.scalar_type() == at::ScalarType::Half);
+
+  int query_stride_token = query.stride(0);
+  int query_stride_head = query.stride(1);
+  int query_stride_dim = query.stride(2);
+  const float attn_scale = 1 / std::sqrt((float)head_dim);
+
+  assert(num_heads % num_kv_heads == 0);
+  int num_queries_per_kv = num_heads / num_kv_heads;
+  int block_table_stride_bsz = block_tables.stride(0);
+  int block_table_stride_seq = block_tables.stride(1);
+  if (key_dimension == 4) {
+    // key/value: num_blocks, num_kv_heads, num_blocks, head_dim)
+    int block_size = value.size(2);
+    int k_cache_stride_0 = key.stride(0);
+    int k_cache_stride_1 = key.stride(1);
+    int k_cache_stride_2 = key.stride(2);
+    int k_cache_stride_3 = key.stride(3);
+
+    int v_cache_stride_0 = value.stride(0);
+    int v_cache_stride_1 = value.stride(1);
+    int v_cache_stride_2 = value.stride(2);
+    int v_cache_stride_3 = value.stride(3);
+    switch (head_dim) {
+      case 128:
+        vllm::context_attention_kernel_v1_reshaped<sycl::half, 32, 128>(
+            query.data_ptr(), key.data_ptr(), value.data_ptr(),
+            block_tables.data_ptr(), attn_scale, query_start_loc.data_ptr(),
+            seq_lens.data_ptr(), context_lens.data_ptr(), block_size,
+            output.data_ptr(), block_table_stride_bsz, block_table_stride_seq,
+            query_stride_token, query_stride_head, query_stride_dim,
+            k_cache_stride_0, k_cache_stride_1, k_cache_stride_2,
+            k_cache_stride_3, v_cache_stride_0, v_cache_stride_1,
+            v_cache_stride_2, v_cache_stride_3, output.stride(0),
+            output.stride(1), num_queries_per_kv, max_input_length, batch_size,
+            num_heads);
+        break;
+      case 64:
+        vllm::context_attention_kernel_v1_reshaped<sycl::half, 32, 64>(
+            query.data_ptr(), key.data_ptr(), value.data_ptr(),
+            block_tables.data_ptr(), attn_scale, query_start_loc.data_ptr(),
+            seq_lens.data_ptr(), context_lens.data_ptr(), block_size,
+            output.data_ptr(), block_table_stride_bsz, block_table_stride_seq,
+            query_stride_token, query_stride_head, query_stride_dim,
+            k_cache_stride_0, k_cache_stride_1, k_cache_stride_2,
+            k_cache_stride_3, v_cache_stride_0, v_cache_stride_1,
+            v_cache_stride_2, v_cache_stride_3, output.stride(0),
+            output.stride(1), num_queries_per_kv, max_input_length, batch_size,
+            num_heads);
+        break;
+      default:
+        throw std::runtime_error("unsupported head_dim");
+    }
+  } else {
+    int x = key.size(4);
+    int block_size = value.size(3);
+    int k_cache_stride_token = key.stride(0);
+    int k_cache_stride_head = key.stride(1);
+    int k_cache_stride_head_dim = key.stride(2);
+    int k_cache_stride_block = key.stride(3);
+    int k_cache_stride_x = key.stride(4);
+
+    int v_cache_stride_token = value.stride(0);
+    int v_cache_stride_head = value.stride(1);
+    int v_cache_stride_head_dim = value.stride(2);
+    int v_cache_stride_block = value.stride(3);
+    switch (head_dim) {
+      case 128:
+        vllm::context_attention_kernel_v1<sycl::half, 32, 128>(
+            query.data_ptr(), key.data_ptr(), value.data_ptr(),
+            block_tables.data_ptr(), attn_scale, query_start_loc.data_ptr(),
+            seq_lens.data_ptr(), context_lens.data_ptr(), block_size, x,
+            output.data_ptr(), block_table_stride_bsz, block_table_stride_seq,
+            query_stride_token, query_stride_head, query_stride_dim,
+            k_cache_stride_token, k_cache_stride_head, k_cache_stride_head_dim,
+            k_cache_stride_block, k_cache_stride_x, v_cache_stride_token,
+            v_cache_stride_head, v_cache_stride_head_dim, v_cache_stride_block,
+            output.stride(0), output.stride(1), num_queries_per_kv,
+            max_input_length, batch_size, num_heads);
+        break;
+      case 64:
+        vllm::context_attention_kernel_v1<sycl::half, 32, 64>(
+            query.data_ptr(), key.data_ptr(), value.data_ptr(),
+            block_tables.data_ptr(), attn_scale, query_start_loc.data_ptr(),
+            seq_lens.data_ptr(), context_lens.data_ptr(), block_size, x,
+            output.data_ptr(), block_table_stride_bsz, block_table_stride_seq,
+            query_stride_token, query_stride_head, query_stride_dim,
+            k_cache_stride_token, k_cache_stride_head, k_cache_stride_head_dim,
+            k_cache_stride_block, k_cache_stride_x, v_cache_stride_token,
+            v_cache_stride_head, v_cache_stride_head_dim, v_cache_stride_block,
+            output.stride(0), output.stride(1), num_queries_per_kv,
+            max_input_length, batch_size, num_heads);
+        break;
+      default:
+        throw std::runtime_error("unsupported head_dim");
+    }
+  }
+  return output;
+}
+
+template<typename IT, const int VS, const int HD>
+void gqa_1_kernel(
+    const void * query, // [num_seqs, num_heads, head_size]
+    const void * key,   // [num_blocks, num_kv_heads, head_size, block_size]
+    const void * value, // [num_blocks, num_kv_heads, head_size, block_size]
+    const void* block_tables, // [num_seqs, max_num_blocks_per_seq]
+    const void* context_lens, // [num_seqs]
+    void * o_a_s,
+    void * o_accs,
+    const int64_t query_bsz_stride,
+    const int64_t query_head_stride,
+    const int64_t kv_token_stride,
+    const int64_t kv_head_stride,
+    const int64_t kv_block_stride,
+    const int64_t block_table_stride_batch,
+    const int64_t o_a_s_bsz_stride,
+    const int64_t o_a_s_head_stride,
+    const int64_t o_accs_bsz_stride,
+    const int64_t o_accs_head_stride,
+    const float scale,
+    const int block_size,
+    const int bsz,
+    const int num_heads,
+    const int num_kv_heads,
+    const int block_num,
+    const at::Device & device
+) {
+    const int group_size = num_heads / num_kv_heads;
+    const int sub_rows = VS / group_size;
+    const int rem_rows = VS % group_size;
+
+    const float attn_scale = scale;
+
+    sycl::range<3> global_size(bsz, num_heads, block_num);
+    sycl::range<3> local_size(1, group_size, 1);
+
+    auto cgf = [&](sycl::handler& handle) {
+        handle.parallel_for(
+            sycl::nd_range<3>(global_size, local_size),
+            [=](sycl::nd_item<3> item) SYCL_ESIMD_KERNEL {
+                slm_init<VS * HD * sizeof(IT)>();
+
+                const int bsz_idx = item.get_global_id(0);
+                const int head_idx = item.get_global_id(1);
+                const int kv_head_idx = item.get_group(1);
+                const int tid = item.get_local_id(1);
+                const int vid = item.get_global_id(2);
+
+                const IT * query_head = (const IT *)query + bsz_idx * query_bsz_stride
+                                                          + head_idx * query_head_stride;
+                
+                IT * o_accs_head = (IT *)o_accs + bsz_idx * o_accs_bsz_stride
+                                                + head_idx * o_accs_head_stride;
+                float * o_a_s_head = (float *)o_a_s + bsz_idx * o_a_s_bsz_stride
+                                                    + head_idx * o_a_s_head_stride;
+
+                const int* block_tables_ptr = (const int*)block_tables;
+                const int* block_table =
+                    block_tables_ptr + bsz_idx * block_table_stride_batch;
+
+                const int* context_lens_ptr = (const int*)context_lens;
+                const int context_length = context_lens_ptr[bsz_idx];
+
+                simd<IT, HD> query_row = block_load<IT, HD>(query_head) * attn_scale;
+
+                // copy k_cache to slm
+                int start_row = std::min(vid * VS + tid * sub_rows + std::min(tid, rem_rows), context_length);
+                int end_row = std::min(start_row + sub_rows + (tid < rem_rows), context_length);
+                for (int r = start_row; r < end_row; ++r) {
+                    int which_block = r / block_size;
+                    int which_slot = r % block_size;
+                    int physical_block_number = block_table[which_block];
+
+                    const IT * key_head = (const IT *)key + physical_block_number * kv_token_stride +
+                      kv_head_idx * kv_head_stride +
+                      which_slot * kv_block_stride;
+
+                    simd<IT, HD> key_row = block_load<IT, HD>(key_head);
+                    slm_block_store<IT, HD>((r - vid * VS) * HD * sizeof(IT), key_row);
+                }
+                barrier();
+
+                simd<float, VS> attns = -sycl::detail::max_v<float>();
+                int row_num = (vid + 1) * VS > context_length ? context_length % VS : VS;
+                // q @ k
+                for (int r = 0; r < row_num; ++r) {
+                    simd<IT, HD> key_row = slm_block_load<IT, HD>(r * HD * sizeof(IT));
+                    float attn = sycl::ext::intel::esimd::detail::sum<float, IT, HD>(query_row * key_row);
+                    attns[r] = attn;
+                }
+
+                float max_attn = hmax<float, float, VS>(attns);
+                const simd<IT, VS> attn_exp = exp(attns - max_attn);
+                barrier();
+
+                // copy v_cache to slm
+                for (int r = start_row; r < end_row; ++r) {
+                    int which_block = r / block_size;
+                    int which_slot = r % block_size;
+                    int physical_block_number = block_table[which_block];
+
+                    const IT * value_head = (const IT *)value + physical_block_number * kv_token_stride +
+                      kv_head_idx * kv_head_stride +
+                      which_slot * kv_block_stride;
+
+                    simd<IT, HD> value_row = block_load<IT, HD>(value_head);
+                    slm_block_store<IT, HD>((r - vid * VS) * HD * sizeof(IT), value_row);
+                }
+                barrier();
+
+                // attn @ v
+                simd<IT, HD> accs = 0;
+                for (int r = 0; r < row_num; ++r) {
+                    simd<IT, HD> value_row = slm_block_load<IT, HD>(r * HD * sizeof(IT));
+                    accs = accs + value_row * attn_exp[r];
+                }
+
+                float softmax = sycl::ext::intel::esimd::detail::sum<float, float, VS>(attn_exp);
+
+                block_store<IT, HD>(o_accs_head + vid * HD, accs);
+                block_store<float, 1>(o_a_s_head + vid * 2, max_attn);
+                block_store<float, 1>(o_a_s_head + vid * 2 + 1, softmax);
+            }
+        );
+    };
+
+    utils::submit_kernel(cgf, device, "gqa kernel 1/2");
+}
+
+template<typename IT, const int GS, const int HD>
+void gqa_2_kernel(
+    void * o_a_s,
+    void * o_accs,
+    void * output,
+    const void* context_lens, // [num_seqs]
+    const int64_t o_a_s_bsz_stride,
+    const int64_t o_a_s_head_stride,
+    const int64_t o_accs_bsz_stride,
+    const int64_t o_accs_head_stride,
+    const int64_t output_bsz_stride,
+    const int64_t output_head_stride,
+    const int bsz,
+    const int num_heads,
+    const int row_block_num,
+    const at::Device & device
+) {
+    constexpr int SUB_HD = 8;
+    static_assert(HD % SUB_HD == 0);
+    static_assert(HD / SUB_HD <= GS);
+
+    const int sub_rows = row_block_num / GS;
+    const int rem_rows = row_block_num % GS;
+
+    constexpr int accs_slm_offset = 0;
+    constexpr int attn_slm_offset = GS * HD * sizeof(float);
+    constexpr int softmax_slm_offset = attn_slm_offset + GS * sizeof(float);
+
+    sycl::range<3> global_size(bsz, num_heads, GS);
+    sycl::range<3> local_size(1, 1, GS);
+
+    auto cgf = [&](sycl::handler& handle) {
+        handle.parallel_for(
+            sycl::nd_range<3>(global_size, local_size),
+            [=](sycl::nd_item<3> item) SYCL_ESIMD_KERNEL {
+                slm_init<GS * HD * sizeof(float) + GS * 2 * sizeof(float)>();
+
+                const int bsz_idx = item.get_global_id(0);
+                const int head_idx = item.get_global_id(1);
+                const int tid = item.get_global_id(2);
+
+                const int* context_lens_ptr = (const int*)context_lens;
+                const int context_length = context_lens_ptr[bsz_idx];
+                constexpr int VS = 32;
+                const int cur_row_block_num = (context_length + VS - 1) / VS;
+                const int cur_sub_rows = cur_row_block_num / GS;
+                const int cur_rem_rows = cur_row_block_num % GS;
+
+                const float * o_a_s_head = (const float *)o_a_s + bsz_idx * o_a_s_bsz_stride
+                                                                + head_idx * o_a_s_head_stride;
+                const IT * o_accs_head = (const IT *)o_accs + bsz_idx * o_accs_bsz_stride
+                                                            + head_idx * o_accs_head_stride;
+                IT * output_head = (IT *)output + bsz_idx * output_bsz_stride
+                                                + head_idx * output_head_stride;
+
+                int start_row = std::min(tid * cur_sub_rows + std::min(tid, cur_rem_rows), cur_row_block_num);
+                int end_row = std::min(start_row + cur_sub_rows + (tid < cur_rem_rows), cur_row_block_num);
+
+                float max_attn = -sycl::detail::max_v<float>();
+                float softmax = 0;
+                simd<float, HD> accs = 0;
+                for (int r = start_row; r < end_row; ++r) {
+                    float sub_attn = o_a_s_head[2 * r];
+                    float sub_softmax = o_a_s_head[2 * r + 1];
+                    simd<float, HD> sub_accs = block_load<IT, HD>(o_accs_head + r * HD);
+                    float new_max_attn = std::max(max_attn, sub_attn);
+                    float exp1 = exp(max_attn - new_max_attn);
+                    float exp2 = exp(sub_attn - new_max_attn);
+                    accs = accs * exp1 + sub_accs * exp2;
+                    softmax = softmax * exp1 + sub_softmax * exp2;
+                    max_attn = new_max_attn;
+                }
+
+                slm_block_store<float, HD>(accs_slm_offset + tid * HD * sizeof(float), accs);
+                slm_block_store<float, 1>(attn_slm_offset + tid * sizeof(float), max_attn);
+                slm_block_store<float, 1>(softmax_slm_offset + tid * sizeof(float), softmax);
+                barrier();
+
+                if (tid < HD / SUB_HD) {
+                    simd<float, GS> max_attns = slm_block_load<float, GS>(attn_slm_offset);
+                    const simd<float, GS> scales = exp(max_attns - hmax<float, float, GS>(max_attns));
+                    simd<float, GS> softmaxs = slm_block_load<float, GS>(softmax_slm_offset);
+                    float softmax_sum = sycl::ext::intel::esimd::detail::sum<float, float, GS>(softmaxs * scales);
+
+                    simd<float, SUB_HD> result = 0;
+                    #pragma unroll
+                    for (int r = 0; r < GS; ++r) {
+                        simd<float, SUB_HD> sub_accs = slm_block_load<float, SUB_HD>(
+                            accs_slm_offset + (r * HD + tid * SUB_HD) * sizeof(float)
+                        );
+                        result = result + sub_accs * scales[r];
+                    }
+                    result = result / softmax_sum;
+                    block_store<IT, SUB_HD>(output_head + tid * SUB_HD, result);
+                }
+            }
+        );
+    };
+
+    utils::submit_kernel(cgf, device, "gqa kernel 2/2");
+}
+
+using AT = at::ScalarType;
+using fp16 = sycl::half;
+template<const int VS, const int GS, const int HD>
+auto dispatch_gqa_kernel(AT it) {
+    switch (it) {
+        case AT::Float: return std::make_tuple(gqa_1_kernel<float, VS, HD>, gqa_2_kernel<float, GS, HD>);
+        case AT::Half: return std::make_tuple(gqa_1_kernel<fp16, VS, HD>, gqa_2_kernel<fp16, GS, HD>);
+        default: throw std::runtime_error("unsupported dtype, only fp32 and fp16 are supported");
+    }
+}
+
+void paged_attention_gqa(
+    torch::Tensor output,
+    torch::Tensor query,
+    torch::Tensor key_cache,
+    torch::Tensor value_cache,
+    int64_t bsz,
+    int64_t num_heads,
+    int64_t num_kv_heads,
+    float scale,
+    torch::Tensor& block_tables,
+    torch::Tensor& context_lens,
+    int block_size,
+    int64_t head_dim,
+    int max_seq_len
+) {
+    constexpr int VS = 32;
+    constexpr int GS = 32;
+
+    const int row_block_num = (max_seq_len + VS - 1) / VS;
+    auto o_a_s = torch::empty({bsz, num_heads, 1, row_block_num * 2},
+                              torch::device(query.device()).dtype(torch::kFloat32));
+    auto o_accs = torch::empty({bsz, num_heads, 1, row_block_num * head_dim},
+                               torch::device(query.device()).dtype(query.dtype()));
+
+    auto [func1, func2] = [&](){
+        switch (head_dim) {
+            case 128: return dispatch_gqa_kernel<VS, GS, 128>(query.scalar_type());
+            case 96: return dispatch_gqa_kernel<VS, GS, 96>(query.scalar_type());
+            case 80: return dispatch_gqa_kernel<VS, GS, 80>(query.scalar_type());
+            case 64: return dispatch_gqa_kernel<VS, GS, 64>(query.scalar_type());
+            default: throw std::runtime_error("unsupported head_dim, only 128, 96, 80 and 64 are supported");
+        }
+    }();
+
+    func1(
+        query.data_ptr(), key_cache.data_ptr(), value_cache.data_ptr(),
+        block_tables.data_ptr(), context_lens.data_ptr(), o_a_s.data_ptr(), o_accs.data_ptr(),
+        query.stride(0), query.stride(1), key_cache.stride(0), key_cache.stride(1), key_cache.stride(2), block_tables.stride(0),
+        o_a_s.stride(0), o_a_s.stride(1), o_accs.stride(0), o_accs.stride(1),
+        scale, block_size, bsz, num_heads, num_kv_heads, row_block_num,
+        query.device()
+    );
+
+    func2(
+        o_a_s.data_ptr(), o_accs.data_ptr(), output.data_ptr(), context_lens.data_ptr(),
+        o_a_s.stride(0), o_a_s.stride(1),
+        o_accs.stride(0), o_accs.stride(1),
+        output.stride(0), output.stride(1),
+        bsz, num_heads, row_block_num,
+        query.device()
+    );
+}
diff --git a/csrc/xpu/attention_xpu_fp8.cpp b/csrc/xpu/attention_xpu_fp8.cpp
new file mode 100644
index 000000000..a2ea5819b
--- /dev/null
+++ b/csrc/xpu/attention_xpu_fp8.cpp
@@ -0,0 +1,324 @@
+// clang-format off
+#ifdef VLLM_DEV
+#undef __SYCL_DEVICE_ONLY__
+#endif
+#include <sycl/sycl.hpp>
+#include <dpct/dpct.hpp>
+#include <ext/intel/esimd.hpp>
+#include "kv.h"
+
+// clang-format on
+#include <float.h>
+#include <torch/extension.h>
+#include <stdexcept>
+#include "utils.h"
+#include "xpu_types.h"
+// #include "dtype_bfloat16.dp.hpp"
+#include "dtype_float16.h"
+#include "dtype_float32.h"
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+#include <c10/xpu/XPUStream.h>
+#endif
+
+#include <functional>
+// #include <ipex.h>
+
+using namespace sycl::ext::intel::esimd;
+using AT = at::ScalarType;
+
+template <typename IT, const int VS, const int HD>
+void gqa_1_kernel_fp8(
+    const void* query,  // [num_seqs, num_heads, head_size]
+    const void* key,    // [num_blocks, num_kv_heads, head_size, block_size]
+    const void* value,  // [num_blocks, num_kv_heads, head_size, block_size]
+    const void* block_tables,  // [num_seqs, max_num_blocks_per_seq]
+    const void* context_lens,  // [num_seqs]
+    void* o_a_s, void* o_accs, const int64_t query_bsz_stride,
+    const int64_t query_head_stride, const int64_t kv_token_stride,
+    const int64_t kv_head_stride, const int64_t kv_block_stride,
+    const int64_t block_table_stride_batch, const int64_t o_a_s_bsz_stride,
+    const int64_t o_a_s_head_stride, const int64_t o_accs_bsz_stride,
+    const int64_t o_accs_head_stride, const float scale, const int block_size,
+    const int bsz, const int num_heads, const int num_kv_heads,
+    const int block_num, const at::Device& device) {
+  const int group_size = num_heads / num_kv_heads;
+  const int sub_rows = VS / group_size;
+  const int rem_rows = VS % group_size;
+
+  const float attn_scale = scale;
+
+  sycl::range<3> global_size(bsz, num_heads, block_num);
+  sycl::range<3> local_size(1, group_size, 1);
+
+  auto cgf = [&](sycl::handler& handle) {
+    handle.parallel_for(
+        sycl::nd_range<3>(global_size, local_size),
+        [=](sycl::nd_item<3> item) SYCL_ESIMD_KERNEL {
+          slm_init<VS * HD * sizeof(IT)>();
+
+          const int bsz_idx = item.get_global_id(0);
+          const int head_idx = item.get_global_id(1);
+          const int kv_head_idx = item.get_group(1);
+          const int tid = item.get_local_id(1);
+          const int vid = item.get_global_id(2);
+
+          const IT* query_head = (const IT*)query + bsz_idx * query_bsz_stride +
+                                 head_idx * query_head_stride;
+
+          IT* o_accs_head = (IT*)o_accs + bsz_idx * o_accs_bsz_stride +
+                            head_idx * o_accs_head_stride;
+          float* o_a_s_head = (float*)o_a_s + bsz_idx * o_a_s_bsz_stride +
+                              head_idx * o_a_s_head_stride;
+
+          const int* block_tables_ptr = (const int*)block_tables;
+          const int* block_table =
+              block_tables_ptr + bsz_idx * block_table_stride_batch;
+
+          const int* context_lens_ptr = (const int*)context_lens;
+          const int context_length = context_lens_ptr[bsz_idx];
+
+          simd<IT, HD> query_row = block_load<IT, HD>(query_head) * attn_scale;
+
+          // copy k_cache to slm
+          int start_row =
+              std::min(vid * VS + tid * sub_rows + std::min(tid, rem_rows),
+                       context_length);
+          int end_row =
+              std::min(start_row + sub_rows + (tid < rem_rows), context_length);
+          for (int r = start_row; r < end_row; ++r) {
+            int which_block = r / block_size;
+            int which_slot = r % block_size;
+            int physical_block_number = block_table[which_block];
+
+            // Load elements in uint8_t
+            const uint8_t* key_head =
+                (const uint8_t*)key + physical_block_number * kv_token_stride +
+                kv_head_idx * kv_head_stride + which_slot * kv_block_stride;
+
+            simd<uint8_t, HD> key_row = block_load<uint8_t, HD>(key_head);
+            simd<IT, HD> key_dequantized = dequantize_key_row<HD>(key_row);
+            slm_block_store<IT, HD>((r - vid * VS) * HD * sizeof(IT), key_dequantized);
+          }
+          barrier();
+
+          simd<float, VS> attns = -sycl::detail::max_v<float>();
+          int row_num =
+              (vid + 1) * VS > context_length ? context_length % VS : VS;
+          // q @ k
+          for (int r = 0; r < row_num; ++r) {
+            simd<IT, HD> key_row = slm_block_load<IT, HD>(r * HD * sizeof(IT));
+            float attn = sycl::ext::intel::esimd::detail::sum<float, IT, HD>(
+                query_row * key_row);
+            attns[r] = attn;
+          }
+
+          float max_attn = hmax<float, float, VS>(attns);
+          const simd<IT, VS> attn_exp = exp(attns - max_attn);
+          barrier();
+
+          // copy v_cache to slm
+          for (int r = start_row; r < end_row; ++r) {
+            int which_block = r / block_size;
+            int which_slot = r % block_size;
+            int physical_block_number = block_table[which_block];
+
+            const uint8_t* value_head =
+                (const uint8_t*)value + physical_block_number * kv_token_stride +
+                kv_head_idx * kv_head_stride + which_slot * kv_block_stride;
+
+            simd<uint8_t, HD> value_row = block_load<uint8_t, HD>(value_head);
+            simd<IT, HD> value_dequantized = dequantize_value_row<HD>(value_row);
+            slm_block_store<IT, HD>((r - vid * VS) * HD * sizeof(IT),
+                                    value_dequantized);
+          }
+          barrier();
+
+          // attn @ v
+          simd<IT, HD> accs = 0;
+          for (int r = 0; r < row_num; ++r) {
+            simd<IT, HD> value_row =
+                slm_block_load<IT, HD>(r * HD * sizeof(IT));
+            accs = accs + value_row * attn_exp[r];
+          }
+
+          float softmax =
+              sycl::ext::intel::esimd::detail::sum<float, float, VS>(attn_exp);
+
+          block_store<IT, HD>(o_accs_head + vid * HD, accs);
+          block_store<float, 1>(o_a_s_head + vid * 2, max_attn);
+          block_store<float, 1>(o_a_s_head + vid * 2 + 1, softmax);
+        });
+  };
+
+  utils::submit_kernel(cgf, device, "gqa kernel 1/2");
+}
+
+template <typename IT, const int GS, const int HD>
+void gqa_2_kernel_fp8(void* o_a_s, void* o_accs, void* output,
+                  const void* context_lens,  // [num_seqs]
+                  const int64_t o_a_s_bsz_stride,
+                  const int64_t o_a_s_head_stride,
+                  const int64_t o_accs_bsz_stride,
+                  const int64_t o_accs_head_stride,
+                  const int64_t output_bsz_stride,
+                  const int64_t output_head_stride, const int bsz,
+                  const int num_heads, const int row_block_num,
+                  const at::Device& device) {
+  constexpr int SUB_HD = 8;
+  static_assert(HD % SUB_HD == 0);
+  static_assert(HD / SUB_HD <= GS);
+
+  const int sub_rows = row_block_num / GS;
+  const int rem_rows = row_block_num % GS;
+
+  constexpr int accs_slm_offset = 0;
+  constexpr int attn_slm_offset = GS * HD * sizeof(float);
+  constexpr int softmax_slm_offset = attn_slm_offset + GS * sizeof(float);
+
+  sycl::range<3> global_size(bsz, num_heads, GS);
+  sycl::range<3> local_size(1, 1, GS);
+
+  auto cgf = [&](sycl::handler& handle) {
+    handle.parallel_for(
+        sycl::nd_range<3>(global_size, local_size),
+        [=](sycl::nd_item<3> item) SYCL_ESIMD_KERNEL {
+          slm_init<GS * HD * sizeof(float) + GS * 2 * sizeof(float)>();
+
+          const int bsz_idx = item.get_global_id(0);
+          const int head_idx = item.get_global_id(1);
+          const int tid = item.get_global_id(2);
+
+          const int* context_lens_ptr = (const int*)context_lens;
+          const int context_length = context_lens_ptr[bsz_idx];
+          constexpr int VS = 32;
+          const int cur_row_block_num = (context_length + VS - 1) / VS;
+          const int cur_sub_rows = cur_row_block_num / GS;
+          const int cur_rem_rows = cur_row_block_num % GS;
+
+          const float* o_a_s_head = (const float*)o_a_s +
+                                    bsz_idx * o_a_s_bsz_stride +
+                                    head_idx * o_a_s_head_stride;
+          const IT* o_accs_head = (const IT*)o_accs +
+                                  bsz_idx * o_accs_bsz_stride +
+                                  head_idx * o_accs_head_stride;
+          IT* output_head = (IT*)output + bsz_idx * output_bsz_stride +
+                            head_idx * output_head_stride;
+
+          int start_row =
+              std::min(tid * cur_sub_rows + std::min(tid, cur_rem_rows),
+                       cur_row_block_num);
+          int end_row =
+              std::min(start_row + cur_sub_rows + (tid < cur_rem_rows),
+                       cur_row_block_num);
+
+          float max_attn = -sycl::detail::max_v<float>();
+          float softmax = 0;
+          simd<float, HD> accs = 0;
+          for (int r = start_row; r < end_row; ++r) {
+            float sub_attn = o_a_s_head[2 * r];
+            float sub_softmax = o_a_s_head[2 * r + 1];
+            simd<float, HD> sub_accs = block_load<IT, HD>(o_accs_head + r * HD);
+            float new_max_attn = std::max(max_attn, sub_attn);
+            float exp1 = exp(max_attn - new_max_attn);
+            float exp2 = exp(sub_attn - new_max_attn);
+            accs = accs * exp1 + sub_accs * exp2;
+            softmax = softmax * exp1 + sub_softmax * exp2;
+            max_attn = new_max_attn;
+          }
+
+          slm_block_store<float, HD>(accs_slm_offset + tid * HD * sizeof(float),
+                                     accs);
+          slm_block_store<float, 1>(attn_slm_offset + tid * sizeof(float),
+                                    max_attn);
+          slm_block_store<float, 1>(softmax_slm_offset + tid * sizeof(float),
+                                    softmax);
+          barrier();
+
+          if (tid < HD / SUB_HD) {
+            simd<float, GS> max_attns =
+                slm_block_load<float, GS>(attn_slm_offset);
+            const simd<float, GS> scales =
+                exp(max_attns - hmax<float, float, GS>(max_attns));
+            simd<float, GS> softmaxs =
+                slm_block_load<float, GS>(softmax_slm_offset);
+            float softmax_sum =
+                sycl::ext::intel::esimd::detail::sum<float, float, GS>(
+                    softmaxs * scales);
+
+            simd<float, SUB_HD> result = 0;
+#pragma unroll
+            for (int r = 0; r < GS; ++r) {
+              simd<float, SUB_HD> sub_accs = slm_block_load<float, SUB_HD>(
+                  accs_slm_offset + (r * HD + tid * SUB_HD) * sizeof(float));
+              result = result + sub_accs * scales[r];
+            }
+            result = result / softmax_sum;
+            block_store<IT, SUB_HD>(output_head + tid * SUB_HD, result);
+          }
+        });
+  };
+
+  utils::submit_kernel(cgf, device, "gqa kernel 2/2");
+}
+
+template <const int VS, const int GS, const int HD>
+auto dispatch_gqa_kernel_fp8(AT it) {
+  switch (it) {
+    case AT::Float:
+      return std::make_tuple(gqa_1_kernel_fp8<float, VS, HD>,
+                             gqa_2_kernel_fp8<float, GS, HD>);
+    case AT::Half:
+      return std::make_tuple(gqa_1_kernel_fp8<fp16, VS, HD>,
+                             gqa_2_kernel_fp8<fp16, GS, HD>);
+    default:
+      throw std::runtime_error(
+          "unsupported dtype, only fp32 and fp16 are supported");
+  }
+}
+
+void paged_attention_gqa_fp8(torch::Tensor output, torch::Tensor query,
+                         torch::Tensor key_cache, torch::Tensor value_cache,
+                         int64_t bsz, int64_t num_heads, int64_t num_kv_heads,
+                         float scale, torch::Tensor& block_tables,
+                         torch::Tensor& context_lens, int block_size,
+                         int64_t head_dim, int max_seq_len) {
+  constexpr int VS = 32;
+  constexpr int GS = 32;
+
+  const int row_block_num = (max_seq_len + VS - 1) / VS;
+  auto o_a_s =
+      torch::empty({bsz, num_heads, 1, row_block_num * 2},
+                   torch::device(query.device()).dtype(torch::kFloat32));
+  auto o_accs =
+      torch::empty({bsz, num_heads, 1, row_block_num * head_dim},
+                   torch::device(query.device()).dtype(query.dtype()));
+
+  auto [func1, func2] = [&]() {
+    switch (head_dim) {
+      case 128:
+        return dispatch_gqa_kernel_fp8<VS, GS, 128>(query.scalar_type());
+      case 96:
+        return dispatch_gqa_kernel_fp8<VS, GS, 96>(query.scalar_type());
+      case 80:
+        return dispatch_gqa_kernel_fp8<VS, GS, 80>(query.scalar_type());
+      case 64:
+        return dispatch_gqa_kernel_fp8<VS, GS, 64>(query.scalar_type());
+      default:
+        throw std::runtime_error(
+            "unsupported head_dim, only 128, 96, 80 and 64 are supported");
+    }
+  }();
+
+  func1(query.data_ptr(), key_cache.data_ptr(), value_cache.data_ptr(),
+        block_tables.data_ptr(), context_lens.data_ptr(), o_a_s.data_ptr(),
+        o_accs.data_ptr(), query.stride(0), query.stride(1),
+        key_cache.stride(0), key_cache.stride(1), key_cache.stride(2),
+        block_tables.stride(0), o_a_s.stride(0), o_a_s.stride(1),
+        o_accs.stride(0), o_accs.stride(1), scale, block_size, bsz, num_heads,
+        num_kv_heads, row_block_num, query.device());
+
+  func2(o_a_s.data_ptr(), o_accs.data_ptr(), output.data_ptr(),
+        context_lens.data_ptr(), o_a_s.stride(0), o_a_s.stride(1),
+        o_accs.stride(0), o_accs.stride(1), output.stride(0), output.stride(1),
+        bsz, num_heads, row_block_num, query.device());
+}
diff --git a/csrc/xpu/base.hpp b/csrc/xpu/base.hpp
new file mode 100644
index 000000000..c364c62e6
--- /dev/null
+++ b/csrc/xpu/base.hpp
@@ -0,0 +1,118 @@
+#pragma once
+
+#include <sycl.hpp>
+#include <sycl/ext/intel/esimd.hpp>
+
+#include "common.h"
+
+using namespace sycl::ext::intel::esimd;
+using fp16 = sycl::half;
+
+constexpr int QK = 64;
+constexpr int SBS = 4;
+
+constexpr int BLOCK_SIZES[GGML_TYPE_COUNT] = {
+    [GGML_TYPE_Q4_0]     = QK / 2,
+    [GGML_TYPE_Q4_0_WOQ] = QK / 2,
+    [GGML_TYPE_FP8E5]  = QK,
+};
+
+constexpr int SCALE_SIZES[GGML_TYPE_COUNT] = {
+    [GGML_TYPE_Q4_0]     = sizeof(fp16),
+    [GGML_TYPE_Q4_0_WOQ] = sizeof(fp16),
+    [GGML_TYPE_FP8E5]  = 0,
+};
+
+template<int QTYPE>
+ESIMD_INLINE auto load_qblocks(const uint8_t * weight, const uint8_t * scale);
+
+template<>
+ESIMD_INLINE auto load_qblocks<GGML_TYPE_Q4_0>(const uint8_t * weight, const uint8_t * scale) {
+    constexpr int BLOCK_SIZE = BLOCK_SIZES[GGML_TYPE_Q4_0];
+    simd<uint8_t, BLOCK_SIZE * SBS> ybytes = block_load<uint8_t, BLOCK_SIZE * SBS>(weight);
+    const simd<fp16, SBS> scales = block_load<fp16, SBS>((const fp16 *)scale);
+
+    simd<fp16, QK * SBS> yvs;
+    #pragma unroll
+    for (int i = 0; i < SBS; ++i) {
+        simd<uint8_t, QK> uyv;
+        uyv.select<QK / 2, 1>(0) = ybytes.template select<QK / 2, 1>(i * QK / 2) & (uint8_t)0xF;
+        uyv.select<QK / 2, 1>(QK / 2) = ybytes.template select<QK / 2, 1>(i * QK / 2) >> (uint8_t)4;
+        yvs.template select<QK, 1>(i * QK) = (uyv.bit_cast_view<int8_t>() - (int8_t)8) * scales[i];
+    }
+    return yvs;
+}
+
+template<>
+ESIMD_INLINE auto load_qblocks<GGML_TYPE_Q4_0_WOQ>(const uint8_t * weight, const uint8_t * scale) {
+    constexpr int BLOCK_SIZE = BLOCK_SIZES[GGML_TYPE_Q4_0_WOQ];
+    simd<uint8_t, BLOCK_SIZE * SBS> ybytes = block_load<uint8_t, BLOCK_SIZE * SBS>(weight);
+    const simd<fp16, SBS> scales = block_load<fp16, SBS>((const fp16 *)scale);
+
+    simd<fp16, QK * SBS> yvs;
+    #pragma unroll
+    for (int i = 0; i < SBS; ++i) {
+        simd<uint8_t, QK> uyv;
+        uyv.select<QK / 2, 2>(0) = ybytes.template select<QK / 2, 1>(i * QK / 2) & (uint8_t)0xF;
+        uyv.select<QK / 2, 2>(1) = ybytes.template select<QK / 2, 1>(i * QK / 2) >> (uint8_t)4;
+        yvs.template select<QK, 1>(i * QK) = (uyv.bit_cast_view<int8_t>() - (int8_t)8) * scales[i];
+    }
+    return yvs;
+}
+
+
+template<>
+ESIMD_INLINE auto load_qblocks<GGML_TYPE_FP8E5>(const uint8_t * weight, const uint8_t * scale) {
+    constexpr int BLOCK_SIZE = BLOCK_SIZES[GGML_TYPE_FP8E5];
+    simd<uint8_t, BLOCK_SIZE * SBS> ybytes = block_load<uint8_t, BLOCK_SIZE * SBS>(weight);
+
+    simd<fp16, QK * SBS> yvs;
+    yvs.template bit_cast_view<uint8_t>().template select<QK * SBS, 2>(0) = 0x80;
+    yvs.template bit_cast_view<uint8_t>().template select<QK * SBS, 2>(1) = ybytes;
+    return yvs;
+}
+
+
+// C++ doesn't support function template partial specialization, so write a new version for SBS=1
+template<int QTYPE>
+ESIMD_INLINE auto load_qblock(const uint8_t * weight, const uint8_t * scale);
+
+template<>
+ESIMD_INLINE auto load_qblock<GGML_TYPE_Q4_0>(const uint8_t * weight, const uint8_t * scale) {
+    constexpr int BLOCK_SIZE = BLOCK_SIZES[GGML_TYPE_Q4_0];
+    simd<uint8_t, BLOCK_SIZE> ybytes = block_load<uint8_t, BLOCK_SIZE>(weight);
+    fp16 scales = *(const fp16 *)scale;
+
+    simd<uint8_t, QK> uyv;
+    uyv.select<QK / 2, 1>(0) = ybytes & (uint8_t)0xF;
+    uyv.select<QK / 2, 1>(QK / 2) = ybytes >> (uint8_t)4;
+    simd<fp16, QK> yv = (uyv.bit_cast_view<int8_t>() - (int8_t)8) * scales;
+
+    return yv;
+}
+
+template<>
+ESIMD_INLINE auto load_qblock<GGML_TYPE_Q4_0_WOQ>(const uint8_t * weight, const uint8_t * scale) {
+    constexpr int BLOCK_SIZE = BLOCK_SIZES[GGML_TYPE_Q4_0_WOQ];
+    simd<uint8_t, BLOCK_SIZE> ybytes = block_load<uint8_t, BLOCK_SIZE>(weight);
+    fp16 scales = *(const fp16 *)scale;
+
+    simd<uint8_t, QK> uyv;
+    uyv.select<QK / 2, 2>(0) = ybytes & (uint8_t)0xF;
+    uyv.select<QK / 2, 2>(1) = ybytes >> (uint8_t)4;
+    simd<fp16, QK> yv = (uyv.bit_cast_view<int8_t>() - (int8_t)8) * scales;
+
+    return yv;
+}
+
+
+template<>
+ESIMD_INLINE auto load_qblock<GGML_TYPE_FP8E5>(const uint8_t * weight, const uint8_t * scale) {
+    constexpr int BLOCK_SIZE = BLOCK_SIZES[GGML_TYPE_FP8E5];
+    simd<uint8_t, BLOCK_SIZE> ybytes = block_load<uint8_t, BLOCK_SIZE>(weight);
+
+    simd<fp16, QK> yvs;
+    yvs.template bit_cast_view<uint8_t>().template select<QK, 2>(0) = 0x80;
+    yvs.template bit_cast_view<uint8_t>().template select<QK, 2>(1) = ybytes;
+    return yvs;
+}
diff --git a/csrc/xpu/cache_ops_xpu.cpp b/csrc/xpu/cache_ops_xpu.cpp
new file mode 100644
index 000000000..a3451c0e7
--- /dev/null
+++ b/csrc/xpu/cache_ops_xpu.cpp
@@ -0,0 +1,579 @@
+// clang-format off
+#ifdef VLLM_DEV
+#undef __SYCL_DEVICE_ONLY__
+#endif
+#include <sycl/sycl.hpp>
+#include <dpct/dpct.hpp>
+#include <ext/intel/esimd.hpp>
+// clang-format on
+#include "xpu_types.h"
+
+#include <torch/extension.h>
+#include "utils.h"
+
+using fp16 = sycl::half;
+using namespace sycl::ext::intel::esimd;
+
+template <typename scalar_t>
+void reshape_and_cache_kernel(
+    const scalar_t* __restrict__ key, // [num_tokens, num_heads, head_size]
+    const scalar_t* __restrict__ value, // [num_tokens, num_heads, head_size]
+    scalar_t* __restrict__ key_cache, // [num_blocks, num_heads, head_size/x,
+                                      // block_size, x]
+    scalar_t* __restrict__ value_cache, // [num_blocks, num_heads, head_size,
+                                        // block_size]
+    const int64_t* __restrict__ slot_mapping, // [num_tokens]
+    const int key_stride,
+    const int value_stride,
+    const int num_heads,
+    const int head_size,
+    const int block_size,
+    const int x,
+    const sycl::nd_item<3>& item_ct1) {
+  const int64_t token_idx = item_ct1.get_group(2);
+  const int64_t slot_idx = slot_mapping[token_idx];
+  if (slot_idx < 0) {
+    // Padding token that should be ignored.
+    return;
+  }
+
+  const int64_t block_idx = slot_idx / block_size;
+  const int64_t block_offset = slot_idx % block_size;
+
+  const int n = num_heads * head_size;
+  for (int i = item_ct1.get_local_id(2); i < n;
+       i += item_ct1.get_local_range(2)) {
+    const int64_t src_key_idx = token_idx * key_stride + i;
+    const int64_t src_value_idx = token_idx * value_stride + i;
+
+    const int head_idx = i / head_size;
+    const int head_offset = i % head_size;
+    const int x_idx = head_offset / x;
+    const int x_offset = head_offset % x;
+
+    const int64_t tgt_key_idx =
+        block_idx * num_heads * (head_size / x) * block_size * x +
+        head_idx * (head_size / x) * block_size * x + x_idx * block_size * x +
+        block_offset * x + x_offset;
+    const int64_t tgt_value_idx =
+        block_idx * num_heads * head_size * block_size +
+        head_idx * head_size * block_size + head_offset * block_size +
+        block_offset;
+    key_cache[tgt_key_idx] = key[src_key_idx];
+    value_cache[tgt_value_idx] = value[src_value_idx];
+  }
+}
+
+template <typename scalar_t>
+void call_reshape_and_cache_kernel(
+    const scalar_t* __restrict__ key,
+    const scalar_t* __restrict__ value,
+    scalar_t* __restrict__ key_cache,
+    scalar_t* __restrict__ value_cache,
+    const int64_t* __restrict__ slot_mapping,
+    const int num_tokens,
+    const int key_stride,
+    const int value_stride,
+    const int num_heads,
+    const int head_size,
+    const int block_size,
+    const int x) {
+  using sycl_t = vllm::xpu::SyclTypeTrait<scalar_t>::Type;
+  sycl::range<3> grid(1, 1, num_tokens);
+  sycl::range<3> block(1, 1, std::min(num_heads * head_size, 512));
+  auto& queue = vllm::xpu::vllmGetQueue();
+  queue.submit([&](sycl::handler& cgh) {
+    cgh.parallel_for(
+        sycl::nd_range<3>(grid * block, block), [=](sycl::nd_item<3> item_ct1) {
+          reshape_and_cache_kernel<sycl_t>(
+              (const sycl_t* __restrict__)key,
+              (const sycl_t* __restrict__)value,
+              (sycl_t* __restrict__)key_cache,
+              (sycl_t* __restrict__)value_cache,
+              slot_mapping,
+              key_stride,
+              value_stride,
+              num_heads,
+              head_size,
+              block_size,
+              x,
+              item_ct1);
+        });
+  });
+}
+
+void reshape_and_cache(
+    torch::Tensor& key,
+    torch::Tensor& value,
+    torch::Tensor& key_cache,
+    torch::Tensor& value_cache,
+    torch::Tensor& slot_mapping,
+    const std::string& kv_cache_dtype,
+    const float kv_scale) {
+  int num_tokens = key.size(0);
+  int num_heads = key.size(1);
+  int head_size = key.size(2);
+  int block_size = key_cache.size(3);
+  int x = key_cache.size(4);
+
+  int key_stride = key.stride(0);
+  int value_stride = value.stride(0);
+
+  VLLM_XPU_DISPATCH_FLOATING_TYPES(
+      key.scalar_type(), "call_reshape_and_cache_kernel", [&] {
+        call_reshape_and_cache_kernel<scalar_t>(
+            key.data_ptr<scalar_t>(),
+            value.data_ptr<scalar_t>(),
+            key_cache.data_ptr<scalar_t>(),
+            value_cache.data_ptr<scalar_t>(),
+            slot_mapping.data_ptr<int64_t>(),
+            num_tokens,
+            key_stride,
+            value_stride,
+            num_heads,
+            head_size,
+            block_size,
+            x);
+      });
+}
+
+template <typename scalar_t>
+void reshape_and_cache_ipexllm_kernel(
+    const scalar_t* __restrict__ key, // [num_tokens, num_heads, head_size]
+    const scalar_t* __restrict__ value, // [num_tokens, num_heads, head_size]
+    scalar_t* __restrict__ key_cache, // [num_blocks, num_kv_heads, block_size, head_size]
+    scalar_t* __restrict__ value_cache, // [num_blocks, num_kv_heads, block_size, head_size]
+    const int64_t* __restrict__ slot_mapping, // [num_tokens]
+    const int key_stride,
+    const int value_stride,
+    const int num_heads,
+    const int head_size,
+    const int block_size,
+    const int x,
+    const sycl::nd_item<3>& item_ct1) {
+  const int64_t token_idx = item_ct1.get_group(2);
+  const int64_t slot_idx = slot_mapping[token_idx];
+  if (slot_idx < 0) {
+    // Padding token that should be ignored.
+    return;
+  }
+
+  const int64_t block_idx = slot_idx / block_size;
+  const int64_t block_offset = slot_idx % block_size;
+
+  const int n = num_heads * head_size;
+  for (int i = item_ct1.get_local_id(2); i < n;
+       i += item_ct1.get_local_range(2)) {
+    const int64_t src_key_idx = token_idx * key_stride + i;
+    const int64_t src_value_idx = token_idx * value_stride + i;
+
+    const int head_idx = i / head_size;
+    const int head_offset = i % head_size;
+
+    // const int64_t tgt_key_idx =
+    //     block_idx * num_heads * (head_size / x) * block_size * x +
+    //     head_idx * (head_size / x) * block_size * x + x_idx * block_size * x +
+    //     block_offset * x + x_offset;
+
+    // const int64_t tgt_value_idx =
+    //     block_idx * num_heads * head_size * block_size +
+    //     head_idx * head_size * block_size + head_offset * block_size +
+    //     block_offset;
+
+    const int64_t tgt_value_idx =
+        block_idx * num_heads * head_size * block_size +
+        head_idx * head_size * block_size + 
+        block_offset * head_size + 
+        head_offset;
+    const int64_t tgt_key_idx = tgt_value_idx;
+    key_cache[tgt_key_idx] = key[src_key_idx];
+    value_cache[tgt_value_idx] = value[src_value_idx];
+  }
+}
+
+template <typename scalar_t>
+void call_reshape_and_cache_ipexllm_kernel(
+    const scalar_t* __restrict__ key,
+    const scalar_t* __restrict__ value,
+    scalar_t* __restrict__ key_cache,
+    scalar_t* __restrict__ value_cache,
+    const int64_t* __restrict__ slot_mapping,
+    const int num_tokens,
+    const int key_stride,
+    const int value_stride,
+    const int num_heads,
+    const int head_size,
+    const int block_size,
+    const int x) {
+  using sycl_t = vllm::xpu::SyclTypeTrait<scalar_t>::Type;
+  sycl::range<3> grid(1, 1, num_tokens);
+  sycl::range<3> block(1, 1, std::min(num_heads * head_size, 512));
+  auto& queue = vllm::xpu::vllmGetQueue();
+  queue.submit([&](sycl::handler& cgh) {
+    cgh.parallel_for(
+        sycl::nd_range<3>(grid * block, block), [=](sycl::nd_item<3> item_ct1) {
+          reshape_and_cache_ipexllm_kernel<sycl_t>(
+              (const sycl_t* __restrict__)key,
+              (const sycl_t* __restrict__)value,
+              (sycl_t* __restrict__)key_cache,
+              (sycl_t* __restrict__)value_cache,
+              slot_mapping,
+              key_stride,
+              value_stride,
+              num_heads,
+              head_size,
+              block_size,
+              x,
+              item_ct1);
+        });
+  });
+}
+
+void reshape_and_cache_ipexllm(
+    torch::Tensor& key,
+    torch::Tensor& value,
+    torch::Tensor& key_cache,
+    torch::Tensor& value_cache,
+    torch::Tensor& slot_mapping,
+    const std::string& kv_cache_dtype,
+    const float kv_scale) {
+  int num_tokens = key.size(0);
+  int num_heads = key.size(1);
+  int head_size = key.size(2);
+  int block_size = key_cache.size(2);
+  // int x = key_cache.size(4);
+  int x = 1;
+
+  int key_stride = key.stride(0);
+  int value_stride = value.stride(0);
+
+  VLLM_XPU_DISPATCH_FLOATING_TYPES(
+      key.scalar_type(), "call_reshape_and_cache_ipexllm_kernel", [&] {
+        call_reshape_and_cache_ipexllm_kernel<scalar_t>(
+            key.data_ptr<scalar_t>(),
+            value.data_ptr<scalar_t>(),
+            key_cache.data_ptr<scalar_t>(),
+            value_cache.data_ptr<scalar_t>(),
+            slot_mapping.data_ptr<int64_t>(),
+            num_tokens,
+            key_stride,
+            value_stride,
+            num_heads,
+            head_size,
+            block_size,
+            x);
+      });
+}
+
+
+template <typename scalar_t>
+void copy_blocks_kernel(
+    int64_t* key_cache_ptrs,
+    int64_t* value_cache_ptrs,
+    const int64_t* __restrict__ block_mapping,
+    const int numel_per_block,
+    const sycl::nd_item<3>& item_ct1) {
+  const int layer_idx = item_ct1.get_group(2);
+  const int pair_idx = item_ct1.get_group(1);
+
+  scalar_t* key_cache = reinterpret_cast<scalar_t*>(key_cache_ptrs[layer_idx]);
+  scalar_t* value_cache =
+      reinterpret_cast<scalar_t*>(value_cache_ptrs[layer_idx]);
+  int64_t src_block_number = block_mapping[2 * pair_idx];
+  int64_t dst_block_number = block_mapping[2 * pair_idx + 1];
+
+  const int64_t src_block_offset = src_block_number * numel_per_block;
+  const int64_t dst_block_offset = dst_block_number * numel_per_block;
+  for (int i = item_ct1.get_local_id(2); i < numel_per_block;
+       i += item_ct1.get_local_range(2)) {
+    int64_t src_offset = src_block_offset + i;
+    int64_t dst_offset = dst_block_offset + i;
+    key_cache[dst_offset] = key_cache[src_offset];
+  }
+  for (int i = item_ct1.get_local_id(2); i < numel_per_block;
+       i += item_ct1.get_local_range(2)) {
+    int64_t src_offset = src_block_offset + i;
+    int64_t dst_offset = dst_block_offset + i;
+    value_cache[dst_offset] = value_cache[src_offset];
+  }
+}
+
+template <typename scalar_t>
+void call_copy_blocks_kernel(
+    std::vector<torch::Tensor>& key_caches,
+    std::vector<torch::Tensor>& value_caches,
+    const std::map<int64_t, std::vector<int64_t>>& block_mapping) {
+  using sycl_t = vllm::xpu::SyclTypeTrait<scalar_t>::Type;
+  int num_layers = key_caches.size();
+  TORCH_CHECK(num_layers == value_caches.size());
+  if (num_layers == 0) {
+    return;
+  }
+  torch::Device cache_device = key_caches[0].device();
+  TORCH_CHECK(cache_device.is_xpu());
+  // Create data structures for the kernel.
+  // Create an array of pointers to the key and value caches.
+  int64_t key_cache_ptrs[num_layers];
+  int64_t value_cache_ptrs[num_layers];
+  for (int layer_idx = 0; layer_idx < num_layers; ++layer_idx) {
+    key_cache_ptrs[layer_idx] =
+        reinterpret_cast<int64_t>(key_caches[layer_idx].data_ptr());
+    value_cache_ptrs[layer_idx] =
+        reinterpret_cast<int64_t>(value_caches[layer_idx].data_ptr());
+  }
+  // Create block mapping array.
+  std::vector<int64_t> block_mapping_vec;
+  for (const auto& pair : block_mapping) {
+    int64_t src_block_number = pair.first;
+    for (int64_t dst_block_number : pair.second) {
+      block_mapping_vec.push_back(src_block_number);
+      block_mapping_vec.push_back(dst_block_number);
+    }
+  }
+  int64_t* block_mapping_array = block_mapping_vec.data();
+  int num_pairs = block_mapping_vec.size() / 2;
+  // Move the data structures to the GPU.
+  // NOTE: This synchronizes the CPU and GPU.
+  torch::Tensor key_cache_ptrs_tensor =
+      torch::from_blob(key_cache_ptrs, {num_layers}, torch::kInt64)
+          .to(cache_device);
+  torch::Tensor value_cache_ptrs_tensor =
+      torch::from_blob(value_cache_ptrs, {num_layers}, torch::kInt64)
+          .to(cache_device);
+  torch::Tensor block_mapping_tensor =
+      torch::from_blob(block_mapping_array, {2 * num_pairs}, torch::kInt64)
+          .to(cache_device);
+  auto k_ptr = key_cache_ptrs_tensor.data_ptr<int64_t>();
+  auto v_ptr = value_cache_ptrs_tensor.data_ptr<int64_t>();
+  auto b_ptr = block_mapping_tensor.data_ptr<int64_t>();
+  // Launch the kernel.
+  const int numel_per_block = key_caches[0][0].numel();
+
+  sycl::range<3> grid(1, num_pairs, num_layers);
+  sycl::range<3> block(1, 1, std::min(1024, numel_per_block));
+  auto& queue = vllm::xpu::vllmGetQueue();
+  queue.submit([&](sycl::handler& cgh) {
+    cgh.parallel_for(
+        sycl::nd_range<3>(grid * block, block), [=](sycl::nd_item<3> item_ct1) {
+          copy_blocks_kernel<sycl_t>(
+              k_ptr, v_ptr, b_ptr, numel_per_block, item_ct1);
+        });
+  });
+}
+
+void copy_blocks(
+    std::vector<torch::Tensor>& key_caches,
+    std::vector<torch::Tensor>& value_caches,
+    const std::map<int64_t, std::vector<int64_t>>& block_mapping) {
+  VLLM_XPU_DISPATCH_FLOATING_TYPES(
+      key_caches[0].scalar_type(), "call_copy_blocks_kernel", [&] {
+        call_copy_blocks_kernel<scalar_t>(
+            key_caches, value_caches, block_mapping);
+      });
+}
+
+void swap_blocks(
+    torch::Tensor& src,
+    torch::Tensor& dst,
+    const std::map<int64_t, int64_t>& block_mapping) {
+  char* src_ptr = (char*)src.data_ptr();
+  char* dst_ptr = (char*)dst.data_ptr();
+
+  const int64_t block_size_in_bytes = src.element_size() * src[0].numel();
+  auto& queue = vllm::xpu::vllmGetQueue();
+
+  // NOTE(woosuk): This can be slow if the number of blocks is large.
+  for (const auto& pair : block_mapping) {
+    int64_t src_block_number = pair.first;
+    int64_t dst_block_number = pair.second;
+    int64_t src_offset = src_block_number * block_size_in_bytes;
+    int64_t dst_offset = dst_block_number * block_size_in_bytes;
+    queue.memcpy(
+        dst_ptr + dst_offset, src_ptr + src_offset, block_size_in_bytes);
+  }
+  queue.wait();
+}
+
+template <typename scalar_t>
+void gather_cached_kv_kernel(
+    scalar_t* __restrict__ key, // [num_tokens, [stride], num_heads, head_size]
+    scalar_t* __restrict__ value, // [num_tokens, [stride], num_heads,
+                                  // head_size]
+    const scalar_t* __restrict__ key_cache, // [num_blocks, num_heads,
+                                            // head_size/x, block_size, x]
+    const scalar_t* __restrict__ value_cache, // [num_blocks, num_heads,
+                                              // head_size, block_size]
+    const int* __restrict__ slot_mapping, // [num_tokens]
+    const int key_stride,
+    const int value_stride,
+    const int num_heads,
+    const int head_size,
+    const int block_size,
+    const int x,
+    const sycl::nd_item<3>& item_ct1) {
+  const int token_idx = item_ct1.get_group(2);
+  const int slot_idx = slot_mapping[token_idx];
+  const int block_idx = slot_idx / block_size;
+  const int block_offset = slot_idx % block_size;
+
+  const int num_tokens = num_heads * head_size;
+  for (int i = item_ct1.get_local_id(2); i < num_tokens;
+       i += item_ct1.get_local_range(2)) {
+    const int tgt_key_idx = token_idx * key_stride + i;
+    const int tgt_value_idx = token_idx * value_stride + i;
+
+    const int head_idx = i / head_size;
+    const int head_offset = i % head_size;
+    const int x_idx =
+        head_offset / x; // the offset of the [head_size/x] dimension
+    const int x_offset = head_offset % x;
+
+    // const int src_key_idx =
+    //     block_idx * num_heads * (head_size / x) * block_size * x +
+    //     head_idx * (head_size / x) * block_size * x + x_idx * block_size * x +
+    //     block_offset * x + x_offset;
+    // const int src_value_idx = block_idx * num_heads * head_size * block_size +
+    //     head_idx * head_size * block_size + head_offset * block_size +
+    //     block_offset;
+
+    const int src_value_idx = 
+        block_idx * num_heads * head_size * block_size + 
+        head_idx * head_size * block_size + 
+        block_offset * head_size + 
+        head_offset;
+    const int src_key_idx = src_value_idx;
+
+    key[tgt_key_idx] = VLLM_LDG(&key_cache[src_key_idx]);
+    value[tgt_value_idx] = VLLM_LDG(&value_cache[src_value_idx]);
+  }
+}
+
+template <typename scalar_t>
+void gather_cached_kv_kernel_optimized(
+    scalar_t* __restrict__ key, // [num_tokens, [stride], num_heads, head_size]
+    scalar_t* __restrict__ value, // [num_tokens, [stride], num_heads,
+                                  // head_size]
+    const scalar_t* __restrict__ key_cache, // [num_blocks, num_heads,
+                                            // head_size/x, block_size, x]
+    const scalar_t* __restrict__ value_cache, // [num_blocks, num_heads,
+                                              // head_size, block_size]
+    const int* __restrict__ slot_mapping, // [num_tokens]
+    const int key_stride,
+    const int value_stride,
+    const int num_heads,
+    const int head_size,
+    const int block_size,
+    const int x,
+    const sycl::nd_item<3>& item_ct1) {
+  const int token_idx = item_ct1.get_group(2);
+  const int slot_idx = slot_mapping[token_idx];
+  const int block_idx = slot_idx / block_size;
+  const int block_offset = slot_idx % block_size;
+
+  const int dim = num_heads * head_size;
+  assert(dim % 4 == 0); // this is true for known use cases
+  const int unroll_factor = 4;
+  const int unrolled_dim = dim / unroll_factor;
+
+  for (int i = item_ct1.get_local_id(2); i < unrolled_dim;
+       i += item_ct1.get_local_range(2)) {
+    int tgt_key_indices[unroll_factor];
+    int tgt_value_indices[unroll_factor];
+    int src_key_indices[unroll_factor];
+    int src_value_indices[unroll_factor];
+    scalar_t keys_to_store[unroll_factor];
+    scalar_t values_to_store[unroll_factor];
+
+#pragma unroll
+    for (int j = 0; j < unroll_factor; ++j) {
+      int index = i + j * unrolled_dim;
+
+      const int tgt_key_idx = token_idx * key_stride + index;
+      const int tgt_value_idx = token_idx * value_stride + index;
+
+      const int head_idx = index / head_size;
+      const int head_offset = index % head_size;
+
+      const int src_value_idx = 
+        block_idx * num_heads * head_size * block_size + 
+        head_idx * head_size * block_size + 
+        block_offset * head_size + 
+        head_offset;
+      const int src_key_idx = src_value_idx;
+
+      tgt_key_indices[j] = tgt_key_idx;
+      tgt_value_indices[j] = tgt_value_idx;
+      src_key_indices[j] = src_key_idx;
+      src_value_indices[j] = src_value_idx;
+
+      keys_to_store[j] = VLLM_LDG(&key_cache[src_key_idx]);
+      values_to_store[j] = VLLM_LDG(&value_cache[src_value_idx]);
+    }
+
+#pragma unroll
+    for (int j = 0; j < unroll_factor; ++j) {
+      key[tgt_key_indices[j]] = keys_to_store[j];
+      value[tgt_value_indices[j]] = values_to_store[j];
+    }
+  }
+}
+
+template <typename scalar_t>
+void call_gather_cached_kv_kernel_optimized(
+    torch::Tensor& key,
+    torch::Tensor& value,
+    torch::Tensor& key_cache,
+    torch::Tensor& value_cache,
+    torch::Tensor& slot_mapping) {
+  using sycl_t = vllm::xpu::SyclTypeTrait<scalar_t>::Type;
+  int num_tokens = key.size(0);
+  int num_heads = key.size(1);
+  int head_size = key.size(2);
+  int block_size = key_cache.size(2);
+  // int x = key_cache.size(4);
+  int x = 1;
+
+  int key_stride = key.stride(0);
+  int value_stride = value.stride(0);
+  auto key_ptr = key.data_ptr<scalar_t>();
+  auto value_ptr = value.data_ptr<scalar_t>();
+  auto key_cache_ptr = key_cache.data_ptr<scalar_t>();
+  auto value_cache_ptr = value_cache.data_ptr<scalar_t>();
+  auto slot_mapping_ptr = slot_mapping.data_ptr<int>();
+  sycl::range<3> grid(1, 1, num_tokens);
+  sycl::range<3> block(1, 1, std::min(num_heads * head_size, 512));
+  auto& queue = vllm::xpu::vllmGetQueue();
+  queue.submit([&](sycl::handler& cgh) {
+    cgh.parallel_for(
+        sycl::nd_range<3>(grid * block, block), [=](sycl::nd_item<3> item_ct1) {
+          gather_cached_kv_kernel_optimized<sycl_t>(
+              (sycl_t* __restrict__)key_ptr,
+              (sycl_t* __restrict__)value_ptr,
+              (const sycl_t* __restrict__)key_cache_ptr,
+              (const sycl_t* __restrict__)value_cache_ptr,
+              slot_mapping_ptr,
+              key_stride,
+              value_stride,
+              num_heads,
+              head_size,
+              block_size,
+              x,
+              item_ct1);
+        });
+  });
+}
+
+void gather_cached_kv(
+    torch::Tensor& key,
+    torch::Tensor& value,
+    torch::Tensor& key_cache,
+    torch::Tensor& value_cache,
+    torch::Tensor& slot_mapping) {
+  VLLM_XPU_DISPATCH_FLOATING_TYPES(
+      key_cache[0].scalar_type(),
+      "call_gather_cached_kv_kernel_optimized",
+      [&] {
+        call_gather_cached_kv_kernel_optimized<scalar_t>(
+            key, value, key_cache, value_cache, slot_mapping);
+      });
+}
diff --git a/csrc/xpu/cache_ops_xpu_fp8.cpp b/csrc/xpu/cache_ops_xpu_fp8.cpp
new file mode 100644
index 000000000..e4a0001fe
--- /dev/null
+++ b/csrc/xpu/cache_ops_xpu_fp8.cpp
@@ -0,0 +1,170 @@
+// clang-format off
+#ifdef VLLM_DEV
+#undef __SYCL_DEVICE_ONLY__
+#endif
+#include <sycl/sycl.hpp>
+#include <dpct/dpct.hpp>
+#include <ext/intel/esimd.hpp>
+// clang-format on
+#include "xpu_types.h"
+
+#include <torch/extension.h>
+#include "utils.h"
+#include "kv.h"
+
+using fp16 = sycl::half;
+using namespace sycl::ext::intel::esimd;
+
+// scalar_t is key.scalar_type() -> half
+template <typename scalar_t, const int HD>
+void reshape_and_cache_ipexllm_kernel_fp8(
+    const scalar_t* __restrict__ key,    // [num_tokens, num_heads, head_size]
+    const scalar_t* __restrict__ value,  // [num_tokens, num_heads, head_size]
+    uint8_t * __restrict__ key_cache,  // [num_blocks, num_kv_heads, block_size,
+                                       // head_size]
+    uint8_t * __restrict__ value_cache,        // [num_blocks, num_kv_heads,
+                                               // block_size, head_size]
+    const int64_t* __restrict__ slot_mapping,  // [num_tokens]
+    const int key_stride, const int value_stride,
+    const int key_head_stride, const int value_head_stride,
+    const int num_heads,
+    const int head_size, const int block_size, const int x,
+    const sycl::nd_item<3>& item_ct1) {
+
+  //                      New Implementation                      //
+  const size_t token_idx = item_ct1.get_global_id(0);
+  const size_t head_idx = item_ct1.get_global_id(1);
+  const int64_t slot_idx = slot_mapping[token_idx];
+  if (slot_idx < 0) {
+    return;
+  }
+  const int64_t block_idx = slot_idx / block_size;
+  const int64_t block_offset = slot_idx % block_size;
+  // The thread is responsible for the HD elements within key/value
+  const scalar_t * key_head = key + token_idx * key_stride + head_idx * key_head_stride;
+
+  const scalar_t * value_head = value + token_idx * value_stride + head_idx * value_head_stride;
+
+  uint8_t * key_output_head = key_cache + block_idx * num_heads * head_size * block_size +
+      head_idx * head_size * block_size + block_offset * head_size;
+  uint8_t * value_output_head = value_cache + block_idx * num_heads * head_size * block_size +
+      head_idx * head_size * block_size + block_offset * head_size;
+
+  simd<fp16, HD> key_row = block_load<scalar_t, HD>(key_head);
+  simd<uint8_t, HD> key_result = quantize_key_row<HD>(key_row);
+  block_store<uint8_t, HD>(key_output_head, key_result);
+
+  simd<fp16, HD> value_row = block_load<scalar_t, HD>(value_head);
+  simd<uint8_t, HD> value_result = quantize_value_row<HD>(value_row);
+  block_store<uint8_t, HD>(value_output_head, value_result);
+}
+
+
+template <typename scalar_t, const int HD>
+void call_reshape_and_cache_ipexllm_kernel_fp8(
+    const scalar_t* __restrict__ key, const scalar_t* __restrict__ value,
+    uint8_t* __restrict__ key_cache, uint8_t* __restrict__ value_cache,
+    const int64_t* __restrict__ slot_mapping, const int num_tokens,
+    const int key_stride, const int value_stride,
+    const int key_head_stride, const int value_head_stride,
+    const int num_heads,
+    const int head_size, const int block_size, const int x) {
+  using sycl_t = vllm::xpu::SyclTypeTrait<scalar_t>::Type;
+  sycl::range<3> grid(num_tokens, num_heads, 1);
+  sycl::range<3> block(1, 1, 1);
+  auto& queue = vllm::xpu::vllmGetQueue();
+  queue.submit([&](sycl::handler& cgh) {
+    cgh.parallel_for(
+        sycl::nd_range<3>(grid * block, block), [=](sycl::nd_item<3> item_ct1) SYCL_ESIMD_KERNEL {
+          reshape_and_cache_ipexllm_kernel_fp8<sycl_t, HD>(
+              (const sycl_t* __restrict__)key,
+              (const sycl_t* __restrict__)value,
+              (uint8_t* __restrict__)key_cache,
+              (uint8_t* __restrict__)value_cache, slot_mapping, key_stride,
+              value_stride, key_head_stride, value_head_stride,
+              num_heads, head_size, block_size, x, item_ct1);
+        });
+  });
+}
+
+void reshape_and_cache_ipexllm_fp8(torch::Tensor& key, torch::Tensor& value,
+                               torch::Tensor& key_cache,
+                               torch::Tensor& value_cache,
+                               torch::Tensor& slot_mapping,
+                               const std::string& kv_cache_dtype,
+                               const float kv_scale) {
+  int num_tokens = key.size(0);
+  int num_heads = key.size(1);
+  int head_size = key.size(2);
+  int block_size = key_cache.size(2);
+  // int x = key_cache.size(4);
+  int x = 1;
+
+  int key_stride = key.stride(0);
+  int value_stride = value.stride(0);
+
+  int key_head_stride = key.stride(1);
+  int value_head_stride = value.stride(1);
+
+  // This actually dispatches on scalar_type, we will then need to dispatch on Head Dim...
+switch (head_size) {
+  case 64:
+    VLLM_XPU_DISPATCH_FLOATING_TYPES(
+        key.scalar_type(), "call_reshape_and_cache_ipexllm_kernel_fp8", [&] {
+          call_reshape_and_cache_ipexllm_kernel_fp8<scalar_t, 64>(
+              key.data_ptr<scalar_t>(), value.data_ptr<scalar_t>(),
+              key_cache.data_ptr<uint8_t>(), value_cache.data_ptr<uint8_t>(),
+              slot_mapping.data_ptr<int64_t>(), num_tokens, key_stride,
+              value_stride, key_head_stride, value_head_stride, num_heads,
+              head_size, block_size, x);
+        });
+    break;
+  case 128:
+    VLLM_XPU_DISPATCH_FLOATING_TYPES(
+        key.scalar_type(), "call_reshape_and_cache_ipexllm_kernel_fp8", [&] {
+          call_reshape_and_cache_ipexllm_kernel_fp8<scalar_t, 128>(
+              key.data_ptr<scalar_t>(), value.data_ptr<scalar_t>(),
+              key_cache.data_ptr<uint8_t>(), value_cache.data_ptr<uint8_t>(),
+              slot_mapping.data_ptr<int64_t>(), num_tokens, key_stride,
+              value_stride, key_head_stride, value_head_stride, num_heads,
+              head_size, block_size, x);
+        });
+    break;
+  case 96:
+    VLLM_XPU_DISPATCH_FLOATING_TYPES(
+        key.scalar_type(), "call_reshape_and_cache_ipexllm_kernel_fp8", [&] {
+          call_reshape_and_cache_ipexllm_kernel_fp8<scalar_t, 96>(
+              key.data_ptr<scalar_t>(), value.data_ptr<scalar_t>(),
+              key_cache.data_ptr<uint8_t>(), value_cache.data_ptr<uint8_t>(),
+              slot_mapping.data_ptr<int64_t>(), num_tokens, key_stride,
+              value_stride, key_head_stride, value_head_stride, num_heads,
+              head_size, block_size, x);
+        });
+    break;
+  case 80:
+    VLLM_XPU_DISPATCH_FLOATING_TYPES(
+        key.scalar_type(), "call_reshape_and_cache_ipexllm_kernel_fp8", [&] {
+          call_reshape_and_cache_ipexllm_kernel_fp8<scalar_t, 80>(
+              key.data_ptr<scalar_t>(), value.data_ptr<scalar_t>(),
+              key_cache.data_ptr<uint8_t>(), value_cache.data_ptr<uint8_t>(),
+              slot_mapping.data_ptr<int64_t>(), num_tokens, key_stride,
+              value_stride, key_head_stride, value_head_stride, num_heads,
+              head_size, block_size, x);
+        });
+    break;
+  default:
+    TORCH_CHECK(false, "Unsupported head_dim: ", head_size);
+}
+  // VLLM_XPU_DISPATCH_FLOATING_TYPES(
+  //     key.scalar_type(), "call_reshape_and_cache_ipexllm_kernel_fp8", [&] {
+  //       call_reshape_and_cache_ipexllm_kernel_fp8<scalar_t, 128>(
+  //           key.data_ptr<scalar_t>(), value.data_ptr<scalar_t>(),
+  //           key_cache.data_ptr<uint8_t>(), value_cache.data_ptr<uint8_t>(),
+  //           slot_mapping.data_ptr<int64_t>(), num_tokens, key_stride,
+  //           value_stride, key_head_stride, value_head_stride,
+  //           num_heads, head_size, block_size, x);
+  //     });
+}
+
+
+
diff --git a/csrc/xpu/common.h b/csrc/xpu/common.h
new file mode 100644
index 000000000..17d6ef643
--- /dev/null
+++ b/csrc/xpu/common.h
@@ -0,0 +1,312 @@
+#pragma once
+
+#include <sycl.hpp>
+#include <torch/extension.h>
+
+typedef union half_t {
+    uint16_t u;
+    sycl::half f;
+} __half_t;
+
+typedef union ufloat32 {
+    unsigned u;
+    float f;
+} __float_t;
+
+#define QK4_0 64
+#define QR4_0 2
+#define QK4_1 64
+#define QR4_1 2
+#define QK5_0 64
+#define QR5_0 2
+#define QK5_1 64
+#define QR5_1 2
+#define QK8_0 64
+#define QR8_0 1
+#define QK8_1 32
+#define QR8_1 1
+#define QI8_1 (QK8_1 / (4 * QR8_1)) // 8
+#define QKFP8 64
+#define QRFP8 1
+#define QKFP6 64
+// for iq2 quantization
+#define WARP_SIZE 32
+#define QK_K 256
+#define QK4_K 32
+#define QR4_K 2
+#define QK6_K 16
+#define QKFP6_K 16
+#define QR2_XXS 8
+#define QI2_XXS (QK_K / (4*QR2_XXS)) // 8
+#define QR2_XS 8
+#define QI2_XS (QK_K / (4*QR2_XS)) // 8
+#define QR2_K 4
+#define QI2_K (QK_K / (4*QR2_K)) // 16
+#define QR1_S 8
+#define QI1_S (QK_K / (4*QR1_S)) // 8
+
+typedef struct {
+    sycl::half d;          // delta
+    uint8_t qs[QK4_0 / 2];    // nibbles / quants
+} block_q4_0;
+
+typedef struct {
+    uint8_t qs[QK4_0 / 2];    // nibbles / quants
+} block_q4_0_qs;
+
+typedef struct {
+    uint8_t qs[QK4_1 / 2];    // nibbles / quants
+} block_q4_1_qs;
+
+typedef struct {
+    sycl::half d;              // delta
+    sycl::half m;              // min
+    uint8_t qs[QK4_1 / 2];     // nibbles / quants
+} block_q4_1;
+
+typedef struct {
+    sycl::half d;
+    uint8_t qh[8];
+    uint8_t qs[QK5_0 / 2];
+} block_q5_0;
+
+typedef struct {
+    sycl::half d;          // delta
+    sycl::half m;          // min
+    uint8_t qh[8];         // 5-th bit of quants
+    uint8_t qs[QK5_1 / 2]; // nibbles / quants
+} block_q5_1;
+
+typedef struct {
+    sycl::half d;           // delta
+    uint8_t qh[8];          // 3-th bit of quants
+    uint8_t qs[QK4_0 / 4];  // nibbles / quants
+} block_nf3;
+
+typedef struct {
+    uint8_t qh[8];          // 3-th bit of quants
+    uint8_t qs[QK4_0 / 4];  // nibbles / quants
+} block_nf3_qs;
+
+typedef struct {
+    float d;       // delta
+    int8_t qs[QK8_0];   // quants
+} block_q8_0;
+
+typedef struct {
+    int8_t qs[QK8_0];   // quants
+} block_q8_0_qs;
+
+typedef struct {
+    sycl::half d;
+    sycl::half sum;
+    int8_t  qs[QK8_1];      // quants
+} block_q8_1;
+
+typedef struct {
+    uint8_t qs[QKFP8];
+} block_fp8_qs;
+
+typedef struct {
+    float d;
+    uint8_t qs[QKFP8];
+} block_fp8;
+
+typedef struct {
+    sycl::half d;
+    uint16_t qs[QK_K/8]; // 32
+} block_iq2_xxs;
+
+typedef struct {
+    sycl::half d;
+    uint16_t qs[QK_K/8]; // 32
+    uint8_t  scales[QK_K/32]; // 8
+} block_iq2_xs;
+
+typedef struct {
+    uint8_t scales[QK_K/16]; // scales and mins, quantized with 4 bits
+    uint8_t qs[QK_K/4];      // quants
+    sycl::half d;            // super-block scale for quantized scales
+    sycl::half min;          // super-block min for quantized mins
+} block_q2_K;
+
+typedef struct {
+    sycl::half d;                 // super-block scale for quantized scales
+    sycl::half dmin;              // super-block scale for quantized mins
+    uint8_t scales[16];           // scales and mins, quantized with 8 bits
+    uint8_t qs[QK_K/2];           // 4--bit quants
+} block_q4_K;
+
+typedef struct {
+    uint8_t qs[QK_K/2];            // 4-bit quants
+} block_q4_K_qs;
+
+typedef struct {
+    uint8_t qs[QK4_K/2];            // 4-bit quants
+} block_q4_K_qs_block;
+
+typedef struct {
+    uint8_t scales[16];            // scales and mins, quantized with 8 bits
+} block_q4_K_scales;
+
+typedef struct {
+    sycl::half d;               // super-block scale for quantized scales
+    sycl::half dmin;            // super-block scale for quantized mins
+    uint8_t scales[12];         // scales and mins, quantized with 6 bits
+    uint8_t qh[QK_K/8];          // quants, high bit
+    uint8_t qs[QK_K/2];          // quants, low 4 bits
+} block_q5_K;
+
+typedef struct {
+    uint8_t ql[QK_K/2];   // quants, lower 4 bits
+    uint8_t qh[QK_K/4];   // quants, upper 2 bits
+    int8_t  scales[QK_K/16]; // scales
+    sycl::half d;            // delta
+} block_q6_K;
+
+typedef struct {
+    uint32_t qh[QK_K/16];      // quants, upper 2 bits
+} block_q6_K_qh;
+
+typedef struct {
+    uint32_t ql[QK_K/8];      // quants, lower 4 bits
+} block_q6_K_ql;
+
+typedef struct {
+    int8_t  scales[QK_K/16]; // scales, quantized with 8 bits
+} block_q6_K_scales;
+
+typedef struct {
+    uint8_t ql[QK_K/2];       // quants, lower 4 bits
+    uint8_t qh[QK_K/4];       // quants, upper 2 bits
+    int8_t  scales[QK_K/16];  // scales, quantized with 8 bits
+    sycl::half d;            // super-block scale
+} block_fp6_K;
+static_assert(sizeof(block_fp6_K) == sizeof(sycl::half) + QK_K / 16 + 3*QK_K/4, "wrong fp6_K block size/padding");
+
+typedef struct {
+    uint32_t ql[QK_K/8];      // quants, lower 4 bits
+} block_fp6_k_ql;
+
+typedef struct {
+    uint32_t qh[QK_K/16];     // quants, upper 2 bits
+} block_fp6_k_qh;
+
+typedef struct {
+    int8_t scales[QK_K/16];  // scales, quantized with 8 bits, 16
+} block_fp6_k_scales;
+
+typedef struct {
+    uint32_t ql[QKFP6_K/8];     // upper 2 bits, 2
+} block_base_fp6_k_ql;
+
+typedef struct {
+    uint32_t qh[QKFP6_K/16];     // upper 2 bits, 1
+} block_base_fp6_k_qh;
+
+#define NGRID_IQ1S 2048
+#define IQ1S_DELTA 0.125f
+#define IQ1M_DELTA 0.125f
+
+typedef struct {
+    sycl::half d;
+    uint8_t  qs[QK_K/8];
+    uint16_t qh[QK_K/32];
+} block_iq1_s;
+
+// 1.8125 bpw
+typedef struct {
+    uint8_t  qs[QK_K/8];      // grid index, low 8 bits
+    uint8_t  qh[QK_K/16];     // grid index, high 3 bits + grid shift bit (for two groups of 8)
+    uint8_t  scales[QK_K/32]; // 4-bit block scales
+} block_iq1_m;
+
+typedef struct {
+    uint8_t ql[QKFP6/2];      // lower 4 bits, 32
+    uint8_t qh[QKFP6/4];      // upper 2 bits, 16
+    sycl::half  d;            // delta
+} block_fp6;
+
+typedef struct {
+    uint32_t qh[QKFP6/16];     // upper 2 bits, 4
+} block_fp6_32_qh;
+
+typedef struct {
+    uint32_t ql[QKFP6/8];      // lower 4 bits, 8
+} block_fp6_32_ql;
+
+enum ggml_type {
+    GGML_TYPE_Q4_0 = 2,
+    GGML_TYPE_Q4_1 = 3,
+    GGML_TYPE_Q5_0 = 6,
+    GGML_TYPE_Q5_1 = 7,
+    GGML_TYPE_Q8_0 = 8,
+    GGML_TYPE_Q8_1 = 9,
+    GGML_TYPE_NF4 = 10,
+    GGML_TYPE_NF3 = 11,
+    GGML_TYPE_FP8E4 = 15,
+    GGML_TYPE_FP4 = 16,
+    GGML_TYPE_FP8E5 = 19,
+    GGML_TYPE_IQ2_XXS = 21,
+    GGML_TYPE_IQ2_XS = 22,
+    GGML_TYPE_Q2_K = 23,
+    GGML_TYPE_IQ1_S = 24,
+    GGML_TYPE_IQ1_M = 25,
+    GGML_TYPE_Q6_K = 26,
+    GGML_TYPE_Q4_K = 27,
+    GGML_TYPE_Q5_K = 28,
+    GGML_TYPE_FP6 = 29,
+    GGML_TYPE_FP6_K = 30,
+    GGML_TYPE_Q4_0_WOQ = 34,
+    GGML_TYPE_COUNT
+};
+
+static const int GGML_BLCK_SIZE[GGML_TYPE_COUNT] = {
+    [GGML_TYPE_Q4_0] = QK4_0,
+    [GGML_TYPE_Q4_1] = QK4_1,
+    [GGML_TYPE_Q5_0] = QK5_0,
+    [GGML_TYPE_Q5_1] = QK5_1,
+    [GGML_TYPE_NF4]  = QK4_0,
+    [GGML_TYPE_NF3]  = QK4_0,
+    [GGML_TYPE_Q8_0] = QK8_0,
+    [GGML_TYPE_Q8_1] = QK8_1,
+    [GGML_TYPE_FP8E4]  = QKFP8,
+    [GGML_TYPE_FP4]  = QK4_0,
+    [GGML_TYPE_FP6]  = QKFP6,
+    [GGML_TYPE_FP8E5]  = QKFP8,
+    [GGML_TYPE_IQ2_XXS] = QK_K,
+    [GGML_TYPE_IQ2_XS] = QK_K,
+    [GGML_TYPE_Q2_K] = QK_K,
+    [GGML_TYPE_IQ1_S] = QK_K,
+    [GGML_TYPE_IQ1_M] = QK_K,
+    [GGML_TYPE_Q6_K] = QK_K,
+    [GGML_TYPE_Q4_K] = QK_K,
+    [GGML_TYPE_Q5_K] = QK_K,
+    [GGML_TYPE_FP6_K] = QK_K,
+    [GGML_TYPE_Q4_0_WOQ] = QK4_0,
+};
+
+static const size_t GGML_TYPE_SIZE[GGML_TYPE_COUNT] = {
+    [GGML_TYPE_Q4_0] = sizeof(block_q4_0),
+    [GGML_TYPE_Q4_1] = sizeof(block_q4_1),
+    [GGML_TYPE_Q5_0] = sizeof(block_q5_1),
+    [GGML_TYPE_Q5_1] = sizeof(block_q5_1),
+    [GGML_TYPE_NF4]  = sizeof(block_q4_0),
+    [GGML_TYPE_NF3]  = sizeof(block_nf3),
+    [GGML_TYPE_Q8_0] = sizeof(block_q8_0),
+    [GGML_TYPE_Q8_1] = sizeof(block_q8_1),
+    [GGML_TYPE_FP8E4]= sizeof(block_fp8),
+    [GGML_TYPE_FP4]  = sizeof(block_q4_0),
+    [GGML_TYPE_FP6]  = sizeof(block_fp6),
+    [GGML_TYPE_FP8E5]  = sizeof(block_fp8),
+    [GGML_TYPE_IQ2_XXS] = sizeof(block_iq2_xxs),
+    [GGML_TYPE_IQ2_XS] = sizeof(block_iq2_xs),
+    [GGML_TYPE_Q2_K] = sizeof(block_q2_K),
+    [GGML_TYPE_IQ1_S] = sizeof(block_iq1_s),
+    [GGML_TYPE_IQ1_M] = sizeof(block_iq1_m),
+    [GGML_TYPE_Q6_K] = sizeof(block_q6_K),
+    [GGML_TYPE_Q4_K] = sizeof(block_q4_K),
+    [GGML_TYPE_Q5_K] = sizeof(block_q5_K),
+    [GGML_TYPE_FP6_K] = sizeof(block_fp6_K),
+    [GGML_TYPE_Q4_0_WOQ] = sizeof(block_q4_0),
+};
diff --git a/csrc/xpu/dequantize.h b/csrc/xpu/dequantize.h
new file mode 100644
index 000000000..9a967312e
--- /dev/null
+++ b/csrc/xpu/dequantize.h
@@ -0,0 +1,74 @@
+#include <dpct/dpct.hpp>
+#include <sycl/sycl.hpp>
+#include "utils.h"
+/*
+Adapted from https://github.com/mit-han-lab/llm-awq
+Modified from NVIDIA FasterTransformer:
+https://github.com/NVIDIA/FasterTransformer/blob/main/src/fastertransformer/cutlass_extensions/include/cutlass_extensions/interleaved_numeric_conversion.h
+@article{lin2023awq,
+  title={AWQ: Activation-aware Weight Quantization for LLM Compression and
+Acceleration}, author={Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang,
+Shang and Dang, Xingyu and Han, Song}, journal={arXiv}, year={2023}
+}
+*/
+
+#pragma once
+
+namespace vllm {
+namespace awq {
+
+sycl::uint4 dequantize_s4_to_fp16x2(uint32_t const& source) {
+  sycl::uint4 result;
+
+  uint32_t* h = reinterpret_cast<uint32_t*>(&result);
+  uint32_t const i4s = reinterpret_cast<uint32_t const&>(source);
+
+  // First, we extract the i4s and construct an intermediate fp16 number.
+  static constexpr uint32_t immLut = (0xf0 & 0xcc) | 0xaa;
+  static constexpr uint32_t BOTTOM_MASK = 0x000f000f;
+  static constexpr uint32_t TOP_MASK = 0x00f000f0;
+  static constexpr uint32_t I4s_TO_F16s_MAGIC_NUM = 0x64006400;
+
+  // Note that the entire sequence only requires 1 shift instruction. This is
+  // thanks to the register packing format and the fact that we force our
+  // integers to be unsigned, and account for this in the fp16 subtractions. In
+  // addition, I exploit the fact that sub and fma have the same throughput in
+  // order to convert elt_23 and elt_67 to fp16 without having to shift them to
+  // the bottom bits before hand.
+
+  // Shift right by 8 to now consider elt_45 and elt_67. Issue first to hide RAW
+  // dependency if we issue immediately before required.
+  const uint32_t top_i4s = i4s >> 8;
+  h[0] = (i4s & BOTTOM_MASK) | I4s_TO_F16s_MAGIC_NUM;
+  h[1] = (i4s & TOP_MASK) | I4s_TO_F16s_MAGIC_NUM;
+  h[2] = (top_i4s & BOTTOM_MASK) | I4s_TO_F16s_MAGIC_NUM;
+  h[3] = (top_i4s & TOP_MASK) | I4s_TO_F16s_MAGIC_NUM;
+
+  // This is the half2 {1032, 1032} represented as an integer.
+  // static constexpr uint32_t FP16_TOP_MAGIC_NUM = 0x64086408;
+  // Haotian: subtract {1024, 1024} instead, we do not need to map to [-8, 7]
+  static constexpr uint32_t FP16_TOP_MAGIC_NUM = 0x64006400;
+  // This is the half2 {1 / 16, 1 / 16} represented as an integer.
+  static constexpr uint32_t ONE_SIXTEENTH = 0x2c002c00;
+  // This is the half2 {-72, -72} represented as an integer.
+  // static constexpr uint32_t NEG_72 = 0xd480d480;
+  // Haotian: Let's use {-64, -64}.
+  static constexpr uint32_t NEG_64 = 0xd400d400;
+  *(sycl::half2*)(&h[0]) = sycl_half_sub2(
+      *(sycl::half2*)(&h[0]), *(sycl::half2*)(&FP16_TOP_MAGIC_NUM));
+  *(sycl::half2*)(&h[1]) = sycl_half_fma2(
+      *(sycl::half2*)(&h[1]),
+      *(sycl::half2*)(&ONE_SIXTEENTH),
+      *(sycl::half2*)(&NEG_64));
+  *(sycl::half2*)(&h[2]) = sycl_half_sub2(
+      *(sycl::half2*)(&h[2]), *(sycl::half2*)(&FP16_TOP_MAGIC_NUM));
+  *(sycl::half2*)(&h[3]) = sycl_half_fma2(
+      *(sycl::half2*)(&h[3]),
+      *(sycl::half2*)(&ONE_SIXTEENTH),
+      *(sycl::half2*)(&NEG_64));
+
+  return result;
+}
+
+} // namespace awq
+} // namespace vllm
\ No newline at end of file
diff --git a/csrc/xpu/dtype_float16.h b/csrc/xpu/dtype_float16.h
new file mode 100644
index 000000000..1b9c1f248
--- /dev/null
+++ b/csrc/xpu/dtype_float16.h
@@ -0,0 +1,458 @@
+/*
+ * Adapted from
+ * https://github.com/NVIDIA/FasterTransformer/blob/release/v5.3_tag/src/fastertransformer/kernels/decoder_masked_multihead_attention/decoder_masked_multihead_attention_template.hpp
+ * and
+ * https://github.com/NVIDIA/FasterTransformer/blob/release/v5.3_tag/src/fastertransformer/kernels/decoder_masked_multihead_attention_utils.h
+ * Copyright (c) 2023, The vLLM team.
+ * Copyright (c) 2020-2023, NVIDIA CORPORATION.  All rights reserved.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+#pragma once
+
+#include <dpct/dpct.hpp>
+#include <sycl/sycl.hpp>
+#include "attention_generic.h"
+#include "dtype_float32.h"
+#include "utils.h"
+
+#include <stdint.h>
+
+namespace vllm {
+
+// FP16 vector types for Q, K, V.
+template <>
+struct Vec<sycl::half, 1> {
+  using Type = sycl::half;
+};
+template <>
+struct Vec<sycl::half, 2> {
+  using Type = sycl::half2;
+};
+template <>
+struct Vec<sycl::half, 4> {
+  using Type = sycl::half4;
+};
+template <>
+struct Vec<sycl::half, 8> {
+  using Type = sycl::half8;
+};
+
+template <>
+struct FloatVec<sycl::half> {
+  using Type = float;
+};
+template <>
+struct FloatVec<sycl::half2> {
+  using Type = sycl::float2;
+};
+
+template <>
+struct FloatVec<sycl::half4> {
+  using Type = Float4_;
+};
+template <>
+struct FloatVec<sycl::half8> {
+  using Type = Float8_;
+};
+
+// Utility functions for type conversions.
+inline sycl::half2 h0_h0(sycl::half a) {
+  return sycl::half2{a, a};
+}
+
+inline float half_to_float(sycl::half h) {
+  return float(h);
+}
+
+inline sycl::float2 half2_to_float2(sycl::half2 v) {
+
+  return sycl::float2(half_to_float(v.x()), half_to_float(v.y()));
+}
+
+inline sycl::half float_to_half(float f) {
+  return sycl::half(f);
+}
+
+inline sycl::half2 float2_to_half2(sycl::float2 f) {
+  return sycl::half2{float_to_half(f.x()), float_to_half(f.y())};
+}
+
+// Vector addition.
+inline sycl::half add(sycl::half a, sycl::half b) {
+  return sycl_half_add(a,b);
+}
+
+inline sycl::half2 add(sycl::half2 a, sycl::half2 b) {
+  auto val = sycl_half_add2(a, b);
+  return (val);
+}
+
+inline sycl::half4 add(sycl::half4 a, sycl::half4 b) {
+  sycl::half4 c;
+  c.x() = add(a.x(), b.x());
+  c.y() = add(a.y(), b.y());
+  c.z() = add(a.z(), b.z());
+  c.w() = add(a.w(), b.w());
+  return c;
+}
+
+inline sycl::half8 add(sycl::half8 a, sycl::half8 b) {
+  sycl::half8 c;
+  c.s0() = add(a.s0(), b.s0());
+  c.s1() = add(a.s1(), b.s1());
+  c.s2() = add(a.s2(), b.s2());
+  c.s3() = add(a.s3(), b.s3());
+  c.s4() = add(a.s4(), b.s4());
+  c.s5() = add(a.s5(), b.s5());
+  c.s6() = add(a.s6(), b.s6());
+  c.s7() = add(a.s7(), b.s7());
+  return c;
+}
+
+inline sycl::float2 add(sycl::half2 a, sycl::float2 fb) {
+  sycl::float2 fa = half2_to_float2(a);
+  return add(fa, fb);
+}
+
+inline Float4_ add(sycl::half4 a, Float4_ fb) {
+  Float4_ fc;
+  fc.x = add(sycl::half2{a.x(), a.y()}, fb.x);
+  fc.y = add(sycl::half2{a.z(), a.w()}, fb.y);
+  return fc;
+}
+
+inline Float8_ add(sycl::half8 a, Float8_ fb) {
+  Float8_ fc;
+  fc.x = add(sycl::half2{a.s0(), a.s1()}, fb.x);
+  fc.y = add(sycl::half2{a.s2(), a.s3()}, fb.y);
+  fc.z = add(sycl::half2{a.s4(), a.s5()}, fb.z);
+  fc.w = add(sycl::half2{a.s6(), a.s7()}, fb.w);
+  return fc;
+}
+
+// Vector multiplication.
+template <>
+inline sycl::half mul(sycl::half a, sycl::half b) {
+  auto val = sycl_half_mul((a), (b));
+  return (val);
+}
+
+template <>
+inline sycl::half2 mul(sycl::half2 a, sycl::half2 b) {
+  auto val = sycl_half_mul2((a), (b));
+  return (val);
+}
+
+template <>
+inline sycl::half2 mul(sycl::half a, sycl::half2 b) {
+  return mul<sycl::half2, sycl::half2, sycl::half2>(h0_h0(a), b);
+}
+
+
+template <>
+inline sycl::half4 mul(sycl::half4 a, sycl::half4 b) {
+  sycl::half4 c;
+  c.x() = mul<sycl::half, sycl::half, sycl::half>(a.x(), b.x());
+  c.y() = mul<sycl::half, sycl::half, sycl::half>(a.y(), b.y());
+  c.z() = mul<sycl::half, sycl::half, sycl::half>(a.z(), b.z());
+  c.w() = mul<sycl::half, sycl::half, sycl::half>(a.w(), b.w());
+  return c;
+}
+
+template <>
+inline sycl::half4 mul(sycl::half a, sycl::half4 b) {
+  sycl::half4 c;
+  c.x() = mul<sycl::half, sycl::half, sycl::half>(a, b.x());
+  c.y() = mul<sycl::half, sycl::half, sycl::half>(a, b.y());
+  c.z() = mul<sycl::half, sycl::half, sycl::half>(a, b.z());
+  c.w() = mul<sycl::half, sycl::half, sycl::half>(a, b.w());
+  return c;
+}
+
+template <>
+inline sycl::half8 mul(sycl::half8 a, sycl::half8 b) {
+  sycl::half8 c;
+  c.s0() = mul<sycl::half, sycl::half, sycl::half>(a.s0(), b.s0());
+  c.s1() = mul<sycl::half, sycl::half, sycl::half>(a.s1(), b.s1());
+  c.s2() = mul<sycl::half, sycl::half, sycl::half>(a.s2(), b.s2());
+  c.s3() = mul<sycl::half, sycl::half, sycl::half>(a.s3(), b.s3());
+  c.s4() = mul<sycl::half, sycl::half, sycl::half>(a.s4(), b.s4());
+  c.s5() = mul<sycl::half, sycl::half, sycl::half>(a.s5(), b.s5());
+  c.s6() = mul<sycl::half, sycl::half, sycl::half>(a.s6(), b.s6());
+  c.s7() = mul<sycl::half, sycl::half, sycl::half>(a.s7(), b.s7());
+  return c;
+}
+
+template <>
+inline sycl::half8 mul(sycl::half a, sycl::half8 b) {
+  sycl::half8 c;
+  c.s0() = mul<sycl::half, sycl::half, sycl::half>(a, b.s0());
+  c.s1() = mul<sycl::half, sycl::half, sycl::half>(a, b.s1());
+  c.s2() = mul<sycl::half, sycl::half, sycl::half>(a, b.s2());
+  c.s3() = mul<sycl::half, sycl::half, sycl::half>(a, b.s3());
+  c.s4() = mul<sycl::half, sycl::half, sycl::half>(a, b.s4());
+  c.s5() = mul<sycl::half, sycl::half, sycl::half>(a, b.s5());
+  c.s6() = mul<sycl::half, sycl::half, sycl::half>(a, b.s6());
+  c.s7() = mul<sycl::half, sycl::half, sycl::half>(a, b.s7());
+  return c;
+}
+
+template <>
+inline float mul(sycl::half a, sycl::half b) {
+  float fa = half_to_float(a);
+  float fb = half_to_float(b);
+  return fa * fb;
+}
+
+template <>
+inline sycl::float2 mul(sycl::half2 a, sycl::half2 b) {
+  sycl::float2 fa = half2_to_float2(a);
+  sycl::float2 fb = half2_to_float2(b);
+  return mul<sycl::float2, sycl::float2, sycl::float2>(fa, fb);
+}
+
+template <>
+inline sycl::float2 mul(sycl::half a, sycl::half2 b) {
+  return mul<sycl::float2, sycl::half2, sycl::half2>(h0_h0(a), b);
+}
+
+template <>
+inline Float4_ mul(sycl::half4 a, sycl::half4 b) {
+  Float4_ fc;
+  fc.x = mul<sycl::float2, sycl::half2, sycl::half2>(
+      sycl::half2{a.x(), a.y()}, sycl::half2{b.x(), b.y()});
+  fc.y = mul<sycl::float2, sycl::half2, sycl::half2>(
+      sycl::half2{a.z(), a.w()}, sycl::half2{b.z(), b.w()});
+  return fc;
+}
+
+template <>
+inline Float4_ mul(sycl::half a, sycl::half4 b) {
+  sycl::half2 s = h0_h0(a);
+  Float4_ fc;
+
+  fc.x =
+      mul<sycl::float2, sycl::half2, sycl::half2>(s, sycl::half2{b.x(), b.y()});
+  fc.y =
+      mul<sycl::float2, sycl::half2, sycl::half2>(s, sycl::half2{b.z(), b.w()});
+  return fc;
+}
+
+template <>
+inline Float8_ mul(sycl::half8 a, sycl::half8 b) {
+  Float8_ fc;
+  fc.x = mul<sycl::float2, sycl::half2, sycl::half2>(
+      sycl::half2{a.s0(), a.s1()}, sycl::half2{b.s0(), b.s1()});
+  fc.y = mul<sycl::float2, sycl::half2, sycl::half2>(
+      sycl::half2{a.s2(), a.s3()}, sycl::half2{b.s2(), b.s3()});
+  fc.z = mul<sycl::float2, sycl::half2, sycl::half2>(
+      sycl::half2{a.s4(), a.s5()}, sycl::half2{b.s4(), b.s5()});
+  fc.w = mul<sycl::float2, sycl::half2, sycl::half2>(
+      sycl::half2{a.s6(), a.s7()}, sycl::half2{b.s6(), b.s7()});
+  return fc;
+}
+
+template <>
+inline Float8_ mul(sycl::half a, sycl::half8 b) {
+  sycl::half2 s = h0_h0(a);
+  Float8_ fc;
+  fc.x = mul<sycl::float2, sycl::half2, sycl::half2>(
+      s, sycl::half2{b.s0(), b.s1()});
+  fc.y = mul<sycl::float2, sycl::half2, sycl::half2>(
+      s, sycl::half2{b.s2(), b.s3()});
+  fc.z = mul<sycl::float2, sycl::half2, sycl::half2>(
+      s, sycl::half2{b.s4(), b.s5()});
+  fc.w = mul<sycl::float2, sycl::half2, sycl::half2>(
+      s, sycl::half2{b.s6(), b.s7()});
+  return fc;
+}
+
+// Vector fused multiply-add.
+inline sycl::half2 fma(sycl::half2 a, sycl::half2 b, sycl::half2 c) {
+  auto val = sycl_half_fma2((a), (b), (c));
+  return (val);
+}
+
+inline sycl::half2 fma(sycl::half a, sycl::half2 b, sycl::half2 c) {
+  return fma(h0_h0(a), b, c);
+}
+
+inline sycl::half4 fma(sycl::half4 a, sycl::half4 b, sycl::half4 c) {
+  sycl::half4 d;
+  d.x() = fma(a.x(), b.x(), c.x());
+  d.y() = fma(a.y(), b.y(), c.y());
+  d.z() = fma(a.z(), b.z(), c.z());
+  d.w() = fma(a.w(), b.w(), c.w());
+  return d;
+}
+
+inline sycl::half4 fma(sycl::half a, sycl::half4 b, sycl::half4 c) {
+  sycl::half4 s = sycl::half4{a, a, a, a};
+  return fma(s, b, c);
+}
+
+inline sycl::half8 fma(sycl::half8 a, sycl::half8 b, sycl::half8 c) {
+  sycl::half8 d;
+  d.s0() = fma(a.s0(), b.s0(), c.s0());
+  d.s1() = fma(a.s1(), b.s1(), c.s1());
+  d.s2() = fma(a.s2(), b.s2(), c.s2());
+  d.s3() = fma(a.s3(), b.s3(), c.s3());
+  d.s4() = fma(a.s4(), b.s4(), c.s4());
+  d.s5() = fma(a.s5(), b.s5(), c.s5());
+  d.s6() = fma(a.s6(), b.s6(), c.s6());
+  d.s7() = fma(a.s7(), b.s7(), c.s7());
+  return d;
+}
+
+inline sycl::half8 fma(sycl::half a, sycl::half8 b, sycl::half8 c) {
+  sycl::half8 d;
+  d.s0() = fma(a, b.s0(), c.s0());
+  d.s1() = fma(a, b.s1(), c.s1());
+  d.s2() = fma(a, b.s2(), c.s2());
+  d.s3() = fma(a, b.s3(), c.s3());
+  d.s4() = fma(a, b.s4(), c.s4());
+  d.s5() = fma(a, b.s5(), c.s5());
+  d.s6() = fma(a, b.s6(), c.s6());
+  d.s7() = fma(a, b.s7(), c.s7());
+  return d;
+}
+
+inline float fma(sycl::half a, sycl::half b, float fc) {
+  float fa = half_to_float(a);
+  float fb = half_to_float(b);
+  return sycl::fma(fa, fb, fc);
+}
+
+inline sycl::float2 fma(sycl::half2 a, sycl::half2 b, sycl::float2 fc) {
+  sycl::float2 fa = half2_to_float2(a);
+  sycl::float2 fb = half2_to_float2(b);
+  return fma(fa, fb, fc);
+}
+
+inline sycl::float2 fma(sycl::half a, sycl::half2 b, sycl::float2 fc) {
+  return fma(h0_h0(a), b, fc);
+}
+
+inline Float4_ fma(sycl::half4 a, sycl::half4 b, Float4_ fc) {
+  Float4_ fd;
+  fd.x = fma(sycl::half2{a.x(), a.y()}, sycl::half2{b.x(), b.y()}, fc.x);
+  fd.y = fma(sycl::half2{a.z(), a.w()}, sycl::half2{b.z(), b.w()}, fc.y);
+  return fd;
+}
+
+inline Float4_ fma(sycl::half a, sycl::half4 b, Float4_ fc) {
+  sycl::half4 s = sycl::half4{a, a, a, a};
+
+  return fma(s, b, fc);
+}
+
+inline Float8_ fma(sycl::half8 a, sycl::half8 b, Float8_ fc) {
+  Float8_ fd;
+  fd.x = fma(sycl::half2{a.s0(), a.s1()}, sycl::half2{b.s0(), b.s1()}, fc.x);
+  fd.y = fma(sycl::half2{a.s2(), a.s3()}, sycl::half2{b.s2(), b.s3()}, fc.y);
+  fd.z = fma(sycl::half2{a.s4(), a.s5()}, sycl::half2{b.s4(), b.s5()}, fc.z);
+  fd.w = fma(sycl::half2{a.s6(), a.s7()}, sycl::half2{b.s6(), b.s7()}, fc.w);
+  return fd;
+}
+
+inline Float8_ fma(sycl::half a, sycl::half8 b, Float8_ fc) {
+  sycl::half8 s = sycl::half8{a, a, a, a, a, a, a, a};
+
+  return fma(s, b, fc);
+}
+
+// Vector sum.
+template <>
+inline float sum(sycl::half v) {
+  return half_to_float(v);
+}
+
+template <>
+inline float sum(sycl::half2 v) {
+  sycl::float2 tmp = half2_to_float2(v);
+  return tmp.x() + tmp.y();
+}
+
+template <>
+inline float sum(sycl::half4 v) {
+  sycl::half2 c = add(sycl::half2{v.x(), v.y()}, sycl::half2{v.z(), v.w()});
+  return sum(c);
+}
+
+template <>
+inline float sum(sycl::half8 v) {
+  return add(
+      sum(sycl::half4{v.s0(), v.s1(), v.s2(), v.s3()}),
+      sum(sycl::half4{v.s4(), v.s5(), v.s6(), v.s7()}));
+}
+
+inline void from_float(sycl::half& dst, float src) {
+  dst = sycl::half(src);
+}
+
+inline void from_float(sycl::half2& dst, sycl::float2 src) {
+  dst = float2_to_half2(src);
+}
+
+inline void from_float(sycl::half4& dst, Float4_ src) {
+  sycl::half2 h0 = float2_to_half2(src.x);
+  sycl::half2 h1 = float2_to_half2(src.y);
+  dst.x() = h0.x();
+  dst.y() = h0.y();
+  dst.z() = h1.x();
+  dst.w() = h1.y();
+}
+
+inline void from_float(sycl::half8& dst, Float8_ src) {
+  dst.s0() = float2_to_half2(src.x).x();
+  dst.s1() = float2_to_half2(src.x).y();
+  dst.s2() = float2_to_half2(src.y).x();
+  dst.s3() = float2_to_half2(src.y).y();
+  dst.s4() = float2_to_half2(src.z).x();
+  dst.s5() = float2_to_half2(src.z).y();
+  dst.s6() = float2_to_half2(src.w).x();
+  dst.s7() = float2_to_half2(src.w).y();
+}
+
+// From float16 to float32.
+inline float to_float(sycl::half u) {
+  return half_to_float(u);
+}
+
+inline sycl::float2 to_float(sycl::half2 u) {
+  return half2_to_float2(u);
+}
+
+inline Float4_ to_float(sycl::half4 u) {
+  Float4_ tmp;
+  tmp.x = half2_to_float2(sycl::half2{u.x(), u.y()});
+  tmp.y = half2_to_float2(sycl::half2{u.z(), u.w()});
+  return tmp;
+}
+
+inline Float8_ to_float(sycl::half8 u) {
+  Float8_ tmp;
+  tmp.x = half2_to_float2(sycl::half2{u.s0(), u.s1()});
+  tmp.y = half2_to_float2(sycl::half2{u.s2(), u.s3()});
+  tmp.z = half2_to_float2(sycl::half2{u.s4(), u.s5()});
+  tmp.w = half2_to_float2(sycl::half2{u.s6(), u.s7()});
+  return tmp;
+}
+
+// Zero-out a variable.
+inline void zero(sycl::half& dst) {
+  dst = sycl::half(0);
+}
+
+} // namespace vllm
\ No newline at end of file
diff --git a/csrc/xpu/dtype_float32.h b/csrc/xpu/dtype_float32.h
new file mode 100644
index 000000000..7b70e4efc
--- /dev/null
+++ b/csrc/xpu/dtype_float32.h
@@ -0,0 +1,268 @@
+/*
+ * Adapted from https://github.com/NVIDIA/FasterTransformer/blob/release/v5.3_tag/src/fastertransformer/kernels/decoder_masked_multihead_attention/decoder_masked_multihead_attention_template.hpp
+ * and https://github.com/NVIDIA/FasterTransformer/blob/release/v5.3_tag/src/fastertransformer/kernels/decoder_masked_multihead_attention_utils.h
+ * Copyright (c) 2023, The vLLM team.
+ * Copyright (c) 2020-2023, NVIDIA CORPORATION.  All rights reserved.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+#pragma once
+
+#include <sycl/sycl.hpp>
+#include <dpct/dpct.hpp>
+#include "attention_generic.h"
+
+#include <stdint.h>
+
+namespace vllm {
+
+// Define custom FP32 vector data types.
+struct Float4_ {
+  sycl::float2 x;
+  sycl::float2 y;
+};
+
+struct Float8_ {
+  sycl::float2 x;
+  sycl::float2 y;
+  sycl::float2 z;
+  sycl::float2 w;
+};
+
+// FP32 vector types for Q, K, V.
+template<>
+struct Vec<float, 1> {
+  using Type = float;
+};
+template<>
+struct Vec<float, 2> {
+  using Type = sycl::float2;
+};
+template<>
+struct Vec<float, 4> {
+  using Type = sycl::float4;
+};
+
+// FP32 accumulator vector types corresponding to Vec.
+template<>
+struct FloatVec<float> {
+  using Type = float;
+};
+template <> struct FloatVec<sycl::float2> {
+  using Type = sycl::float2;
+};
+template <> struct FloatVec<sycl::float4> {
+  using Type = sycl::float4;
+};
+
+// Vector addition.
+inline float add(float a, float b) {
+  return a + b;
+}
+
+inline sycl::float2 add(sycl::float2 a, sycl::float2 b) {
+  sycl::float2 c;
+  c.x() = add(a.x(), b.x());
+  c.y() = add(a.y(), b.y());
+  return c;
+}
+
+inline sycl::float4 add(sycl::float4 a, sycl::float4 b) {
+  sycl::float4 c;
+  c.x() = add(a.x(), b.x());
+  c.y() = add(a.y(), b.y());
+  c.z() = add(a.z(), b.z());
+  c.w() = add(a.w(), b.w());
+  return c;
+}
+
+// Vector multiplication.
+template<>
+inline float mul<float, float>(float a, float b) {
+  return a * b;
+}
+
+template <> inline sycl::float2 mul(sycl::float2 a, sycl::float2 b) {
+  sycl::float2 c;
+  c.x() = a.x() * b.x();
+  c.y() = a.y() * b.y();
+  return c;
+}
+
+template <> inline sycl::float2 mul(float a, sycl::float2 b) {
+  sycl::float2 c;
+  c.x() = a * b.x();
+  c.y() = a * b.y();
+  return c;
+}
+
+template <> inline sycl::float4 mul(sycl::float4 a, sycl::float4 b) {
+  sycl::float4 c;
+  c.x() = a.x() * b.x();
+  c.y() = a.y() * b.y();
+  c.z() = a.z() * b.z();
+  c.w() = a.w() * b.w();
+  return c;
+}
+
+template <> inline sycl::float4 mul(float a, sycl::float4 b) {
+  sycl::float4 c;
+  c.x() = a * b.x();
+  c.y() = a * b.y();
+  c.z() = a * b.z();
+  c.w() = a * b.w();
+  return c;
+}
+
+// Vector fused multiply-add.
+inline float fma(float a, float b, float c) {
+  return a * b + c;
+}
+
+inline sycl::float2 fma(sycl::float2 a, sycl::float2 b, sycl::float2 c) {
+  sycl::float2 d;
+  d.x() = fma(a.x(), b.x(), c.x());
+  d.y() = fma(a.y(), b.y(), c.y());
+  return d;
+}
+
+inline sycl::float2 fma(float a, sycl::float2 b, sycl::float2 c) {
+  sycl::float2 d;
+  d.x() = fma(a, b.x(), c.x());
+  d.y() = fma(a, b.y(), c.y());
+  return d;
+}
+
+inline sycl::float4 fma(sycl::float4 a, sycl::float4 b, sycl::float4 c) {
+  sycl::float4 d;
+  d.x() = fma(a.x(), b.x(), c.x());
+  d.y() = fma(a.y(), b.y(), c.y());
+  d.z() = fma(a.z(), b.z(), c.z());
+  d.w() = fma(a.w(), b.w(), c.w());
+  return d;
+}
+
+inline sycl::float4 fma(float a, sycl::float4 b, sycl::float4 c) {
+  sycl::float4 d;
+  d.x() = fma(a, b.x(), c.x());
+  d.y() = fma(a, b.y(), c.y());
+  d.z() = fma(a, b.z(), c.z());
+  d.w() = fma(a, b.w(), c.w());
+  return d;
+}
+
+inline Float4_ fma(float a, Float4_ b, Float4_ c) {
+  Float4_ d;
+  d.x = fma(a, b.x, c.x);
+  d.y = fma(a, b.y, c.y);
+  return d;
+}
+
+inline Float8_ fma(float a, Float8_ b, Float8_ c) {
+  Float8_ d;
+  d.x = fma(a, b.x, c.x);
+  d.y = fma(a, b.y, c.y);
+  d.z = fma(a, b.z, c.z);
+  d.w = fma(a, b.w, c.w);
+  return d;
+}
+
+// Vector sum.
+template<>
+inline float sum(float v) {
+  return v;
+}
+
+template <> inline float sum(sycl::float2 v) {
+  return v.x() + v.y();
+}
+
+template <> inline float sum(sycl::float4 v) {
+  return v.x() + v.y() + v.z() + v.w();
+}
+
+template<>
+inline float sum(Float4_ v) {
+  return v.x.x() + v.x.y() + v.y.x() + v.y.y();
+}
+
+template<>
+inline float sum(Float8_ v) {
+  return v.x.x() + v.x.y() + v.y.x() + v.y.y() + v.z.x() + v.z.y() + v.w.x() +
+         v.w.y();
+}
+
+// Vector dot product.
+inline float dot(float a, float b) {
+  return a * b;
+}
+
+inline float dot(sycl::float2 a, sycl::float2 b) {
+  sycl::float2 c = mul<sycl::float2, sycl::float2, sycl::float2>(a, b);
+  return c.x() + c.y();
+}
+
+inline float dot(Float4_ a, Float4_ b) {
+  sycl::float2 acc = mul<sycl::float2, sycl::float2, sycl::float2>(a.x, b.x);
+  acc = fma(a.y, b.y, acc);
+  return acc.x() + acc.y();
+}
+
+inline float dot(Float8_ a, Float8_ b) {
+  sycl::float2 acc = mul<sycl::float2, sycl::float2, sycl::float2>(a.x, b.x);
+  acc = fma(a.y, b.y, acc);
+  acc = fma(a.z, b.z, acc);
+  acc = fma(a.w, b.w, acc);
+  return acc.x() + acc.y();
+}
+
+// From float to float.
+inline void from_float(float& dst, float src) {
+  dst = src;
+}
+
+inline void from_float(sycl::float2 &dst, sycl::float2 src) {
+  dst = src;
+}
+
+inline void from_float(sycl::float4 &dst, sycl::float4 src) {
+  dst = src;
+}
+
+// From float to float.
+inline float to_float(float u) {
+  return u;
+}
+
+inline sycl::float2 to_float(sycl::float2 u) {
+  return u;
+}
+
+inline sycl::float4 to_float(sycl::float4 u) {
+  return u;
+}
+
+inline Float4_ to_float(Float4_ u) {
+  return u;
+}
+
+inline Float8_ to_float(Float8_ u) {
+  return u;
+}
+
+// Zero-out a variable.
+inline void zero(float& dst) {
+  dst = 0.f;
+}
+
+} // namespace vllm
\ No newline at end of file
diff --git a/csrc/xpu/fused_moe.cpp b/csrc/xpu/fused_moe.cpp
new file mode 100644
index 000000000..3a39d0e13
--- /dev/null
+++ b/csrc/xpu/fused_moe.cpp
@@ -0,0 +1,269 @@
+#include "utils.h"
+#include "base.hpp"
+
+using ST = at::ScalarType;
+
+#include <sycl/sycl.hpp>
+#include "xpu_types.h"
+#include <torch/extension.h>
+
+template <typename T>
+__inline__ T silu_xpu(const T& x) {
+  // x * sigmoid(x)
+  return (T)(((float)x) / (1.0f + sycl::exp((float)-x)));
+}
+
+template <typename scalar_t>
+void silu_and_mul_kernel(
+    scalar_t* __restrict__ out, // [..., d]
+    const scalar_t* __restrict__ input, // [..., 2, d]
+    const int d,
+    const sycl::nd_item<3>& item_ct1) {
+  const int64_t token_idx = item_ct1.get_group(2);
+  for (int64_t idx = item_ct1.get_local_id(2); idx < d;
+       idx += item_ct1.get_local_range(2)) {
+    const scalar_t x = input[token_idx * 2 * d + idx];
+    const scalar_t y = input[token_idx * 2 * d + d + idx];
+    out[token_idx * d + idx] = silu_xpu(x) * y;
+  }
+}
+
+template <typename scalar_t>
+void call_silu_and_mul_kernel(
+    int num_tokens,
+    int d,
+    const scalar_t* __restrict__ input,
+    scalar_t* __restrict__ output) {
+  using sycl_t = vllm::xpu::SyclTypeTrait<scalar_t>::Type;
+  sycl::range<3> grid(1, 1, num_tokens);
+  sycl::range<3> block(1, 1, std::min(d, 1024));
+  auto& queue = vllm::xpu::vllmGetQueue();
+  queue.submit([&](sycl::handler& cgh) {
+    cgh.parallel_for(
+        sycl::nd_range<3>(grid * block, block), [=](sycl::nd_item<3> item_ct1) {
+          silu_and_mul_kernel<sycl_t>(
+              (sycl_t*)output, (const sycl_t*)input, d, item_ct1);
+        });
+  });
+}
+
+void _silu_and_mul(torch::Tensor& out, torch::Tensor& input) {
+  int num_tokens = input.numel() / input.size(-1);
+  int d = input.size(-1) / 2;
+
+  VLLM_XPU_DISPATCH_FLOATING_TYPES(
+      input.scalar_type(), "call_silu_and_mul_kernel", [&] {
+        call_silu_and_mul_kernel(
+            num_tokens,
+            d,
+            input.data_ptr<scalar_t>(),
+            out.data_ptr<scalar_t>());
+      });
+}
+
+template <typename IT, const int VS, const int GS, const int ES, const int QTYPE>
+static void moe_forward_kernel(
+    const void* input_ptr,
+    const int64_t* indexs,
+    const uint64_t* qweights,
+    void * output_ptr,
+    const int num_tokens,
+    const int state_size,
+    const int output_size,
+    at::Device device
+) {
+    static_assert(ES == 8 || ES == 16 || ES == 32);
+    assert(output_size % VS == 0);
+
+    const int nb = state_size / QK;
+    const int nsb = nb / SBS;
+
+    constexpr int BLOCK_SIZE = BLOCK_SIZES[QTYPE];
+    constexpr int SCALE_SIZE = SCALE_SIZES[QTYPE];
+
+    sycl::range<2> global_size(num_tokens, output_size / VS * GS);
+    sycl::range<2> local_size(1, GS);
+
+    auto cgf = [&](sycl::handler& handle) {
+        handle.parallel_for(
+            sycl::nd_range<2>(global_size, local_size),
+            [=](sycl::nd_item<2> item) SYCL_ESIMD_KERNEL {
+                slm_init<GS * VS * sizeof(float)>();
+
+                const int eid = item.get_global_id(0);
+                const int tid = item.get_local_id(1);
+                const int vid = item.get_group(1) * VS;
+
+                if (indexs[eid] >= 0) {
+                    const uint8_t* weight = (const uint8_t *)(qweights[indexs[eid]]);
+                    const uint8_t* scales = weight + (int64_t)output_size * nb * BLOCK_SIZE;
+                    const IT* input = static_cast<const IT *>(input_ptr) + eid * state_size;
+                    IT* output = static_cast<IT *>(output_ptr) + eid * output_size;
+
+                    const uint8_t * weight_base = weight + nb * BLOCK_SIZE * vid;
+                    const uint8_t * scale_base = scales + nb * SCALE_SIZE * vid;
+
+                    simd<IT, VS * ES> accvs{};
+
+                    for (int s = tid; s < nsb; s += GS) {
+                        simd<IT, SBS * QK> xvs = block_load<IT, SBS * QK>(input + s * SBS * QK);
+
+                        #pragma unroll
+                        for (int v = 0; v < VS; ++v) {
+                            simd<fp16, SBS * QK> yvs = load_qblocks<QTYPE>(
+                                weight_base + v * nb * BLOCK_SIZE + s * SBS * BLOCK_SIZE,
+                                scale_base + v * nb * SCALE_SIZE + s * SBS * SCALE_SIZE
+                            );
+
+                            #pragma unroll
+                            for (int i = 0; i < SBS * QK; i += ES) {
+                                accvs.template select<ES, 1>(v * ES) +=
+                                    xvs.template select<ES, 1>(i) *
+                                    yvs.template select<ES, 1>(i);
+                            }
+                        }
+                    }
+
+                    for (int b = nsb * SBS + tid; b < nb; b += GS) {
+                        simd<IT, QK> xv = block_load<IT, QK>(input + b * QK);
+
+                        #pragma unroll
+                        for (int v = 0; v < VS; ++v) {
+                            simd<fp16, QK> yv = load_qblock<QTYPE>(
+                                weight_base + v * nb * BLOCK_SIZE + b * BLOCK_SIZE,
+                                scale_base + v * nb * SCALE_SIZE + b * SCALE_SIZE
+                            );
+
+                            #pragma unroll
+                            for (int i = 0; i < QK; i += ES) {
+                                accvs.template select<ES, 1>(v * ES) +=
+                                    xv.template select<ES, 1>(i) *
+                                    yv.template select<ES, 1>(i);
+                            }
+                        }
+                    }
+
+                    simd<float, VS> accs;
+                    #pragma unroll
+                    for(int v = 0; v < VS; ++v) {
+                        accs[v] = sycl::ext::intel::esimd::detail::sum<float, IT, ES>(
+                            accvs.template select<ES, 1>(v * ES)
+                        );
+                    }
+
+                    slm_block_store<float, VS>(tid * VS * sizeof(float), accs);
+
+                    barrier();
+
+                    if (tid == 0) {
+                        #pragma unroll
+                        for (int i = 1; i < GS; ++i) {
+                            accs += slm_block_load<float, VS>(i * VS * sizeof(float));
+                        }
+
+                        block_store<IT, VS>(output + vid, accs);
+                    }
+                }
+
+                
+            }
+        );
+    };
+
+    utils::submit_kernel(cgf, device, "moe forward down kernel");
+}
+
+
+template <int QTYPE>
+static auto dispatch_moe_forward(ST scalar_t) {
+    switch (scalar_t) {
+        case ST::Float: return std::make_tuple(moe_forward_kernel<float, 4, 4, 16, QTYPE>);
+        case ST::Half: return std::make_tuple(moe_forward_kernel<fp16, 4, 4, 32, QTYPE>);
+        default: throw std::runtime_error("unsupported dtype, only fp32 and fp16 are supported");
+    }
+}
+
+
+torch::Tensor moe_forward(
+    torch::Tensor input,
+    torch::Tensor indexs,
+    torch::Tensor qweights_attr,
+    int64_t state_size,
+    int64_t output_size,
+    int64_t qtype
+) {
+    auto [func] = [&] () {
+        switch (qtype) {
+            case GGML_TYPE_Q4_0:
+                return dispatch_moe_forward<GGML_TYPE_Q4_0>(input.scalar_type());
+            case GGML_TYPE_Q4_0_WOQ:
+                return dispatch_moe_forward<GGML_TYPE_Q4_0_WOQ>(input.scalar_type());
+            case GGML_TYPE_FP8E5:
+                return dispatch_moe_forward<GGML_TYPE_FP8E5>(input.scalar_type());
+            default: throw std::runtime_error("unsupported qtype: " + std::to_string(qtype));
+        }
+    } ();
+
+    int64_t num_tokens = indexs.numel();
+
+    torch::Tensor output = torch::zeros({num_tokens, output_size},
+                                    torch::device(input.device()).dtype(input.dtype()));
+
+    func(
+        input.data_ptr(), indexs.data_ptr<int64_t>(),
+        qweights_attr.data_ptr<uint64_t>(), output.data_ptr(),
+        num_tokens, state_size, output_size, input.device()
+    );
+
+    return output;
+}
+
+
+torch::Tensor fused_moe_forward(
+    torch::Tensor input,
+    torch::Tensor indexs,
+    torch::Tensor qweights1_attr,
+    torch::Tensor qweights2_attr,
+    int64_t hidden_size,
+    int64_t intermediate_size,
+    int64_t qtype
+) {
+    auto [gmm_func] = [&] () {
+        switch (qtype) {
+            case GGML_TYPE_Q4_0:
+                return dispatch_moe_forward<GGML_TYPE_Q4_0>(input.scalar_type());
+            case GGML_TYPE_Q4_0_WOQ:
+                return dispatch_moe_forward<GGML_TYPE_Q4_0_WOQ>(input.scalar_type());
+            case GGML_TYPE_FP8E5:
+                return dispatch_moe_forward<GGML_TYPE_FP8E5>(input.scalar_type());
+            default: throw std::runtime_error("unsupported qtype: " + std::to_string(qtype));
+        }
+    } ();
+
+    int64_t num_tokens = indexs.numel();
+
+    torch::Tensor w1_output = torch::zeros({num_tokens, intermediate_size * 2},
+                                    torch::device(input.device()).dtype(input.dtype()));
+    
+    torch::Tensor tmp = torch::zeros({num_tokens, intermediate_size},
+                                    torch::device(input.device()).dtype(input.dtype()));
+    
+    torch::Tensor w2_output = torch::zeros({num_tokens, hidden_size},
+                                    torch::device(input.device()).dtype(input.dtype()));
+
+    gmm_func(
+        input.data_ptr(), indexs.data_ptr<int64_t>(),
+        qweights1_attr.data_ptr<uint64_t>(), w1_output.data_ptr(),
+        num_tokens, hidden_size, intermediate_size * 2, input.device()
+    );
+
+    _silu_and_mul(tmp, w1_output);
+
+    gmm_func(
+        tmp.data_ptr(), indexs.data_ptr<int64_t>(),
+        qweights2_attr.data_ptr<uint64_t>(), w2_output.data_ptr(),
+        num_tokens, intermediate_size, hidden_size, input.device()
+    );
+
+    return w2_output;
+}
diff --git a/csrc/xpu/gemm_kernels_xpu.cpp b/csrc/xpu/gemm_kernels_xpu.cpp
new file mode 100644
index 000000000..d96aa5880
--- /dev/null
+++ b/csrc/xpu/gemm_kernels_xpu.cpp
@@ -0,0 +1,125 @@
+/*
+Adapted from https://github.com/mit-han-lab/llm-awq
+@article{lin2023awq,
+  title={AWQ: Activation-aware Weight Quantization for LLM Compression and
+Acceleration}, author={Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang,
+Shang and Dang, Xingyu and Han, Song}, journal={arXiv}, year={2023}
+}
+ */
+
+#include <dpct/dpct.hpp>
+#include <sycl/sycl.hpp>
+#include <torch/extension.h>
+//#include <c10/cuda/CUDAGuard.h>
+#include "dequantize.h"
+#include "utils.h"
+#include "xpu_types.h"
+
+void awq_dequantize_impl(
+    int* __restrict__ input,
+    sycl::half* __restrict__ scaling_factors,
+    int* __restrict__ zeros,
+    sycl::half* __restrict__ output,
+    int G,
+    sycl::nd_item<3> item_ct1) {
+  int j_factors1 = 4;
+  int row_stride2 = 4;
+  int split_k_iters = 1;
+  sycl::half2 ZERO_HALF2{0, 0};
+  sycl::half input_shared[8];
+
+  int N = item_ct1.get_local_range(2) * item_ct1.get_group_range(2);
+  int col = item_ct1.get_group(2) * item_ct1.get_local_range(2) +
+      item_ct1.get_local_id(2);
+  int row = item_ct1.get_group(1) * item_ct1.get_local_range(1) +
+      item_ct1.get_local_id(1);
+  int index1 = 8 * col + 8 * row * N;
+  sycl::half* output_ptr2 = output + index1;
+
+  int index2 = col + row * N;
+  int* input_ptr2 = input + index2;
+
+  int index3 = col + (int)(row / G) * N;
+  int* zeros_ptr2 = zeros + index3;
+  int index4 = 8 * col + (int)(row / G) * N * 8;
+  sycl::half* scale_loaded = scaling_factors + index4;
+
+  uint32_t zeros_loaded = *(uint32_t*)(zeros_ptr2);
+  sycl::uint4 zero_loaded_u4 = vllm::awq::dequantize_s4_to_fp16x2(zeros_loaded);
+  // sycl::uint4 scale_loaded_u4 = *(sycl::uint4*)(scaling_factors_ptr2);
+  // int j = 0;
+
+  uint32_t input_loaded = *(uint32_t*)(input_ptr2);
+  sycl::uint4 input_loaded_fp16 =
+      vllm::awq::dequantize_s4_to_fp16x2(input_loaded);
+
+  sycl::half2* input_loaded_h2 = (sycl::half2*)(&input_loaded_fp16);
+  sycl::half2* zero_loaded_h2 = (sycl::half2*)(&zero_loaded_u4);
+  sycl::half2* scale_loaded_h2 = (sycl::half2*)scale_loaded;
+  for (int i = 0; i < 4; i++) {
+    input_loaded_h2[i] = sycl_half_sub2(input_loaded_h2[i], zero_loaded_h2[i]);
+    input_loaded_h2[i] =
+        sycl_half_fma2(input_loaded_h2[i], scale_loaded_h2[i], ZERO_HALF2);
+  }
+  *(sycl::uint4*)(input_shared) = input_loaded_fp16;
+
+  for (int i = 0; i < 8; ++i) {
+    *(output_ptr2 + i) = input_shared[i];
+  }
+}
+
+torch::Tensor awq_dequantize(
+    torch::Tensor _kernel,
+    torch::Tensor _scaling_factors,
+    torch::Tensor _zeros,
+    int split_k_iters,
+    int thx,
+    int thy) {
+  int in_c = _kernel.size(0);
+  int qout_c = _kernel.size(1);
+  int out_c = qout_c * 8;
+  int G = in_c / _scaling_factors.size(0);
+
+  int x_thread = thx;
+  int y_thread = thy;
+
+  int x_blocks = 1;
+  int y_blocks = 1;
+  if (thx == 0) {
+    x_thread = qout_c;
+  }
+  if (thy == 0) {
+    y_thread = in_c;
+  }
+  if (thx == 0 && thy == 0) {
+    x_thread = 8;
+    y_thread = 8;
+    x_blocks = (int)(qout_c / 8);
+    y_blocks = (int)(in_c / 8);
+  }
+
+  auto options = torch::TensorOptions()
+                     .dtype(_scaling_factors.dtype())
+                     .device(_scaling_factors.device());
+  at::Tensor _de_kernel = torch::empty({in_c, out_c}, options);
+  auto kernel = reinterpret_cast<int*>(_kernel.data_ptr<int>());
+  auto de_kernel =
+      reinterpret_cast<sycl::half*>(_de_kernel.data_ptr<at::Half>());
+  auto scaling_factors =
+      reinterpret_cast<sycl::half*>(_scaling_factors.data_ptr<at::Half>());
+  auto zeros = reinterpret_cast<int*>(_zeros.data_ptr<int>());
+
+  sycl::range<3> num_blocks(1, y_blocks, x_blocks);
+  sycl::range<3> threads_per_block(1, y_thread, x_thread);
+  auto& queue = vllm::xpu::vllmGetQueue();
+
+  queue.submit([&](sycl::handler& cgh) {
+    cgh.parallel_for(
+        sycl::nd_range<3>(num_blocks * threads_per_block, threads_per_block),
+        [=](sycl::nd_item<3> item_ct1) {
+          awq_dequantize_impl(
+              kernel, scaling_factors, zeros, de_kernel, G, item_ct1);
+        });
+  });
+  return _de_kernel;
+}
\ No newline at end of file
diff --git a/csrc/xpu/kv.h b/csrc/xpu/kv.h
new file mode 100644
index 000000000..9616ad7ef
--- /dev/null
+++ b/csrc/xpu/kv.h
@@ -0,0 +1,76 @@
+#pragma once
+
+#include <torch/extension.h>
+#include <ext/intel/esimd.hpp>
+
+using fp16 = sycl::half;
+
+constexpr uint8_t FP16_EXP_OFFSET = 15;
+constexpr uint8_t K_EXP_OFFSET = 9;
+constexpr uint8_t V_EXP_OFFSET = 12;
+constexpr uint8_t K_OFFSET = (FP16_EXP_OFFSET - K_EXP_OFFSET) << 3;
+constexpr uint8_t V_OFFSET = (FP16_EXP_OFFSET - V_EXP_OFFSET) << 3;
+constexpr uint16_t K_MAX =
+    (uint16_t)0x3FC0 + ((uint16_t)(FP16_EXP_OFFSET - K_EXP_OFFSET) << 10);
+constexpr uint16_t K_MIN =
+    (uint16_t)0x0040 + ((uint16_t)(FP16_EXP_OFFSET - K_EXP_OFFSET) << 10);
+constexpr uint16_t V_MAX =
+    (uint16_t)0x3FC0 + ((uint16_t)(FP16_EXP_OFFSET - V_EXP_OFFSET) << 10);
+constexpr uint16_t V_MIN =
+    (uint16_t)0x0040 + ((uint16_t)(FP16_EXP_OFFSET - V_EXP_OFFSET) << 10);
+
+template <const int HD>
+ESIMD_INLINE __ESIMD_NS::simd<uint8_t, HD> quantize_key_row(
+    __ESIMD_NS::simd<fp16, HD> key_row) {
+  const __ESIMD_NS::simd<fp16, HD> kmax = sycl::bit_cast<fp16, uint16_t>(K_MAX);
+  const __ESIMD_NS::simd<fp16, HD> kmin = sycl::bit_cast<fp16, uint16_t>(K_MIN);
+  __ESIMD_NS::simd<fp16, HD> key =
+      __ESIMD_NS::max(__ESIMD_NS::min(__ESIMD_NS::abs(key_row), kmax), kmin);
+  key.template bit_cast_view<uint16_t>() <<= 1;
+  __ESIMD_NS::simd<uint8_t, HD> sign =
+      key_row.template bit_cast_view<uint8_t>().template select<HD, 2>(1) &
+      (uint8_t)0x80;
+  return (key.template bit_cast_view<uint8_t>().template select<HD, 2>(1) -
+          K_OFFSET) |
+         sign;
+}
+
+template <const int HD>
+ESIMD_INLINE __ESIMD_NS::simd<uint8_t, HD> quantize_value_row(
+    __ESIMD_NS::simd<fp16, HD> value_row) {
+  const __ESIMD_NS::simd<fp16, HD> vmax = sycl::bit_cast<fp16, uint16_t>(V_MAX);
+  const __ESIMD_NS::simd<fp16, HD> vmin = sycl::bit_cast<fp16, uint16_t>(V_MIN);
+  __ESIMD_NS::simd<fp16, HD> value =
+      __ESIMD_NS::max(__ESIMD_NS::min(__ESIMD_NS::abs(value_row), vmax), vmin);
+  value.template bit_cast_view<uint16_t>() <<= 1;
+  __ESIMD_NS::simd<uint8_t, HD> sign =
+      value_row.template bit_cast_view<uint8_t>().template select<HD, 2>(1) &
+      (uint8_t)0x80;
+  return (value.template bit_cast_view<uint8_t>().template select<HD, 2>(1) -
+          V_OFFSET) |
+         sign;
+}
+
+template <const int HD>
+ESIMD_INLINE __ESIMD_NS::simd<fp16, HD> dequantize_key_row(
+    const __ESIMD_NS::simd<uint8_t, HD>& key_row) {
+  __ESIMD_NS::simd<uint16_t, HD> result = 0x80;
+  result.template bit_cast_view<uint8_t>().template select<HD, 2>(1) =
+      (key_row & (uint8_t)0x7F) + K_OFFSET;
+  result >>= 1;
+  __ESIMD_NS::simd<uint8_t, HD> sign = key_row & (uint8_t)0x80;
+  result.template bit_cast_view<uint8_t>().template select<HD, 2>(1) |= sign;
+  return result.template bit_cast_view<fp16>();
+}
+
+template <const int HD>
+ESIMD_INLINE __ESIMD_NS::simd<fp16, HD> dequantize_value_row(
+    const __ESIMD_NS::simd<uint8_t, HD>& value_row) {
+  __ESIMD_NS::simd<uint16_t, HD> result = 0x80;
+  result.template bit_cast_view<uint8_t>().template select<HD, 2>(1) =
+      (value_row & (uint8_t)0x7F) + V_OFFSET;
+  result >>= 1;
+  __ESIMD_NS::simd<uint8_t, HD> sign = value_row & (uint8_t)0x80;
+  result.template bit_cast_view<uint8_t>().template select<HD, 2>(1) |= sign;
+  return result.template bit_cast_view<fp16>();
+}
\ No newline at end of file
diff --git a/csrc/xpu/layernorm_xpu.cpp b/csrc/xpu/layernorm_xpu.cpp
new file mode 100644
index 000000000..9a6a2af0a
--- /dev/null
+++ b/csrc/xpu/layernorm_xpu.cpp
@@ -0,0 +1,188 @@
+// clang-format off
+#ifdef VLLM_DEV
+#undef __SYCL_DEVICE_ONLY__
+#endif
+#include <sycl/sycl.hpp>
+#include <dpct/dpct.hpp>
+
+#include <torch/extension.h>
+#include <algorithm>
+#include "utils.h"
+#include "xpu_types.h"
+#include "reduction_utils.h"
+
+namespace vllm {
+
+template <typename scalar_t>
+void rms_norm_kernel(
+    scalar_t* __restrict__ out, // [..., hidden_size]
+    const scalar_t* __restrict__ input, // [..., hidden_size]
+    const scalar_t* __restrict__ weight, // [hidden_size]
+    const float epsilon,
+    const int num_tokens,
+    const int hidden_size,
+    const sycl::nd_item<3>& item_ct1,
+    float* s_variance,
+    float* shared_vals) {
+  float variance = 0.0f;
+
+  for (int idx = item_ct1.get_local_id(2); idx < hidden_size;
+       idx += item_ct1.get_local_range(2)) {
+    const float x = (float)input[item_ct1.get_group(2) * hidden_size + idx];
+    variance += x * x;
+  }
+
+  variance = blockReduceSum<float>(variance, item_ct1, shared_vals);
+  if (item_ct1.get_local_id(2) == 0) {
+    *s_variance = sycl::rsqrt(variance / hidden_size + epsilon);
+  }
+
+  // item_ct1.barrier();
+  item_ct1.barrier(sycl::access::fence_space::local_space);
+
+  for (int idx = item_ct1.get_local_id(2); idx < hidden_size;
+       idx += item_ct1.get_local_range(2)) {
+    float x = (float)input[item_ct1.get_group(2) * hidden_size + idx];
+    out[item_ct1.get_group(2) * hidden_size + idx] =
+        ((scalar_t)(x * (*s_variance))) * weight[idx];
+  }
+}
+
+template <typename scalar_t>
+void call_rms_norm_kernel(
+    torch::Tensor& out,
+    torch::Tensor& input,
+    torch::Tensor& weight,
+    float epsilon) {
+  using sycl_t = vllm::xpu::SyclTypeTrait<scalar_t>::Type;
+  int hidden_size = input.size(-1);
+  int num_tokens = input.numel() / hidden_size;
+  auto out_ptr = out.data_ptr<scalar_t>();
+  auto input_ptr = input.data_ptr<scalar_t>();
+  auto weight_ptr = weight.data_ptr<scalar_t>();
+  sycl::range<3> grid(1, 1, num_tokens);
+  sycl::range<3> block(1, 1, std::min(hidden_size, 1024));
+  auto& queue = vllm::xpu::vllmGetQueue();
+  queue.submit([&](sycl::handler& cgh) {
+    sycl::local_accessor<float, 1> shared_vals( sycl::range<1>(32), cgh);
+    sycl::local_accessor<float, 1> s_variance( sycl::range<1>(1), cgh);
+    cgh.parallel_for(
+        sycl::nd_range<3>(grid * block, block),
+        [=](sycl::nd_item<3> item_ct1) [[intel::reqd_sub_group_size(32)]] {
+          rms_norm_kernel<sycl_t>(
+              (sycl_t*)out_ptr,
+              (const sycl_t*)input_ptr,
+              (const sycl_t*)weight_ptr,
+              epsilon,
+              num_tokens,
+              hidden_size,
+              item_ct1,
+              s_variance.get_pointer(),
+              shared_vals.get_pointer());
+        });
+  });
+}
+
+
+template <typename scalar_t>
+void fused_add_rms_norm_kernel(
+    scalar_t* __restrict__ input,   // [..., hidden_size]
+    scalar_t* __restrict__ residual,        // [..., hidden_size]
+    const scalar_t* __restrict__ weight, // [hidden_size]
+    const float epsilon,
+    const int num_tokens,
+    const int hidden_size,
+    const sycl::nd_item<3>& item_ct1,
+    float* s_variance,
+    float* shared_vals) {
+  float variance = 0.0f;
+
+  for (int idx = item_ct1.get_local_id(2); idx < hidden_size;
+       idx += item_ct1.get_local_range(2)) {
+    float x = (float)input[item_ct1.get_group(2) * hidden_size + idx];
+    x+=(float)residual[item_ct1.get_group(2) * hidden_size + idx];
+    variance += x * x;
+    residual[item_ct1.get_group(2) * hidden_size + idx] = (scalar_t)x;
+  }
+
+  variance = blockReduceSum<float>(variance, item_ct1, shared_vals);
+  if (item_ct1.get_local_id(2) == 0) {
+    *s_variance = sycl::rsqrt(variance / hidden_size + epsilon);
+  }
+
+  // item_ct1.barrier();
+  item_ct1.barrier(sycl::access::fence_space::local_space);
+
+  for (int idx = item_ct1.get_local_id(2); idx < hidden_size;
+       idx += item_ct1.get_local_range(2)) {
+    float x = (float)residual[item_ct1.get_group(2) * hidden_size + idx];
+    input[item_ct1.get_group(2) * hidden_size + idx] =
+        ((scalar_t)(x * (*s_variance))) * weight[idx];
+  }
+}
+
+template <typename scalar_t>
+void call_fused_add_rms_norm_kernel(
+    torch::Tensor& input,
+    torch::Tensor& residual,
+    torch::Tensor& weight,
+    float epsilon){
+  using sycl_t = vllm::xpu::SyclTypeTrait<scalar_t>::Type;
+  int hidden_size = input.size(-1);
+  int num_tokens = input.numel() / hidden_size;
+  auto input_ptr = input.data_ptr<scalar_t>();
+  auto residual_ptr = residual.data_ptr<scalar_t>();
+  auto weight_ptr = weight.data_ptr<scalar_t>();
+  sycl::range<3> grid(1, 1, num_tokens);
+  sycl::range<3> block(1, 1, std::min(hidden_size, 1024));
+  auto& queue = vllm::xpu::vllmGetQueue();
+  queue.submit([&](sycl::handler& cgh) {
+    sycl::local_accessor<float, 1> shared_vals( sycl::range<1>(32), cgh);
+    sycl::local_accessor<float, 1> s_variance( sycl::range<1>(1), cgh);
+    cgh.parallel_for(
+        sycl::nd_range<3>(grid * block, block), [=](sycl::nd_item<3> item_ct1)[[intel::reqd_sub_group_size(32)]] {
+          fused_add_rms_norm_kernel<sycl_t>(
+              (sycl_t*)input_ptr,
+              (sycl_t*)residual_ptr,
+              (const sycl_t*)weight_ptr,
+              epsilon,
+              num_tokens,
+              hidden_size,
+              item_ct1,
+              s_variance.get_pointer(),
+              shared_vals.get_pointer());
+        });
+  });
+}
+
+} // namespace vllm
+
+void rms_norm(
+    torch::Tensor& out,
+    torch::Tensor& input,
+    torch::Tensor& weight,
+    float epsilon) {
+  VLLM_XPU_DISPATCH_FLOATING_TYPES(
+      input.scalar_type(), "call_rms_norm_kernel", [&] {
+        vllm::call_rms_norm_kernel<scalar_t>(out, input, weight, epsilon);
+      });
+}
+
+void fused_add_rms_norm(
+    torch::Tensor& input,
+    torch::Tensor& residual,
+    torch::Tensor& weight,
+    float epsilon) {
+  int hidden_size = input.size(-1);
+  int num_tokens = input.numel() / hidden_size;
+
+  VLLM_XPU_DISPATCH_FLOATING_TYPES(
+      input.scalar_type(), "call_fused_add_rms_norm_kernel", [&] {
+        vllm::call_fused_add_rms_norm_kernel<scalar_t>(
+            input,
+            residual,
+            weight,
+               epsilon);
+      });
+}
+
diff --git a/csrc/xpu/pos_encoding_xpu.cpp b/csrc/xpu/pos_encoding_xpu.cpp
new file mode 100644
index 000000000..3232cacbc
--- /dev/null
+++ b/csrc/xpu/pos_encoding_xpu.cpp
@@ -0,0 +1,333 @@
+// clang-format off
+#ifdef VLLM_DEV
+#undef __SYCL_DEVICE_ONLY__
+#endif
+#include <sycl/sycl.hpp>
+// clang-format on
+#include "xpu_types.h"
+
+#include <torch/extension.h>
+#include "utils.h"
+
+template <typename scalar_t, bool IS_NEOX>
+inline void apply_rotary_embedding(
+    scalar_t* __restrict__ arr,
+    const scalar_t* __restrict__ cos_ptr,
+    const scalar_t* __restrict__ sin_ptr,
+    int rot_offset,
+    int embed_dim) {
+  int x_index, y_index;
+  scalar_t cos, sin;
+  if (IS_NEOX) {
+    // GPT-NeoX style rotary embedding.
+    x_index = rot_offset;
+    y_index = embed_dim + rot_offset;
+    cos = VLLM_LDG(cos_ptr + x_index);
+    sin = VLLM_LDG(sin_ptr + x_index);
+  } else {
+    // GPT-J style rotary embedding.
+    x_index = 2 * rot_offset;
+    y_index = 2 * rot_offset + 1;
+    cos = VLLM_LDG(cos_ptr + x_index / 2);
+    sin = VLLM_LDG(sin_ptr + x_index / 2);
+  }
+
+  const scalar_t x = arr[x_index];
+  const scalar_t y = arr[y_index];
+  arr[x_index] = x * cos - y * sin;
+  arr[y_index] = y * cos + x * sin;
+}
+
+template <typename scalar_t, bool IS_NEOX>
+void rotary_embedding_kernel(
+    const int64_t* __restrict__ positions, // [batch_size, seq_len] or
+                                           // [num_tokens]
+    scalar_t* __restrict__ query, // [batch_size, seq_len, num_heads, head_size]
+                                  // or [num_tokens, num_heads, head_size]
+    scalar_t* __restrict__ key, // [batch_size, seq_len, num_kv_heads,
+                                // head_size] or [num_tokens, num_kv_heads,
+                                // head_size]
+    const scalar_t* __restrict__ cos_sin_cache, // [max_position, 2, rot_dim //
+                                                // 2]
+    const int rot_dim,
+    const int query_stride,
+    const int key_stride,
+    const int num_heads,
+    const int num_kv_heads,
+    const int head_size,
+    const sycl::nd_item<3>& item_ct1) {
+  // Each thread block is responsible for one token.
+  const int token_idx = item_ct1.get_group(2);
+  int64_t pos = positions[token_idx];
+  const scalar_t* cache_ptr = cos_sin_cache + pos * rot_dim;
+
+  const int embed_dim = rot_dim / 2;
+  const scalar_t* cos_ptr = cache_ptr;
+  const scalar_t* sin_ptr = cache_ptr + embed_dim;
+
+  const int nq = num_heads * embed_dim;
+  for (int i = item_ct1.get_local_id(2); i < nq;
+       i += item_ct1.get_local_range(2)) {
+    const int head_idx = i / embed_dim;
+    const int token_head = token_idx * query_stride + head_idx * head_size;
+    const int rot_offset = i % embed_dim;
+    apply_rotary_embedding<scalar_t, IS_NEOX>(
+        query + token_head, cos_ptr, sin_ptr, rot_offset, embed_dim);
+  }
+
+  const int nk = num_kv_heads * embed_dim;
+  for (int i = item_ct1.get_local_id(2); i < nk;
+       i += item_ct1.get_local_range(2)) {
+    const int head_idx = i / embed_dim;
+    const int token_head = token_idx * key_stride + head_idx * head_size;
+    const int rot_offset = i % embed_dim;
+    apply_rotary_embedding<scalar_t, IS_NEOX>(
+        key + token_head, cos_ptr, sin_ptr, rot_offset, embed_dim);
+  }
+}
+
+template <typename scalar_t, bool IS_NEOX>
+void batched_rotary_embedding_kernel(
+    const int64_t* __restrict__ positions, // [batch_size, seq_len] or
+                                           // [num_tokens]
+    scalar_t* __restrict__ query, // [batch_size, seq_len, num_heads, head_size]
+                                  // or [num_tokens, num_heads, head_size]
+    scalar_t* __restrict__ key, // [batch_size, seq_len, num_kv_heads,
+                                // head_size] or [num_tokens, num_kv_heads,
+                                // head_size]
+    const scalar_t* __restrict__ cos_sin_cache, // [max_position, 2, rot_dim //
+                                                // 2]
+    const int64_t* __restrict__ cos_sin_cache_offsets,  // [batch_size, seq_len] or [num_tokens]
+    const int rot_dim,
+    const int query_stride,
+    const int key_stride,
+    const int num_heads,
+    const int num_kv_heads,
+    const int head_size,
+    const sycl::nd_item<3>& item_ct1) {
+  // Each thread block is responsible for one token.
+  const int token_idx = item_ct1.get_group(2);
+  int64_t cos_sin_cache_offset = cos_sin_cache_offsets[token_idx];
+  int64_t pos = positions[token_idx];
+  const scalar_t* cache_ptr = cos_sin_cache + (cos_sin_cache_offset + pos) * rot_dim;
+
+  const int embed_dim = rot_dim / 2;
+  const scalar_t* cos_ptr = cache_ptr;
+  const scalar_t* sin_ptr = cache_ptr + embed_dim;
+
+  const int nq = num_heads * embed_dim;
+  for (int i = item_ct1.get_local_id(2); i < nq;
+       i += item_ct1.get_local_range(2)) {
+    const int head_idx = i / embed_dim;
+    const int token_head = token_idx * query_stride + head_idx * head_size;
+    const int rot_offset = i % embed_dim;
+    apply_rotary_embedding<scalar_t, IS_NEOX>(
+        query + token_head, cos_ptr, sin_ptr, rot_offset, embed_dim);
+  }
+
+  const int nk = num_kv_heads * embed_dim;
+  for (int i = item_ct1.get_local_id(2); i < nk;
+       i += item_ct1.get_local_range(2)) {
+    const int head_idx = i / embed_dim;
+    const int token_head = token_idx * key_stride + head_idx * head_size;
+    const int rot_offset = i % embed_dim;
+    apply_rotary_embedding<scalar_t, IS_NEOX>(
+        key + token_head, cos_ptr, sin_ptr, rot_offset, embed_dim);
+  }
+}
+
+template <typename scalar_t>
+void call_rotary_embedding_kernel(
+    const int64_t* __restrict__ positions, // [num_tokens]
+    scalar_t* __restrict__ query, // [num_tokens, num_heads, head_size]
+    scalar_t* __restrict__ key, // [num_tokens, num_kv_heads, head_size]
+    const scalar_t* __restrict__ cos_sin_cache, // [max_position, 2, rot_dim //
+                                                // 2]
+    const int rot_dim,
+    const int query_stride,
+    const int key_stride,
+    const int num_heads,
+    const int num_kv_heads,
+    const int head_size,
+    const int num_tokens,
+    const int sin_cos_dim,
+    bool is_neox) {
+  using sycl_t = vllm::xpu::SyclTypeTrait<scalar_t>::Type;
+  sycl::range<3> grid(1, 1, num_tokens);
+  sycl::range<3> block(1, 1, std::min(num_heads * rot_dim / 2, 512));
+  auto& queue = vllm::xpu::vllmGetQueue();
+  if (is_neox) {
+    queue.submit([&](sycl::handler& cgh) {
+      cgh.parallel_for(
+          sycl::nd_range<3>(grid * block, block),
+          [=](sycl::nd_item<3> item_ct1) {
+            rotary_embedding_kernel<sycl_t, true>(
+                positions,
+                (sycl_t* __restrict__)query,
+                (sycl_t* __restrict__)key,
+                (const sycl_t* __restrict__)cos_sin_cache,
+                rot_dim,
+                query_stride,
+                key_stride,
+                num_heads,
+                num_kv_heads,
+                head_size,
+                item_ct1);
+          });
+    });
+  } else {
+    queue.submit([&](sycl::handler& cgh) {
+      cgh.parallel_for(
+          sycl::nd_range<3>(grid * block, block),
+          [=](sycl::nd_item<3> item_ct1) {
+            rotary_embedding_kernel<sycl_t, false>(
+                positions,
+                (sycl_t* __restrict__)query,
+                (sycl_t* __restrict__)key,
+                (const sycl_t* __restrict__)cos_sin_cache,
+                rot_dim,
+                query_stride,
+                key_stride,
+                num_heads,
+                num_kv_heads,
+                head_size,
+                item_ct1);
+          });
+    });
+  }
+}
+
+template <typename scalar_t>
+void call_batched_rotary_embedding_kernel(
+    const int64_t* __restrict__ positions, // [num_tokens]
+    scalar_t* __restrict__ query, // [num_tokens, num_heads, head_size]
+    scalar_t* __restrict__ key, // [num_tokens, num_kv_heads, head_size]
+    const scalar_t* __restrict__ cos_sin_cache, // [max_position, 2, rot_dim //
+                                                // 2]
+    const int64_t* __restrict__ cos_sin_cache_offsets,  // [batch_size, seq_len] or [num_tokens]
+    const int rot_dim,
+    const int query_stride,
+    const int key_stride,
+    const int num_heads,
+    const int num_kv_heads,
+    const int head_size,
+    const int num_tokens,
+    const int sin_cos_dim,
+    bool is_neox) {
+  using sycl_t = vllm::xpu::SyclTypeTrait<scalar_t>::Type;
+  sycl::range<3> grid(1, 1, num_tokens);
+  sycl::range<3> block(1, 1, std::min(num_heads * rot_dim / 2, 512));
+  auto& queue = vllm::xpu::vllmGetQueue();
+  if (is_neox) {
+    queue.submit([&](sycl::handler& cgh) {
+      cgh.parallel_for(
+          sycl::nd_range<3>(grid * block, block),
+          [=](sycl::nd_item<3> item_ct1) {
+            batched_rotary_embedding_kernel<sycl_t, true>(
+                positions,
+                (sycl_t* __restrict__)query,
+                (sycl_t* __restrict__)key,
+                (const sycl_t* __restrict__)cos_sin_cache,
+                cos_sin_cache_offsets,
+                rot_dim,
+                query_stride,
+                key_stride,
+                num_heads,
+                num_kv_heads,
+                head_size,
+                item_ct1);
+          });
+    });
+  } else {
+    queue.submit([&](sycl::handler& cgh) {
+      cgh.parallel_for(
+          sycl::nd_range<3>(grid * block, block),
+          [=](sycl::nd_item<3> item_ct1) {
+            batched_rotary_embedding_kernel<sycl_t, false>(
+                positions,
+                (sycl_t* __restrict__)query,
+                (sycl_t* __restrict__)key,
+                (const sycl_t* __restrict__)cos_sin_cache,
+                cos_sin_cache_offsets,
+                rot_dim,
+                query_stride,
+                key_stride,
+                num_heads,
+                num_kv_heads,
+                head_size,
+                item_ct1);
+          });
+    });
+  }
+}
+
+void rotary_embedding(
+    torch::Tensor& positions,
+    torch::Tensor& query,
+    torch::Tensor& key,
+    int head_size,
+    torch::Tensor& cos_sin_cache,
+    bool is_neox) {
+
+  int num_tokens = query.numel() / query.size(-1);
+  int rot_dim = cos_sin_cache.size(1);
+  int num_heads = query.size(-1) / head_size;
+  int num_kv_heads = key.size(-1) / head_size;
+  int key_stride = key.stride(-2);
+  int query_stride = query.stride(-2);
+  int cos_sin_dim = cos_sin_cache.size(0);
+
+  VLLM_XPU_DISPATCH_FLOATING_TYPES(
+      query.scalar_type(), "call_rotary_embedding_kernel", [&] {
+        call_rotary_embedding_kernel<scalar_t>(
+            positions.data_ptr<int64_t>(),
+            query.data_ptr<scalar_t>(),
+            key.data_ptr<scalar_t>(),
+            cos_sin_cache.data_ptr<scalar_t>(),
+            rot_dim,
+            query_stride,
+            key_stride,
+            num_heads,
+            num_kv_heads,
+            head_size,
+            num_tokens,
+            cos_sin_dim,
+            is_neox);
+      });
+}
+
+void batched_rotary_embedding(
+  torch::Tensor& positions,
+  torch::Tensor& query,
+  torch::Tensor& key,
+  int head_size,
+  torch::Tensor& cos_sin_cache,
+  bool is_neox,
+  int rot_dim,
+  torch::Tensor& cos_sin_cache_offsets) {
+  int64_t num_tokens = cos_sin_cache_offsets.size(0);
+  int num_heads = query.size(-1) / head_size;
+  int num_kv_heads = key.size(-1) / head_size;
+  int key_stride = key.stride(-2);
+  int query_stride = query.stride(-2);
+  int cos_sin_dim = cos_sin_cache.size(0);
+
+  VLLM_XPU_DISPATCH_FLOATING_TYPES(
+    query.scalar_type(), "call_batched_rotary_embedding_kernel", [&] {
+      call_batched_rotary_embedding_kernel<scalar_t>(
+          positions.data_ptr<int64_t>(),
+          query.data_ptr<scalar_t>(),
+          key.data_ptr<scalar_t>(),
+          cos_sin_cache.data_ptr<scalar_t>(),
+          cos_sin_cache_offsets.data_ptr<int64_t>(),
+          rot_dim,
+          query_stride,
+          key_stride,
+          num_heads,
+          num_kv_heads,
+          head_size,
+          num_tokens,
+          cos_sin_dim,
+          is_neox);
+    });
+}
\ No newline at end of file
diff --git a/csrc/xpu/pybind.cpp b/csrc/xpu/pybind.cpp
new file mode 100644
index 000000000..bf9e94612
--- /dev/null
+++ b/csrc/xpu/pybind.cpp
@@ -0,0 +1,112 @@
+// #include "cache.h"
+#include "xpu_ops.h"
+#include <torch/extension.h>
+
+PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
+  // vLLM custom ops
+  pybind11::module ops = m.def_submodule("ops", "vLLM custom operators");
+
+  // Attention ops
+  ops.def(
+    "paged_attention_v1",
+    &paged_attention_v1,
+    "Compute the attention between an input query and the cached keys/values using PagedAttention.");
+  ops.def(
+    "paged_attention_v2",
+    &paged_attention_v2,
+    "PagedAttention V2.");
+
+  ops.def("context_attention_forward_v1", &context_attention_forward_v1,
+          "Context attention forward_v1");
+
+  ops.def("context_attention_forward_v2", &context_attention_forward_v2,
+          "Context attention forward_v2");
+
+  ops.def(
+    "paged_attention_gqa",
+    &paged_attention_gqa,
+    "PagedAttention GQA.");
+
+  ops.def("paged_attention_gqa_fp8", &paged_attention_gqa_fp8, "PagedAttention GQA fp8.");
+
+  // Activation ops
+  ops.def(
+    "silu_and_mul",
+    &silu_and_mul,
+    "Activation function used in SwiGLU.");
+  ops.def(
+    "gelu_and_mul",
+    &gelu_and_mul,
+    "Activation function used in GeGLU with `none` approximation.");
+  ops.def(
+    "gelu_tanh_and_mul",
+    &gelu_tanh_and_mul,
+    "Activation function used in GeGLU with `tanh` approximation.");
+  ops.def(
+    "gelu_new",
+    &gelu_new,
+    "GELU implementation used in GPT-2.");
+  ops.def(
+    "gelu_fast",
+    &gelu_fast,
+    "Approximate GELU implementation.");
+
+  // Layernorm
+  ops.def(
+    "rms_norm",
+    &rms_norm,
+    "Apply Root Mean Square (RMS) Normalization to the input tensor.");
+
+  ops.def(
+    "fused_add_rms_norm",
+    &fused_add_rms_norm,
+    "In-place fused Add and RMS Normalization");
+
+  // Rotary embedding
+  ops.def(
+    "rotary_embedding",
+    &rotary_embedding,
+    "Apply GPT-NeoX or GPT-J style rotary embedding to query and key");
+
+  // Cache ops
+  pybind11::module cache_ops = m.def_submodule("cache_ops", "vLLM cache ops");
+  cache_ops.def(
+    "swap_blocks",
+    &swap_blocks,
+    "Swap in (out) the cache blocks from src to dst");
+  cache_ops.def(
+    "copy_blocks",
+    &copy_blocks,
+    "Copy the cache blocks from src to dst");
+  cache_ops.def(
+    "reshape_and_cache",
+    &reshape_and_cache,
+    "Reshape the key and value tensors and cache them");
+  cache_ops.def(
+    "reshape_and_cache_ipexllm",
+    &reshape_and_cache_ipexllm,
+    "Reshape the key and value tensors and cache them for ipex_llm");
+
+  cache_ops.def(
+    "reshape_and_cache_ipexllm_fp8",
+    &reshape_and_cache_ipexllm_fp8,
+    "Reshape the key and value tensors and cache them for ipex_llm with fp8");
+
+  // Quant
+  ops.def(
+    "awq_dequantize",
+    &awq_dequantize,
+    "dequant method for awq");
+
+
+  ops.def(
+    "moe_forward",
+    &moe_forward,
+    "PagedAttention GQA.");
+
+  ops.def(
+    "fused_moe_forward",
+    &fused_moe_forward,
+    "PagedAttention GQA.");
+
+}
diff --git a/csrc/xpu/reduction_utils.h b/csrc/xpu/reduction_utils.h
new file mode 100644
index 000000000..93c64d759
--- /dev/null
+++ b/csrc/xpu/reduction_utils.h
@@ -0,0 +1,56 @@
+/*
+ * Copyright (c) 2023, The vLLM team.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+#pragma once
+
+#include <dpct/dpct.hpp>
+#include <stdint.h>
+#include <sycl/sycl.hpp>
+
+namespace vllm {
+
+template <typename T>
+__inline__ T warpReduceSum(T val, const sycl::nd_item<3>& item_ct1) {
+#pragma unroll
+  for (int mask = 16; mask > 0; mask >>= 1)
+    val += dpct::permute_sub_group_by_xor(
+        item_ct1.get_sub_group(), val, mask, 32);
+  return val;
+}
+
+/* Calculate the sum of all elements in a block */
+template<typename T>
+__inline__ T blockReduceSum(T val, const sycl::nd_item<3> &item_ct1, T *shared) {
+
+  int lane = item_ct1.get_local_id(2) & 0x1f;
+  int wid = item_ct1.get_local_id(2) >> 5;
+
+  val = warpReduceSum<T>(val, item_ct1);
+
+  if (lane == 0) {
+    shared[wid] = val;
+  }
+  item_ct1.barrier();
+
+  // Modify from blockDim.x << 5 to blockDim.x / 32. to prevent
+  // blockDim.x is not divided by 32
+  val = (item_ct1.get_local_id(2) < (item_ct1.get_local_range(2) / 32.f))
+            ? shared[lane]
+            : (T)(0.0f);
+  val = warpReduceSum<T>(val, item_ct1);
+  return val;
+}
+
+} // namespace vllm
\ No newline at end of file
diff --git a/csrc/xpu/utils.cpp b/csrc/xpu/utils.cpp
new file mode 100644
index 000000000..5f613af55
--- /dev/null
+++ b/csrc/xpu/utils.cpp
@@ -0,0 +1,34 @@
+#include "utils.h"
+#include <sycl/ext/intel/math.hpp>
+
+sycl::half sycl_half_mul(sycl::half a, sycl::half b) {
+  return sycl::ext::intel::math::hmul(a, b);
+}
+sycl::half sycl_half_add(sycl::half a, sycl::half b) {
+  return sycl::ext::intel::math::hadd(a, b);
+}
+sycl::half sycl_half_sub(sycl::half a, sycl::half b) {
+  return sycl::ext::intel::math::hsub(a, b);
+}
+sycl::half sycl_half_fma(sycl::half a, sycl::half b, sycl::half c) {
+  return sycl::ext::intel::math::hfma(a, b, c);
+}
+
+sycl::half2 sycl_half_mul2(sycl::half2 a, sycl::half2 b) {
+  return sycl::ext::intel::math::hmul2(a, b);
+}
+sycl::half2 sycl_half_add2(sycl::half2 a, sycl::half2 b) {
+  return sycl::ext::intel::math::hadd2(a, b);
+}
+sycl::half2 sycl_half_sub2(sycl::half2 a, sycl::half2 b) {
+  return sycl::ext::intel::math::hsub2(a, b);
+}
+
+sycl::half2 sycl_half_fma2(sycl::half2 a, sycl::half2 b, sycl::half2 c) {
+  return sycl::ext::intel::math::hfma2(a, b, c);
+}
+
+int get_max_shared_memory_per_block_device_attribute(int device_id) {
+  const sycl::device& device = vllm::xpu::vllmGetQueue().get_device();
+  return device.get_info<sycl::info::device::local_mem_size>();
+}
diff --git a/csrc/xpu/utils.h b/csrc/xpu/utils.h
new file mode 100644
index 000000000..fa3ead51c
--- /dev/null
+++ b/csrc/xpu/utils.h
@@ -0,0 +1,82 @@
+#pragma once
+
+#include <sycl/sycl.hpp>
+#include <functional>
+#include <memory>
+// #include <ipex.h>
+#include <ATen/ATen.h>
+#include <torch/torch.h>
+
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+#include <c10/xpu/XPUStream.h>
+#endif
+
+
+#define VLLM_LDG(arg) *(arg)
+namespace vllm {
+namespace xpu {
+
+static inline sycl::queue& vllmGetQueue() {
+  auto device_type = c10::DeviceType::XPU;
+  c10::impl::VirtualGuardImpl impl(device_type);
+  c10::Stream c10_stream = impl.getStream(c10::Device(device_type));
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+  return at::xpu::XPUStream(c10_stream).queue();
+#else
+  return ::xpu::get_queue_from_stream(c10_stream);
+#endif
+}
+template <typename T>
+struct SyclTypeTrait{
+  using Type = T;
+};
+
+template <>
+struct SyclTypeTrait<c10::Half>{
+  using Type = sycl::half;
+};
+
+template <>
+struct SyclTypeTrait<c10::BFloat16>{
+  using Type = sycl::ext::oneapi::bfloat16;
+};
+
+
+} // namespace xpu
+
+} // namespace vllm
+
+SYCL_EXTERNAL sycl::half sycl_half_mul(sycl::half a, sycl::half b);
+SYCL_EXTERNAL sycl::half sycl_half_add(sycl::half a, sycl::half b);
+SYCL_EXTERNAL sycl::half sycl_half_sub(sycl::half a, sycl::half b);
+SYCL_EXTERNAL sycl::half sycl_half_fma(sycl::half a, sycl::half b, sycl::half c);
+
+SYCL_EXTERNAL sycl::half2 sycl_half_mul2(sycl::half2 a, sycl::half2 b);
+SYCL_EXTERNAL sycl::half2 sycl_half_add2(sycl::half2 a, sycl::half2 b);
+SYCL_EXTERNAL sycl::half2 sycl_half_sub2(sycl::half2 a, sycl::half2 b);
+SYCL_EXTERNAL sycl::half2 sycl_half_fma2(sycl::half2 a, sycl::half2 b, sycl::half2 c);
+
+int get_max_shared_memory_per_block_device_attribute(int device_id);
+
+namespace utils {
+    static inline sycl::queue& get_queue(const at::Device& device) {
+        c10::impl::VirtualGuardImpl impl(device.type());
+        c10::Stream c10_stream = impl.getStream(c10::Device(device));
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+        return at::xpu::XPUStream(c10_stream).queue();
+#else
+        return ::xpu::get_queue_from_stream(c10_stream);
+#endif
+    }
+
+    static inline sycl::event submit_kernel(std::function<void(sycl::handler&)> kernel, const at::Device& device, const char * desc) {
+        sycl::queue& queue = get_queue(device);
+        sycl::event event = queue.submit(kernel);
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+        // xpu::profiler_record(desc, event);
+#else
+        ::xpu::profiler_record(desc, event);
+#endif
+        return event;
+    }
+}
diff --git a/csrc/xpu/xpu_ops.h b/csrc/xpu/xpu_ops.h
new file mode 100644
index 000000000..603d4f23d
--- /dev/null
+++ b/csrc/xpu/xpu_ops.h
@@ -0,0 +1,194 @@
+#pragma once
+#include <torch/extension.h>
+
+void rotary_embedding(torch::Tensor &positions, torch::Tensor &query,
+                          torch::Tensor &key, int head_size,
+                          torch::Tensor &cos_sin_cache, bool is_neox);
+void batched_rotary_embedding(
+  torch::Tensor& positions,
+  torch::Tensor& query,
+  torch::Tensor& key,
+  int head_size,
+  torch::Tensor& cos_sin_cache,
+  bool is_neox,
+  int rot_dim,
+  torch::Tensor& cos_sin_cache_offsets);
+
+void silu_and_mul(torch::Tensor &out, torch::Tensor &input);
+void gelu_and_mul(torch::Tensor &out, torch::Tensor &input);
+
+void gelu_new(torch::Tensor &out, torch::Tensor &input);
+
+void gelu_fast(torch::Tensor &out, torch::Tensor &input);
+
+
+void gelu_tanh_and_mul(
+  torch::Tensor& out,
+  torch::Tensor& input);
+
+void paged_attention_v1(
+    torch::Tensor &out, torch::Tensor &query, torch::Tensor &key_cache,
+    torch::Tensor &value_cache, int num_kv_heads, float scale,
+    torch::Tensor &block_tables, torch::Tensor &context_lens, int block_size,
+    int max_context_len, const c10::optional<torch::Tensor> &alibi_slopes,
+    const std::string& kv_cache_dtype, const float kv_scale, const float attn_logit_softcapping);
+
+void paged_attention_v2(
+    torch::Tensor &out, torch::Tensor &exp_sums, torch::Tensor &max_logits,
+    torch::Tensor &tmp_out, torch::Tensor &query, torch::Tensor &key_cache,
+    torch::Tensor &value_cache, int num_kv_heads, float scale,
+    torch::Tensor &block_tables, torch::Tensor &context_lens, int block_size,
+    int max_context_len, const c10::optional<torch::Tensor> &alibi_slopes,
+    const std::string& kv_cache_dtype, const float kv_scale, const float attn_logit_softcapping);
+
+torch::Tensor context_attention_forward_v1(
+    torch::Tensor query,  // [num_tokens, num_kv_head, head_dim]
+    torch::Tensor key,    // [num_tokens, num_kv_heads * head_size]
+    torch::Tensor value,  // [num_tokens, num_kv_heads * head_size]
+    torch::Tensor block_tables, torch::Tensor query_start_loc,
+    torch::Tensor seq_lens, torch::Tensor context_lens, int max_input_length,
+    int max_context_length);
+
+torch::Tensor context_attention_forward_v2(
+    torch::Tensor query,  // [num_tokens, num_kv_head, head_dim]
+    torch::Tensor key,    // [num_tokens, num_kv_heads * head_size]
+    torch::Tensor value,  // [num_tokens, num_kv_heads * head_size]
+    torch::Tensor block_tables, torch::Tensor query_start_loc,
+    torch::Tensor seq_lens, torch::Tensor context_lens, int max_input_length,
+    int max_context_length, int max_q_length);
+
+void copy_blocks(
+    std::vector<torch::Tensor> &key_caches,
+    std::vector<torch::Tensor> &value_caches,
+    const std::map<int64_t, std::vector<int64_t>> &block_mapping);
+
+void reshape_and_cache(torch::Tensor &key, torch::Tensor &value,
+                           torch::Tensor &key_cache, torch::Tensor &value_cache,
+                           torch::Tensor &slot_mapping,
+                           const std::string& kv_cache_dtype, const float kv_scale);
+void reshape_and_cache_ipexllm(torch::Tensor &key, torch::Tensor &value,
+                           torch::Tensor &key_cache, torch::Tensor &value_cache,
+                           torch::Tensor &slot_mapping,
+                           const std::string& kv_cache_dtype, const float kv_scale);
+
+void reshape_and_cache_ipexllm_fp8(torch::Tensor& key, torch::Tensor& value,
+                                   torch::Tensor& key_cache,
+                                   torch::Tensor& value_cache,
+                                   torch::Tensor& slot_mapping,
+                                   const std::string& kv_cache_dtype,
+                                   const float kv_scale);
+
+void moe_align_block_size(
+  torch::Tensor topk_ids,
+  int num_experts,
+  int block_size,
+  torch::Tensor sorted_token_ids,
+  torch::Tensor experts_ids,
+  torch::Tensor num_tokens_post_pad) {
+  TORCH_CHECK(false, "moe_align_block_size is not supported on XPU.");
+}
+void swap_blocks(torch::Tensor &src, torch::Tensor &dst,
+                     const std::map<int64_t, int64_t> &block_mapping);
+
+void gather_cached_kv(torch::Tensor &key, torch::Tensor &value,
+                          torch::Tensor &key_cache, torch::Tensor &value_cache,
+                          torch::Tensor &slot_mapping);
+
+void convert_fp8_e5m2(torch::Tensor& src_cache, torch::Tensor& dst_cache) {
+  TORCH_CHECK(false, "Quantization is not supported on XPU.");
+}
+
+void rms_norm(torch::Tensor &out, torch::Tensor &input,
+                  torch::Tensor &weight, float epsilon);
+
+void fused_add_rms_norm(torch::Tensor &input, torch::Tensor &residual,
+                            torch::Tensor &weight, float epsilon);
+
+torch::Tensor awq_gemm(torch::Tensor _in_feats, torch::Tensor _kernel,
+                       torch::Tensor _scaling_factors, torch::Tensor _zeros,
+                       int split_k_iters) {
+  TORCH_CHECK(false, "awq_gemm is not supported on XPU.");                            
+}
+
+torch::Tensor marlin_gemm(
+    torch::Tensor& a, 
+    torch::Tensor& b_q_weight,
+    torch::Tensor& b_scales, 
+    torch::Tensor& workspace,
+    int64_t size_m, 
+    int64_t size_n, 
+    int64_t size_k) {
+  TORCH_CHECK(false, "marlin_gemm is not supported on XPU.");                            
+}
+
+torch::Tensor awq_dequantize(torch::Tensor _kernel, 
+    torch::Tensor _scaling_factors,
+    torch::Tensor _zeros,
+    int split_k_iters,
+    int thx,
+    int thy);
+
+void squeezellm_gemm(torch::Tensor vec, torch::Tensor mat,
+                         torch::Tensor mul, torch::Tensor lookup_table) {
+  TORCH_CHECK(false, "squeezellm_gemm is not supported on XPU.");
+}
+
+torch::Tensor gptq_gemm(
+  torch::Tensor a,
+  torch::Tensor b_q_weight,
+  torch::Tensor b_gptq_qzeros,
+  torch::Tensor b_gptq_scales,
+  torch::Tensor b_g_idx,
+  bool use_exllama,
+  int bit) {
+  TORCH_CHECK(false, "gptq_gemm is not supported on XPU.");
+}
+
+void gptq_shuffle(
+  torch::Tensor q_weight,
+  torch::Tensor q_perm,
+  int bit) {
+  TORCH_CHECK(false, "gptq_shuffle is not supported on XPU.");
+}
+
+void paged_attention_gqa(
+    torch::Tensor output,
+    torch::Tensor query,
+    torch::Tensor key_cache,
+    torch::Tensor value_cache,
+    int64_t bsz,
+    int64_t num_heads,
+    int64_t num_kv_heads,
+    float scale,
+    torch::Tensor& block_tables,
+    torch::Tensor& context_lens,
+    int block_size,
+    int64_t head_dim,
+    int max_seq_len
+);
+
+
+torch::Tensor moe_forward(
+    torch::Tensor input,
+    torch::Tensor indexs,
+    torch::Tensor qweights_attr,
+    int64_t state_size,
+    int64_t output_size,
+    int64_t qtype
+);
+
+torch::Tensor fused_moe_forward(
+    torch::Tensor input,
+    torch::Tensor indexs,
+    torch::Tensor qweights1_attr,
+    torch::Tensor qweights2_attr,
+    int64_t hidden_size,
+    int64_t intermediate_size,
+    int64_t qtype
+);
+void paged_attention_gqa_fp8(torch::Tensor output, torch::Tensor query,
+                         torch::Tensor key_cache, torch::Tensor value_cache,
+                         int64_t bsz, int64_t num_heads, int64_t num_kv_heads,
+                         float scale, torch::Tensor& block_tables,
+                         torch::Tensor& context_lens, int block_size,
+                         int64_t head_dim, int max_seq_len);
diff --git a/csrc/xpu/xpu_types.h b/csrc/xpu/xpu_types.h
new file mode 100644
index 000000000..23f5b805c
--- /dev/null
+++ b/csrc/xpu/xpu_types.h
@@ -0,0 +1,25 @@
+
+#ifndef XPU_TYPES_H
+#define XPU_TYPES_H
+
+#include <torch/extension.h>
+
+// FIXME: FP16 is not fully supported in Torch-CPU
+#define VLLM_XPU_DISPATCH_CASE_FLOATING_TYPES(...)     \
+  AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__) \
+  AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)  \
+  AT_DISPATCH_CASE(at::ScalarType::BFloat16, __VA_ARGS__)
+
+#define VLLM_XPU_DISPATCH_CASE_FLOATING_TYPES_FLOAT_ONLY(...) \
+  AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)        \
+  AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)
+
+#define VLLM_XPU_DISPATCH_FLOATING_TYPES(TYPE, NAME, ...) \
+  AT_DISPATCH_SWITCH(                                     \
+      TYPE, NAME, VLLM_XPU_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__))
+
+#define VLLM_XPU_DISPATCH_FLOATING_TYPES_FLOAT_ONLY(TYPE, NAME, ...) \
+  AT_DISPATCH_SWITCH(                                     \
+      TYPE, NAME, VLLM_XPU_DISPATCH_CASE_FLOATING_TYPES_FLOAT_ONLY(__VA_ARGS__))
+
+#endif
\ No newline at end of file
diff --git a/docker/Dockerfile.xpu b/docker/Dockerfile.xpu
index 681102b9d..7b0b1de2d 100644
--- a/docker/Dockerfile.xpu
+++ b/docker/Dockerfile.xpu
@@ -1,9 +1,14 @@
-# oneapi 2025.0.2 docker base image use rolling 2448 package. https://dgpu-docs.intel.com/releases/packages.html?release=Rolling+2448.13&os=Ubuntu+22.04, and we don't need install driver manually.
-FROM intel/deep-learning-essentials:2025.0.2-0-devel-ubuntu22.04 AS vllm-base
+FROM intel/deep-learning-essentials:2025.0.2-0-devel-ubuntu24.04 AS vllm-base
 
-RUN rm /etc/apt/sources.list.d/intel-graphics.list
+RUN wget -O- https://apt.repos.intel.com/intel-gpg-keys/GPG-PUB-KEY-INTEL-SW-PRODUCTS.PUB | gpg --dearmor | tee /usr/share/keyrings/oneapi-archive-keyring.gpg > /dev/null && \
+    echo "deb [signed-by=/usr/share/keyrings/oneapi-archive-keyring.gpg] https://apt.repos.intel.com/oneapi all main" | tee /etc/apt/sources.list.d/oneAPI.list && \
+    add-apt-repository -y ppa:kobuk-team/intel-graphics-testing
 
 RUN apt-get update -y && \
+    apt-get install -y software-properties-common && \
+    add-apt-repository ppa:deadsnakes/ppa && \
+    apt-get install -y python3.10 python3.10-distutils && \
+    curl -sS https://bootstrap.pypa.io/get-pip.py | python3.10 && \
     apt-get install -y --no-install-recommends --fix-missing \
     curl \
     ffmpeg \
@@ -14,11 +19,17 @@ RUN apt-get update -y && \
     libgl1 \
     lsb-release \
     numactl \
-    python3 \
-    python3-dev \
-    python3-pip \
+    python3.10-dev \
     wget
 
+RUN update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.10 1
+
+RUN apt install -y libze1 libze-dev libze-intel-gpu1 intel-opencl-icd libze-intel-gpu-raytracing
+RUN wget https://github.com/uxlfoundation/oneCCL/releases/download/2021.15.3/intel-oneccl-2021.15.3.11_offline.sh
+RUN bash intel-oneccl-2021.15.3.11_offline.sh -a --silent --eula accept && echo "source /opt/intel/oneapi/setvars.sh --force" >> /root/.bashrc
+SHELL ["bash", "-c"]
+CMD ["bash", "-c", "source /root/.bashrc && exec bash"]
+
 WORKDIR /workspace/vllm
 COPY requirements/xpu.txt /workspace/vllm/requirements/xpu.txt
 COPY requirements/common.txt /workspace/vllm/requirements/common.txt
@@ -35,6 +46,7 @@ RUN --mount=type=bind,source=.git,target=.git \
     if [ "$GIT_REPO_CHECK" != 0 ]; then bash tools/check_repo.sh; fi
 
 ENV VLLM_TARGET_DEVICE=xpu
+ENV VLLM_WORKER_MULTIPROC_METHOD=spawn
 
 RUN --mount=type=cache,target=/root/.cache/pip \
     --mount=type=bind,source=.git,target=.git \
@@ -48,6 +60,12 @@ FROM vllm-base AS vllm-openai
 RUN --mount=type=cache,target=/root/.cache/pip \
     pip install accelerate hf_transfer 'modelscope!=1.15.0'
 
+RUN --mount=type=cache,target=/root/.cache/pip \
+    pip uninstall pytorch-triton-xpu triton -y
+
+RUN --mount=type=cache,target=/root/.cache/pip \
+    pip install https://ubit-artifactory-ba.intel.com/artifactory/aipc_releases-ba-local/gpu-new/validation/IPEX/adhoc/vllm/triton-3.3.0+gitbd88137b-cp310-cp310-linux_x86_64.whl
+
 ENV VLLM_USAGE_SOURCE production-docker-image \
     TRITON_XPU_PROFILE 1
 # install development dependencies (for testing)
diff --git a/docs/features/quantization/fp8.md b/docs/features/quantization/fp8.md
index 01d5d9da0..361fdda10 100644
--- a/docs/features/quantization/fp8.md
+++ b/docs/features/quantization/fp8.md
@@ -133,4 +133,4 @@ print(result[0].outputs[0].text)
 ```
 
 !!! warning
-    Currently, we load the model at original precision before quantizing down to 8-bits, so you need enough memory to load the whole model.
+    Currently, by default we load the model at original precision before quantizing down to 8-bits, so you need enough memory to load the whole model. To avoid this, adding `VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT=1` can allow offloading weights to cpu before quantization and quantized weights will be kept in device.
diff --git a/docs/models/supported_models.md b/docs/models/supported_models.md
index 7594c6e6f..6d9d4dea0 100644
--- a/docs/models/supported_models.md
+++ b/docs/models/supported_models.md
@@ -297,79 +297,103 @@ See [this page][generative-models] for more information on how to use generative
 
 #### Text Generation
 
-Specified using `--task generate`.
-
-| Architecture                                      | Models                                              | Example HF Models                                                                                                                                                            | [LoRA][lora-adapter]   | [PP][distributed-serving]   |
-|---------------------------------------------------|-----------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------|-----------------------------|
-| `AquilaForCausalLM`                               | Aquila, Aquila2                                     | `BAAI/Aquila-7B`, `BAAI/AquilaChat-7B`, etc.                                                                                                                                 | ✅︎                     | ✅︎                          |
-| `ArcticForCausalLM`                               | Arctic                                              | `Snowflake/snowflake-arctic-base`, `Snowflake/snowflake-arctic-instruct`, etc.                                                                                               | ✅︎                     |                             |
-| `BaiChuanForCausalLM`                             | Baichuan2, Baichuan                                 | `baichuan-inc/Baichuan2-13B-Chat`, `baichuan-inc/Baichuan-7B`, etc.                                                                                                          | ✅︎                     | ✅︎                          |
-| `BambaForCausalLM`                                | Bamba                                               | `ibm-ai-platform/Bamba-9B-fp8`, `ibm-ai-platform/Bamba-9B`                                                                                                                   |                        |                             |
-| `BloomForCausalLM`                                | BLOOM, BLOOMZ, BLOOMChat                            | `bigscience/bloom`, `bigscience/bloomz`, etc.                                                                                                                                | ✅︎                     |                             |
-| `BartForConditionalGeneration`                    | BART                                                | `facebook/bart-base`, `facebook/bart-large-cnn`, etc.                                                                                                                        |                        |                             |
-| `ChatGLMModel`, `ChatGLMForConditionalGeneration` | ChatGLM                                             | `THUDM/chatglm2-6b`, `THUDM/chatglm3-6b`, `ShieldLM-6B-chatglm3`, etc.                                                                                                       | ✅︎                     | ✅︎                          |
-| `CohereForCausalLM`, `Cohere2ForCausalLM`         | Command-R                                           | `CohereForAI/c4ai-command-r-v01`, `CohereForAI/c4ai-command-r7b-12-2024`, etc.                                                                                               | ✅︎                     | ✅︎                          |
-| `DbrxForCausalLM`                                 | DBRX                                                | `databricks/dbrx-base`, `databricks/dbrx-instruct`, etc.                                                                                                                     | ✅︎                     |                             |
-| `DeciLMForCausalLM`                               | DeciLM                                              | `nvidia/Llama-3_3-Nemotron-Super-49B-v1`, etc.                                                                                                                               | ✅︎                     |                             |
-| `DeepseekForCausalLM`                             | DeepSeek                                            | `deepseek-ai/deepseek-llm-67b-base`, `deepseek-ai/deepseek-llm-7b-chat` etc.                                                                                                 | ✅︎                     |                             |
-| `DeepseekV2ForCausalLM`                           | DeepSeek-V2                                         | `deepseek-ai/DeepSeek-V2`, `deepseek-ai/DeepSeek-V2-Chat` etc.                                                                                                               | ✅︎                     |                             |
-| `DeepseekV3ForCausalLM`                           | DeepSeek-V3                                         | `deepseek-ai/DeepSeek-V3-Base`, `deepseek-ai/DeepSeek-V3` etc.                                                                                                               | ✅︎                     |                             |
-| `ExaoneForCausalLM`                               | EXAONE-3                                            | `LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct`, etc.                                                                                                                                 | ✅︎                     | ✅︎                          |
-| `FalconForCausalLM`                               | Falcon                                              | `tiiuae/falcon-7b`, `tiiuae/falcon-40b`, `tiiuae/falcon-rw-7b`, etc.                                                                                                         | ✅︎                     |                             |
-| `FalconMambaForCausalLM`                          | FalconMamba                                         | `tiiuae/falcon-mamba-7b`, `tiiuae/falcon-mamba-7b-instruct`, etc.                                                                                                            | ✅︎                     | ✅︎                          |
-| `FalconH1ForCausalLM`                             | Falcon-H1                                           | `tiiuae/Falcon-H1-34B-Base`, `tiiuae/Falcon-H1-34B-Instruct`, etc.                                                                                                           | ✅︎                     | ✅︎                          |
-| `GemmaForCausalLM`                                | Gemma                                               | `google/gemma-2b`, `google/gemma-1.1-2b-it`, etc.                                                                                                                            | ✅︎                     | ✅︎                          |
-| `Gemma2ForCausalLM`                               | Gemma 2                                             | `google/gemma-2-9b`, `google/gemma-2-27b`, etc.                                                                                                                              | ✅︎                     | ✅︎                          |
-| `Gemma3ForCausalLM`                               | Gemma 3                                             | `google/gemma-3-1b-it`, etc.                                                                                                                                                 | ✅︎                     | ✅︎                          |
-| `GlmForCausalLM`                                  | GLM-4                                               | `THUDM/glm-4-9b-chat-hf`, etc.                                                                                                                                               | ✅︎                     | ✅︎                          |
-| `Glm4ForCausalLM`                                 | GLM-4-0414                                          | `THUDM/GLM-4-32B-0414`, etc.                                                                                                                                                 | ✅︎                     | ✅︎                          |
-| `GPT2LMHeadModel`                                 | GPT-2                                               | `gpt2`, `gpt2-xl`, etc.                                                                                                                                                      | ✅︎                     |                             |
-| `GPTBigCodeForCausalLM`                           | StarCoder, SantaCoder, WizardCoder                  | `bigcode/starcoder`, `bigcode/gpt_bigcode-santacoder`, `WizardLM/WizardCoder-15B-V1.0`, etc.                                                                                 | ✅︎                     | ✅︎                          |
-| `GPTJForCausalLM`                                 | GPT-J                                               | `EleutherAI/gpt-j-6b`, `nomic-ai/gpt4all-j`, etc.                                                                                                                            | ✅︎                     |                             |
-| `GPTNeoXForCausalLM`                              | GPT-NeoX, Pythia, OpenAssistant, Dolly V2, StableLM | `EleutherAI/gpt-neox-20b`, `EleutherAI/pythia-12b`, `OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5`, `databricks/dolly-v2-12b`, `stabilityai/stablelm-tuned-alpha-7b`, etc. | ✅︎                     |                             |
-| `GraniteForCausalLM`                              | Granite 3.0, Granite 3.1, PowerLM                   | `ibm-granite/granite-3.0-2b-base`, `ibm-granite/granite-3.1-8b-instruct`, `ibm/PowerLM-3b`, etc.                                                                             | ✅︎                     | ✅︎                          |
-| `GraniteMoeForCausalLM`                           | Granite 3.0 MoE, PowerMoE                           | `ibm-granite/granite-3.0-1b-a400m-base`, `ibm-granite/granite-3.0-3b-a800m-instruct`, `ibm/PowerMoE-3b`, etc.                                                                | ✅︎                     | ✅︎                          |
-| `GraniteMoeHybridForCausalLM`                     | Granite 4.0 MoE Hybrid                              | `ibm-granite/granite-4.0-tiny-preview`, etc.                                                                                                                                 | ✅︎                     | ✅︎                          |
-| `GraniteMoeSharedForCausalLM`                     | Granite MoE Shared                                  | `ibm-research/moe-7b-1b-active-shared-experts` (test model)                                                                                                                  | ✅︎                     | ✅︎                          |
-| `GritLM`                                          | GritLM                                              | `parasail-ai/GritLM-7B-vllm`.                                                                                                                                                | ✅︎                     | ✅︎                          |
-| `Grok1ModelForCausalLM`                           | Grok1                                               | `hpcai-tech/grok-1`.                                                                                                                                                         | ✅︎                     | ✅︎                          |
-| `InternLMForCausalLM`                             | InternLM                                            | `internlm/internlm-7b`, `internlm/internlm-chat-7b`, etc.                                                                                                                    | ✅︎                     | ✅︎                          |
-| `InternLM2ForCausalLM`                            | InternLM2                                           | `internlm/internlm2-7b`, `internlm/internlm2-chat-7b`, etc.                                                                                                                  | ✅︎                     | ✅︎                          |
-| `InternLM3ForCausalLM`                            | InternLM3                                           | `internlm/internlm3-8b-instruct`, etc.                                                                                                                                       | ✅︎                     | ✅︎                          |
-| `JAISLMHeadModel`                                 | Jais                                                | `inceptionai/jais-13b`, `inceptionai/jais-13b-chat`, `inceptionai/jais-30b-v3`, `inceptionai/jais-30b-chat-v3`, etc.                                                         | ✅︎                     |                             |
-| `JambaForCausalLM`                                | Jamba                                               | `ai21labs/AI21-Jamba-1.5-Large`, `ai21labs/AI21-Jamba-1.5-Mini`, `ai21labs/Jamba-v0.1`, etc.                                                                                 | ✅︎                     | ✅︎                          |
-| `LlamaForCausalLM`                                | Llama 3.1, Llama 3, Llama 2, LLaMA, Yi              | `meta-llama/Meta-Llama-3.1-405B-Instruct`, `meta-llama/Meta-Llama-3.1-70B`, `meta-llama/Meta-Llama-3-70B-Instruct`, `meta-llama/Llama-2-70b-hf`, `01-ai/Yi-34B`, etc.        | ✅︎                     | ✅︎                          |
-| `MambaForCausalLM`                                | Mamba                                               | `state-spaces/mamba-130m-hf`, `state-spaces/mamba-790m-hf`, `state-spaces/mamba-2.8b-hf`, etc.                                                                               | ✅︎                     |                             |
-| `MiniCPMForCausalLM`                              | MiniCPM                                             | `openbmb/MiniCPM-2B-sft-bf16`, `openbmb/MiniCPM-2B-dpo-bf16`, `openbmb/MiniCPM-S-1B-sft`, etc.                                                                               | ✅︎                     | ✅︎                          |
-| `MiniCPM3ForCausalLM`                             | MiniCPM3                                            | `openbmb/MiniCPM3-4B`, etc.                                                                                                                                                  | ✅︎                     | ✅︎                          |
-| `MistralForCausalLM`                              | Mistral, Mistral-Instruct                           | `mistralai/Mistral-7B-v0.1`, `mistralai/Mistral-7B-Instruct-v0.1`, etc.                                                                                                      | ✅︎                     | ✅︎                          |
-| `MixtralForCausalLM`                              | Mixtral-8x7B, Mixtral-8x7B-Instruct                 | `mistralai/Mixtral-8x7B-v0.1`, `mistralai/Mixtral-8x7B-Instruct-v0.1`, `mistral-community/Mixtral-8x22B-v0.1`, etc.                                                          | ✅︎                     | ✅︎                          |
-| `MPTForCausalLM`                                  | MPT, MPT-Instruct, MPT-Chat, MPT-StoryWriter        | `mosaicml/mpt-7b`, `mosaicml/mpt-7b-storywriter`, `mosaicml/mpt-30b`, etc.                                                                                                   | ✅︎                     |                             |
-| `NemotronForCausalLM`                             | Nemotron-3, Nemotron-4, Minitron                    | `nvidia/Minitron-8B-Base`, `mgoin/Nemotron-4-340B-Base-hf-FP8`, etc.                                                                                                         | ✅︎                     | ✅︎                          |
-| `OLMoForCausalLM`                                 | OLMo                                                | `allenai/OLMo-1B-hf`, `allenai/OLMo-7B-hf`, etc.                                                                                                                             | ✅︎                     |                             |
-| `OLMo2ForCausalLM`                                | OLMo2                                               | `allenai/OLMo-2-0425-1B`, etc.                                                                                                                                               | ✅︎                     |                             |
-| `OLMoEForCausalLM`                                | OLMoE                                               | `allenai/OLMoE-1B-7B-0924`, `allenai/OLMoE-1B-7B-0924-Instruct`, etc.                                                                                                        | ✅︎                     | ✅︎                          |
-| `OPTForCausalLM`                                  | OPT, OPT-IML                                        | `facebook/opt-66b`, `facebook/opt-iml-max-30b`, etc.                                                                                                                         | ✅︎                     |                             |
-| `OrionForCausalLM`                                | Orion                                               | `OrionStarAI/Orion-14B-Base`, `OrionStarAI/Orion-14B-Chat`, etc.                                                                                                             | ✅︎                     |                             |
-| `PhiForCausalLM`                                  | Phi                                                 | `microsoft/phi-1_5`, `microsoft/phi-2`, etc.                                                                                                                                 | ✅︎                     | ✅︎                          |
-| `Phi3ForCausalLM`                                 | Phi-4, Phi-3                                        | `microsoft/Phi-4-mini-instruct`, `microsoft/Phi-4`, `microsoft/Phi-3-mini-4k-instruct`, `microsoft/Phi-3-mini-128k-instruct`, `microsoft/Phi-3-medium-128k-instruct`, etc.   | ✅︎                     | ✅︎                          |
-| `Phi3SmallForCausalLM`                            | Phi-3-Small                                         | `microsoft/Phi-3-small-8k-instruct`, `microsoft/Phi-3-small-128k-instruct`, etc.                                                                                             | ✅︎                     |                             |
-| `PhiMoEForCausalLM`                               | Phi-3.5-MoE                                         | `microsoft/Phi-3.5-MoE-instruct`, etc.                                                                                                                                       | ✅︎                     | ✅︎                          |
-| `PersimmonForCausalLM`                            | Persimmon                                           | `adept/persimmon-8b-base`, `adept/persimmon-8b-chat`, etc.                                                                                                                   | ✅︎                     |                             |
-| `Plamo2ForCausalLM`                               | PLaMo2                                              | `pfnet/plamo-2-1b`, `pfnet/plamo-2-8b`, etc.                                                                                                                                 |                        |                             |
-| `QWenLMHeadModel`                                 | Qwen                                                | `Qwen/Qwen-7B`, `Qwen/Qwen-7B-Chat`, etc.                                                                                                                                    | ✅︎                     | ✅︎                          |
-| `Qwen2ForCausalLM`                                | QwQ, Qwen2                                          | `Qwen/QwQ-32B-Preview`, `Qwen/Qwen2-7B-Instruct`, `Qwen/Qwen2-7B`, etc.                                                                                                      | ✅︎                     | ✅︎                          |
-| `Qwen2MoeForCausalLM`                             | Qwen2MoE                                            | `Qwen/Qwen1.5-MoE-A2.7B`, `Qwen/Qwen1.5-MoE-A2.7B-Chat`, etc.                                                                                                                | ✅︎                     |                             |
-| `Qwen3ForCausalLM`                                | Qwen3                                               | `Qwen/Qwen3-8B`, etc.                                                                                                                                                        | ✅︎                     | ✅︎                          |
-| `Qwen3MoeForCausalLM`                             | Qwen3MoE                                            | `Qwen/Qwen3-30B-A3B`, etc.                                                                                                                                                   | ✅︎                     |                             |
-| `StableLmForCausalLM`                             | StableLM                                            | `stabilityai/stablelm-3b-4e1t`, `stabilityai/stablelm-base-alpha-7b-v2`, etc.                                                                                                | ✅︎                     |                             |
-| `Starcoder2ForCausalLM`                           | Starcoder2                                          | `bigcode/starcoder2-3b`, `bigcode/starcoder2-7b`, `bigcode/starcoder2-15b`, etc.                                                                                             | ✅︎                     |                             |
-| `SolarForCausalLM`                                | Solar Pro                                           | `upstage/solar-pro-preview-instruct`, etc.                                                                                                                                   | ✅︎                     | ✅︎                          |
-| `TeleChat2ForCausalLM`                            | TeleChat2                                           | `Tele-AI/TeleChat2-3B`, `Tele-AI/TeleChat2-7B`, `Tele-AI/TeleChat2-35B`, etc.                                                                                                | ✅︎                     | ✅︎                          |
-| `TeleFLMForCausalLM`                              | TeleFLM                                             | `CofeAI/FLM-2-52B-Instruct-2407`, `CofeAI/Tele-FLM`, etc.                                                                                                                    | ✅︎                     | ✅︎                          |
-| `XverseForCausalLM`                               | XVERSE                                              | `xverse/XVERSE-7B-Chat`, `xverse/XVERSE-13B-Chat`, `xverse/XVERSE-65B-Chat`, etc.                                                                                            | ✅︎                     | ✅︎                          |
-| `MiniMaxText01ForCausalLM`                        | MiniMax-Text                                        | `MiniMaxAI/MiniMax-Text-01`, etc.                                                                                                                                            | ✅︎                     |                             |
-| `Zamba2ForCausalLM`                               | Zamba2                                              | `Zyphra/Zamba2-7B-instruct`, `Zyphra/Zamba2-2.7B-instruct`, `Zyphra/Zamba2-1.2B-instruct`, etc.                                                                              |                        |                             |
+These models primarily accept the [`LLM.generate`](./generative_models.md#llmgenerate) API. Chat/Instruct models additionally support the [`LLM.chat`](./generative_models.md#llmchat) API.
+
+<style>
+th {
+  white-space: nowrap;
+  min-width: 0 !important;
+}
+</style>
+
+| Architecture | Models | Example HF Models | [LoRA](../features/lora.md) | [PP](../serving/parallelism_scaling.md) | [V1](gh-issue:8779) |
+|--------------|--------|-------------------|----------------------|---------------------------|---------------------|
+| `AquilaForCausalLM` | Aquila, Aquila2 | `BAAI/Aquila-7B`, `BAAI/AquilaChat-7B`, etc. | ✅︎ | ✅︎ | ✅︎ |
+| `ArceeForCausalLM` | Arcee (AFM) | `arcee-ai/AFM-4.5B-Base`, etc. | ✅︎ | ✅︎ | ✅︎ |
+| `ArcticForCausalLM` | Arctic | `Snowflake/snowflake-arctic-base`, `Snowflake/snowflake-arctic-instruct`, etc. | | ✅︎ | ✅︎ |
+| `BaiChuanForCausalLM` | Baichuan2, Baichuan | `baichuan-inc/Baichuan2-13B-Chat`, `baichuan-inc/Baichuan-7B`, etc. | ✅︎ | ✅︎ | ✅︎ |
+| `BailingMoeForCausalLM` | Ling | `inclusionAI/Ling-lite-1.5`, `inclusionAI/Ling-plus`, etc. | ✅︎ | ✅︎ | ✅︎ |
+| `BambaForCausalLM` | Bamba | `ibm-ai-platform/Bamba-9B-fp8`, `ibm-ai-platform/Bamba-9B` | ✅︎ | ✅︎ | ✅︎ |
+| `BloomForCausalLM` | BLOOM, BLOOMZ, BLOOMChat | `bigscience/bloom`, `bigscience/bloomz`, etc. | | ✅︎ | |
+| `BartForConditionalGeneration` | BART | `facebook/bart-base`, `facebook/bart-large-cnn`, etc. | | | |
+| `ChatGLMModel`, `ChatGLMForConditionalGeneration` | ChatGLM | `zai-org/chatglm2-6b`, `zai-org/chatglm3-6b`, `ShieldLM-6B-chatglm3`, etc. | ✅︎ | ✅︎ | ✅︎ |
+| `CohereForCausalLM`, `Cohere2ForCausalLM` | Command-R | `CohereForAI/c4ai-command-r-v01`, `CohereForAI/c4ai-command-r7b-12-2024`, etc. | ✅︎ | ✅︎ | ✅︎ |
+| `DbrxForCausalLM` | DBRX | `databricks/dbrx-base`, `databricks/dbrx-instruct`, etc. | | ✅︎ | ✅︎ |
+| `DeciLMForCausalLM` | DeciLM | `nvidia/Llama-3_3-Nemotron-Super-49B-v1`, etc. | ✅︎ | ✅︎ | ✅︎ |
+| `DeepseekForCausalLM` | DeepSeek | `deepseek-ai/deepseek-llm-67b-base`, `deepseek-ai/deepseek-llm-7b-chat`, etc. | | ✅︎ | ✅︎ |
+| `DeepseekV2ForCausalLM` | DeepSeek-V2 | `deepseek-ai/DeepSeek-V2`, `deepseek-ai/DeepSeek-V2-Chat`, etc. | | ✅︎ | ✅︎ |
+| `DeepseekV3ForCausalLM` | DeepSeek-V3 | `deepseek-ai/DeepSeek-V3-Base`, `deepseek-ai/DeepSeek-V3`, etc. | | ✅︎ | ✅︎ |
+| `Dots1ForCausalLM` | dots.llm1 | `rednote-hilab/dots.llm1.base`, `rednote-hilab/dots.llm1.inst`, etc. | | ✅︎ | ✅︎ |
+| `Ernie4_5ForCausalLM` | Ernie4.5 | `baidu/ERNIE-4.5-0.3B-PT`, etc. | ✅︎ | ✅︎ | ✅︎ |
+| `Ernie4_5_MoeForCausalLM` | Ernie4.5MoE | `baidu/ERNIE-4.5-21B-A3B-PT`, `baidu/ERNIE-4.5-300B-A47B-PT`, etc. |✅︎| ✅︎ | ✅︎ |
+| `ExaoneForCausalLM` | EXAONE-3 | `LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct`, etc. | ✅︎ | ✅︎ | ✅︎ |
+| `Exaone4ForCausalLM` | EXAONE-4 | `LGAI-EXAONE/EXAONE-4.0-32B`, etc. | ✅︎ | ✅︎ | ✅︎ |
+| `Fairseq2LlamaForCausalLM` | Llama (fairseq2 format) | `mgleize/fairseq2-dummy-Llama-3.2-1B`, etc. | ✅︎ | ✅︎ | ✅︎ |
+| `FalconForCausalLM` | Falcon | `tiiuae/falcon-7b`, `tiiuae/falcon-40b`, `tiiuae/falcon-rw-7b`, etc. | | ✅︎ | ✅︎ |
+| `FalconMambaForCausalLM` | FalconMamba | `tiiuae/falcon-mamba-7b`, `tiiuae/falcon-mamba-7b-instruct`, etc. | | ✅︎ | ✅︎ |
+| `FalconH1ForCausalLM` | Falcon-H1 | `tiiuae/Falcon-H1-34B-Base`, `tiiuae/Falcon-H1-34B-Instruct`, etc. | ✅︎ | ✅︎ | ✅︎ |
+| `GemmaForCausalLM` | Gemma | `google/gemma-2b`, `google/gemma-1.1-2b-it`, etc. | ✅︎ | ✅︎ | ✅︎ |
+| `Gemma2ForCausalLM` | Gemma 2 | `google/gemma-2-9b`, `google/gemma-2-27b`, etc. | ✅︎ | ✅︎ | ✅︎ |
+| `Gemma3ForCausalLM` | Gemma 3 | `google/gemma-3-1b-it`, etc. | ✅︎ | ✅︎ | ✅︎ |
+| `Gemma3nForConditionalGeneration` | Gemma 3n | `google/gemma-3n-E2B-it`, `google/gemma-3n-E4B-it`, etc. | | | ✅︎ |
+| `GlmForCausalLM` | GLM-4 | `zai-org/glm-4-9b-chat-hf`, etc. | ✅︎ | ✅︎ | ✅︎ |
+| `Glm4ForCausalLM` | GLM-4-0414 | `zai-org/GLM-4-32B-0414`, etc. | ✅︎ | ✅︎ | ✅︎ |
+| `Glm4MoeForCausalLM` | GLM-4.5 | `zai-org/GLM-4.5`, etc. | ✅︎ | ✅︎ | ✅︎ |
+| `GPT2LMHeadModel` | GPT-2 | `gpt2`, `gpt2-xl`, etc. | | ✅︎ | ✅︎ |
+| `GPTBigCodeForCausalLM` | StarCoder, SantaCoder, WizardCoder | `bigcode/starcoder`, `bigcode/gpt_bigcode-santacoder`, `WizardLM/WizardCoder-15B-V1.0`, etc. | ✅︎ | ✅︎ | ✅︎ |
+| `GPTJForCausalLM` | GPT-J | `EleutherAI/gpt-j-6b`, `nomic-ai/gpt4all-j`, etc. | | ✅︎ | ✅︎ |
+| `GPTNeoXForCausalLM` | GPT-NeoX, Pythia, OpenAssistant, Dolly V2, StableLM | `EleutherAI/gpt-neox-20b`, `EleutherAI/pythia-12b`, `OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5`, `databricks/dolly-v2-12b`, `stabilityai/stablelm-tuned-alpha-7b`, etc. | | ✅︎ | ✅︎ |
+| `GptOssForCausalLM` | GPT-OSS | `openai/gpt-oss-120b`, `openai/gpt-oss-20b` | | | ✅︎ |
+| `GraniteForCausalLM` | Granite 3.0, Granite 3.1, PowerLM | `ibm-granite/granite-3.0-2b-base`, `ibm-granite/granite-3.1-8b-instruct`, `ibm/PowerLM-3b`, etc. | ✅︎ | ✅︎ | ✅︎ |
+| `GraniteMoeForCausalLM` | Granite 3.0 MoE, PowerMoE | `ibm-granite/granite-3.0-1b-a400m-base`, `ibm-granite/granite-3.0-3b-a800m-instruct`, `ibm/PowerMoE-3b`, etc. | ✅︎ | ✅︎ | ✅︎ |
+| `GraniteMoeHybridForCausalLM` | Granite 4.0 MoE Hybrid | `ibm-granite/granite-4.0-tiny-preview`, etc. | ✅︎ | ✅︎ | ✅︎ |
+| `GraniteMoeSharedForCausalLM` | Granite MoE Shared | `ibm-research/moe-7b-1b-active-shared-experts` (test model) | ✅︎ | ✅︎ | ✅︎ |
+| `GritLM` | GritLM | `parasail-ai/GritLM-7B-vllm`. | ✅︎ | ✅︎ | |
+| `Grok1ModelForCausalLM` | Grok1 | `hpcai-tech/grok-1`. | ✅︎ | ✅︎ | ✅︎ |
+| `HunYuanDenseV1ForCausalLM` | Hunyuan-7B-Instruct-0124 | `tencent/Hunyuan-7B-Instruct-0124` | ✅︎ | | ✅︎ |
+| `HunYuanMoEV1ForCausalLM` | Hunyuan-80B-A13B | `tencent/Hunyuan-A13B-Instruct`, `tencent/Hunyuan-A13B-Pretrain`, `tencent/Hunyuan-A13B-Instruct-FP8`, etc. | ✅︎ | | ✅︎ |
+| `HCXVisionForCausalLM` | HyperCLOVAX-SEED-Vision-Instruct-3B | `naver-hyperclovax/HyperCLOVAX-SEED-Vision-Instruct-3B` | | | ✅︎ |
+| `InternLMForCausalLM` | InternLM | `internlm/internlm-7b`, `internlm/internlm-chat-7b`, etc. | ✅︎ | ✅︎ | ✅︎ |
+| `InternLM2ForCausalLM` | InternLM2 | `internlm/internlm2-7b`, `internlm/internlm2-chat-7b`, etc. | ✅︎ | ✅︎ | ✅︎ |
+| `InternLM3ForCausalLM` | InternLM3 | `internlm/internlm3-8b-instruct`, etc. | ✅︎ | ✅︎ | ✅︎ |
+| `JAISLMHeadModel` | Jais | `inceptionai/jais-13b`, `inceptionai/jais-13b-chat`, `inceptionai/jais-30b-v3`, `inceptionai/jais-30b-chat-v3`, etc. | | ✅︎ | ✅︎ |
+| `JambaForCausalLM` | Jamba | `ai21labs/AI21-Jamba-1.5-Large`, `ai21labs/AI21-Jamba-1.5-Mini`, `ai21labs/Jamba-v0.1`, etc. | ✅︎ | ✅︎ | ✅︎ |
+| `LlamaForCausalLM` | Llama 3.1, Llama 3, Llama 2, LLaMA, Yi | `meta-llama/Meta-Llama-3.1-405B-Instruct`, `meta-llama/Meta-Llama-3.1-70B`, `meta-llama/Meta-Llama-3-70B-Instruct`, `meta-llama/Llama-2-70b-hf`, `01-ai/Yi-34B`, etc. | ✅︎ | ✅︎ | ✅︎ |
+| `MambaForCausalLM` | Mamba | `state-spaces/mamba-130m-hf`, `state-spaces/mamba-790m-hf`, `state-spaces/mamba-2.8b-hf`, etc. | | ✅︎ | ✅︎ |
+| `Mamba2ForCausalLM` | Mamba2 | `mistralai/Mamba-Codestral-7B-v0.1`, etc. | | ✅︎ | ✅︎ |
+| `MiMoForCausalLM` | MiMo | `XiaomiMiMo/MiMo-7B-RL`, etc. | ✅︎ | ✅︎ | ✅︎ |
+| `MiniCPMForCausalLM` | MiniCPM | `openbmb/MiniCPM-2B-sft-bf16`, `openbmb/MiniCPM-2B-dpo-bf16`, `openbmb/MiniCPM-S-1B-sft`, etc. | ✅︎ | ✅︎ | ✅︎ |
+| `MiniCPM3ForCausalLM` | MiniCPM3 | `openbmb/MiniCPM3-4B`, etc. | ✅︎ | ✅︎ | ✅︎ |
+| `MistralForCausalLM` | Mistral, Mistral-Instruct | `mistralai/Mistral-7B-v0.1`, `mistralai/Mistral-7B-Instruct-v0.1`, etc. | ✅︎ | ✅︎ | ✅︎ |
+| `MixtralForCausalLM` | Mixtral-8x7B, Mixtral-8x7B-Instruct | `mistralai/Mixtral-8x7B-v0.1`, `mistralai/Mixtral-8x7B-Instruct-v0.1`, `mistral-community/Mixtral-8x22B-v0.1`, etc. | ✅︎ | ✅︎ | ✅︎ |
+| `MPTForCausalLM` | MPT, MPT-Instruct, MPT-Chat, MPT-StoryWriter | `mosaicml/mpt-7b`, `mosaicml/mpt-7b-storywriter`, `mosaicml/mpt-30b`, etc. | | ✅︎ | ✅︎ |
+| `NemotronForCausalLM` | Nemotron-3, Nemotron-4, Minitron | `nvidia/Minitron-8B-Base`, `mgoin/Nemotron-4-340B-Base-hf-FP8`, etc. | ✅︎ | ✅︎ | ✅︎ |
+| `NemotronHForCausalLM` | Nemotron-H | `nvidia/Nemotron-H-8B-Base-8K`, `nvidia/Nemotron-H-47B-Base-8K`, `nvidia/Nemotron-H-56B-Base-8K`, etc. | ✅︎ | ✅︎ | ✅︎ |
+| `OLMoForCausalLM` | OLMo | `allenai/OLMo-1B-hf`, `allenai/OLMo-7B-hf`, etc. | | ✅︎ | ✅︎ |
+| `OLMo2ForCausalLM` | OLMo2 | `allenai/OLMo-2-0425-1B`, etc. | | ✅︎ | ✅︎ |
+| `OLMoEForCausalLM` | OLMoE | `allenai/OLMoE-1B-7B-0924`, `allenai/OLMoE-1B-7B-0924-Instruct`, etc. | | ✅︎ | ✅︎ |
+| `OPTForCausalLM` | OPT, OPT-IML | `facebook/opt-66b`, `facebook/opt-iml-max-30b`, etc. | | ✅︎ | ✅︎ |
+| `OrionForCausalLM` | Orion | `OrionStarAI/Orion-14B-Base`, `OrionStarAI/Orion-14B-Chat`, etc. | | ✅︎ | ✅︎ |
+| `PhiForCausalLM` | Phi | `microsoft/phi-1_5`, `microsoft/phi-2`, etc. | ✅︎ | ✅︎ | ✅︎ |
+| `Phi3ForCausalLM` | Phi-4, Phi-3 | `microsoft/Phi-4-mini-instruct`, `microsoft/Phi-4`, `microsoft/Phi-3-mini-4k-instruct`, `microsoft/Phi-3-mini-128k-instruct`, `microsoft/Phi-3-medium-128k-instruct`, etc. | ✅︎ | ✅︎ | ✅︎ |
+| `PhiMoEForCausalLM` | Phi-3.5-MoE | `microsoft/Phi-3.5-MoE-instruct`, etc. | ✅︎ | ✅︎ | ✅︎ |
+| `Phi4FlashForCausalLM` | Phi-4-mini-flash-reasoning | `microsoft/microsoft/Phi-4-mini-instruct`, etc. | | | |
+| `PersimmonForCausalLM` | Persimmon | `adept/persimmon-8b-base`, `adept/persimmon-8b-chat`, etc. | | ✅︎ | ✅︎ |
+| `Plamo2ForCausalLM` | PLaMo2 | `pfnet/plamo-2-1b`, `pfnet/plamo-2-8b`, etc. | | ✅︎ | |
+| `QWenLMHeadModel` | Qwen | `Qwen/Qwen-7B`, `Qwen/Qwen-7B-Chat`, etc. | ✅︎ | ✅︎ | ✅︎ |
+| `Qwen2ForCausalLM` | QwQ, Qwen2 | `Qwen/QwQ-32B-Preview`, `Qwen/Qwen2-7B-Instruct`, `Qwen/Qwen2-7B`, etc. | ✅︎ | ✅︎ | ✅︎ |
+| `Qwen2MoeForCausalLM` | Qwen2MoE | `Qwen/Qwen1.5-MoE-A2.7B`, `Qwen/Qwen1.5-MoE-A2.7B-Chat`, etc. | ✅︎ | ✅︎ | ✅︎ |
+| `Qwen3ForCausalLM` | Qwen3 | `Qwen/Qwen3-8B`, etc. | ✅︎ | ✅︎ | ✅︎ |
+| `Qwen3MoeForCausalLM` | Qwen3MoE | `Qwen/Qwen3-30B-A3B`, etc. | ✅︎ | ✅︎ | ✅︎ |
+| `StableLmForCausalLM` | StableLM | `stabilityai/stablelm-3b-4e1t`, `stabilityai/stablelm-base-alpha-7b-v2`, etc. | | | ✅︎ |
+| `Starcoder2ForCausalLM` | Starcoder2 | `bigcode/starcoder2-3b`, `bigcode/starcoder2-7b`, `bigcode/starcoder2-15b`, etc. | | ✅︎ | ✅︎ |
+| `SolarForCausalLM` | Solar Pro | `upstage/solar-pro-preview-instruct`, etc. | ✅︎ | ✅︎ | ✅︎ |
+| `TeleChat2ForCausalLM` | TeleChat2 | `Tele-AI/TeleChat2-3B`, `Tele-AI/TeleChat2-7B`, `Tele-AI/TeleChat2-35B`, etc. | ✅︎ | ✅︎ | ✅︎ |
+| `TeleFLMForCausalLM` | TeleFLM | `CofeAI/FLM-2-52B-Instruct-2407`, `CofeAI/Tele-FLM`, etc. | ✅︎ | ✅︎ | ✅︎ |
+| `XverseForCausalLM` | XVERSE | `xverse/XVERSE-7B-Chat`, `xverse/XVERSE-13B-Chat`, `xverse/XVERSE-65B-Chat`, etc. | ✅︎ | ✅︎ | ✅︎ |
+| `MiniMaxM1ForCausalLM` | MiniMax-Text | `MiniMaxAI/MiniMax-M1-40k`, `MiniMaxAI/MiniMax-M1-80k`, etc. | | | ✅︎ |
+| `MiniMaxText01ForCausalLM` | MiniMax-Text | `MiniMaxAI/MiniMax-Text-01`, etc. | | | ✅︎ |
+| `Zamba2ForCausalLM` | Zamba2 | `Zyphra/Zamba2-7B-instruct`, `Zyphra/Zamba2-2.7B-instruct`, `Zyphra/Zamba2-1.2B-instruct`, etc. | | | ✅︎ |
 
 !!! note
     Currently, the ROCm version of vLLM supports Mistral and Mixtral only for context lengths up to 4096.
@@ -510,46 +534,61 @@ See [this page][generative-models] for more information on how to use generative
 
 Specified using `--task generate`.
 
-| Architecture                                 | Models                                                                   | Inputs                                                                | Example HF Models                                                                                                                                       | [LoRA][lora-adapter]   | [PP][distributed-serving]   | [V1](gh-issue:8779)   |
-|----------------------------------------------|--------------------------------------------------------------------------|-----------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------|-----------------------------|-----------------------|
-| `AriaForConditionalGeneration`               | Aria                                                                     | T + I<sup>+</sup>                                                     | `rhymes-ai/Aria`                                                                                                                                        | ✅︎                     | ✅︎                          |                       |
-| `AyaVisionForConditionalGeneration`          | Aya Vision                                                               | T + I<sup>+</sup>                                                     | `CohereForAI/aya-vision-8b`, `CohereForAI/aya-vision-32b`, etc.                                                                                         | ✅︎                     | ✅︎                          |                       |
-| `Blip2ForConditionalGeneration`              | BLIP-2                                                                   | T + I<sup>E</sup>                                                     | `Salesforce/blip2-opt-2.7b`, `Salesforce/blip2-opt-6.7b`, etc.                                                                                          | ✅︎                     | ✅︎                          |                       |
-| `ChameleonForConditionalGeneration`          | Chameleon                                                                | T + I                                                                 | `facebook/chameleon-7b` etc.                                                                                                                            | ✅︎                     | ✅︎                          |                       |
-| `DeepseekVLV2ForCausalLM`<sup>^</sup>        | DeepSeek-VL2                                                             | T + I<sup>+</sup>                                                     | `deepseek-ai/deepseek-vl2-tiny`, `deepseek-ai/deepseek-vl2-small`, `deepseek-ai/deepseek-vl2` etc.                                                      | ✅︎                     | ✅︎                          |                       |
-| `Florence2ForConditionalGeneration`          | Florence-2                                                               | T + I                                                                 | `microsoft/Florence-2-base`, `microsoft/Florence-2-large` etc.                                                                                          |                        |                             |                       |
-| `FuyuForCausalLM`                            | Fuyu                                                                     | T + I                                                                 | `adept/fuyu-8b` etc.                                                                                                                                    | ✅︎                     | ✅︎                          |                       |
-| `Gemma3ForConditionalGeneration`             | Gemma 3                                                                  | T + I<sup>+</sup>                                                     | `google/gemma-3-4b-it`, `google/gemma-3-27b-it`, etc.                                                                                                   | ✅︎                     | ✅︎                          | ⚠️                    |
-| `GLM4VForCausalLM`<sup>^</sup>               | GLM-4V                                                                   | T + I                                                                 | `THUDM/glm-4v-9b`, `THUDM/cogagent-9b-20241220` etc.                                                                                                    | ✅︎                     | ✅︎                          | ✅︎                    |
-| `GraniteSpeechForConditionalGeneration`      | Granite Speech                                                           | T + A                                                                 | `ibm-granite/granite-speech-3.3-8b`                                                                                                                     | ✅︎                     | ✅︎                          | ✅︎                    |
-| `H2OVLChatModel`                             | H2OVL                                                                    | T + I<sup>E+</sup>                                                    | `h2oai/h2ovl-mississippi-800m`, `h2oai/h2ovl-mississippi-2b`, etc.                                                                                      | ✅︎                     | ✅︎\*                        |                       |
-| `Idefics3ForConditionalGeneration`           | Idefics3                                                                 | T + I                                                                 | `HuggingFaceM4/Idefics3-8B-Llama3` etc.                                                                                                                 | ✅︎                     | ✅︎                          |                       |
-| `InternVLChatModel`                          | InternVL 3.0, InternVideo 2.5, InternVL 2.5, Mono-InternVL, InternVL 2.0 | T + I<sup>E+</sup> + (V<sup>E+</sup>)                                                    | `OpenGVLab/InternVL3-9B`, `OpenGVLab/InternVideo2_5_Chat_8B`, `OpenGVLab/InternVL2_5-4B`, `OpenGVLab/Mono-InternVL-2B`, `OpenGVLab/InternVL2-4B`, etc.  | ✅︎                     | ✅︎                          |                       |
-| `KimiVLForConditionalGeneration`             | Kimi-VL-A3B-Instruct, Kimi-VL-A3B-Thinking                               | T + I<sup>+</sup>                                                     | `moonshotai/Kimi-VL-A3B-Instruct`, `moonshotai/Kimi-VL-A3B-Thinking`                                                                                    | ✅︎                     |                             |                       |
-| `Llama4ForConditionalGeneration`             | Llama 4                                                                  | T + I<sup>+</sup>                                                     | `meta-llama/Llama-4-Scout-17B-16E-Instruct`, `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8`, `meta-llama/Llama-4-Maverick-17B-128E-Instruct`, etc. | ✅︎                     | ✅︎                          |                       |
-| `LlavaForConditionalGeneration`              | LLaVA-1.5                                                                | T + I<sup>E+</sup>                                                    | `llava-hf/llava-1.5-7b-hf`, `TIGER-Lab/Mantis-8B-siglip-llama3` (see note), etc.                                                                        | ✅︎                     | ✅︎                          |                       |
-| `LlavaNextForConditionalGeneration`          | LLaVA-NeXT                                                               | T + I<sup>E+</sup>                                                    | `llava-hf/llava-v1.6-mistral-7b-hf`, `llava-hf/llava-v1.6-vicuna-7b-hf`, etc.                                                                           | ✅︎                     | ✅︎                          |                       |
-| `LlavaNextVideoForConditionalGeneration`     | LLaVA-NeXT-Video                                                         | T + V                                                                 | `llava-hf/LLaVA-NeXT-Video-7B-hf`, etc.                                                                                                                 | ✅︎                     | ✅︎                          |                       |
-| `LlavaOnevisionForConditionalGeneration`     | LLaVA-Onevision                                                          | T + I<sup>+</sup> + V<sup>+</sup>                                     | `llava-hf/llava-onevision-qwen2-7b-ov-hf`, `llava-hf/llava-onevision-qwen2-0.5b-ov-hf`, etc.                                                            | ✅︎                     | ✅︎                          |                       |
-| `MiniCPMO`                                   | MiniCPM-O                                                                | T + I<sup>E+</sup> + V<sup>E+</sup> + A<sup>E+</sup>                  | `openbmb/MiniCPM-o-2_6`, etc.                                                                                                                           | ✅︎                     | ✅︎                          | ✅︎                    |
-| `MiniCPMV`                                   | MiniCPM-V                                                                | T + I<sup>E+</sup> + V<sup>E+</sup>                                   | `openbmb/MiniCPM-V-2` (see note), `openbmb/MiniCPM-Llama3-V-2_5`, `openbmb/MiniCPM-V-2_6`, etc.                                                         | ✅︎                     | ✅︎                          | ✅︎                    |
-| `MiniMaxVL01ForConditionalGeneration`        | MiniMax-VL                                                               | T + I<sup>E+</sup>                                                    | `MiniMaxAI/MiniMax-VL-01`, etc.                                                                                                                         | ✅︎                     | ✅︎                          |                       |
-| `Mistral3ForConditionalGeneration`           | Mistral3                                                                 | T + I<sup>+</sup>                                                     | `mistralai/Mistral-Small-3.1-24B-Instruct-2503`, etc.                                                                                                   | ✅︎                     | ✅︎                          | ✅︎                    |
-| `MllamaForConditionalGeneration`             | Llama 3.2                                                                | T + I<sup>+</sup>                                                     | `meta-llama/Llama-3.2-90B-Vision-Instruct`, `meta-llama/Llama-3.2-11B-Vision`, etc.                                                                     |                        |                             |                       |
-| `MolmoForCausalLM`                           | Molmo                                                                    | T + I<sup>+</sup>                                                     | `allenai/Molmo-7B-D-0924`, `allenai/Molmo-7B-O-0924`, etc.                                                                                              | ✅︎                     | ✅︎                          | ✅︎                    |
-| `NVLM_D_Model`                               | NVLM-D 1.0                                                               | T + I<sup>+</sup>                                                     | `nvidia/NVLM-D-72B`, etc.                                                                                                                               | ✅︎                     | ✅︎                          |                       |
-| `Ovis`                                       | Ovis2, Ovis1.6                                                           | T + I<sup>+</sup>                                                     | `AIDC-AI/Ovis2-1B`, `AIDC-AI/Ovis1.6-Llama3.2-3B`, etc.                                                                                                 | ✅︎                     |                             |                       |
-| `PaliGemmaForConditionalGeneration`          | PaliGemma, PaliGemma 2                                                   | T + I<sup>E</sup>                                                     | `google/paligemma-3b-pt-224`, `google/paligemma-3b-mix-224`, `google/paligemma2-3b-ft-docci-448`, etc.                                                  | ✅︎                     | ⚠️                          |                       |
-| `Phi3VForCausalLM`                           | Phi-3-Vision, Phi-3.5-Vision                                             | T + I<sup>E+</sup>                                                    | `microsoft/Phi-3-vision-128k-instruct`, `microsoft/Phi-3.5-vision-instruct`, etc.                                                                       | ✅︎                     | ✅︎                          |                       |
-| `Phi4MMForCausalLM`                          | Phi-4-multimodal                                                         | T + I<sup>+</sup> / T + A<sup>+</sup> / I<sup>+</sup> + A<sup>+</sup> | `microsoft/Phi-4-multimodal-instruct`, etc.                                                                                                             | ✅︎                     | ✅︎                          |                       |
-| `PixtralForConditionalGeneration`            | Pixtral                                                                  | T + I<sup>+</sup>                                                     | `mistralai/Mistral-Small-3.1-24B-Instruct-2503`, `mistral-community/pixtral-12b`, etc.                                                                  | ✅︎                     | ✅︎                          |                       |
-| `QwenVLForConditionalGeneration`<sup>^</sup> | Qwen-VL                                                                  | T + I<sup>E+</sup>                                                    | `Qwen/Qwen-VL`, `Qwen/Qwen-VL-Chat`, etc.                                                                                                               | ✅︎                     | ✅︎                          | ✅︎                    |
-| `Qwen2AudioForConditionalGeneration`         | Qwen2-Audio                                                              | T + A<sup>+</sup>                                                     | `Qwen/Qwen2-Audio-7B-Instruct`                                                                                                                          | ✅︎                     | ✅︎                          |                       |
-| `Qwen2VLForConditionalGeneration`            | QVQ, Qwen2-VL                                                            | T + I<sup>E+</sup> + V<sup>E+</sup>                                   | `Qwen/QVQ-72B-Preview`, `Qwen/Qwen2-VL-7B-Instruct`, `Qwen/Qwen2-VL-72B-Instruct`, etc.                                                                 | ✅︎                     | ✅︎                          | ✅︎                    |
-| `Qwen2_5_VLForConditionalGeneration`         | Qwen2.5-VL                                                               | T + I<sup>E+</sup> + V<sup>E+</sup>                                   | `Qwen/Qwen2.5-VL-3B-Instruct`, `Qwen/Qwen2.5-VL-72B-Instruct`, etc.                                                                                     | ✅︎                     | ✅︎                          | ✅︎                    |
-| `Qwen2_5OmniThinkerForConditionalGeneration` | Qwen2.5-Omni                                                             | T + I<sup>E+</sup> + V<sup>E+</sup> + A<sup>+</sup>                   | `Qwen/Qwen2.5-Omni-7B`                                                                                                                                  | ✅︎                     | ✅︎\*                        |                       |
-| `SkyworkR1VChatModel`                        | Skywork-R1V-38B                                                          | T + I                                                                 | `Skywork/Skywork-R1V-38B`                                                                                                                               | ✅︎                     | ✅︎                          |                       |
-| `SmolVLMForConditionalGeneration`            | SmolVLM2                                                                 | T + I                                                                 | `SmolVLM2-2.2B-Instruct`                                                                                                                                | ✅︎                     | ✅︎                          |                       |
+| Architecture | Models | Inputs | Example HF Models | [LoRA](../features/lora.md) | [PP](../serving/distributed_serving.md) | [V1](gh-issue:8779) |
+|--------------|--------|--------|-------------------|----------------------|---------------------------|---------------------|
+| `AriaForConditionalGeneration` | Aria | T + I<sup>+</sup> | `rhymes-ai/Aria` | | | ✅︎ |
+| `AyaVisionForConditionalGeneration` | Aya Vision | T + I<sup>+</sup> | `CohereForAI/aya-vision-8b`, `CohereForAI/aya-vision-32b`, etc. | | ✅︎ | ✅︎ |
+| `Blip2ForConditionalGeneration` | BLIP-2 | T + I<sup>E</sup> | `Salesforce/blip2-opt-2.7b`, `Salesforce/blip2-opt-6.7b`, etc. | | ✅︎ | ✅︎ |
+| `ChameleonForConditionalGeneration` | Chameleon | T + I | `facebook/chameleon-7b`, etc. | | ✅︎ | ✅︎ |
+| `DeepseekVLV2ForCausalLM`<sup>^</sup> | DeepSeek-VL2 | T + I<sup>+</sup> | `deepseek-ai/deepseek-vl2-tiny`, `deepseek-ai/deepseek-vl2-small`, `deepseek-ai/deepseek-vl2`, etc. | | ✅︎ | ✅︎ |
+| `Florence2ForConditionalGeneration` | Florence-2 | T + I | `microsoft/Florence-2-base`, `microsoft/Florence-2-large`, etc. | | | |
+| `FuyuForCausalLM` | Fuyu | T + I | `adept/fuyu-8b`, etc. | | ✅︎ | ✅︎ |
+| `Gemma3ForConditionalGeneration` | Gemma 3 | T + I<sup>+</sup> | `google/gemma-3-4b-it`, `google/gemma-3-27b-it`, etc. | ✅︎ | ✅︎ | ⚠️ |
+| `GLM4VForCausalLM`<sup>^</sup> | GLM-4V | T + I | `zai-org/glm-4v-9b`, `zai-org/cogagent-9b-20241220`, etc. | ✅︎ | ✅︎ | ✅︎ |
+| `Glm4vForConditionalGeneration` | GLM-4.1V-Thinking | T + I<sup>E+</sup> + V<sup>E+</sup> | `zai-org/GLM-4.1V-9B-Thinking`, etc. | ✅︎ | ✅︎ | ✅︎ |
+| `Glm4vMoeForConditionalGeneration` | GLM-4.5V | T + I<sup>E+</sup> + V<sup>E+</sup> | `zai-org/GLM-4.5V`, etc. | ✅︎ | ✅︎ | ✅︎ |
+| `GraniteSpeechForConditionalGeneration` | Granite Speech | T + A | `ibm-granite/granite-speech-3.3-8b` | ✅︎ | ✅︎ | ✅︎ |
+| `H2OVLChatModel` | H2OVL | T + I<sup>E+</sup> | `h2oai/h2ovl-mississippi-800m`, `h2oai/h2ovl-mississippi-2b`, etc. | | ✅︎ | ✅︎ |
+| `Idefics3ForConditionalGeneration` | Idefics3 | T + I | `HuggingFaceM4/Idefics3-8B-Llama3`, etc. | ✅︎ | | ✅︎ |
+| `InternS1ForConditionalGeneration` | Intern-S1 | T + I<sup>E+</sup> + V<sup>E+</sup> | `internlm/Intern-S1`, etc. | ✅︎ | ✅︎ | ✅︎ |
+| `InternVLChatModel` | InternVL 3.0, InternVideo 2.5, InternVL 2.5, Mono-InternVL, InternVL 2.0 | T + I<sup>E+</sup> + (V<sup>E+</sup>) | `OpenGVLab/InternVL3-9B`, `OpenGVLab/InternVideo2_5_Chat_8B`, `OpenGVLab/InternVL2_5-4B`, `OpenGVLab/Mono-InternVL-2B`, `OpenGVLab/InternVL2-4B`, etc. | ✅︎ | ✅︎ | ✅︎ |
+| `KeyeForConditionalGeneration` | Keye-VL-8B-Preview | T + I<sup>E+</sup> + V<sup>E+</sup> | `Kwai-Keye/Keye-VL-8B-Preview` | | | ✅︎ |
+| `KimiVLForConditionalGeneration` | Kimi-VL-A3B-Instruct, Kimi-VL-A3B-Thinking | T + I<sup>+</sup> | `moonshotai/Kimi-VL-A3B-Instruct`, `moonshotai/Kimi-VL-A3B-Thinking` | | | ✅︎ |
+| `Llama4ForConditionalGeneration` | Llama 4 | T + I<sup>+</sup> | `meta-llama/Llama-4-Scout-17B-16E-Instruct`, `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8`, `meta-llama/Llama-4-Maverick-17B-128E-Instruct`, etc. | | ✅︎ | ✅︎ |
+| `Llama_Nemotron_Nano_VL` | Llama Nemotron Nano VL | T + I<sup>E+</sup> | `nvidia/Llama-3.1-Nemotron-Nano-VL-8B-V1` | ✅︎ | ✅︎ | ✅︎ |
+| `LlavaForConditionalGeneration` | LLaVA-1.5, Pixtral (HF Transformers) | T + I<sup>E+</sup> | `llava-hf/llava-1.5-7b-hf`, `TIGER-Lab/Mantis-8B-siglip-llama3` (see note), `mistral-community/pixtral-12b`, etc. | | ✅︎ | ✅︎ |
+| `LlavaNextForConditionalGeneration` | LLaVA-NeXT | T + I<sup>E+</sup> | `llava-hf/llava-v1.6-mistral-7b-hf`, `llava-hf/llava-v1.6-vicuna-7b-hf`, etc. | | ✅︎ | ✅︎ |
+| `LlavaNextVideoForConditionalGeneration` | LLaVA-NeXT-Video | T + V | `llava-hf/LLaVA-NeXT-Video-7B-hf`, etc. | | ✅︎ | ✅︎ |
+| `LlavaOnevisionForConditionalGeneration` | LLaVA-Onevision | T + I<sup>+</sup> + V<sup>+</sup> | `llava-hf/llava-onevision-qwen2-7b-ov-hf`, `llava-hf/llava-onevision-qwen2-0.5b-ov-hf`, etc. | | ✅︎ | ✅︎ |
+| `MiniCPMO` | MiniCPM-O | T + I<sup>E+</sup> + V<sup>E+</sup> + A<sup>E+</sup> | `openbmb/MiniCPM-o-2_6`, etc. | ✅︎ | ✅︎ | ✅︎ |
+| `MiniCPMV` | MiniCPM-V | T + I<sup>E+</sup> + V<sup>E+</sup> | `openbmb/MiniCPM-V-2` (see note), `openbmb/MiniCPM-Llama3-V-2_5`, `openbmb/MiniCPM-V-2_6`, etc. | ✅︎ | | ✅︎ |
+| `MiniMaxVL01ForConditionalGeneration` | MiniMax-VL | T + I<sup>E+</sup> | `MiniMaxAI/MiniMax-VL-01`, etc. | | ✅︎ | ✅︎ |
+| `Mistral3ForConditionalGeneration` | Mistral3 (HF Transformers) | T + I<sup>+</sup> | `mistralai/Mistral-Small-3.1-24B-Instruct-2503`, etc. | ✅︎ | ✅︎ | ✅︎ |
+| `MllamaForConditionalGeneration` | Llama 3.2 | T + I<sup>+</sup> | `meta-llama/Llama-3.2-90B-Vision-Instruct`, `meta-llama/Llama-3.2-11B-Vision`, etc. | | | |
+| `MolmoForCausalLM` | Molmo | T + I<sup>+</sup> | `allenai/Molmo-7B-D-0924`, `allenai/Molmo-7B-O-0924`, etc. | ✅︎ | ✅︎ | ✅︎ |
+| `NVLM_D_Model` | NVLM-D 1.0 | T + I<sup>+</sup> | `nvidia/NVLM-D-72B`, etc. | | ✅︎ | ✅︎ |
+| `Ovis` | Ovis2, Ovis1.6 | T + I<sup>+</sup> | `AIDC-AI/Ovis2-1B`, `AIDC-AI/Ovis1.6-Llama3.2-3B`, etc. | | ✅︎ | ✅︎ |
+| `PaliGemmaForConditionalGeneration` | PaliGemma, PaliGemma 2 | T + I<sup>E</sup> | `google/paligemma-3b-pt-224`, `google/paligemma-3b-mix-224`, `google/paligemma2-3b-ft-docci-448`, etc. | | ✅︎ | ⚠️ |
+| `Phi3VForCausalLM` | Phi-3-Vision, Phi-3.5-Vision | T + I<sup>E+</sup> | `microsoft/Phi-3-vision-128k-instruct`, `microsoft/Phi-3.5-vision-instruct`, etc. | | ✅︎ | ✅︎ |
+| `Phi4MMForCausalLM` | Phi-4-multimodal | T + I<sup>+</sup> / T + A<sup>+</sup> / I<sup>+</sup> + A<sup>+</sup> | `microsoft/Phi-4-multimodal-instruct`, etc. | ✅︎ | ✅︎ | ✅︎ |
+| `Phi4MultimodalForCausalLM` | Phi-4-multimodal (HF Transformers) | T + I<sup>+</sup> / T + A<sup>+</sup> / I<sup>+</sup> + A<sup>+</sup> | `microsoft/Phi-4-multimodal-instruct` (with revision `refs/pr/70`), etc. | ✅︎ | ✅︎ | ✅︎ |
+| `PixtralForConditionalGeneration` | Mistral 3 (Mistral format), Pixtral (Mistral format) | T + I<sup>+</sup> | `mistralai/Mistral-Small-3.1-24B-Instruct-2503`, `mistralai/Pixtral-12B-2409`, etc. | | ✅︎ | ✅︎ |
+| `QwenVLForConditionalGeneration`<sup>^</sup> | Qwen-VL | T + I<sup>E+</sup> | `Qwen/Qwen-VL`, `Qwen/Qwen-VL-Chat`, etc. | ✅︎ | ✅︎ | ✅︎ |
+| `Qwen2AudioForConditionalGeneration` | Qwen2-Audio | T + A<sup>+</sup> | `Qwen/Qwen2-Audio-7B-Instruct` | | ✅︎ | ✅︎ |
+| `Qwen2VLForConditionalGeneration` | QVQ, Qwen2-VL | T + I<sup>E+</sup> + V<sup>E+</sup> | `Qwen/QVQ-72B-Preview`, `Qwen/Qwen2-VL-7B-Instruct`, `Qwen/Qwen2-VL-72B-Instruct`, etc. | ✅︎ | ✅︎ | ✅︎ |
+| `Qwen2_5_VLForConditionalGeneration` | Qwen2.5-VL | T + I<sup>E+</sup> + V<sup>E+</sup> | `Qwen/Qwen2.5-VL-3B-Instruct`, `Qwen/Qwen2.5-VL-72B-Instruct`, etc. | ✅︎ | ✅︎ | ✅︎ |
+| `Qwen2_5OmniThinkerForConditionalGeneration` | Qwen2.5-Omni | T + I<sup>E+</sup> + V<sup>E+</sup> + A<sup>+</sup> | `Qwen/Qwen2.5-Omni-7B` | | ✅︎ | ✅︎ |
+| `SkyworkR1VChatModel` | Skywork-R1V-38B | T + I | `Skywork/Skywork-R1V-38B` | | ✅︎ | ✅︎ |
+| `SmolVLMForConditionalGeneration` | SmolVLM2 | T + I | `SmolVLM2-2.2B-Instruct` | ✅︎ | | ✅︎ |
+| `Step3VLForConditionalGeneration` | Step3-VL | T + I<sup>+</sup> | `stepfun-ai/step3` | | ✅︎ | ✅︎ |
+| `TarsierForConditionalGeneration` | Tarsier | T + I<sup>E+</sup> | `omni-search/Tarsier-7b`, `omni-search/Tarsier-34b` | | ✅︎ | ✅︎ |
+| `Tarsier2ForConditionalGeneration`<sup>^</sup> | Tarsier2 | T + I<sup>E+</sup> + V<sup>E+</sup> | `omni-research/Tarsier2-Recap-7b`, `omni-research/Tarsier2-7b-0115` | | ✅︎ | ✅︎ |
+
+Some models are supported only via the [Transformers backend](#transformers). The purpose of the table below is to acknowledge models which we officially support in this way. The logs will say that the Transformers backend is being used, and you will see no warning that this is fallback behaviour. This means that, if you have issues with any of the models listed below, please [make an issue](https://github.com/vllm-project/vllm/issues/new/choose) and we'll do our best to fix it!
+
+| Architecture | Models | Inputs | Example HF Models | [LoRA](../features/lora.md) | [PP](../serving/distributed_serving.md) | [V1](gh-issue:8779) |
+|--------------|--------|--------|-------------------|-----------------------------|-----------------------------------------|---------------------|
+| `Emu3ForConditionalGeneration` | Emu3 | T + I | `BAAI/Emu3-Chat-hf` | ✅︎ | ✅︎ | ✅︎ |
 
 <sup>^</sup> You need to set the architecture name via `--hf-overrides` to match the one in vLLM.  
 &nbsp;&nbsp;&nbsp;&nbsp;• For example, to use DeepSeek-VL2 series models:  
diff --git a/examples/offline_inference/multilora_inference.py b/examples/offline_inference/multilora_inference.py
index 1fa2f16f8..7e3bf7041 100644
--- a/examples/offline_inference/multilora_inference.py
+++ b/examples/offline_inference/multilora_inference.py
@@ -98,14 +98,14 @@ def initialize_engine() -> LLMEngine:
     #   numbers will cause higher memory usage. If you know that all LoRAs will
     #   use the same rank, it is recommended to set this as low as possible.
     # max_cpu_loras: controls the size of the CPU LoRA cache.
-    engine_args = EngineArgs(
-        model="meta-llama/Llama-2-7b-hf",
-        enable_lora=True,
-        max_loras=1,
-        max_lora_rank=8,
-        max_cpu_loras=2,
-        max_num_seqs=256,
-    )
+    engine_args = EngineArgs(model="meta-llama/Llama-2-7b-hf",
+                             enable_lora=True,
+                             max_loras=1,
+                             max_lora_rank=8,
+                             max_cpu_loras=2,
+                             max_num_seqs=256,
+                             enforce_eager=True,
+                             block_size=64)
     return LLMEngine.from_engine_args(engine_args)
 
 
diff --git a/examples/offline_inference/vision_language.py b/examples/offline_inference/vision_language.py
index f05045016..30ee8b868 100644
--- a/examples/offline_inference/vision_language.py
+++ b/examples/offline_inference/vision_language.py
@@ -247,6 +247,42 @@ def run_glm4v(questions: list[str], modality: str) -> ModelRequestData:
     )
 
 
+# GLM-4.1V
+def run_glm4_1v(questions: list[str], modality: str) -> ModelRequestData:
+    model_name = "THUDM/GLM-4.1V-9B-Thinking"
+
+    engine_args = EngineArgs(
+        model=model_name,
+        max_model_len=4096,
+        max_num_seqs=2,
+        mm_processor_kwargs={
+            "size": {"shortest_edge": 12544, "longest_edge": 47040000},
+            "fps": 1,
+        },
+        limit_mm_per_prompt={modality: 1},
+        enforce_eager=True,
+    )
+
+    if modality == "image":
+        placeholder = "<|begin_of_image|><|image|><|end_of_image|>"
+    elif modality == "video":
+        placeholder = "<|begin_of_video|><|video|><|end_of_video|>"
+
+    prompts = [
+        (
+            "[gMASK]<sop><|system|>\nYou are a helpful assistant.<|user|>\n"
+            f"{placeholder}"
+            f"{question}<|assistant|>assistant\n"
+        )
+        for question in questions
+    ]
+
+    return ModelRequestData(
+        engine_args=engine_args,
+        prompts=prompts,
+    )
+
+
 # H2OVL-Mississippi
 def run_h2ovl(questions: list[str], modality: str) -> ModelRequestData:
     assert modality == "image"
@@ -954,7 +990,9 @@ def run_qwen2_5_vl(questions: list[str], modality: str) -> ModelRequestData:
             "max_pixels": 1280 * 28 * 28,
             "fps": 1,
         },
-        limit_mm_per_prompt={modality: 1},
+        limit_mm_per_prompt={"image": 1},
+        enforce_eager=True,
+        disable_mm_preprocessor_cache=args.disable_mm_preprocessor_cache,
     )
 
     if modality == "image":
@@ -1063,6 +1101,7 @@ model_example_map = {
     "fuyu": run_fuyu,
     "gemma3": run_gemma3,
     "glm4v": run_glm4v,
+    "glm4_1v": run_glm4_1v,
     "h2ovl_chat": run_h2ovl,
     "idefics3": run_idefics3,
     "internvl_chat": run_internvl,
@@ -1119,10 +1158,11 @@ def get_multi_modal_input(args):
     if args.modality == "video":
         # Input video and question
         video = VideoAsset(name="baby_reading", num_frames=args.num_frames).np_ndarrays
+        metadata = VideoAsset(name="baby_reading", num_frames=args.num_frames).metadata
         vid_questions = ["Why is this video funny?"]
 
         return {
-            "data": video,
+            "data": [(video, metadata)] if args.model_type == "glm4_1v" else video,
             "questions": vid_questions,
         }
 
diff --git a/examples/offline_inference/vision_language_multi_image.py b/examples/offline_inference/vision_language_multi_image.py
index e776ff7fe..6aeaa8bc2 100644
--- a/examples/offline_inference/vision_language_multi_image.py
+++ b/examples/offline_inference/vision_language_multi_image.py
@@ -652,12 +652,14 @@ def load_qwen2_5_vl(question: str, image_urls: list[str]) -> ModelRequestData:
         )
         process_vision_info = None
 
-    model_name = "Qwen/Qwen2.5-VL-3B-Instruct"
+    model_name = "Qwen/Qwen2.5-VL-7B-Instruct"
 
     engine_args = EngineArgs(
         model=model_name,
         max_model_len=32768 if process_vision_info is None else 4096,
-        max_num_seqs=5,
+        max_num_seqs=2,
+        enforce_eager=True,
+        gpu_memory_utilization=0.8,
         limit_mm_per_prompt={"image": len(image_urls)},
     )
 
diff --git a/requirements/xpu.txt b/requirements/xpu.txt
index 04c4d4ff8..fbbc12325 100644
--- a/requirements/xpu.txt
+++ b/requirements/xpu.txt
@@ -3,21 +3,20 @@
 
 ray>=2.9
 cmake>=3.26
-packaging>=24.2
+packaging
 setuptools-scm>=8
-setuptools>=77.0.3,<80.0.0
+setuptools>=75.8.0
 wheel
 jinja2>=3.1.6
 datasets # for benchmark scripts
+numba == 0.60.0 # v0.61 doesn't support Python 3.9. Required for N-gram speculative decoding
 
-torch==2.7.0+xpu
+torch == 2.7.0
 torchaudio
 torchvision
-pytorch-triton-xpu
 --extra-index-url=https://download.pytorch.org/whl/xpu
 
-# Please refer xpu doc, we need manually install intel-extension-for-pytorch 2.6.10+xpu due to there are some conflict dependencies with torch 2.6.0+xpu
-# FIXME: This will be fix in ipex 2.7. just leave this here for awareness.
-intel-extension-for-pytorch==2.7.10+xpu
+
+intel-extension-for-pytorch @ https://download.pytorch-extension.intel.com/ipex_dev/xpu/intel_extension_for_pytorch-2.7.0.post1%2Bxpu-cp310-cp310-linux_x86_64.whl
 oneccl_bind_pt==2.7.0+xpu
 --extra-index-url=https://pytorch-extension.intel.com/release-whl/stable/xpu/us/
diff --git a/run_benchmark_with_profile.sh b/run_benchmark_with_profile.sh
new file mode 100644
index 000000000..fe4dbc268
--- /dev/null
+++ b/run_benchmark_with_profile.sh
@@ -0,0 +1,3 @@
+export VLLM_TORCH_PROFILER_DIR=$PWD/profile
+
+VLLM_USE_V1=1 python3 benchmarks/benchmark_throughput.py --model facebook/opt-125m --dataset_name random  --enforce-eager --max-num-seqs 32 --gpu-memory-util 0.8 --num-prompts 16 --max-model-len 2000 --input-len 1024 --output-len 10 --max-num-batched-tokens 32768  --disable-sliding-window --dtype float16 --profile
\ No newline at end of file
diff --git a/setup.py b/setup.py
index 180f2f978..d44435874 100644
--- a/setup.py
+++ b/setup.py
@@ -147,6 +147,7 @@ class cmake_build_ext(build_ext):
         cmake_args = [
             '-DCMAKE_BUILD_TYPE={}'.format(cfg),
             '-DVLLM_TARGET_DEVICE={}'.format(VLLM_TARGET_DEVICE),
+            "-DCMAKE_CXX_STANDARD=17",
         ]
 
         verbose = envs.VERBOSE
@@ -467,7 +468,7 @@ def _is_xpu() -> bool:
 
 
 def _build_custom_ops() -> bool:
-    return _is_cuda() or _is_hip() or _is_cpu()
+    return _is_cuda() or _is_hip() or _is_cpu() or _is_xpu()
 
 
 def get_rocm_version():
diff --git a/tests/entrypoints/openai/test_video.py b/tests/entrypoints/openai/test_video.py
index 53f057a29..dd0311ede 100644
--- a/tests/entrypoints/openai/test_video.py
+++ b/tests/entrypoints/openai/test_video.py
@@ -49,7 +49,7 @@ async def client(server):
 @pytest.fixture(scope="session")
 def base64_encoded_video() -> dict[str, str]:
     return {
-        video_url: encode_video_base64(fetch_video(video_url))
+        video_url: encode_video_base64(fetch_video(video_url)[0])
         for video_url in TEST_VIDEO_URLS
     }
 
diff --git a/tests/models/multimodal/generation/test_common.py b/tests/models/multimodal/generation/test_common.py
index e4e48f995..ebc1c2ff3 100644
--- a/tests/models/multimodal/generation/test_common.py
+++ b/tests/models/multimodal/generation/test_common.py
@@ -304,6 +304,34 @@ VLM_TEST_SETTINGS = {
         num_logprobs=10,
         marks=[large_gpu_mark(min_gb=32)],
     ),
+    "glm4_1v": VLMTestInfo(
+        models=["THUDM/GLM-4.1V-9B-Thinking"],
+        test_type=(VLMTestType.IMAGE, VLMTestType.MULTI_IMAGE),
+        prompt_formatter=lambda img_prompt: f"<|user|>\n{img_prompt}<|assistant|>",  # noqa: E501
+        img_idx_to_prompt=lambda idx: "<|begin_of_image|><|image|><|end_of_image|>", # noqa: E501
+        video_idx_to_prompt=lambda idx: "<|begin_of_video|><|video|><|end_of_video|>", # noqa: E501
+        max_model_len=2048,
+        max_num_seqs=2,
+        get_stop_token_ids=lambda tok: [151329, 151336, 151338],
+        num_logprobs=10,
+        image_size_factors=[(), (0.25,), (0.25, 0.25, 0.25), (0.25, 0.2, 0.15)],
+        auto_cls=AutoModelForImageTextToText,
+    ),
+    "glm4_1v-video": VLMTestInfo(
+        models=["THUDM/GLM-4.1V-9B-Thinking"],
+        # GLM4.1V require include video metadata for input
+        test_type=VLMTestType.CUSTOM_INPUTS,
+        max_model_len=4096,
+        max_num_seqs=2,
+        auto_cls=AutoModelForImageTextToText,
+        patch_hf_runner=model_utils.glm4_1v_patch_hf_runner,
+        custom_test_opts=[CustomTestOptions(
+            inputs=custom_inputs.video_with_metadata_glm4_1v(),
+            limit_mm_per_prompt={"video": 1},
+        )],
+        # This is needed to run on machine with 24GB VRAM
+        vllm_runner_kwargs={"gpu_memory_utilization": 0.95},
+    ),
     "h2ovl": VLMTestInfo(
         models = [
             "h2oai/h2ovl-mississippi-800m",
diff --git a/tests/models/multimodal/generation/vlm_utils/custom_inputs.py b/tests/models/multimodal/generation/vlm_utils/custom_inputs.py
index cc1045561..1291f84ea 100644
--- a/tests/models/multimodal/generation/vlm_utils/custom_inputs.py
+++ b/tests/models/multimodal/generation/vlm_utils/custom_inputs.py
@@ -128,3 +128,23 @@ def windows_attention_image_qwen2_5_vl():
 
     wrapped_sf = ImageSizeWrapper(type=SizeType.SIZE_FACTOR, data=[0.5])
     return build_single_image_inputs([image], [prompt], wrapped_sf)
+
+
+def video_with_metadata_glm4_1v():
+    video_array = VIDEO_ASSETS[0].np_ndarrays
+    metadata = VIDEO_ASSETS[0].metadata
+    question = "Describe the video."
+    video_prompt = "<|begin_of_video|><|video|><|end_of_video|>"
+    formatted_prompt = f"<|user|>\n{video_prompt}{question}<|assistant|>\n"
+
+    scales = [0.1, 0.2, 0.25]
+    video_input = [[(rescale_video_size(video_array, scale), metadata)]
+                   for scale in scales]
+    prompts = [formatted_prompt] * len(video_input)
+
+    return [
+        PromptWithMultiModalInput(
+            prompts=prompts,
+            video_data=video_input,
+        )
+    ]
diff --git a/tests/models/multimodal/generation/vlm_utils/model_utils.py b/tests/models/multimodal/generation/vlm_utils/model_utils.py
index dc1ea5208..8bb34bbb3 100644
--- a/tests/models/multimodal/generation/vlm_utils/model_utils.py
+++ b/tests/models/multimodal/generation/vlm_utils/model_utils.py
@@ -13,10 +13,12 @@ import regex as re
 import torch
 from PIL.Image import Image
 from transformers import (AutoConfig, AutoTokenizer, BatchFeature,
-                          GenerationConfig)
+                          GenerationConfig, GenerationMixin)
+from transformers.video_utils import VideoMetadata
 
 from vllm.sequence import SampleLogprobs
 from vllm.transformers_utils.tokenizer import patch_padding_side
+from vllm.utils import is_list_of
 
 from .....conftest import HfRunner, ImageAsset, ImageTestAssets
 from .types import RunnerOutput
@@ -361,6 +363,28 @@ def glm4v_patch_hf_runner(hf_model: HfRunner) -> HfRunner:
     return hf_model
 
 
+def glm4_1v_patch_hf_runner(hf_model: HfRunner) -> HfRunner:
+    """Patches and returns an instance of the HfRunner to use for GLM4.1V."""
+    hf_processor = hf_model.processor
+
+    def processor(*args, videos=None, **kwargs):
+        if videos is not None and is_list_of(videos, tuple):
+            # If videos is a list of tuples, we assume each tuple contains
+            # (video_array, metadata) as in the case of GLM4.1V.
+            video_metadata = [[VideoMetadata(**video[1])] for video in videos]
+            videos = [[video[0]] for video in videos]
+        else:
+            video_metadata = None
+
+        return hf_processor(*args,
+                            videos=videos,
+                            video_metadata=video_metadata,
+                            **kwargs)
+
+    hf_model.processor = processor
+    return hf_model
+
+
 def h2ovl_patch_hf_runner(hf_model: HfRunner) -> HfRunner:
     """Patches and returns an instance of the HfRunner to use for H2OVL."""
 
diff --git a/tests/models/multimodal/processing/test_common.py b/tests/models/multimodal/processing/test_common.py
index 572fa366d..3619a0b9b 100644
--- a/tests/models/multimodal/processing/test_common.py
+++ b/tests/models/multimodal/processing/test_common.py
@@ -23,6 +23,22 @@ from ....multimodal.utils import random_audio, random_image, random_video
 from ...registry import HF_EXAMPLE_MODELS
 
 
+def glm4_1v_patch_mm_data(mm_data: MultiModalDataDict) -> MultiModalDataDict:
+    """
+    Patch the multimodal data for GLM4.1V model.
+    """
+    # Ensure video metadata is included
+    if "video" in mm_data:
+        video = mm_data["video"]
+        mm_data["video"] = (video, {
+            "total_num_frames": len(video),
+            "fps": len(video),
+            "duration": 1,
+            "video_backend": "opencv"
+        })
+    return mm_data
+
+
 def _test_processing_correctness(
     model_id: str,
     hit_rate: float,
@@ -153,6 +169,11 @@ _IGNORE_MM_KEYS = {
     "ultravox": {"audio_features"},
 }
 
+MM_DATA_PATCHES = {
+    # GLM4.1V requires video metadata to be included in the input
+    "glm4v": glm4_1v_patch_mm_data,
+}
+
 
 def _test_processing_correctness_one(
     model_config: ModelConfig,
@@ -165,6 +186,8 @@ def _test_processing_correctness_one(
 ):
     model_type = model_config.hf_config.model_type
     ignore_mm_keys = _IGNORE_MM_KEYS.get(model_type, set[str]())
+    if model_type in MM_DATA_PATCHES:
+        mm_data = MM_DATA_PATCHES[model_type](mm_data)
 
     if isinstance(prompt, str):
         text_prompt = prompt
@@ -244,7 +267,8 @@ def _test_processing_correctness_one(
     "adept/fuyu-8b",
     "google/gemma-3-4b-it",
     "THUDM/glm-4v-9b",
-    "ibm-granite/granite-speech-3.3-8b",
+    "THUDM/GLM-4.1V-9B-Thinking",
+    "ibm-granite/granite-speech-3.3-2b",
     "h2oai/h2ovl-mississippi-800m",
     "OpenGVLab/InternVL2-1B",
     "OpenGVLab/InternVL3-1B",
diff --git a/tests/models/registry.py b/tests/models/registry.py
index a49e3ad6b..86dcd79d5 100644
--- a/tests/models/registry.py
+++ b/tests/models/registry.py
@@ -331,6 +331,9 @@ _MULTIMODAL_EXAMPLE_MODELS = {
     "GLM4VForCausalLM": _HfExamplesInfo("THUDM/glm-4v-9b",
                                         trust_remote_code=True,
                                         hf_overrides={"architectures": ["GLM4VForCausalLM"]}),  # noqa: E501
+    "Glm4vForConditionalGeneration": _HfExamplesInfo("zai-org/GLM-4.1V-9B-Thinking"),  # noqa: E501
+    "Glm4vMoeForConditionalGeneration": _HfExamplesInfo("zai-org/GLM-4.5V",
+                                          is_available_online=False),   # noqa: E501
     "H2OVLChatModel": _HfExamplesInfo("h2oai/h2ovl-mississippi-800m",
                                       extras={"2b": "h2oai/h2ovl-mississippi-2b"},  # noqa: E501
                                       max_transformers_version="4.48",  # noqa: E501
diff --git a/tests/multimodal/test_utils.py b/tests/multimodal/test_utils.py
index f1e45da30..c7fb3c2e1 100644
--- a/tests/multimodal/test_utils.py
+++ b/tests/multimodal/test_utils.py
@@ -149,7 +149,9 @@ async def test_fetch_video_http(video_url: str, num_frames: int):
     video_sync = connector.fetch_video(video_url, num_frames=num_frames)
     video_async = await connector.fetch_video_async(video_url,
                                                     num_frames=num_frames)
-    assert np.array_equal(video_sync, video_async)
+    # Check that the video frames are equal and metadata are same
+    assert np.array_equal(video_sync[0], video_async[0])
+    assert video_sync[1] == video_async[1]
 
 
 # Used for the next two tests related to `merge_and_sort_multimodal_metadata`.
diff --git a/tests/quantization/test_cpu_offload.py b/tests/quantization/test_cpu_offload.py
index a05eb494c..82a0e0cd8 100644
--- a/tests/quantization/test_cpu_offload.py
+++ b/tests/quantization/test_cpu_offload.py
@@ -1,4 +1,5 @@
 # SPDX-License-Identifier: Apache-2.0
+# SPDX-FileCopyrightText: Copyright contributors to the vLLM project
 
 # Expanded quantized model tests for CPU offloading
 # Base tests: tests/basic_correctness/test_cpu_offload.py
@@ -10,6 +11,16 @@ from tests.quantization.utils import is_quant_method_supported
 from ..utils import compare_two_settings
 
 
+@pytest.mark.skipif(not is_quant_method_supported("fp8"),
+                    reason="fp8 is not supported on this GPU type.")
+def test_offload_weights_before_quant_fp8():
+    # Test quantization of an unquantized checkpoint
+    compare_two_settings("meta-llama/Llama-3.2-1B-Instruct",
+                         ["--quantization", "fp8"], ["--quantization", "fp8"],
+                         {"VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT": "1"},
+                         max_wait_seconds=480)
+
+
 @pytest.mark.skipif(not is_quant_method_supported("fp8"),
                     reason="fp8 is not supported on this GPU type.")
 def test_cpu_offload_fp8():
diff --git a/tests/quantization/test_ipex_quant.py b/tests/quantization/test_ipex_quant.py
index 0e3913676..7b0953983 100644
--- a/tests/quantization/test_ipex_quant.py
+++ b/tests/quantization/test_ipex_quant.py
@@ -24,7 +24,7 @@ DTYPE = ["bfloat16"]
 @pytest.mark.parametrize("model", MODELS)
 @pytest.mark.parametrize("dtype", DTYPE)
 def test_ipex_quant(vllm_runner, model, dtype):
-    with vllm_runner(model, dtype=dtype) as llm:
+    with vllm_runner(model, dtype=dtype, enforce_eager=True, block_size=64) as llm:
         output = llm.generate_greedy(["The capital of France is"],
                                      max_tokens=32)
     assert output
diff --git a/tests/v1/e2e/test_correctness_sliding_window.py b/tests/v1/e2e/test_correctness_sliding_window.py
index a125d3fb7..5e85a46ae 100644
--- a/tests/v1/e2e/test_correctness_sliding_window.py
+++ b/tests/v1/e2e/test_correctness_sliding_window.py
@@ -25,7 +25,7 @@ model_config = {
     "model",
     [
         "bigcode/starcoder2-3b",  # sliding window only
-        "google/gemma-2-2b-it",  # sliding window + full attention
+        #"google/gemma-2-2b-it",  # sliding window + full attention
     ])
 @pytest.mark.parametrize("batch_size", [5])
 @pytest.mark.parametrize("seed", [1])
@@ -41,7 +41,7 @@ def test_sliding_window_retrival(monkeypatch, model, batch_size, seed):
 
         test_config = model_config[model]
 
-        llm = LLM(model=model)
+        llm = LLM(model=model, enforce_eager=True, block_size=64)
         sampling_params = SamplingParams(temperature=0.0, max_tokens=100)
 
         prompts, answer, indices = prep_prompts(batch_size,
diff --git a/tests/v1/e2e/test_spec_decode.py b/tests/v1/e2e/test_spec_decode.py
index 2fad37d68..c15ea4d2c 100644
--- a/tests/v1/e2e/test_spec_decode.py
+++ b/tests/v1/e2e/test_spec_decode.py
@@ -73,7 +73,7 @@ def test_ngram_correctness(
     with monkeypatch.context() as m:
         m.setenv("VLLM_USE_V1", "1")
 
-        ref_llm = LLM(model=model_name, max_model_len=1024)
+        ref_llm = LLM(model=model_name, enforce_eager=True, block_size=32, dtype="float16")
         ref_outputs = ref_llm.chat(test_prompts, sampling_config)
         del ref_llm
 
@@ -86,11 +86,17 @@ def test_ngram_correctness(
                 "num_speculative_tokens": 3,
             },
             max_model_len=1024,
+            enforce_eager=True,
+            block_size=64,
+            dtype="float16",
+            gpu_memory_utilization=0.6,
         )
         spec_outputs = spec_llm.chat(test_prompts, sampling_config)
         matches = 0
         misses = 0
         for ref_output, spec_output in zip(ref_outputs, spec_outputs):
+            print(ref_output.outputs[0].text)
+            print(spec_output.outputs[0].text)
             if ref_output.outputs[0].text == spec_output.outputs[0].text:
                 matches += 1
             else:
@@ -119,7 +125,7 @@ def test_eagle_correctness(
     with monkeypatch.context() as m:
         m.setenv("VLLM_USE_V1", "1")
 
-        ref_llm = LLM(model=model_name, max_model_len=2048)
+        ref_llm = LLM(model=model_name, max_model_len=2048, enforce_eager=True, block_size=64, dtype="float16")
         ref_outputs = ref_llm.chat(test_prompts, sampling_config)
         del ref_llm
 
@@ -135,6 +141,10 @@ def test_eagle_correctness(
                 "max_model_len": 2048,
             },
             max_model_len=2048,
+            enforce_eager=True,
+            block_size=64,
+            dtype="float16",
+            gpu_memory_utilization=0.6,
         )
         spec_outputs = spec_llm.chat(test_prompts, sampling_config)
         matches = 0
diff --git a/tests/v1/engine/test_fast_incdec_prefix_err.py b/tests/v1/engine/test_fast_incdec_prefix_err.py
new file mode 100644
index 000000000..5c844e0e7
--- /dev/null
+++ b/tests/v1/engine/test_fast_incdec_prefix_err.py
@@ -0,0 +1,80 @@
+# SPDX-License-Identifier: Apache-2.0
+# SPDX-FileCopyrightText: Copyright contributors to the vLLM project
+
+from transformers import AutoTokenizer
+
+from vllm.sampling_params import SamplingParams
+from vllm.v1.engine import EngineCoreRequest
+from vllm.v1.engine.detokenizer import IncrementalDetokenizer
+
+# ruff: noqa: E501
+
+
+def test_fast_inc_detok_invalid_utf8_err_case():
+    """
+    Test edge case where tokenizer can produce non-monotonic,
+    invalid UTF-8 output, which breaks the internal state of
+    tokenizers' DecodeStream.
+    See https://github.com/vllm-project/vllm/issues/17448.
+
+    Thanks to reproducer from @fpaupier:
+    https://gist.github.com/fpaupier/0ed1375bd7633c5be6c894b1c7ac1be3.
+    """
+    tokenizer = AutoTokenizer.from_pretrained("google/gemma-3-1b-it")
+
+    # Create a test request
+    prompt_token_ids = [107, 4606, 236787, 107]
+    params = SamplingParams(skip_special_tokens=True)
+    request = EngineCoreRequest(
+        "test",
+        prompt_token_ids,
+        None,
+        None,
+        None,
+        params,
+        None,
+        0.0,
+        None,
+        cache_salt=None,
+        data_parallel_rank=None,
+    )
+
+    detokenizer = IncrementalDetokenizer.from_new_request(tokenizer, request)
+
+    assert detokenizer.__class__.__name__ == "FastIncrementalDetokenizer", \
+        "Should use FastIncrementalDetokenizer by default"
+
+    # Process tokens incrementally
+    test_tokens = [
+        236840, 107, 138, 236782, 107, 140, 236775, 6265, 1083, 623, 121908,
+        147418, 827, 107, 140, 236775, 6265, 236779, 2084, 1083, 623, 203292,
+        827, 107, 140, 236775, 6265, 236779, 7777, 1083, 623, 121908, 147418,
+        569, 537, 236789, 65880, 569, 537, 236789, 62580, 853, 115693, 210118,
+        35178, 16055, 1270, 759, 215817, 4758, 1925, 1117, 827, 107, 140,
+        236775, 5654, 1083, 623, 110733, 46291, 827, 107, 140, 236775, 5654,
+        236779, 2084, 1083, 623, 136955, 56731, 827, 107, 140, 236775, 5654,
+        236779, 7777, 1083, 623, 194776, 2947, 496, 109811, 1608, 890, 215817,
+        4758, 1925, 1117, 2789, 432, 398, 602, 31118, 569, 124866, 134772, 509,
+        19478, 1640, 33779, 236743, 236770, 236819, 236825, 236771, 432, 398,
+        432, 237167, 827, 107, 140, 236775, 77984, 1083, 623, 2709, 236745,
+        2555, 513, 236789, 602, 31118, 569
+    ]
+
+    output = ""
+    for i, token_id in enumerate(test_tokens):
+        detokenizer.update([token_id], False)
+
+        finished = i == len(test_tokens) - 1
+        output += detokenizer.get_next_output_text(finished, delta=True)
+
+
+# fmt: off
+    assert output == r'''[
+  {
+    "source": "Résultats",
+    "source_type": "CONCEPT",
+    "source_description": "Résultats de l'analyse de l'impact des opérations israéliennes sur la frontière libanaise",
+    "target": "Israël",
+    "target_type": "ORGANIZATION",
+    "target_description": "Pays qui a obtenu à sa frontière libanaise « un niveau de calme inédit depuis les années 1960 »",
+    "relationship": "Obtention d'un niveau de'''
diff --git a/vllm/_custom_ops.py b/vllm/_custom_ops.py
index 3c8e6b95c..b42ff5de4 100644
--- a/vllm/_custom_ops.py
+++ b/vllm/_custom_ops.py
@@ -14,7 +14,8 @@ from vllm.scalar_type import ScalarType
 
 logger = init_logger(__name__)
 
-if not current_platform.is_tpu() and not current_platform.is_hpu():
+if not current_platform.is_tpu() and not current_platform.is_hpu()\
+        and not current_platform.is_xpu():
     try:
         import vllm._C
     except ImportError as e:
@@ -1184,7 +1185,10 @@ def scaled_fp8_quant(
                 output, input, scale, scale_ub)
         else:
             scale = torch.zeros(1, device=input.device, dtype=torch.float32)
-            torch.ops._C.dynamic_scaled_fp8_quant(output, input, scale)
+            if current_platform.is_xpu():
+                torch.ops.torch_ipex.dynamic_scaled_fp8_quant(output, input, scale)
+            else:
+                torch.ops._C.dynamic_scaled_fp8_quant(output, input, scale)
     else:
         # num_token_padding not implemented for this case
         assert (scale.numel() == 1 or num_token_padding is None)
diff --git a/vllm/_ipex_ops.py b/vllm/_ipex_ops.py
index a9a624b85..a445f3694 100644
--- a/vllm/_ipex_ops.py
+++ b/vllm/_ipex_ops.py
@@ -13,6 +13,49 @@ try:
 except ImportError as e:
     logger.warning("Import error msg: %s", e.msg)
 
+from vllm.utils import direct_register_custom_op
+import vllm._C.ops
+
+
+def ipex_fp8_gemm(
+    a: torch.Tensor,
+    trans_a: bool,
+    b: torch.Tensor,
+    trans_b: bool,
+    d: Optional[torch.Tensor],
+    dtype: torch.dtype,
+    a_scale_inv: Optional[torch.Tensor],
+    b_scale_inv: Optional[torch.Tensor],
+    bias: Optional[torch.Tensor],
+    acc: bool,
+) -> torch.Tensor:
+    return torch.ops.torch_ipex.fp8_gemm2(a, trans_a, b, trans_b, d, dtype,
+                                          a_scale_inv, b_scale_inv, bias, acc)
+
+
+def ipex_fp8_gemm_fake_(
+    a: torch.Tensor,
+    trans_a: bool,
+    b: torch.Tensor,
+    trans_b: bool,
+    d: Optional[torch.Tensor],
+    dtype: torch.dtype,
+    a_scale_inv: Optional[torch.Tensor],
+    b_scale_inv: Optional[torch.Tensor],
+    bias: Optional[torch.Tensor],
+    acc: bool,
+) -> torch.Tensor:
+    dim_0 = a.size(1) if trans_a else a.size(0)
+    dim_1 = b.size(0) if trans_b else b.size(1)
+    return torch.zeros((dim_0, dim_1), device=a.device, dtype=a.dtype)
+
+
+direct_register_custom_op(op_name="fp8_gemm",
+                          op_func=ipex_fp8_gemm,
+                          mutates_args=[],
+                          fake_impl=ipex_fp8_gemm_fake_,
+                          dispatch_key="XPU")
+
 
 class ipex_ops:
 
@@ -76,7 +119,8 @@ class ipex_ops:
         assert kv_cache_dtype == "auto"
         num_heads = out.size(1)
         num_queries_per_tokens = num_heads // num_kv_heads
-        ipex.llm.modules.PagedAttention.single_query_kv_attention(
+        #ipex.llm.modules.PagedAttention.single_query_kv_attention(
+        torch.xpu.paged_attention_v1(
             out,
             query.contiguous(),
             key_cache.view_as(value_cache),
@@ -141,10 +185,12 @@ class ipex_ops:
         cos_sin_cache: torch.Tensor,  # [cos_sin_dim, rot_dim]
         is_neox: bool,
     ) -> None:
-        rot_dim = cos_sin_cache.size(1)
-        ipex.llm.functional.rotary_embedding_batched(positions, query, key,
-                                                     head_size, cos_sin_cache,
-                                                     is_neox, rot_dim)
+        # rot_dim = cos_sin_cache.size(1)
+        # ipex.llm.functional.rotary_embedding_batched(positions, query, key,
+        #                                              head_size, cos_sin_cache,
+        #                                              is_neox, rot_dim)
+        import vllm._C.ops
+        vllm._C.ops.rotary_embedding(positions, query, key, head_size, cos_sin_cache, is_neox)
 
     @staticmethod
     def batched_rotary_embedding(positions: torch.Tensor, query: torch.Tensor,
@@ -160,14 +206,18 @@ class ipex_ops:
     @staticmethod
     def rms_norm(input: torch.Tensor, weight: torch.Tensor,
                  epsilon: float) -> torch.Tensor:
-        return ipex.llm.functional.rms_norm(input, weight, epsilon)
+        # return ipex.llm.functional.rms_norm(input, weight, epsilon)
+        tmp = torch.empty_like(input)
+        vllm._C.ops.rms_norm(tmp, input, weight, epsilon)
+        return tmp
 
     @staticmethod
     def fused_add_rms_norm(input: torch.Tensor, residual: torch.Tensor,
                            weight: torch.Tensor, epsilon: float) -> None:
-        tmp = ipex.llm.functional.add_rms_norm(residual, input, weight, None,
-                                               epsilon, True)
-        input.copy_(tmp)
+        # tmp = ipex.llm.functional.add_rms_norm(residual, input, weight, None,
+        #                                        epsilon, True)
+        # input.copy_(tmp)
+        vllm._C.ops.fused_add_rms_norm(input, residual, weight, epsilon)
 
     @staticmethod
     def varlen_attention(
@@ -227,6 +277,64 @@ class ipex_ops:
         ipex.llm.modules.PagedAttention.reshape_and_cache(
             key, value, key_cache, value_cache, slot_mapping)
 
+    @staticmethod
+    def reshape_and_cache_flash(
+        key: torch.Tensor,
+        value: torch.Tensor,
+        key_cache: torch.Tensor,
+        value_cache: torch.Tensor,
+        slot_mapping: torch.Tensor,
+        kv_cache_dtype: str,
+        k_scale: float,
+        v_scale: float,
+    ) -> None:
+        ipex.llm.modules.PagedAttention.reshape_and_cache_flash(
+            key, value, key_cache, value_cache, slot_mapping, kv_cache_dtype,
+            k_scale, v_scale)
+
+    @staticmethod
+    def chunked_prefill(
+        query: torch.Tensor,
+        key_cache: torch.Tensor,
+        value_cache: torch.Tensor,
+        output: torch.Tensor,
+        cu_seqlens_q: torch.Tensor,
+        cu_seqlens_k: torch.Tensor,
+        seq_used_k: Optional[torch.Tensor],
+        block_table: torch.Tensor,
+        alibi_slopes: Optional[torch.Tensor],
+        max_seqlen_q: int,
+        max_seqlen_k: int,
+        p_dropout: float,
+        softmax_scale: float,
+        zero_tensors: bool,
+        window_size_left: int,
+        window_size_right: int,
+        is_causal: bool,
+        return_softmax: bool,
+        gen_: Optional[torch.Generator],
+        kv_cache_dtype: str,
+    ):
+        return ipex.llm.modules.PagedAttention.flash_attn_varlen_func(
+            output,
+            query.contiguous(),
+            key_cache,
+            value_cache,
+            cu_seqlens_q,
+            cu_seqlens_k,
+            max_seqlen_q,
+            max_seqlen_k,
+            softmax_scale,
+            is_causal,
+            block_table,
+            alibi_slopes,
+            kv_cache_dtype=kv_cache_dtype,  # "fp8"
+            window_size_left=window_size_left,
+            window_size_right=window_size_right,
+            k_scale=1.0,
+            v_scale=1.0,
+        )
+
     @staticmethod
     def copy_blocks(key_caches: list[torch.Tensor],
                     value_caches: list[torch.Tensor],
@@ -241,3 +349,125 @@ class ipex_ops:
     def swap_blocks(src: torch.Tensor, dst: torch.Tensor,
                     block_mapping: torch.Tensor) -> None:
         torch.xpu.swap_blocks(src, dst, block_mapping)  # type: ignore
+
+    @staticmethod
+    def bgmv_shrink(inputs: torch.Tensor,
+                    lora_a_weights: torch.Tensor,
+                    output_tensor: torch.Tensor,
+                    lora_indices_tensor: torch.Tensor,
+                    scaling: float = 1.0) -> None:
+        ipex.llm.functional.bgmv_shrink(inputs, lora_a_weights, output_tensor,
+                                        lora_indices_tensor, scaling)
+
+    @staticmethod
+    def bgmv_expand(inputs: torch.Tensor,
+                    lora_b_weights: torch.Tensor,
+                    output_tensor: torch.Tensor,
+                    lora_indices_tensor: torch.Tensor,
+                    add_inputs: bool = True) -> None:
+        ipex.llm.functional.bgmv_expand(inputs, lora_b_weights, output_tensor,
+                                        lora_indices_tensor, add_inputs)
+
+    @staticmethod
+    def bgmv_expand_slice(inputs: torch.Tensor,
+                          lora_b_weights: torch.Tensor,
+                          output_tensor: torch.Tensor,
+                          lora_indices_tensor: torch.Tensor,
+                          slice_offset: int,
+                          slice_size: int,
+                          add_inputs: bool = True) -> None:
+        ipex.llm.functional.bgmv_expand_slice(inputs, lora_b_weights,
+                                              output_tensor,
+                                              lora_indices_tensor,
+                                              slice_offset, slice_size,
+                                              add_inputs)
+
+    @staticmethod
+    def sgmv_shrink(inputs: torch.Tensor,
+                    lora_a_weights: torch.Tensor,
+                    output_tensor: torch.Tensor,
+                    b_seq_start_loc: torch.Tensor,
+                    seq_len_tensor: torch.Tensor,
+                    lora_indices_tensor: torch.Tensor,
+                    batches: int,
+                    max_seq_length: int,
+                    token_nums: int,
+                    scaling: float = 1.0) -> None:
+        assert inputs.size(0) == token_nums
+        ipex.llm.functional.sgmv_shrink(inputs, lora_a_weights, output_tensor,
+                                        b_seq_start_loc, seq_len_tensor,
+                                        lora_indices_tensor, batches,
+                                        max_seq_length, scaling)
+
+    @staticmethod
+    def sgmv_expand(inputs: torch.Tensor,
+                    lora_b_weights: torch.Tensor,
+                    output_tensor: torch.Tensor,
+                    b_seq_start_loc: torch.Tensor,
+                    seq_len_tensor: torch.Tensor,
+                    lora_indices_tensor: torch.Tensor,
+                    batches: int,
+                    max_seq_length: int,
+                    token_nums: int,
+                    add_inputs: bool = False) -> None:
+        assert inputs.size(0) == token_nums
+        ipex.llm.functional.sgmv_expand(inputs, lora_b_weights, output_tensor,
+                                        b_seq_start_loc, seq_len_tensor,
+                                        lora_indices_tensor, batches,
+                                        max_seq_length, add_inputs)
+
+    @staticmethod
+    def sgmv_expand_slice(inputs: torch.Tensor,
+                          lora_b_weights: torch.Tensor,
+                          output_tensor: torch.Tensor,
+                          b_seq_start_loc: torch.Tensor,
+                          seq_len_tensor: torch.Tensor,
+                          lora_indices_tensor: torch.Tensor,
+                          batches: int,
+                          max_seq_length: int,
+                          token_nums: int,
+                          slice_offset: int,
+                          slice_size: int,
+                          add_inputs: bool = False) -> None:
+        assert inputs.size(0) == token_nums
+        ipex.llm.functional.sgmv_expand_slice(inputs, lora_b_weights,
+                                              output_tensor, b_seq_start_loc,
+                                              seq_len_tensor,
+                                              lora_indices_tensor, batches,
+                                              max_seq_length, slice_offset,
+                                              slice_size, add_inputs)
+
+    # @staticmethod
+    # def lora_expand(inputs: torch.Tensor,
+    #                 lora_b_weights: List[torch.Tensor],
+    #                 output_tensor: torch.Tensor,
+    #                 token_lora_mapping: torch.Tensor,
+    #                 token_indices_sorted_by_lora_ids: torch.Tensor,
+    #                 num_tokens_per_lora: torch.Tensor,
+    #                 lora_token_start_loc: torch.Tensor,
+    #                 lora_ids: torch.Tensor,
+    #                 offset_start: int = 0,
+    #                 add_inputs: bool = False) -> None:
+    #     ipex.llm.functional.lora_expand(inputs, lora_b_weights,
+    #                                     output_tensor, token_lora_mapping,
+    #                                     token_indices_sorted_by_lora_ids,
+    #                                     num_tokens_per_lora, num_tokens_per_lora,
+    #                                     lora_token_start_loc, lora_ids,
+    #                                     offset_start, add_inputs)
+
+    # @staticmethod
+    # def lora_shrink(inputs: torch.Tensor,
+    #                 lora_a_weights: List[torch.Tensor],
+    #                 output_tensor: torch.Tensor,
+    #                 token_lora_mapping: torch.Tensor,
+    #                 token_indices_sorted_by_lora_ids: torch.Tensor,
+    #                 num_tokens_per_lora: torch.Tensor,
+    #                 lora_token_start_loc: torch.Tensor,
+    #                 lora_ids: torch.Tensor,
+    #                 scaling: float) -> None:
+    #     ipex.llm.functional.lora_shrink(inputs, lora_a_weights,
+    #                                     output_tensor, token_lora_mapping,
+    #                                     token_indices_sorted_by_lora_ids,
+    #                                     num_tokens_per_lora, num_tokens_per_lora,
+    #                                     lora_token_start_loc, lora_ids,
+    #                                     scaling)
diff --git a/vllm/assets/video.py b/vllm/assets/video.py
index bf06746a9..0316de407 100644
--- a/vllm/assets/video.py
+++ b/vllm/assets/video.py
@@ -2,7 +2,7 @@
 
 from dataclasses import dataclass
 from functools import lru_cache
-from typing import ClassVar, Literal, Optional
+from typing import Any, ClassVar, Literal, Optional
 
 import cv2
 import numpy as np
@@ -76,6 +76,24 @@ def video_to_pil_images_list(path: str,
     ]
 
 
+def video_get_metadata(path: str) -> dict[str, Any]:
+    cap = cv2.VideoCapture(path)
+    if not cap.isOpened():
+        raise ValueError(f"Could not open video file {path}")
+
+    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
+    fps = cap.get(cv2.CAP_PROP_FPS)
+    duration = total_frames / fps if fps > 0 else 0
+
+    metadata = {
+        "total_num_frames": total_frames,
+        "fps": fps,
+        "duration": duration,
+        "video_backend": "opencv"
+    }
+    return metadata
+
+
 VideoAssetName = Literal["baby_reading"]
 
 
@@ -104,6 +122,12 @@ class VideoAsset:
         ret = video_to_ndarrays(video_path, self.num_frames)
         return ret
 
+    @property
+    def metadata(self) -> dict[str, Any]:
+        video_path = download_video_asset(self.filename)
+        ret = video_get_metadata(video_path)
+        return ret
+
     def get_audio(self, sampling_rate: Optional[float] = None) -> npt.NDArray:
         """
         Read audio data from the video asset, used in Qwen2.5-Omni examples.
diff --git a/vllm/attention/backends/ipex_attn.py b/vllm/attention/backends/ipex_attn.py
index f322c7b3d..5377d22cf 100644
--- a/vllm/attention/backends/ipex_attn.py
+++ b/vllm/attention/backends/ipex_attn.py
@@ -19,7 +19,11 @@ from vllm.logger import init_logger
 logger = init_logger(__name__)
 
 _PARTITION_SIZE = 512
-
+_IPEX_SUPPORTED_ATTENTION_TYPE = [AttentionType.DECODER,
+                                  AttentionType.ENCODER_ONLY,
+                                  AttentionType.ENCODER,
+                                  AttentionType.ENCODER_DECODER
+                                  ]
 
 class IpexAttnBackend(AttentionBackend):
 
@@ -80,6 +84,25 @@ class IpexAttnMetadata(AttentionMetadata, PagedAttentionMetadata):
     seq_lens: Optional[List[int]]
     seqlen_q: Optional[torch.Tensor]
     max_seqlen: Optional[int]
+    # Begin encoder attn & enc/dec cross-attn fields...
+
+    # Encoder sequence lengths representation
+    encoder_seq_lens: Optional[List[int]] = None
+    encoder_seq_lens_tensor: Optional[torch.Tensor] = None
+    # (batch_size + 1,). The cumulative sequence lengths of the sequences in
+    # the batch, used to index into sequence. E.g., if the sequence length is
+    # [4, 6], it is [0, 4, 10].
+    encoder_seq_start_loc: Optional[torch.Tensor] = None
+    seq_start_loc: Optional[torch.Tensor] = None
+    # Maximum sequence length among encoder sequences
+    max_encoder_seq_len: Optional[int] = None
+    # Number of tokens input to encoder
+    num_encoder_tokens: Optional[int] = None
+
+    # Cross-attention memory-mapping data structures: slot mapping
+    # and block tables
+    cross_slot_mapping: Optional[torch.Tensor] = None
+    cross_block_tables: Optional[torch.Tensor] = None
 
     def __post_init__(self):
         # Set during the execution of the first attention op.
@@ -88,6 +111,8 @@ class IpexAttnMetadata(AttentionMetadata, PagedAttentionMetadata):
         # from xformer API.
         # will not appear in the __repr__ and __init__
         self.attn_bias: Optional[List[torch.Tensor]] = None
+        self.encoder_attn_bias: Optional[List[torch.Tensor]] = None
+        self.cross_attn_bias: Optional[List[torch.Tensor]] = None
 
     @property
     def prefill_metadata(self) -> Optional["IpexAttnMetadata"]:
@@ -108,6 +133,155 @@ class IpexAttnMetadata(AttentionMetadata, PagedAttentionMetadata):
         return self
 
 
+    @property
+    def is_all_encoder_attn_metadata_set(self):
+        '''
+        All attention metadata required for encoder attention is set.
+        '''
+        return ((self.encoder_seq_lens is not None)
+                and (self.encoder_seq_lens_tensor is not None)
+                and (self.max_encoder_seq_len is not None))
+
+
+    @property
+    def is_all_cross_attn_metadata_set(self):
+        '''
+        All attention metadata required for enc/dec cross-attention is set.
+        Superset of encoder attention required metadata.
+        '''
+        return (self.is_all_encoder_attn_metadata_set
+                and (self.cross_slot_mapping is not None)
+                and (self.cross_block_tables is not None))
+
+
+    def get_attn_bias(
+        self,
+        attn_type: str,
+    ) -> Optional[List[torch.Tensor]]:
+        '''
+        Extract appropriate attention bias from attention metadata
+        according to attention type.
+        Arguments:
+        * attn_metadata: Attention metadata structure associated with attention
+        * attn_type: encoder attention, decoder self-attention,
+                    encoder/decoder cross-attention
+        Returns:
+        * Appropriate attention bias value given the attention type
+        '''
+
+        if (attn_type == AttentionType.DECODER
+                or attn_type == AttentionType.ENCODER_ONLY):
+            return self.attn_bias
+        elif attn_type == AttentionType.ENCODER:
+            return self.encoder_attn_bias
+        elif attn_type == AttentionType.ENCODER_DECODER:
+            return self.cross_attn_bias
+        else:
+            raise AttributeError(f"Invalid attention type {str(attn_type)}")
+
+
+    def set_attn_bias(
+        self,
+        attn_bias: List[torch.Tensor],
+        attn_type: str,
+    ) -> None:
+        '''
+        Update appropriate attention bias field of attention metadata,
+        according to attention type.
+        Arguments:
+        * attn_metadata: Attention metadata structure associated with attention
+        * attn_bias: The desired attention bias value
+        * attn_type: encoder attention, decoder self-attention,
+                    encoder/decoder cross-attention
+        '''
+
+        if (attn_type == AttentionType.DECODER
+                or attn_type == AttentionType.ENCODER_ONLY):
+            self.attn_bias = attn_bias
+        elif attn_type == AttentionType.ENCODER:
+            self.encoder_attn_bias = attn_bias
+        elif attn_type == AttentionType.ENCODER_DECODER:
+            self.cross_attn_bias = attn_bias
+        else:
+            raise AttributeError(f"Invalid attention type {str(attn_type)}")
+
+
+    def get_seq_lens(
+        self,
+        attn_type: str,
+    ):
+        '''
+        Extract appropriate sequence lengths from attention metadata
+        according to attention type.
+
+        Arguments:
+
+        * attn_metadata: Attention metadata structure associated with attention
+        * attn_type: encoder attention, decoder self-attention,
+                    encoder/decoder cross-attention
+
+        Returns:
+        * Appropriate sequence lengths tensor for query
+        * Appropriate sequence lengths tensor for key & value
+        '''
+
+        if (attn_type == AttentionType.DECODER
+                or attn_type == AttentionType.ENCODER_ONLY):
+            seq_lens_q = self.seq_lens
+            seq_lens_kv = self.seq_lens
+        elif attn_type == AttentionType.ENCODER:
+            seq_lens_q = self.encoder_seq_lens
+            seq_lens_kv = self.encoder_seq_lens
+        elif attn_type == AttentionType.ENCODER_DECODER:
+            seq_lens_q = self.seq_lens
+            seq_lens_kv = self.encoder_seq_lens
+        else:
+            raise AttributeError(f"Invalid attention type {str(attn_type)}")
+        return seq_lens_q, seq_lens_kv
+
+
+    def get_seq_len_block_table_args(
+        self,
+        attn_type: str,
+    ) -> tuple:
+        '''
+        The particular choice of sequence-length- and block-table-related
+        attributes which should be extracted from attn_metadata is dependent
+        on the type of attention operation.
+        Decoder attn -> select entirely decoder self-attention-related fields
+        Encoder/decoder cross-attn -> select encoder sequence lengths &
+                                    cross-attn block-tables fields
+        Encoder attn -> select encoder sequence lengths fields & no block tables
+        Arguments:
+        * attn_metadata: Attention metadata structure associated with attention
+        * is_prompt: True if prefill, False otherwise
+        * attn_type: encoder attention, decoder self-attention,
+                    encoder/decoder cross-attention
+        Returns:
+        * Appropriate sequence-lengths tensor
+        * Appropriate max sequence-length scalar
+        * Appropriate block tables (or None)
+        '''
+
+        if (attn_type == AttentionType.DECODER
+                or attn_type == AttentionType.ENCODER_ONLY):
+            # Decoder self-attention
+            # Choose max_seq_len based on whether we are in prompt_run
+            return (self.seq_lens_tensor, self.max_decode_seq_len,
+                    self.block_tables)
+        elif attn_type == AttentionType.ENCODER_DECODER:
+            # Enc/dec cross-attention KVs match encoder sequence length;
+            # cross-attention utilizes special "cross" block tables
+            return (self.encoder_seq_lens_tensor, self.max_encoder_seq_len,
+                    self.cross_block_tables)
+        elif attn_type == AttentionType.ENCODER:
+            # No block tables associated with encoder attention
+            return (self.encoder_seq_lens_tensor, self.max_encoder_seq_len,
+                    None)
+        else:
+            raise AttributeError(f"Invalid attention type {str(attn_type)}")
+
+
 class IpexAttnBackendImpl(AttentionImpl[IpexAttnMetadata]):
 
     def __init__(
@@ -143,7 +317,9 @@ class IpexAttnBackendImpl(AttentionImpl[IpexAttnMetadata]):
 
         assert self.num_heads % self.num_kv_heads == 0
         self.num_queries_per_kv = self.num_heads // self.num_kv_heads
-        self.need_mask = (self.sliding_window is not None)
+        self.need_mask = (self.sliding_window is not None or
+                          self.alibi_slopes is not None)
+        self.attn_type = attn_type
         if logits_soft_cap is None:
             logits_soft_cap = -1
         self.logits_soft_cap = logits_soft_cap
@@ -157,11 +333,9 @@ class IpexAttnBackendImpl(AttentionImpl[IpexAttnMetadata]):
             raise NotImplementedError(
                 "IPEX backend does not support FP8 KV cache. "
                 "Please use xFormers backend instead.")
-        if attn_type != AttentionType.DECODER:
-            raise NotImplementedError("Encoder self-attention and "
-                                      "encoder/decoder cross-attention "
-                                      "are not implemented for "
-                                      "IpexAttnBackendImpl")
+        if attn_type not in _IPEX_SUPPORTED_ATTENTION_TYPE:
+            raise NotImplementedError("Current attn type {attn_type} " \
+                                      "is not implemented for IpexAttnBackendImpl")
 
     def split_kv_cache(
         self,
@@ -203,25 +377,66 @@ class IpexAttnBackendImpl(AttentionImpl[IpexAttnMetadata]):
             shape = [num_tokens, num_heads * head_size]
         """
         assert layer._k_scale_float == 1.0 and layer._v_scale_float == 1.0
+        attn_type = self.attn_type
         num_tokens, hidden_size = query.shape
         # Reshape the query, key, and value tensors.
         query = query.view(-1, self.num_heads, self.head_size)
-        key = key.view(-1, self.num_kv_heads, self.head_size)
-        value = value.view(-1, self.num_kv_heads, self.head_size)
+        if key is not None:
+            assert value is not None
+            key = key.view(-1, self.num_kv_heads, self.head_size)
+            value = value.view(-1, self.num_kv_heads, self.head_size)
 
-        if kv_cache.numel() > 0:
+        if kv_cache.numel() > 0 and attn_type != AttentionType.ENCODER:
             key_cache, value_cache = self.split_kv_cache(
                 kv_cache, self.num_kv_heads, self.head_size)
-            ipex_ops.reshape_and_cache(
-                key,
-                value,
-                key_cache,
-                value_cache,
-                attn_metadata.slot_mapping.flatten(),
-                self.kv_cache_dtype,
-                layer._k_scale_float,
-                layer._v_scale_float,
-            )
+
+            if (key is not None) and (value is not None):
+                if attn_type == AttentionType.ENCODER_DECODER:
+                    # Update cross-attention KV cache (prefill-only)
+                    # During cross-attention decode, key & value will be None,
+                    # preventing this IF-statement branch from running
+                    updated_slot_mapping = attn_metadata.cross_slot_mapping
+                else:
+                    # Update self-attention KV cache (prefill/decode)
+                    updated_slot_mapping = attn_metadata.slot_mapping
+                ipex_ops.reshape_and_cache(
+                    key,
+                    value,
+                    key_cache,
+                    value_cache,
+                    updated_slot_mapping.flatten(),
+                    self.kv_cache_dtype,
+                    layer._k_scale,
+                    layer._v_scale,
+                )
+
+        if attn_type != AttentionType.ENCODER:
+            # Decoder self-attention supports chunked prefill.
+            # Encoder/decoder cross-attention requires no chunked
+            # prefill (100% prefill or 100% decode tokens, no mix)
+            num_prefill_tokens = attn_metadata.num_prefill_tokens
+            num_decode_tokens = attn_metadata.num_decode_tokens
+        else:
+            # Encoder attention - chunked prefill is not applicable;
+            # derive token-count from query shape & and treat them
+            # as 100% prefill tokens
+            assert attn_metadata.num_encoder_tokens is not None
+            num_prefill_tokens = attn_metadata.num_encoder_tokens
+            num_decode_tokens = 0
+
+
+        if attn_type == AttentionType.DECODER:
+            # Only enforce this shape-constraint for decoder
+            # self-attention
+            assert key.shape[0] == num_prefill_tokens + num_decode_tokens
+            assert value.shape[0] == num_prefill_tokens + num_decode_tokens
+
+        output = torch.empty_like(query)
+        decode_query = query[num_prefill_tokens:]
+
+        is_causal = not self.need_mask
+        if self.attn_type == AttentionType.ENCODER_ONLY:
+            is_causal = False
 
         if attn_metadata.is_prompt:
             assert attn_metadata.seq_lens is not None
@@ -232,34 +447,51 @@ class IpexAttnBackendImpl(AttentionImpl[IpexAttnMetadata]):
                     value = value.repeat_interleave(self.num_queries_per_kv,
                                                     dim=1)
 
-                if attn_metadata.attn_bias is None:
-                    if self.sliding_window is not None:
-                        att_masks = _make_sliding_window_bias(
+                attn_masks = attn_metadata.get_attn_bias(attn_type)
+                if attn_masks is None:
+                    if self.alibi_slopes is not None:
+                        attn_masks = _make_alibi_bias(
+                            self.alibi_slopes, query.dtype,
+                            attn_metadata.seq_lens)  # type: ignore
+                    elif self.sliding_window is not None:
+                        assert attn_metadata.seq_lens is not None
+                        attn_masks = _make_sliding_window_bias(
                             attn_metadata.seq_lens, self.sliding_window,
                             query.dtype)  # type: ignore
                     else:
-                        att_masks = _make_sliding_window_bias(
-                            attn_metadata.seq_lens, None, dtype=query.dtype)
-                    attn_metadata.attn_bias = att_masks
-
-                output = torch.empty(
-                    (num_tokens, self.num_heads, self.head_size),
-                    dtype=query.dtype,
-                    device=query.device)
+                        seq_lens, _ = attn_metadata.get_seq_lens(attn_type)
+                        attn_masks = [None] * len(seq_lens)
+                    attn_metadata.set_attn_bias(attn_masks, attn_type)
+
+                is_causal = (attn_type == AttentionType.DECODER)
+                seq_lens_q, seq_lens_kv = attn_metadata.get_seq_lens(attn_type)
+                tmp = [0]
+                tmp.extend(seq_lens_q)
+                seqlen = torch.tensor(tmp)
+                seqlen_q = torch.cumsum(seqlen, dim=0).to(device=query.device)
+                max_seqlen_q = (seqlen_q[1:] - seqlen_q[:-1]).max().item()
+
+                tmp = [0]
+                tmp.extend(seq_lens_kv)
+                seqlen = torch.tensor(tmp)
+                seqlen_kv = torch.cumsum(seqlen, dim=0).to(device=query.device)
+                max_seqlen_kv = (seqlen_kv[1:] - seqlen_kv[:-1]).max().item()
                 ipex_ops.varlen_attention(
                     query,
                     key,
                     value,
                     output,
-                    attn_metadata.seqlen_q,
-                    attn_metadata.seqlen_q,
+                    # seq_lens_q,
+                    # seq_lens_kv,
+                    seqlen_q,
+                    seqlen_kv,
                     self.alibi_slopes,
-                    attn_metadata.max_seqlen,
-                    attn_metadata.max_seqlen,
+                    max_seqlen_q,
+                    max_seqlen_kv,
                     pdropout=0.0,
                     softmax_scale=self.scale,
                     zero_tensors=False,
-                    is_causal=True,
+                    is_causal=is_causal,
                     return_softmax=False,
                     gen_=None,
                     window_size_left=-1,
@@ -273,8 +505,16 @@ class IpexAttnBackendImpl(AttentionImpl[IpexAttnMetadata]):
 
         else:
             # Decoding run.
+            assert attn_type != AttentionType.ENCODER_ONLY, (
+                "Encoder-only models should not have decode metadata.")
             max_seq_len = attn_metadata.max_decode_seq_len
-            output = torch.empty_like(query)
+            (
+                seq_lens_arg,
+                max_seq_len_arg,
+                block_tables_arg,
+            ) = attn_metadata.get_seq_len_block_table_args(attn_type)
+            #output = torch.empty_like(query)
+            out = torch.empty_like(decode_query)
             block_size = value_cache.shape[3]
             num_seqs, num_heads, head_size = query.shape
             max_num_partitions = ((max_seq_len + _PARTITION_SIZE - 1) //
@@ -292,20 +532,21 @@ class IpexAttnBackendImpl(AttentionImpl[IpexAttnMetadata]):
             if use_v1:
                 # Run PagedAttention V1.
                 ipex_ops.paged_attention_v1(
-                    output,
-                    query,
+                    out,
+                    decode_query,
                     key_cache,
                     value_cache,
                     self.num_kv_heads,
                     self.scale,
-                    attn_metadata.block_tables,
-                    attn_metadata.seq_lens_tensor,
+                    block_tables_arg,
+                    seq_lens_arg,
                     block_size,
-                    max_seq_len,
+                    max_seq_len_arg,
                     self.alibi_slopes,
                     self.kv_cache_dtype,
                     layer._k_scale_float,
                     layer._v_scale_float,
+                    self.logits_soft_cap,
                 )
             else:
                 # Run PagedAttention V2.
@@ -342,7 +583,9 @@ class IpexAttnBackendImpl(AttentionImpl[IpexAttnMetadata]):
                 )
 
             # Reshape the output tensor.
-        return output.view(-1, self.num_heads * self.head_size)
+            output[num_prefill_tokens:] = out
+        output = output.view(-1, self.num_heads * self.head_size)
+        return output
 
 
 def _make_alibi_bias(
diff --git a/vllm/attention/layer.py b/vllm/attention/layer.py
index 9e4fbe0b4..b2914cf82 100644
--- a/vllm/attention/layer.py
+++ b/vllm/attention/layer.py
@@ -143,7 +143,7 @@ class Attention(nn.Module):
         # opaque custom op. For other platforms, we directly call them
         # and let torch.compile handle them.
         self.use_direct_call = not current_platform.is_cuda_alike(
-        ) and not current_platform.is_cpu()
+        ) and not current_platform.is_cpu() and not current_platform.is_xpu()
 
         self.use_output = attn_backend.accept_output_buffer
         compilation_config = get_current_vllm_config().compilation_config
@@ -369,6 +369,87 @@ def maybe_save_kv_layer_to_connector(
                             attn_metadata[layer_name])
 
 
+class SelfMultiHeadAttention(nn.Module):
+    """Multi-headed attention without any cache, used for ViT."""
+
+    def __init__(
+        self,
+        num_heads: int,
+        head_size: int,
+        scale: float,
+        num_kv_heads: Optional[int] = None,
+    ):
+        super().__init__()
+        self.num_heads = num_heads
+        self.head_size = head_size
+        self.scale = scale
+        self.num_kv_heads = num_heads if num_kv_heads is None else num_kv_heads
+
+        assert self.num_heads % self.num_kv_heads == 0
+        self.num_queries_per_kv = self.num_heads // self.num_kv_heads
+
+        self.attn_backend =  _Backend.TORCH_SDPA
+
+    def forward(
+        self,
+        query: torch.Tensor,
+        key: torch.Tensor,
+        value: torch.Tensor,
+    ) -> torch.Tensor:
+        """Input shape: batch_size x seq_len x hidden_size"""
+        # TODO(Isotr0py): Use existing backend implementations and support FA3
+        bsz, q_len, _ = query.size()
+        kv_len = key.size(1)
+
+        q = query.view(bsz, q_len, self.num_heads, self.head_size)
+        k = key.view(bsz, kv_len, self.num_kv_heads, self.head_size)
+        v = value.view(bsz, kv_len, self.num_kv_heads, self.head_size)
+        from einops import rearrange
+        q, k, v = (rearrange(x, "b s ... -> (b s) ...") for x in [q, k, v])
+        from vllm._ipex_ops import ipex_ops
+        output = torch.empty(
+                    (q.shape[0], q.shape[1], q.shape[2]),
+                    dtype=q.dtype,
+                    device=q.device)
+        import math
+        head_dim = q.shape[-1]
+        scale = 1 / math.sqrt(head_dim) if self.scale is None else self.scale
+        tmp = [0]
+        tmp.append(q_len)
+        seqlen = torch.tensor(tmp)
+        cu_seqlens = torch.cumsum(seqlen, dim=0).to(device=query.device)
+        max_seqlen = q_len
+        ipex_ops.varlen_attention(q, k, v, output,
+                                cu_seqlens,
+                                cu_seqlens,
+                                None,
+                                max_seqlen,
+                                max_seqlen,
+                                pdropout=0,
+                                softmax_scale=scale,
+                                zero_tensors=False,
+                                is_causal=False,
+                                return_softmax=False,
+                                window_size_left=-1,
+                                window_size_right=-1,
+                                gen_=None,
+                                logits_soft_cap=0
+                                )
+
+        # out = rearrange(output,
+        #                             "(b s) ... -> b s ...",
+        #                             b=batch_size)
+        # query, key, value = (x.transpose(1, 2)
+        #                         for x in (query, key, value))
+        # out = F.scaled_dot_product_attention(query,
+        #                                         key,
+        #                                         value,
+        #                                         scale=self.scale)
+        # out = out.transpose(1, 2)
+
+        return output.reshape(bsz, q_len, -1)
+
+
 def unified_attention(
     query: torch.Tensor,
     key: torch.Tensor,
diff --git a/vllm/attention/utils/fa_utils.py b/vllm/attention/utils/fa_utils.py
index ca88549f3..d900cad15 100644
--- a/vllm/attention/utils/fa_utils.py
+++ b/vllm/attention/utils/fa_utils.py
@@ -10,6 +10,8 @@ logger = init_logger(__name__)
 def get_flash_attn_version(requires_alibi: bool = False) -> Optional[int]:
     # import here to avoid circular dependencies
     from vllm.platforms import current_platform
+    if current_platform.is_xpu():
+        return 2
     try:
         from vllm.vllm_flash_attn.flash_attn_interface import (
             fa_version_unsupported_reason, is_fa_version_supported)
diff --git a/vllm/compilation/backends.py b/vllm/compilation/backends.py
index 8114cddcd..5634f941e 100644
--- a/vllm/compilation/backends.py
+++ b/vllm/compilation/backends.py
@@ -15,7 +15,7 @@ import vllm.envs as envs
 from vllm.config import CompilationConfig, VllmConfig
 from vllm.logger import init_logger
 from vllm.platforms import current_platform
-from vllm.utils import resolve_obj_by_qualname
+from vllm.utils import is_torch_equal_or_newer, resolve_obj_by_qualname
 
 from .compiler_interface import (CompilerInterface, EagerAdaptor,
                                  InductorAdaptor, InductorStandaloneAdaptor)
@@ -28,7 +28,9 @@ logger = init_logger(__name__)
 
 def make_compiler(compilation_config: CompilationConfig) -> CompilerInterface:
     if compilation_config.use_inductor:
-        if envs.VLLM_TEST_STANDALONE_COMPILE:
+        # For XPU 2.8.0.dev wheel, it's lower than 2.8.0 so we change to 2.7.9 here
+        if envs.VLLM_USE_STANDALONE_COMPILE and is_torch_equal_or_newer(
+                "2.7.9"):
             logger.info("Using InductorStandaloneAdaptor")
             return InductorStandaloneAdaptor()
         else:
@@ -340,7 +342,7 @@ class VllmBackend:
         vllm_config: VllmConfig,
     ):
         global global_graph_pool
-        if global_graph_pool is None:
+        if global_graph_pool is None and not current_platform.is_xpu():
             global_graph_pool = current_platform.graph_pool_handle()
 
         # TODO: in the future, if we want to use multiple
diff --git a/vllm/compilation/compiler_interface.py b/vllm/compilation/compiler_interface.py
index 21af5eb76..09a81858d 100644
--- a/vllm/compilation/compiler_interface.py
+++ b/vllm/compilation/compiler_interface.py
@@ -154,7 +154,7 @@ class InductorStandaloneAdaptor(CompilerInterface):
     This is not on by default yet, but we plan to turn it on by default for
     PyTorch 2.8.
 
-    Use VLLM_TEST_STANDALONE_COMPILE to toggle this on or off.
+    Use VLLM_USE_STANDALONE_COMPILE to toggle this on or off.
     """
     name = "inductor_standalone"
 
diff --git a/vllm/compilation/fix_functionalization.py b/vllm/compilation/fix_functionalization.py
index 70f3b8b6d..63c79eb9f 100644
--- a/vllm/compilation/fix_functionalization.py
+++ b/vllm/compilation/fix_functionalization.py
@@ -8,6 +8,7 @@ import torch
 from torch._higher_order_ops.auto_functionalize import auto_functionalized
 
 from vllm.logger import init_logger
+from vllm.platforms import current_platform
 
 from .fx_utils import is_func
 from .vllm_inductor_pass import VllmInductorPass
@@ -31,6 +32,8 @@ class FixFunctionalizationPass(VllmInductorPass):
         self.nodes_to_remove: list[torch.fx.Node] = []
         count = 0
         for node in graph.nodes:
+            if current_platform.is_xpu():
+                continue
             if not is_func(node, auto_functionalized):
                 continue  # Avoid deep if-elif nesting
 
diff --git a/vllm/compilation/pass_manager.py b/vllm/compilation/pass_manager.py
index 07ebd3e1b..ae117f928 100644
--- a/vllm/compilation/pass_manager.py
+++ b/vllm/compilation/pass_manager.py
@@ -4,11 +4,11 @@ from torch import fx as fx
 
 from vllm.config import VllmConfig
 from vllm.logger import init_logger
+from vllm.platforms import current_platform
 
 from .activation_quant_fusion import ActivationQuantFusionPass
 from .collective_fusion import AsyncTPPass
 from .fix_functionalization import FixFunctionalizationPass
-from .fusion import FusionPass
 from .inductor_pass import CustomGraphPass, InductorPass, get_pass_context
 from .noop_elimination import NoOpEliminationPass
 from .sequence_parallelism import SequenceParallelismPass
@@ -16,6 +16,11 @@ from .vllm_inductor_pass import VllmInductorPass
 
 logger = init_logger(__name__)
 
+try:
+    from .fusion import FusionPass
+except AttributeError:
+    logger.warning("import FusionPass error.")
+
 
 class PostGradPassManager(CustomGraphPass):
     """
@@ -49,7 +54,7 @@ class PostGradPassManager(CustomGraphPass):
         if self.pass_config.enable_noop:
             self.passes += [NoOpEliminationPass(config)]
 
-        if self.pass_config.enable_fusion:
+        if self.pass_config.enable_fusion and not current_platform.is_xpu():
             self.passes += [FusionPass.instance(config)]
             self.passes += [ActivationQuantFusionPass(config)]
 
diff --git a/vllm/config.py b/vllm/config.py
index db35c848b..2eb278a1a 100644
--- a/vllm/config.py
+++ b/vllm/config.py
@@ -830,7 +830,7 @@ class ModelConfig:
         optimized_quantization_methods = [
             "fp8", "marlin", "modelopt", "gptq_marlin_24", "gptq_marlin",
             "awq_marlin", "fbgemm_fp8", "compressed-tensors", "experts_int8",
-            "quark", "modelopt_fp4", "bitblas", "gptq_bitblas"
+            "quark", "modelopt_fp4", "bitblas", "gptq_bitblas", "ipex"
         ]
         if self.quantization is not None:
             self.quantization = cast(QuantizationMethods,
@@ -1194,7 +1194,8 @@ class ModelConfig:
             self, parallel_config: "ParallelConfig") -> tuple[int, int]:
         from vllm.distributed.utils import get_pp_indices
         if (self.hf_text_config.model_type == "deepseek_mtp"
-                or self.hf_config.model_type == "mimo_mtp"):
+                or self.hf_config.model_type == "mimo_mtp"
+                or self.hf_config.model_type == "glm4_moe_mtp"):
             total_num_hidden_layers = getattr(self.hf_text_config,
                                               "num_nextn_predict_layers", 0)
         else:
@@ -1454,6 +1455,9 @@ class CacheConfig:
     """The number of blocks to allocate for GPU memory."""
     num_cpu_blocks: Optional[int] = field(default=None, init=False)
     """The number of blocks to allocate for CPU memory."""
+    threshold_mem: Optional[int] = field(default=None, init=False)
+    """The mem threshold to do empty cache."""
+
 
     def compute_hash(self) -> str:
         """
@@ -1491,6 +1495,10 @@ class CacheConfig:
             raise ValueError("CPU offload space must be non-negative"
                              f", but got {self.cpu_offload_gb}")
 
+        if self.cpu_offload_gb > 0 and envs.VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT:
+            raise ValueError("CPU offload can't work together with"
+                             "OFFLOAD_WEIGHTS_BEFORE_QUANT")
+
         if self.gpu_memory_utilization > 1.0:
             raise ValueError(
                 "GPU memory utilization must be less than 1.0. Got "
@@ -2415,8 +2423,15 @@ class SpeculativeConfig:
                 "n_predict": n_predict,
                 "architectures": ["MiMoMTPModel"]
             })
-            return hf_config
 
+        if hf_config.architectures[0] == "Glm4MoeForCausalLM":
+            hf_config.model_type = "glm4_moe_mtp"
+            n_predict = getattr(hf_config, "num_nextn_predict_layers", None)
+            hf_config.update({
+                "num_hidden_layers": 0,
+                "n_predict": n_predict,
+                "architectures": ["Glm4MoeMTPModel"]
+            })
         return hf_config
 
     def __post_init__(self):
@@ -2525,8 +2540,8 @@ class SpeculativeConfig:
                 elif (self.draft_model_config.hf_config.model_type ==
                       "mlp_speculator"):
                     self.method = "mlp_speculator"
-                elif (self.draft_model_config.hf_config.model_type ==
-                      "deepseek_mtp"):
+                elif (self.draft_model_config.hf_config.model_type in
+                      ("deepseek_mtp", "glm4_moe_mtp")):
                     self.method = "deepseek_mtp"
                     if self.num_speculative_tokens > 1:
                         logger.warning(
diff --git a/vllm/distributed/parallel_state.py b/vllm/distributed/parallel_state.py
index b674d05a7..aee919b32 100644
--- a/vllm/distributed/parallel_state.py
+++ b/vllm/distributed/parallel_state.py
@@ -43,6 +43,7 @@ from vllm.distributed.utils import StatelessProcessGroup
 from vllm.logger import init_logger
 from vllm.utils import (direct_register_custom_op, resolve_obj_by_qualname,
                         supports_custom_op)
+from vllm.envs import CCL_P2P_CPU
 
 
 @dataclass
@@ -667,6 +668,8 @@ class GroupCoordinator:
                     and tensor.numel() % all_gather_size == 0):
                 tensor = tensor.reshape(all_gather_size, -1)[all_gather_rank]
 
+            if envs.CCL_P2P_CPU:
+                tensor = tensor.cpu()
             if tensor.is_cpu:
                 # use metadata_group for CPU tensors
                 torch.distributed.send(tensor,
@@ -707,9 +710,12 @@ class GroupCoordinator:
         tensor_dict: dict[str, Any] = {}
         for key, value in recv_metadata_list:
             if isinstance(value, TensorMetadata):
+                tensor_device = value.device
+                if envs.CCL_P2P_CPU:
+                    tensor_device = 'cpu'
                 tensor = torch.empty(value.size,
                                      dtype=value.dtype,
-                                     device=value.device)
+                                     device=tensor_device)
                 if tensor.numel() == 0:
                     # Skip broadcasting empty tensors.
                     tensor_dict[key] = tensor
@@ -734,6 +740,8 @@ class GroupCoordinator:
                     torch.distributed.recv(tensor,
                                            src=self.ranks[src],
                                            group=group)
+                if envs.CCL_P2P_CPU:
+                    tensor = tensor.to(value.device)
                 if use_all_gather:
                     # do the allgather
                     tensor = all_gather_group.all_gather(  # type: ignore
diff --git a/vllm/engine/arg_utils.py b/vllm/engine/arg_utils.py
index 442e4100f..020756f82 100644
--- a/vllm/engine/arg_utils.py
+++ b/vllm/engine/arg_utils.py
@@ -1261,7 +1261,7 @@ class EngineArgs:
                 and not envs.is_set("VLLM_ATTENTION_BACKEND")
             ) or envs.VLLM_ATTENTION_BACKEND == "FLASH_ATTN_VLLM_V1"
             supported = False
-            if current_platform.is_rocm():
+            if current_platform.is_rocm() or current_platform.is_xpu():
                 supported = True
             elif fp8_attention and will_use_fa:
                 from vllm.attention.utils.fa_utils import (
@@ -1353,6 +1353,7 @@ class EngineArgs:
             "FLASHMLA",
             "FLASHINFER",
             "FLASHINFER_VLLM_V1",
+            "IPEX_V1",
             "ROCM_AITER_MLA",
         ]
         if (envs.is_set("VLLM_ATTENTION_BACKEND")
diff --git a/vllm/entrypoints/chat_utils.py b/vllm/entrypoints/chat_utils.py
index ec1b327da..3faff1215 100644
--- a/vllm/entrypoints/chat_utils.py
+++ b/vllm/entrypoints/chat_utils.py
@@ -510,6 +510,8 @@ class BaseMultiModalItemTracker(ABC, Generic[_T]):
         if modality in ("image", "image_embeds"):
             if model_type == "chatglm":
                 return "<|begin_of_image|><|endoftext|><|end_of_image|>"
+            if model_type == "glm4v":
+                return "<|begin_of_image|><|image|><|end_of_image|>"
             if model_type in ("phi3_v", "phi4mm"):
                 return f"<|image_{current_count}|>"
             if model_type in ("minicpmo", "minicpmv"):
@@ -558,6 +560,8 @@ class BaseMultiModalItemTracker(ABC, Generic[_T]):
         elif modality == "video":
             if model_type == "internvl_chat":
                 return "<video>"
+            if model_type == "glm4v":
+                return "<|begin_of_video|><|video|><|end_of_video|>"
             if model_type in ("qwen2_vl", "qwen2_5_vl"):
                 return "<|vision_start|><|video_pad|><|vision_end|>"
             if model_type == "qwen2_5_omni":
diff --git a/vllm/entrypoints/openai/tool_parsers/__init__.py b/vllm/entrypoints/openai/tool_parsers/__init__.py
index 054c0b006..03106c1d3 100644
--- a/vllm/entrypoints/openai/tool_parsers/__init__.py
+++ b/vllm/entrypoints/openai/tool_parsers/__init__.py
@@ -5,6 +5,7 @@ from .deepseekv3_tool_parser import DeepSeekV3ToolParser
 from .granite_20b_fc_tool_parser import Granite20bFCToolParser
 from .granite_tool_parser import GraniteToolParser
 from .hermes_tool_parser import Hermes2ProToolParser
+from .glm4_moe_tool_parser import Glm4MoeModelToolParser
 from .internlm2_tool_parser import Internlm2ToolParser
 from .jamba_tool_parser import JambaToolParser
 from .llama4_pythonic_tool_parser import Llama4PythonicToolParser
@@ -18,5 +19,5 @@ __all__ = [
     "GraniteToolParser", "Hermes2ProToolParser", "MistralToolParser",
     "Internlm2ToolParser", "Llama3JsonToolParser", "JambaToolParser",
     "Llama4PythonicToolParser", "PythonicToolParser", "Phi4MiniJsonToolParser",
-    "DeepSeekV3ToolParser"
+    "DeepSeekV3ToolParser", "Glm4MoeModelToolParser"
 ]
diff --git a/vllm/entrypoints/openai/tool_parsers/glm4_moe_tool_parser.py b/vllm/entrypoints/openai/tool_parsers/glm4_moe_tool_parser.py
new file mode 100644
index 000000000..6a03b02a4
--- /dev/null
+++ b/vllm/entrypoints/openai/tool_parsers/glm4_moe_tool_parser.py
@@ -0,0 +1,402 @@
+# SPDX-License-Identifier: Apache-2.0
+# SPDX-FileCopyrightText: Copyright contributors to the vLLM project
+# code modified from deepseekv3_tool_parser.py
+
+from collections.abc import Sequence
+from typing import Union
+
+import regex as re
+
+from vllm.entrypoints.openai.protocol import (ChatCompletionRequest,
+                                              DeltaFunctionCall, DeltaMessage,
+                                              DeltaToolCall,
+                                              ExtractedToolCallInformation,
+                                              FunctionCall, ToolCall)
+from vllm.entrypoints.openai.tool_parsers.abstract_tool_parser import (
+    ToolParser, ToolParserManager)
+from vllm.logger import init_logger
+from vllm.transformers_utils.tokenizer import AnyTokenizer
+
+logger = init_logger(__name__)
+
+
+@ToolParserManager.register_module("glm45")
+class Glm4MoeModelToolParser(ToolParser):
+
+    def __init__(self, tokenizer: AnyTokenizer):
+        super().__init__(tokenizer)
+        self.current_tool_name_sent = False
+        self.prev_tool_call_arr: list[dict] = []
+        self.current_tool_id = -1
+        self.streamed_args_for_tool: list[str] = []
+        self.tool_call_start_token = "<tool_call>"
+        self.tool_call_end_token = "</tool_call>"
+
+        self.tool_calls_start_token = self.tool_call_start_token
+
+        # Updated regex for the XML-based format
+        self.tool_call_regex = re.compile(
+            r"<tool_call>\s*"
+            r"(?P<function_name>[^\n<]+)\s*"  # 函数名（到换行或 <）
+            r"(?P<arguments>(?:\s*<arg_key>[^<]+</arg_key>\s*"
+            r"<arg_value>[^<]*</arg_value>\s*)*)\s*"
+            r"</tool_call>",
+            re.DOTALL,
+        )
+
+        # Regex for parsing individual arguments
+        self.arg_regex = re.compile(
+            r"<arg_key>(?P<key>[^<]+)</arg_key>\s*<arg_value>(?P<value>[^<]*)</arg_value>",
+            re.DOTALL,
+        )
+
+        # Streaming regex
+        self.stream_tool_call_portion_regex = re.compile(
+            r"(?P<function_name>[^\n<]+)\s*"
+            r"(?P<arguments>(?:\s*<arg_key>[^<]+</arg_key>\s*"
+            r"<arg_value>[^<]*</arg_value>\s*)*)",
+            re.DOTALL,
+        )
+
+        # For streaming, we also need a regex to match just the function name
+        self.stream_tool_call_name_regex = re.compile(
+            r"(?P<function_name>[^\n<]+)",
+            re.DOTALL,
+        )
+
+        if not self.model_tokenizer:
+            raise ValueError(
+                "The model tokenizer must be passed to the ToolParser "
+                "constructor during construction.")
+
+        self.tool_call_start_token_id = self.vocab.get(
+            self.tool_call_start_token)
+        self.tool_call_end_token_id = self.vocab.get(self.tool_call_end_token)
+
+    def _parse_arguments(self, args_text: str) -> str:
+        """Parse XML-based arguments into JSON format."""
+        if not args_text or not args_text.strip():
+            return "{}"
+
+        args_dict = {}
+        matches = self.arg_regex.findall(args_text)
+
+        for key, value in matches:
+            args_dict[key.strip()] = value.strip()
+
+        import json
+        return json.dumps(args_dict, ensure_ascii=False)
+
+    def extract_tool_calls(
+        self,
+        model_output: str,
+        request: ChatCompletionRequest,
+    ) -> ExtractedToolCallInformation:
+
+        # sanity check; avoid unnecessary processing
+        if self.tool_calls_start_token not in model_output:
+            return ExtractedToolCallInformation(tools_called=False,
+                                                tool_calls=[],
+                                                content=model_output)
+
+        try:
+            # Find all tool calls in the output
+            function_call_matches = self.tool_call_regex.findall(model_output)
+
+            logger.debug("function_call_matches: %s", function_call_matches)
+
+            if not function_call_matches:
+                return ExtractedToolCallInformation(
+                    tools_called=False,
+                    tool_calls=[],
+                    content=model_output,
+                )
+
+            tool_calls = []
+            for i, match in enumerate(function_call_matches):
+                function_name, function_args_xml = match
+                function_name = function_name.strip()
+
+                # Parse XML arguments to JSON
+                function_args_json = self._parse_arguments(function_args_xml)
+
+                tool_calls.append(
+                    ToolCall(
+                        id=f"call_{i}",
+                        type='function',
+                        function=FunctionCall(name=function_name,
+                                              arguments=function_args_json),
+                    ))
+
+            # Extract content before the first tool call
+            content = model_output[:model_output.find(self.
+                                                      tool_calls_start_token)]
+            return ExtractedToolCallInformation(
+                tools_called=bool(tool_calls),
+                tool_calls=tool_calls,
+                content=content.strip() if content.strip() else None,
+            )
+
+        except Exception:
+            logger.exception("Error in extracting tool call from response.")
+            return ExtractedToolCallInformation(tools_called=False,
+                                                tool_calls=[],
+                                                content=model_output)
+
+    def extract_tool_calls_streaming(
+        self,
+        previous_text: str,
+        current_text: str,
+        delta_text: str,
+        previous_token_ids: Sequence[int],
+        current_token_ids: Sequence[int],
+        delta_token_ids: Sequence[int],
+        request: ChatCompletionRequest,
+    ) -> Union[DeltaMessage, None]:
+
+        logger.debug("delta_text: %s", delta_text)
+        logger.debug("delta_token_ids: %s", delta_token_ids)
+        # check to see if we should be streaming a tool call - is there a
+        if self.tool_call_start_token_id not in current_token_ids:
+            logger.debug("No tool call tokens found!")
+            return DeltaMessage(content=delta_text)
+        delta_text = delta_text.replace(self.tool_calls_start_token,
+                                        "").replace(self.tool_call_end_token,
+                                                    "")
+        try:
+
+            # figure out where we are in the parsing by counting tool call
+            # start & end tags
+            prev_tool_start_count = previous_token_ids.count(
+                self.tool_call_start_token_id)
+            prev_tool_end_count = previous_token_ids.count(
+                self.tool_call_end_token_id)
+            cur_tool_start_count = current_token_ids.count(
+                self.tool_call_start_token_id)
+            cur_tool_end_count = current_token_ids.count(
+                self.tool_call_end_token_id)
+            tool_call_portion = None
+            text_portion = None
+
+            # case: if we're generating text, OR rounding out a tool call
+            if (cur_tool_start_count == cur_tool_end_count
+                    and prev_tool_end_count == cur_tool_end_count
+                    and self.tool_call_end_token not in delta_text):
+                logger.debug("Generating text content! skipping tool parsing.")
+                return DeltaMessage(content=delta_text)
+
+            if self.tool_call_end_token in delta_text:
+                logger.debug("tool_call_end_token in delta_text")
+                full_text = current_text + delta_text
+                tool_call_portion = full_text.split(
+                    self.tool_call_start_token)[-1].split(
+                        self.tool_call_end_token)[0].rstrip()
+                delta_text = delta_text.split(
+                    self.tool_call_end_token)[0].rstrip()
+                text_portion = delta_text.split(
+                    self.tool_call_end_token)[-1].lstrip()
+
+            # case -- we're starting a new tool call
+            if (cur_tool_start_count > cur_tool_end_count
+                    and cur_tool_start_count > prev_tool_start_count):
+                if len(delta_token_ids) > 1:
+                    tool_call_portion = current_text.split(
+                        self.tool_call_start_token)[-1]
+                else:
+                    tool_call_portion = None
+                    delta = None
+
+                text_portion = None
+
+                # set cursors and state appropriately
+                self.current_tool_id += 1
+                self.current_tool_name_sent = False
+                self.streamed_args_for_tool.append("")
+                logger.debug("Starting on a new tool %s", self.current_tool_id)
+
+            # case -- we're updating an existing tool call
+            elif (cur_tool_start_count > cur_tool_end_count
+                  and cur_tool_start_count == prev_tool_start_count):
+
+                # get the portion of the text that's the tool call
+                tool_call_portion = current_text.split(
+                    self.tool_call_start_token)[-1]
+                text_portion = None
+
+            # case -- the current tool call is being closed.
+            elif (cur_tool_start_count == cur_tool_end_count
+                  and cur_tool_end_count >= prev_tool_end_count):
+                if self.prev_tool_call_arr is None or len(
+                        self.prev_tool_call_arr) == 0:
+                    logger.debug(
+                        "attempting to close tool call, but no tool call")
+                    return None
+                diff = self.prev_tool_call_arr[self.current_tool_id].get(
+                    "arguments")
+                if diff:
+                    diff = (diff.encode("utf-8").decode("unicode_escape")
+                            if diff is str else diff)
+                    if '"}' not in delta_text:
+                        return None
+                    end_loc = delta_text.rindex('"}')
+                    diff = delta_text[:end_loc] + '"}'
+                    logger.debug(
+                        "Finishing tool and found diff that had not "
+                        "been streamed yet: %s",
+                        diff,
+                    )
+                    self.streamed_args_for_tool[self.current_tool_id] += diff
+                    return DeltaMessage(tool_calls=[
+                        DeltaToolCall(
+                            index=self.current_tool_id,
+                            function=DeltaFunctionCall(
+                                arguments=diff).model_dump(exclude_none=True),
+                        )
+                    ])
+
+            # case -- otherwise we're just generating text
+            else:
+                text = delta_text.replace(self.tool_call_start_token, "")
+                text = text.replace(self.tool_call_end_token, "")
+                delta = DeltaMessage(tool_calls=[], content=text)
+                return delta
+
+            current_tool_call = dict()
+            if tool_call_portion:
+                current_tool_call_matches = (
+                    self.stream_tool_call_portion_regex.match(
+                        tool_call_portion))
+                if current_tool_call_matches:
+                    tool_id, tool_args = (current_tool_call_matches.groups())
+                    tool_name = tool_id.split('.')[1].split(':')[0]
+                    current_tool_call['id'] = tool_id
+                    current_tool_call["name"] = tool_name
+                    current_tool_call["arguments"] = tool_args
+                else:
+                    current_tool_call_name_matches = (
+                        self.stream_tool_call_name_regex.match(
+                            tool_call_portion))
+                    if current_tool_call_name_matches:
+                        tool_id_str, = current_tool_call_name_matches.groups()
+                        tool_name = tool_id_str.split('.')[1].split(':')[0]
+                        current_tool_call['id'] = tool_id_str
+                        current_tool_call["name"] = tool_name
+                        current_tool_call["arguments"] = ""
+                    else:
+                        logger.debug("Not enough token")
+                        return None
+
+            # case - we haven't sent the tool name yet. If it's available, send
+            #   it. otherwise, wait until it's available.
+            if not self.current_tool_name_sent:
+                if current_tool_call is None:
+                    return None
+                function_name: Union[str, None] = current_tool_call.get("name")
+                tool_id = current_tool_call.get("id")
+                if function_name:
+                    self.current_tool_name_sent = True
+                    return DeltaMessage(tool_calls=[
+                        DeltaToolCall(
+                            index=self.current_tool_id,
+                            type="function",
+                            id=tool_id,
+                            function=DeltaFunctionCall(
+                                name=function_name).model_dump(
+                                    exclude_none=True),
+                        )
+                    ])
+                else:
+                    return None
+
+            # case -- otherwise, send the tool call delta
+
+            # if the tool call portion is None, send the delta as text
+            if tool_call_portion is None:
+                # if there's text but not tool calls, send that -
+                # otherwise None to skip chunk
+                delta = (DeltaMessage(
+                    content=delta_text) if text_portion is not None else None)
+                return delta
+
+            # now, the nitty-gritty of tool calls
+            # now we have the portion to parse as tool call.
+
+            logger.debug("Trying to parse current tool call with ID %s",
+                         self.current_tool_id)
+
+            # if we're starting a new tool call, push an empty object in as
+            #   a placeholder for the arguments
+            if len(self.prev_tool_call_arr) <= self.current_tool_id:
+                self.prev_tool_call_arr.append({})
+
+            # main logic for tool parsing here - compare prev. partially-parsed
+            #   JSON to the current partially-parsed JSON
+            prev_arguments = self.prev_tool_call_arr[self.current_tool_id].get(
+                "arguments")
+            cur_arguments = current_tool_call.get("arguments")
+
+            logger.debug("diffing old arguments: %s", prev_arguments)
+            logger.debug("against new ones: %s", cur_arguments)
+
+            # case -- no arguments have been created yet. skip sending a delta.
+            if not cur_arguments and not prev_arguments:
+                logger.debug("Skipping text %s - no arguments", delta_text)
+                delta = None
+
+            # case -- prev arguments are defined, but non are now.
+            #   probably impossible, but not a fatal error - just keep going
+            elif not cur_arguments and prev_arguments:
+                logger.error("should be impossible to have arguments reset "
+                             "mid-call. skipping streaming anything.")
+                delta = None
+
+            # case -- we now have the first info about arguments available from
+            #   autocompleting the JSON
+            elif cur_arguments and not prev_arguments:
+
+                delta = DeltaMessage(tool_calls=[
+                    DeltaToolCall(
+                        index=self.current_tool_id,
+                        function=DeltaFunctionCall(
+                            arguments=cur_arguments).model_dump(
+                                exclude_none=True),
+                    )
+                ])
+                self.streamed_args_for_tool[
+                    self.current_tool_id] = cur_arguments
+
+            # last case -- we have an update to existing arguments.
+            elif cur_arguments and prev_arguments:
+                if (isinstance(delta_text, str)
+                        and cur_arguments != prev_arguments
+                        and len(cur_arguments) > len(prev_arguments)
+                        and cur_arguments.startswith(prev_arguments)):
+                    delta_arguments = cur_arguments[len(prev_arguments):]
+                    logger.debug("got diff %s", delta_text)
+
+                    delta = DeltaMessage(tool_calls=[
+                        DeltaToolCall(
+                            index=self.current_tool_id,
+                            function=DeltaFunctionCall(
+                                arguments=delta_arguments).model_dump(
+                                    exclude_none=True),
+                        )
+                    ])
+                    self.streamed_args_for_tool[
+                        self.current_tool_id] = cur_arguments
+                else:
+                    delta = None
+
+            # handle saving the state for the current tool into
+            # the "prev" list for use in diffing for the next iteration
+            if self.current_tool_id == len(self.prev_tool_call_arr) - 1:
+                self.prev_tool_call_arr[
+                    self.current_tool_id] = current_tool_call
+            else:
+                self.prev_tool_call_arr.append(current_tool_call)
+
+            return delta
+
+        except Exception:
+            logger.exception("Error trying to handle streaming tool call.")
+            return None  # do not stream a delta. skip this token ID.
\ No newline at end of file
diff --git a/vllm/envs.py b/vllm/envs.py
index b007bf8c5..0990ae666 100644
--- a/vllm/envs.py
+++ b/vllm/envs.py
@@ -118,6 +118,11 @@ if TYPE_CHECKING:
     VLLM_NIXL_SIDE_CHANNEL_PORT: int = 5557
     VLLM_ALL2ALL_BACKEND: str = "naive"
     VLLM_MAX_TOKENS_PER_EXPERT_FP4_MOE: int = 163840
+    VLLM_XPU_FP8_DTYPE: str = "e5m2"
+    VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT: bool = False
+    CCL_P2P_CPU: bool = False
+    VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT: bool = True
+    VLLM_QUANTIZE_Q40_LIB: str = "/opt/lib/vllm_int4_for_multi_arc.so"
 
 
 def get_default_cache_root():
@@ -142,10 +147,10 @@ def maybe_convert_int(value: Optional[str]) -> Optional[int]:
 
 def get_vllm_port() -> Optional[int]:
     """Get the port from VLLM_PORT environment variable.
-    
+
     Returns:
         The port number as an integer if VLLM_PORT is set, None otherwise.
-        
+
     Raises:
         ValueError: If VLLM_PORT is a URI, suggest k8s service discovery issue.
     """
@@ -300,9 +305,11 @@ environment_variables: dict[str, Callable[[], Any]] = {
     lambda: bool(
         os.environ.get("VLLM_TEST_DYNAMO_FULLGRAPH_CAPTURE", "1") != "0"),
 
-    # Internal flag to enable/disable Inductor standalone compile
-    "VLLM_TEST_STANDALONE_COMPILE":
-    lambda: os.environ.get("VLLM_TEST_STANDALONE_COMPILE", "0") != "0",
+    # Feature flag to enable/disable Inductor standalone compile.
+    # In torch <= 2.7 we ignore this flag; in torch >= 2.8 this is
+    # enabled by default.
+    "VLLM_USE_STANDALONE_COMPILE":
+    lambda: os.environ.get("VLLM_USE_STANDALONE_COMPILE", "1") == "1",
 
     # local rank of the process in the distributed setting, used to determine
     # the GPU device id
@@ -822,6 +829,22 @@ environment_variables: dict[str, Callable[[], Any]] = {
     # This is used to prevent the kernel from running out of memory.
     "VLLM_MAX_TOKENS_PER_EXPERT_FP4_MOE":
     lambda: int(os.getenv("VLLM_MAX_TOKENS_PER_EXPERT_FP4_MOE", "163840")),
+
+    # fp8 dtype for XPU platform
+    "VLLM_XPU_FP8_DTYPE":
+    lambda: os.environ.get("VLLM_XPU_FP8_DTYPE", "e5m2"),
+
+    # Offload model weights to cpu before online fp8 quantization
+    "VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT":
+    lambda: os.environ.get("VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT", "1") == "1",
+
+    # Path for finding libs for vLLM sym_int4 quantization support
+    "VLLM_QUANTIZE_Q40_LIB":
+    lambda: os.environ.get("VLLM_QUANTIZE_Q40_LIB", "/opt/lib/vllm_int4_for_multi_arc.so"),
+
+    # Do send/recv on CPU backend
+    "CCL_P2P_CPU":
+    lambda: os.environ.get("CCL_P2P_CPU", "1") == "1"
 }
 
 # --8<-- [end:env-vars-definition]
@@ -884,7 +907,7 @@ def compute_hash() -> str:
         "VLLM_USE_TRITON_AWQ",
         "VLLM_DP_RANK",
         "VLLM_DP_SIZE",
-        "VLLM_TEST_STANDALONE_COMPILE",
+        "VLLM_USE_STANDALONE_COMPILE",
     ]
     for key in environment_variables_to_hash:
         if key in environment_variables:
diff --git a/vllm/executor/ray_distributed_executor.py b/vllm/executor/ray_distributed_executor.py
index 8e67c7a41..14dd9c386 100644
--- a/vllm/executor/ray_distributed_executor.py
+++ b/vllm/executor/ray_distributed_executor.py
@@ -73,9 +73,14 @@ class RayDistributedExecutor(DistributedExecutorBase):
     def _init_executor(self) -> None:
         self.forward_dag: Optional[ray.dag.CompiledDAG] = None
         if envs.VLLM_USE_V1:
-            # V1 uses SPMD worker and compiled DAG
-            os.environ["VLLM_USE_RAY_SPMD_WORKER"] = "1"
-            os.environ["VLLM_USE_RAY_COMPILED_DAG"] = "1"
+            if current_platform.is_xpu():
+                # For XPU platform, COMPILED_DAG functionality does not worked well...
+                os.environ["VLLM_USE_RAY_SPMD_WORKER"] = "1"
+                os.environ["VLLM_USE_RAY_COMPILED_DAG"] = "0"
+            else:
+                # V1 uses SPMD worker and compiled DAG
+                os.environ["VLLM_USE_RAY_SPMD_WORKER"] = "1"
+                os.environ["VLLM_USE_RAY_COMPILED_DAG"] = "1"
 
             # For TPU, avoid compiling NVIDIA's NCCL
             if current_platform.is_tpu():
@@ -91,15 +96,6 @@ class RayDistributedExecutor(DistributedExecutorBase):
         # be executed in a remote Ray worker. Currently this requires
         # USE_RAY_COMPILED_DAG=True.
         self.use_ray_spmd_worker = envs.VLLM_USE_RAY_SPMD_WORKER
-        if self.use_ray_compiled_dag:
-            assert self.use_ray_spmd_worker, (
-                "VLLM_USE_RAY_COMPILED_DAG=1 requires "
-                "VLLM_USE_RAY_SPMD_WORKER=1")
-        if self.use_ray_spmd_worker:
-            # TODO: Support SPMD worker for non-DAG Ray executor.
-            assert self.use_ray_compiled_dag, (
-                "VLLM_USE_RAY_SPMD_WORKER=1 requires "
-                "VLLM_USE_RAY_COMPILED_DAG=1")
 
         assert self.uses_ray
         initialize_ray_cluster(self.parallel_config)
@@ -119,7 +115,7 @@ class RayDistributedExecutor(DistributedExecutorBase):
         self.use_v1 = envs.VLLM_USE_V1
 
         self.pp_locks: Optional[List[asyncio.Lock]] = None
-        if not self.use_ray_compiled_dag:
+        if not self.use_ray_spmd_worker:
             self.driver_exec_method = make_async(
                 self.driver_worker.execute_method)
 
diff --git a/vllm/lora/ops/triton_ops/lora_expand_op.py b/vllm/lora/ops/triton_ops/lora_expand_op.py
index 9feb9e462..8aca2a57e 100644
--- a/vllm/lora/ops/triton_ops/lora_expand_op.py
+++ b/vllm/lora/ops/triton_ops/lora_expand_op.py
@@ -12,6 +12,7 @@ import triton.language as tl
 
 from vllm.lora.ops.triton_ops.kernel_utils import do_expand_kernel
 from vllm.lora.ops.triton_ops.utils import _get_lora_b_ptr
+from vllm.platforms import current_platform
 from vllm.utils import direct_register_custom_op
 
 
@@ -255,6 +256,7 @@ def _lora_expand(
         num_warps=NUM_WARPS,
         num_ctas=NUM_CTAS,
         num_stages=NUM_STAGES,
+        #maxnreg=MAX_NREG,
     )
 
     return
@@ -282,6 +284,7 @@ try:
         op_func=_lora_expand,
         mutates_args=["output_tensor"],
         fake_impl=_lora_expand_fake,
+        dispatch_key=current_platform.dispatch_key,
     )
     lora_expand = torch.ops.vllm.lora_expand
 
diff --git a/vllm/lora/ops/triton_ops/lora_shrink_op.py b/vllm/lora/ops/triton_ops/lora_shrink_op.py
index c3871bd58..ca2c2ab5d 100644
--- a/vllm/lora/ops/triton_ops/lora_shrink_op.py
+++ b/vllm/lora/ops/triton_ops/lora_shrink_op.py
@@ -12,6 +12,7 @@ import triton.language as tl
 
 from vllm.lora.ops.triton_ops.kernel_utils import do_shrink_kernel
 from vllm.lora.ops.triton_ops.utils import _get_lora_a_ptr
+from vllm.platforms import current_platform
 from vllm.utils import direct_register_custom_op
 
 
@@ -210,6 +211,7 @@ def _lora_shrink(
         num_warps=NUM_WARPS,
         num_ctas=NUM_CTAS,
         num_stages=NUM_STAGES,
+        #maxnreg=MAX_NREG,
     )
 
     return
@@ -236,6 +238,7 @@ try:
         op_func=_lora_shrink,
         mutates_args=["output_tensor"],
         fake_impl=_lora_shrink_fake,
+        dispatch_key=current_platform.dispatch_key,
     )
     lora_shrink = torch.ops.vllm.lora_shrink
 
diff --git a/vllm/lora/ops/triton_ops/utils.py b/vllm/lora/ops/triton_ops/utils.py
index 6225635c2..7a6d676d3 100644
--- a/vllm/lora/ops/triton_ops/utils.py
+++ b/vllm/lora/ops/triton_ops/utils.py
@@ -34,7 +34,7 @@ def _get_lora_a_ptr(lora_a_weights: list[torch.Tensor], device: torch.device):
         lora_strides_d1.append(lora_a_weight.stride(1))
         lora_strides_d2.append(lora_a_weight.stride(2))
     if len(lora_a_weights) > 1:
-        lora_ptr_tensor = torch.tensor(tensor_ptrs, device=device)
+        lora_ptr_tensor = torch.tensor(tensor_ptrs, device=device, dtype=torch.uint64)
     else:
         lora_ptr_tensor = lora_a_weights[0]
 
@@ -88,7 +88,7 @@ def _get_lora_b_ptr(lora_weights: list[torch.Tensor], offset_start: int,
 
     if len(lora_weights) > 1:
         # note these are device tensors
-        lora_ptr_tensor = torch.tensor(tensor_ptrs, device=device)
+        lora_ptr_tensor = torch.tensor(tensor_ptrs, device=device, dtype=torch.uint64)
         slice_start_tensor = torch.tensor(slice_offset_lst, device=device)
     else:
         slice_start_tensor = slice_offset_lst[0]
diff --git a/vllm/lora/punica.py b/vllm/lora/punica.py
new file mode 100644
index 000000000..5f711bfe5
--- /dev/null
+++ b/vllm/lora/punica.py
@@ -0,0 +1,616 @@
+"""
+Based on:
+Chen, L., Ye, Z., Wu, Y., Zhuo, D., Ceze, L., & Krishnamurthy, A. (2023). 
+Punica: Multi-Tenant LoRA Serving. 
+https://arxiv.org/abs/2310.18547
+"""
+
+from typing import TYPE_CHECKING, Callable, List, Optional, Tuple, Union
+
+import torch
+
+from vllm.triton_utils import HAS_TRITON
+from vllm.utils import is_xpu
+
+if HAS_TRITON and not is_xpu():
+    from vllm.lora.ops.bgmv_expand import bgmv_expand
+    from vllm.lora.ops.bgmv_expand_slice import bgmv_expand_slice
+    from vllm.lora.ops.bgmv_shrink import bgmv_shrink
+    from vllm.lora.ops.sgmv_expand import sgmv_expand
+    from vllm.lora.ops.sgmv_expand_slice import sgmv_expand_slice
+    from vllm.lora.ops.sgmv_shrink import sgmv_shrink
+elif is_xpu():
+    from vllm._ipex_ops import ipex_ops
+    bgmv_expand = ipex_ops.bgmv_expand
+    bgmv_expand_slice = ipex_ops.bgmv_expand_slice
+    bgmv_shrink = ipex_ops.bgmv_shrink
+    sgmv_expand = ipex_ops.sgmv_expand
+    sgmv_expand_slice = ipex_ops.sgmv_expand_slice
+    sgmv_shrink = ipex_ops.sgmv_shrink
+
+if TYPE_CHECKING:
+    # avoid circuit import
+    from vllm.lora.layers import LoRAMapping
+    from vllm.lora.models import LongContextLoRAContext
+
+
+def compute_meta(
+    token_lora_tensor: torch.Tensor
+) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, int, int, bool]:
+    """
+    Get the information required for the sgmv kernel. With the  features:
+    1. If consecutive requests in the batch use the same LoRA, this function
+    will combine them into a single request, improving sgmv kernel inference
+    performance.
+    2. At the beginning of each prefill stage inference, recalculations are
+    needed based on the input, but only once.
+    """
+
+    lora_indices_tensor, seq_length_tensor = torch.unique_consecutive(
+        token_lora_tensor, return_counts=True)
+    cum_result = torch.cumsum(seq_length_tensor, dim=0)
+    b_seq_start_tensor = torch.zeros_like(seq_length_tensor)
+    b_seq_start_tensor[1:].copy_(cum_result[:-1])
+    max_length = seq_length_tensor.max().item()
+
+    batch_size = lora_indices_tensor.size(0)
+    no_lora = False
+    # -1 means no lora should be applied. Use `no_lora` to determine whether
+    # the current step requires LoRA. If LoRA is not needed, the prefill stage
+    # does not need to launch the triton kernel, which can improve performance
+    if batch_size == 1 and lora_indices_tensor == -1:
+        no_lora = True
+    return (b_seq_start_tensor, seq_length_tensor, lora_indices_tensor,
+            batch_size, max_length, no_lora)
+
+
+# TODO see if this can be vectorized
+def convert_mapping(
+    mapping: "LoRAMapping",
+    lora_index_to_id: List[Optional[int]],
+    max_loras: int,
+    vocab_size: int,
+    extra_vocab_size: int,
+    long_lora_context: Optional["LongContextLoRAContext"] = None,
+    device: torch.device = "cuda",
+) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor,
+           Optional[torch.Tensor], List[int]]:
+    """Converts LoRAMapping to index tensors.
+
+    Args:
+        mapping: LoRAMapping mapping rows in a batch to LoRA ids.
+        lora_index_to_id: List mapping LoRA ids to LoRA indices.
+        max_loras: Maximum number of LoRAs.
+        vocab_size: Model vocab size.
+        extra_vocab_size: Extra vocab size each LoRA can have.
+        long_lora_context: Passed if there are long context lora in a batch.
+
+    Returns:
+        A tuple of tensors:
+            base_indices: Tensor of shape [batch_size] mapping batch rows to
+                LoRA indices.
+            sampler_indices: Tensor of shape [batch_size] mapping requests to
+                LoRA indices for sampler. For generation, this will be the
+                same as base_indicies. For prefill, this will map requests
+                to LoRA indices.
+            sampler_indices_padded: Tensor of shape [batch_size] mapping
+                requests to LoRA indices for sampler with padding.
+                Same as sampler_indicies, but -1 is replaced with
+                max_loras.
+            embeddings_indices: Tensor of shape [2, batch_size] mapping
+                requests to embedding indices. First row is for embeddings
+                added by the LoRAs, second row is for the LoRA.lora_a
+                embeddings.
+            long_lora_indices: Tensor of shape [batch_size] mapping
+                requests to RoPE offsets and rot dims for long LoRAs.
+                None if long context lora doesn't exist.
+            indices_len: List of lengths of the above tensors. It contains
+                (base_indices, sampler_indices, sampler_indices_padded,
+                embeddings_indices, long_lora_indices).
+    """
+    index_mapping_indices: List[int] = list(mapping.index_mapping).copy()
+    embedding_indices = index_mapping_indices.copy()
+    lora_indices = index_mapping_indices.copy()
+    long_lora_offsets: Optional[torch.Tensor] = None
+    if long_lora_context:
+        long_lora_offsets = torch.zeros(len(index_mapping_indices),
+                                        device=device,
+                                        dtype=torch.long)
+    prompt_mapping: List[int] = [
+        lora_index_to_id.index(x) if x > 0 else -1
+        for x in mapping.prompt_mapping
+    ]
+    lora_idx = None
+    for i in range(len(index_mapping_indices)):
+        # TODO index can be slow. optimize
+        lora_idx = (lora_index_to_id.index(index_mapping_indices[i])
+                    if index_mapping_indices[i] > 0 else -1)
+        embedding_indices[i] = lora_idx if index_mapping_indices[i] > 0 else 0
+        lora_indices[i] = lora_idx
+        if long_lora_context:
+            assert long_lora_offsets is not None
+            lora_offset: int = long_lora_context.offsets_by_lora_id.get(
+                index_mapping_indices[i], 0)
+            long_lora_offsets[i] = lora_offset
+
+    indices_list: List[Union[List[int], torch.Tensor]] = [
+        index_mapping_indices,
+        lora_indices,
+        embedding_indices,
+    ]
+    if long_lora_context:
+        assert long_lora_offsets is not None
+        indices_list.append(long_lora_offsets)
+    indices = torch.tensor(indices_list, dtype=torch.long, device=device)
+    prompt_mapping_tensor = torch.tensor(prompt_mapping,
+                                         device=device,
+                                         dtype=torch.long)
+    embeddings_indices = torch.stack([
+        indices[2] * extra_vocab_size,
+        indices[2] * (vocab_size + extra_vocab_size),
+    ])
+    embeddings_indices[embeddings_indices == -1] = max_loras - 1
+    base_indices = indices[1]
+    sampler_indices = prompt_mapping_tensor
+    sampler_indices_padded = sampler_indices.clone()
+    sampler_indices_padded[sampler_indices_padded == -1] = max_loras - 1
+    sampler_indices_padded = torch.arange(
+        0, len(sampler_indices_padded), device=device, dtype=torch.long) + (
+            sampler_indices_padded * len(sampler_indices_padded))
+    long_lora_indices = None
+    long_lora_indices_len: Optional[int] = None
+    if long_lora_context:
+        long_lora_indices = indices[3]
+        long_lora_indices_len = long_lora_indices.shape[-1]
+    # Contain length of indices tensors. Used to index into each tensor.
+    indices_len = [
+        base_indices.shape[-1],
+        sampler_indices.shape[-1],
+        sampler_indices_padded.shape[-1],
+        embeddings_indices.shape[-1],
+    ]
+    if long_lora_indices_len is not None:
+        indices_len.append(long_lora_indices_len)
+    else:
+        # If long_lora doesn't exist,append None
+        indices_len.append(None)
+
+    return (
+        base_indices,
+        sampler_indices,
+        sampler_indices_padded,
+        embeddings_indices,
+        long_lora_indices,
+        indices_len,
+    )
+
+
+class PunicaWrapper:
+    """
+    PunicaWrapper is designed to manage and provide metadata for the punica 
+    kernel. The main function  is to maintain the state information for 
+    Multi-LoRA, and to provide the interface for the punica kernel.
+    """
+
+    def __init__(self, max_num_batched_tokens: int, max_batches: int,
+                 device: torch.device):
+        self.device = device
+        self._token_lora_indices = torch.empty(max_num_batched_tokens,
+                                               dtype=torch.long,
+                                               device=device)
+        self._sampler_indices = torch.empty(max_num_batched_tokens,
+                                            dtype=torch.long,
+                                            device=device)
+        self._sampler_indices_padded = torch.empty(max_num_batched_tokens,
+                                                   dtype=torch.long,
+                                                   device=device)
+        self._embeddings_indices = torch.empty(2,
+                                               max_num_batched_tokens,
+                                               dtype=torch.long,
+                                               device=device)
+        self._long_lora_indices = torch.empty(max_num_batched_tokens,
+                                              dtype=torch.long,
+                                              device=device)
+
+        # 5 is the number of indicies tensors.
+        # base_indices, sampler_indices, sampler_indices_padded,
+        # embeddings_indices,long_lora_indices
+        self.indices_len: List[Optional[int]] = [None] * 5
+        # these attributes are the information required for sgmv kernel
+        self._seq_start_locs = torch.empty(max_batches,
+                                           dtype=torch.long,
+                                           device=device)
+        self._seq_lengths = torch.empty(max_batches,
+                                        dtype=torch.long,
+                                        device=device)
+        self._lora_indices_per_batch = torch.empty(max_batches,
+                                                   dtype=torch.long,
+                                                   device=device)
+        self.max_length: int = 0
+        self.batch_size: int = -1
+        self.is_prefill = False
+        self.no_lora = False
+
+    def update_metadata(
+        self,
+        mapping: "LoRAMapping",
+        lora_index_to_id: List[Optional[int]],
+        max_loras: int,
+        vocab_size: int,
+        extra_vocab_size: int,
+        long_lora_context: Optional["LongContextLoRAContext"] = None,
+    ):
+
+        self._update_base_metadata(mapping, lora_index_to_id, max_loras,
+                                   vocab_size, extra_vocab_size,
+                                   long_lora_context)
+        if mapping.is_prefill:
+            # Update metadata required for prefill-related operators.
+            self._update_prefill_metada(self.token_lora_indices)
+            self.is_prefill = True
+        else:
+            self.is_prefill = False
+
+    def _update_base_metadata(
+        self,
+        mapping: "LoRAMapping",
+        lora_index_to_id: List[Optional[int]],
+        max_loras: int,
+        vocab_size: int,
+        extra_vocab_size: int,
+        long_lora_context: Optional["LongContextLoRAContext"] = None,
+    ):
+        (
+            base_indices,
+            sampler_indices,
+            sampler_indices_padded,
+            embeddings_indices,
+            long_lora_offsets_tensor,
+            indices_len,
+        ) = convert_mapping(
+            mapping,
+            lora_index_to_id,
+            max_loras,
+            vocab_size,
+            extra_vocab_size,
+            long_lora_context,
+            self.device,
+        )
+        self._token_lora_indices[:base_indices.shape[0]].copy_(base_indices)
+        self._sampler_indices[:sampler_indices.shape[0]].copy_(sampler_indices)
+        self._sampler_indices_padded[:sampler_indices_padded.shape[0]].copy_(
+            sampler_indices_padded)
+        self._embeddings_indices[:embeddings_indices.
+                                 shape[0], :embeddings_indices.shape[1]].copy_(
+                                     embeddings_indices)
+        if long_lora_offsets_tensor is not None:
+            self._long_lora_indices[:long_lora_offsets_tensor.shape[0]].copy_(
+                long_lora_offsets_tensor)
+        else:
+            self._long_lora_indices.zero_()
+
+        self.indices_len[:] = indices_len
+
+    def _update_prefill_metada(self, token_lora_tensor: torch.Tensor) -> None:
+
+        (b_seq_start_tensor, seq_length_tensor, lora_indices_tensor,
+         batch_size, max_length, no_lora) = compute_meta(token_lora_tensor)
+
+        self._seq_start_locs[:b_seq_start_tensor.shape[0]].copy_(
+            b_seq_start_tensor)
+        self._seq_lengths[:seq_length_tensor.shape[0]].copy_(seq_length_tensor)
+        self._lora_indices_per_batch[:lora_indices_tensor.shape[0]].copy_(
+            lora_indices_tensor)
+        self.batch_size = batch_size
+        self.max_length = max_length
+        self.no_lora = no_lora
+
+    @property
+    def prefill_metadata(
+            self) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, int, int]:
+        """
+        This property provides a convenient way to access the necessary 
+        metadata for prefill-related  kernel computations.
+            1. seq_start_locs: Tensor of sequence start positions
+            2. seq_lengths: Tensor of sequence lengths
+            3. lora_indices_per_batch: Tensor of lora indices, and an index of 
+                -1 means no lora should be applied.
+            4. batch_size: batch size after clustering identical lora indices
+            5. max_length: The maximum sequence length in the batch
+        """
+        return (self._seq_start_locs[:self.batch_size],
+                self._seq_lengths[:self.batch_size],
+                self._lora_indices_per_batch[:self.batch_size],
+                self.batch_size, self.max_length)
+
+    @property
+    def token_lora_indices(self) -> torch.Tensor:
+        """
+        This property provides the lora indices corresponding to each token 
+        in the batch. An index of -1 means no lora should be applied.
+        """
+        token_lora_len = self.indices_len[0]
+        return self._token_lora_indices[:token_lora_len]
+
+    @property
+    def sampler_indices(self) -> torch.Tensor:
+        """ 
+        This property is used to access the lora indices specifically for 
+        LogitsProcessorWithLoRA
+        """
+        sampler_indices_len = self.indices_len[1]
+        return self._sampler_indices[:sampler_indices_len]
+
+    @property
+    def sampler_indices_padded(self) -> torch.Tensor:
+        """
+        This property provides access to padded sampler indices
+        """
+        indices_padded_len = self.indices_len[2]
+        return self._sampler_indices_padded[:indices_padded_len]
+
+    @property
+    def embeddings_indices(self) -> torch.Tensor:
+        """
+        This property provides access to the indices used for lora embeddings, 
+        specifically for VocabParallelEmbeddingWithLoRA
+        """
+        embeddings_indices_len = self.indices_len[3]
+        return self._embeddings_indices[:, :embeddings_indices_len]
+
+    @property
+    def long_lora_indices(self) -> torch.Tensor:
+        """ 
+        This property provides access to the indices used for long context 
+        lora, specifically for LinearScalingRotaryEmbeddingWithLora
+        """
+        long_lora_len = self.indices_len[4]
+        return self._long_lora_indices[:long_lora_len]
+
+    def shrink_prefill(
+        self,
+        y: torch.Tensor,
+        x: torch.Tensor,
+        w_t_all: torch.Tensor,
+        scale: float,
+    ):
+        #No LoRA request, so return directly
+        if self.no_lora:
+            return
+        sgmv_shrink(
+            x,
+            w_t_all,
+            y,
+            *self.prefill_metadata,
+            scale,
+        )
+
+    def shrink_decode(
+        self,
+        y: torch.Tensor,
+        x: torch.Tensor,
+        w_t_all: torch.Tensor,
+        scale: float,
+    ):
+        bgmv_shrink(x, w_t_all, y, self.token_lora_indices, scale)
+
+    def expand_prefill(
+        self,
+        y: torch.Tensor,
+        x: torch.Tensor,
+        w_t_all: torch.Tensor,
+        add_input: bool,
+    ):
+        #No LoRA request, so return directly
+        if self.no_lora:
+            return
+        sgmv_expand(
+            x,
+            w_t_all,
+            y,
+            *self.prefill_metadata,
+            add_input,
+        )
+
+    def expand_decode(
+        self,
+        y: torch.Tensor,
+        x: torch.Tensor,
+        w_t_all: torch.Tensor,
+        add_input: bool,
+    ):
+        bgmv_expand(x, w_t_all, y, self.token_lora_indices, add_input)
+
+    def expand_slice_prefill(
+        self,
+        y: torch.Tensor,
+        x: torch.Tensor,
+        w_t_all: torch.Tensor,
+        y_offset: Optional[int],
+        y_slice_size: Optional[int],
+        add_input: bool,
+    ):
+        #No LoRA request, so return directly
+        if self.no_lora:
+            return
+        sgmv_expand_slice(
+            x,
+            w_t_all,
+            y,
+            *self.prefill_metadata,
+            y_offset,
+            y_slice_size,
+            add_input,
+        )
+
+    def expand_slice_decode(
+        self,
+        y: torch.Tensor,
+        x: torch.Tensor,
+        w_t_all: torch.Tensor,
+        y_offset: Optional[int],
+        y_slice_size: Optional[int],
+        add_input: bool,
+    ):
+        bgmv_expand_slice(x, w_t_all, y, self.token_lora_indices, y_offset,
+                          y_slice_size, add_input)
+
+    def add_shrink(
+        self,
+        y: torch.Tensor,
+        x: torch.Tensor,
+        w_t_all: torch.Tensor,
+        scale: float,
+    ):
+        """
+        Perform the ` y+=x@w_t_all` computation, which is suitable for the
+        GEMM of lora'a.
+        When `is_prefill is` true, it indicates that it is currently the
+        prefill stage, and the `shrink_prefill` function should be called.
+        Otherwise, it is the decode stage, and the shrink_decode function
+        should be called.
+        """
+        shrink_fun: Callable = (self.shrink_prefill
+                                if self.is_prefill else self.shrink_decode)
+        shrink_fun(y, x, w_t_all, scale)
+
+    def add_expand(
+        self,
+        y: torch.Tensor,
+        x: torch.Tensor,
+        w_t_all: torch.Tensor,
+        add_input: bool = True,
+    ):
+        """
+        Perform the ` y+=x@w_t_all` computation, which is suitable for the
+        GEMM of lora'b.
+        When `is_prefill` is true, it indicates that it is currently the
+        prefill stage, and the `expand_prefill` function should be called.
+        Otherwise, it is the decode stage, and the expand_decode function
+        should be called.
+        """
+
+        expand_fun: Callable = (self.expand_prefill
+                                if self.is_prefill else self.expand_decode)
+        expand_fun(y, x, w_t_all, add_input)
+
+    def add_expand_slice(self,
+                         y: torch.Tensor,
+                         x: torch.Tensor,
+                         w_t_all: torch.Tensor,
+                         y_offset: Optional[int],
+                         y_slice_size: Optional[int],
+                         add_input: bool = True):
+        """
+        Similar to `add_expand`
+        """
+
+        expand_slice_fun: Callable = (self.expand_slice_prefill
+                                      if self.is_prefill else
+                                      self.expand_slice_decode)
+        expand_slice_fun(y, x, w_t_all, y_offset, y_slice_size, add_input)
+
+    def add_lora(self,
+                 y: torch.Tensor,
+                 x: torch.Tensor,
+                 wa_t_all: torch.Tensor,
+                 wb_t_all: torch.Tensor,
+                 scale: float,
+                 y_offset: Optional[int] = None,
+                 y_slice_size: Optional[int] = None,
+                 *,
+                 buffer: Optional[torch.Tensor] = None) -> None:
+        """
+        Semantics:
+        y[i] += (
+            x[i].unsqueeze(0)
+            @ wa_t_all[indices[i], layer_idx, :, :].transpose(-1, -2)
+            @ wb_t_all[indices[i], layer_idx, :, :].transpose(-1, -2)
+            * scale
+            ).squeeze(0)
+        Args:
+            y (torch.Tensor):  Output tensor. Will be changed in-place.
+            x (torch.Tensor): Input tensor
+            wa_t_all (torch.Tensor): lora_a's weight
+            wb_t_all (torch.Tensor): lora_b's weight
+            scale (float): Scaling factor.
+            y_offset (Optional[int], optional): Offset to apply to the starting
+                column of y.
+            y_slice_size (Optional[int], optional): Size of the y column slice..
+            buffer (Optional[torch.Tensor], optional): Defaults to None.
+        """
+        y_org = y
+        y = y.view(-1, y.shape[-1])
+        x = x.view(-1, x.shape[-1])
+        r = wb_t_all.size(-1)
+        if buffer is None:
+            # We set the buffer to be float32 by default ,refer to:
+            # https://github.com/triton-lang/triton/issues/1387
+            buffer = torch.zeros((x.size(0), r),
+                                 dtype=torch.float32,
+                                 device=x.device)
+
+        self.add_shrink(buffer, x, wa_t_all, scale)
+        if y_offset is None and y_slice_size is None:
+            self.add_expand(y, buffer, wb_t_all, add_input=True)
+        else:
+            self.add_expand_slice(y,
+                                  buffer,
+                                  wb_t_all,
+                                  y_offset,
+                                  y_slice_size,
+                                  add_input=True)
+        y = y.view_as(y_org)
+
+    def add_lora_packed_nslice(self, y: torch.Tensor, x: torch.Tensor,
+                               lora_a_stacked: Tuple[torch.Tensor,
+                                                     torch.Tensor,
+                                                     torch.Tensor],
+                               lora_b_stacked: Tuple[torch.Tensor,
+                                                     torch.Tensor,
+                                                     torch.Tensor],
+                               scale: float,
+                               output_slices: Tuple[int, ...]) -> None:
+        """
+        Applies lora to each input. Similar to add_lora, This method is 
+        used for layers that are composed of multiple sublayers
+        (slices) packed together.
+        """
+        y_org = y
+        x = x.view(-1, x.shape[-1])
+        y = y.view(-1, y.shape[-1])
+        offset_left = 0
+        # TODO fuse these kernels
+        for slice_idx in range(len(output_slices)):
+            self.add_lora(y, x, lora_a_stacked[slice_idx],
+                          lora_b_stacked[slice_idx], scale, offset_left,
+                          output_slices[slice_idx])
+            offset_left += output_slices[slice_idx]
+
+        y = y.view_as(y_org)
+
+    def add_lora_logits(self,
+                        y: torch.Tensor,
+                        x: torch.Tensor,
+                        wa_t_all: torch.Tensor,
+                        wb_t_all: torch.Tensor,
+                        scale,
+                        *,
+                        buffer: Optional[torch.Tensor] = None) -> None:
+        """
+        LogitsProcessorWithLoRA always using bgmv
+        """
+        y_org = y
+        y = y.view(-1, y.shape[-1])
+        x = x.view(-1, x.shape[-1])
+        r = wb_t_all.size(-1)
+        if buffer is None:
+            # We set the buffer to be float32 by default ,refer to:
+            # https://github.com/triton-lang/triton/issues/1387
+            buffer = torch.zeros((x.size(0), r),
+                                 dtype=torch.float32,
+                                 device=x.device)
+
+        bgmv_shrink(x, wa_t_all, buffer, self.sampler_indices, scale)
+        bgmv_expand(buffer, wb_t_all, y, self.sampler_indices, add_inputs=True)
+        y = y.view_as(y_org)
diff --git a/vllm/lora/punica_wrapper/punica_gpu.py b/vllm/lora/punica_wrapper/punica_gpu.py
index 224640ec7..cb1f5a506 100644
--- a/vllm/lora/punica_wrapper/punica_gpu.py
+++ b/vllm/lora/punica_wrapper/punica_gpu.py
@@ -6,17 +6,36 @@ Punica: Multi-Tenant LoRA Serving.
 https://arxiv.org/abs/2310.18547
 """
 
-from typing import TYPE_CHECKING, Optional, Union, final
+from typing import TYPE_CHECKING, Optional, Union, Tuple, final
 
 import torch
 
 import vllm.envs as envs
 from vllm.lora.layers import LoRAMapping
+from vllm.platforms import current_platform
 from vllm.triton_utils import HAS_TRITON
 
-if HAS_TRITON:
+is_xpu = current_platform.is_xpu()
+#is_xpu = False
+if HAS_TRITON and not is_xpu:
     from vllm.lora.ops.triton_ops import (LoRAKernelMeta, lora_expand,
                                           lora_shrink)
+elif is_xpu:
+    from vllm._ipex_ops import ipex_ops
+    try:
+        lora_expand = ipex_ops.lora_expand
+        lora_shrink = ipex_ops.lora_shrink
+        XPU_KERNEL_V = 1
+    except AttributeError:
+        from vllm._ipex_ops import ipex_ops
+        bgmv_expand = ipex_ops.bgmv_expand
+        bgmv_expand_slice = ipex_ops.bgmv_expand_slice
+        bgmv_shrink = ipex_ops.bgmv_shrink
+        sgmv_expand = ipex_ops.sgmv_expand
+        sgmv_expand_slice = ipex_ops.sgmv_expand_slice
+        sgmv_shrink = ipex_ops.sgmv_shrink
+        XPU_KERNEL_V = 0
+
 
 from .punica_base import PunicaWrapperBase
 
@@ -40,9 +59,10 @@ class PunicaWrapperGPU(PunicaWrapperBase):
 
         self.max_loras = kwargs['max_loras']
 
-        self.token_mapping_meta = LoRAKernelMeta.make(self.max_loras,
-                                                      max_num_batched_tokens,
-                                                      device=device)
+        if not (is_xpu and XPU_KERNEL_V == 0):
+            self.token_mapping_meta = LoRAKernelMeta.make(self.max_loras,
+                                                          max_num_batched_tokens,
+                                                          device=device)
 
         # When cudagraph capture size is greater than max_num_seqs (max_batches,
         # here), V0 captures the graph as if max_num_seqs is set to
@@ -50,9 +70,10 @@ class PunicaWrapperGPU(PunicaWrapperBase):
         # V1 doesn't have this problem and always respects max_num_seqs.
         max_num_prompts = (max_batches
                            if envs.VLLM_USE_V1 else max_num_batched_tokens)
-        self.prompt_mapping_meta = LoRAKernelMeta.make(self.max_loras,
-                                                       max_num_prompts,
-                                                       device=device)
+        if not (is_xpu and XPU_KERNEL_V == 0):
+            self.prompt_mapping_meta = LoRAKernelMeta.make(self.max_loras,
+                                                           max_num_prompts,
+                                                           device=device)
 
     def update_metadata(
             self,
@@ -65,13 +86,79 @@ class PunicaWrapperGPU(PunicaWrapperBase):
             **kwargs):
 
         self.is_prefill = mapping.is_prefill
-        self._update_base_metadata(mapping, lora_index_to_id, max_loras,
-                                   vocab_size, extra_vocab_size,
-                                   long_lora_context)
+        if is_xpu and XPU_KERNEL_V == 0:
+            PunicaWrapperBase.update_metadata(self, mapping, lora_index_to_id,
+                                              max_loras, vocab_size,
+                                              extra_vocab_size,
+                                              long_lora_context, **kwargs)
+        else:
+            self._update_base_metadata(mapping, lora_index_to_id, max_loras,
+                                       vocab_size, extra_vocab_size,
+                                       long_lora_context)
+            # Prepare cuda kernel metadata tensors
+            self.token_mapping_meta.prepare_tensors(self.token_lora_indices)
+            self.prompt_mapping_meta.prepare_tensors(self.sampler_indices)
+
+    def _apply_shrink_prefill(
+        self,
+        y: torch.Tensor,
+        x: torch.Tensor,
+        w_t_all: Tuple[torch.Tensor, ...],
+        scale: float,
+    ):
+        #No LoRA request, so return directly
+        if self.no_lora:
+            return
+        sgmv_shrink(
+            x,
+            w_t_all,
+            y,
+            *self.prefill_metadata,
+            scale,
+        )
+
+    def _apply_shrink_decode(
+        self,
+        y: torch.Tensor,
+        x: torch.Tensor,
+        w_t_all: torch.Tensor,
+        scale: float,
+    ):
+        bgmv_shrink(x, w_t_all, y, self.token_lora_indices, scale)
+
+    def _apply_expand_prefill(
+        self,
+        y: torch.Tensor,
+        x: torch.Tensor,
+        w_t_all: Tuple[torch.Tensor, ...],
+        offset_start: int,
+        add_inputs: bool,
+    ):
+        #No LoRA request, so return directly
+        if self.no_lora:
+            return
+
+        sgmv_expand(
+            x,
+            w_t_all,
+            y,
+            *self.prefill_metadata,
+            offset_start=offset_start,
+            add_inputs=add_inputs,
+        )
+
+    def _apply_expand_decode(
+        self,
+        y: torch.Tensor,
+        x: torch.Tensor,
+        w_t_all: torch.Tensor,
+        y_offset: Optional[int],
+        y_slice_size: Optional[int],
+        add_inputs: bool,
+    ):
+        bgmv_expand_slice(x, w_t_all, y, self.token_lora_indices, y_offset,
+                          y_slice_size, add_inputs)
 
-        # Prepare cuda kernel metadata tensors
-        self.token_mapping_meta.prepare_tensors(self.token_lora_indices)
-        self.prompt_mapping_meta.prepare_tensors(self.sampler_indices)
 
     def add_shrink(self, y: torch.Tensor, x: torch.Tensor,
                    lora_a_stacked: tuple[torch.Tensor,
@@ -91,13 +178,20 @@ class PunicaWrapperGPU(PunicaWrapperBase):
         """
 
         x = x.view(-1, x.shape[-1])
-        lora_shrink(
-            x,
-            lora_a_stacked,
-            y,
-            *self.token_mapping_meta.meta_args(x.size(0)),
-            scale,
-        )
+        if is_xpu and XPU_KERNEL_V == 0:
+            for slice_idx in range(len(lora_a_stacked)):
+                self._apply_shrink_decode(y[slice_idx], x,
+                                          lora_a_stacked[slice_idx], scale)
+        else:
+            meta_args = self.token_mapping_meta.meta_args(x.size(0))
+
+            lora_shrink(
+                x,
+                lora_a_stacked,
+                y,
+                *self.token_mapping_meta.meta_args(x.size(0)),
+                scale,
+            )
 
     def add_expand(self,
                    y: torch.Tensor,
@@ -137,17 +231,29 @@ class PunicaWrapperGPU(PunicaWrapperBase):
 
         assert x.ndim == 3
         assert x.size(0) == len(output_slices)
-        num_tokens = x.size(1)  # first dimension is the num slices
-
-        lora_expand(
-            x,
-            lora_b_stacked,
-            y,
-            *self.token_mapping_meta.meta_args(num_tokens),
-            offset_start=offset_start,
-            add_inputs=True,
-        )
 
+        if is_xpu and XPU_KERNEL_V == 0:
+            # TODO fuse these kernels
+            for slice_idx in range(len(lora_b_stacked)):
+                self._apply_expand_decode(
+                    y,
+                    x[slice_idx],
+                    lora_b_stacked[slice_idx],
+                    offset_start,
+                    output_slices[slice_idx],
+                    add_inputs=add_inputs,
+                )
+                offset_start += output_slices[slice_idx]
+        else:
+            num_tokens = x.size(1)  # first dimension is the num slices
+            lora_expand(
+                x,
+                lora_b_stacked,
+                y,
+                *self.token_mapping_meta.meta_args(num_tokens),
+                offset_start=offset_start,
+                add_inputs=True,
+            )
         y = y.view_as(y_org)
 
     def add_lora_embedding(self,
@@ -169,14 +275,18 @@ class PunicaWrapperGPU(PunicaWrapperBase):
             add_inputs (bool): Default to True.
         """
 
-        lora_expand(
-            x.unsqueeze(dim=0),
-            (lora_b_stacked, ),
-            y,
-            *self.token_mapping_meta.meta_args(x.size(0)),
-            offset_start=0,
-            add_inputs=add_inputs,
-        )
+        if is_xpu and XPU_KERNEL_V == 0:
+            bgmv_expand(x, lora_b_stacked, y, self.token_lora_indices,
+                            add_inputs)
+        else:
+            lora_expand(
+                x.unsqueeze(dim=0),
+                (lora_b_stacked, ),
+                y,
+                *self.token_mapping_meta.meta_args(x.size(0)),
+                offset_start=0,
+                add_inputs=add_inputs,
+            )
 
     def add_lora_linear(self,
                         y: torch.Tensor,
@@ -279,11 +389,19 @@ class PunicaWrapperGPU(PunicaWrapperBase):
                                  dtype=torch.float32,
                                  device=x.device)
 
-        lora_shrink(x, [lora_a_stacked], buffer.unsqueeze(dim=0),
-                    *self.prompt_mapping_meta.meta_args(x.size(0)), scale)
-
-        lora_expand(buffer.unsqueeze(dim=0), [lora_b_stacked],
-                    y,
-                    *self.prompt_mapping_meta.meta_args(buffer.size(0)),
-                    add_inputs=True)
+        if is_xpu and XPU_KERNEL_V == 0:
+            bgmv_shrink(x, lora_a_stacked, buffer, self.sampler_indices, scale)
+            bgmv_expand(buffer,
+                        lora_b_stacked,
+                        y,
+                        self.sampler_indices,
+                        add_inputs=True)
+        else:
+            lora_shrink(x, [lora_a_stacked], buffer.unsqueeze(dim=0),
+                        *self.prompt_mapping_meta.meta_args(x.size(0)), scale)
+
+            lora_expand(buffer.unsqueeze(dim=0), [lora_b_stacked],
+                        y,
+                        *self.prompt_mapping_meta.meta_args(buffer.size(0)),
+                        add_inputs=True)
         y = y.view_as(y_org)
diff --git a/vllm/model_executor/layers/fused_moe/layer.py b/vllm/model_executor/layers/fused_moe/layer.py
index 29b41e720..c3cfa1942 100644
--- a/vllm/model_executor/layers/fused_moe/layer.py
+++ b/vllm/model_executor/layers/fused_moe/layer.py
@@ -30,7 +30,7 @@ from vllm.utils import direct_register_custom_op
 
 has_pplx = importlib.util.find_spec("pplx_kernels") is not None
 
-if current_platform.is_cuda_alike():
+if current_platform.is_cuda_alike() or current_platform.is_xpu():
     from .fused_batched_moe import (BatchedPrepareAndFinalize,
                                     BatchedTritonExperts)
     from .fused_moe import TritonExperts, fused_experts
@@ -425,7 +425,14 @@ class UnquantizedFusedMoEMethod(FusedMoEMethodBase, CustomOp):
             layer.w13_weight.data = shuffled_w13
             layer.w2_weight.data = shuffled_w2
 
-        if current_platform.is_cpu():
+        if current_platform.is_xpu():
+            import intel_extension_for_pytorch as ipex
+            layer.ipex_fusion = ipex.llm.modules.GatedMLPMOE(
+                layer.w13_weight,
+                layer.w2_weight,
+                use_prepack=True,
+            )
+        elif current_platform.is_cpu():
             if current_platform.get_cpu_architecture() == CpuArchEnum.X86:
                 import intel_extension_for_pytorch as ipex
                 layer.ipex_fusion = ipex.llm.modules.GatedMLPMOE(
@@ -593,6 +600,30 @@ class UnquantizedFusedMoEMethod(FusedMoEMethodBase, CustomOp):
         return layer.hpu_fused_moe(x, layer.w13_weight, layer.w2_weight,
                                    router_logits, top_k)
 
+    def forward_xpu(
+            self,
+            layer: torch.nn.Module,
+            x: torch.Tensor,
+            use_grouped_topk: bool,
+            top_k: int,
+            router_logits: torch.Tensor,
+            renormalize: bool,
+            topk_group: Optional[int] = None,
+            num_expert_group: Optional[int] = None,
+            custom_routing_function: Optional[Callable] = None,
+            **kwargs,
+    ):
+        return layer.ipex_fusion(
+            x,
+            use_grouped_topk,
+            top_k,
+            router_logits,
+            renormalize,
+            topk_group,
+            num_expert_group,
+            custom_routing_function=custom_routing_function
+        )
+
     def forward_tpu(
         self,
         layer: torch.nn.Module,
diff --git a/vllm/model_executor/layers/quantization/__init__.py b/vllm/model_executor/layers/quantization/__init__.py
index 407b9c72f..204e225d3 100644
--- a/vllm/model_executor/layers/quantization/__init__.py
+++ b/vllm/model_executor/layers/quantization/__init__.py
@@ -34,6 +34,7 @@ QuantizationMethods = Literal[
     "moe_wna16",
     "torchao",
     "auto-round",
+    "sym_int4"
 ]
 QUANTIZATION_METHODS: list[str] = list(get_args(QuantizationMethods))
 
@@ -111,6 +112,7 @@ def get_quantization_config(quantization: str) -> type[QuantizationConfig]:
     from .qqq import QQQConfig
     from .torchao import TorchAOConfig
     from .tpu_int8 import Int8TpuConfig
+    from .sym_int4 import SymInt4Config
 
     method_to_config: dict[str, type[QuantizationConfig]] = {
         "aqlm": AQLMConfig,
@@ -141,6 +143,7 @@ def get_quantization_config(quantization: str) -> type[QuantizationConfig]:
         "moe_wna16": MoeWNA16Config,
         "torchao": TorchAOConfig,
         "auto-round": AutoRoundConfig,
+        "sym_int4": SymInt4Config,
     }
     # Update the `method_to_config` with customized quantization methods.
     method_to_config.update(_CUSTOMIZED_METHOD_TO_QUANT_CONFIG)
diff --git a/vllm/model_executor/layers/quantization/fp8.py b/vllm/model_executor/layers/quantization/fp8.py
index ac9b74945..22990e235 100644
--- a/vllm/model_executor/layers/quantization/fp8.py
+++ b/vllm/model_executor/layers/quantization/fp8.py
@@ -179,7 +179,7 @@ class Fp8LinearMethod(LinearMethodBase):
         self.use_marlin = (not current_platform.has_device_capability(89)
                            or envs.VLLM_TEST_FORCE_FP8_MARLIN)
         # Disable marlin for rocm
-        if current_platform.is_rocm():
+        if current_platform.is_rocm() or current_platform.is_xpu():
             self.use_marlin = False
 
         # AITER is only supported on ROCm and only for FP8_FNUZ
@@ -245,10 +245,14 @@ class Fp8LinearMethod(LinearMethodBase):
                         if self.quant_config.is_checkpoint_fp8_serialized else
                         params_dtype)
 
+        # Force offloading weights to cpu if VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT
+        # enabled, otherwise use original device config which can be gpu or cpu
+        # (may happen when cpu_offload_gb > 0)
         weight = ModelWeightParameter(data=torch.empty(
             output_size_per_partition,
             input_size_per_partition,
-            dtype=weight_dtype),
+            dtype=weight_dtype,
+            device="cpu" if envs.VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT else None),
                                       input_dim=1,
                                       output_dim=0,
                                       weight_loader=weight_loader)
@@ -330,12 +334,15 @@ class Fp8LinearMethod(LinearMethodBase):
                                                requires_grad=False)
 
         # If checkpoint not serialized fp8, quantize the weights.
-        elif not self.quant_config.is_checkpoint_fp8_serialized:
+        if not self.quant_config.is_checkpoint_fp8_serialized:
             qweight, weight_scale = ops.scaled_fp8_quant(layer.weight,
                                                          scale=None)
 
             # Update the layer with the new values.
-            layer.weight = Parameter(qweight.t(), requires_grad=False)
+            if current_platform.is_xpu():
+                layer.weight = Parameter(qweight, requires_grad=False)
+            else:
+                layer.weight = Parameter(qweight.t(), requires_grad=False)
             layer.weight_scale = Parameter(weight_scale, requires_grad=False)
             layer.input_scale = None
 
@@ -388,6 +395,14 @@ class Fp8LinearMethod(LinearMethodBase):
               layer: torch.nn.Module,
               x: torch.Tensor,
               bias: Optional[torch.Tensor] = None) -> torch.Tensor:
+        if not hasattr(layer, "weight_scale"):
+            return F.linear(x, layer.weight, bias)
+
+        if current_platform.is_xpu():
+            weight = layer.weight.data
+            scale = layer.weight_scale.data
+            output = torch.ops.vllm.fp8_gemm(x, False, weight, True, None, x.dtype, None, scale, bias, False)
+            return output
 
         if self.use_marlin:
             return apply_fp8_marlin_linear(
@@ -506,8 +521,10 @@ class Fp8MoEMethod(FusedMoEMethodBase):
             num_experts,
             2 * intermediate_size_per_partition,
             hidden_size,
-            dtype=params_dtype),
+            dtype=params_dtype,
+            device="cpu" if envs.VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT else None),
                                         requires_grad=False)
+
         layer.register_parameter("w13_weight", w13_weight)
         set_weight_attrs(w13_weight, extra_weight_attrs)
 
@@ -515,7 +532,8 @@ class Fp8MoEMethod(FusedMoEMethodBase):
             num_experts,
             hidden_size,
             intermediate_size_per_partition,
-            dtype=params_dtype),
+            dtype=params_dtype,
+            device="cpu" if envs.VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT else None),
                                        requires_grad=False)
         layer.register_parameter("w2_weight", w2_weight)
         set_weight_attrs(w2_weight, extra_weight_attrs)
@@ -680,6 +698,23 @@ class Fp8MoEMethod(FusedMoEMethodBase):
                                                       requires_grad=False)
                 layer.w2_weight = torch.nn.Parameter(shuffled_w2,
                                                      requires_grad=False)
+            
+            if current_platform.is_xpu():
+                import intel_extension_for_pytorch as ipex
+                layer.ipex_fusion = ipex.llm.modules.GatedMLPMOE(
+                    layer.w13_weight,
+                    layer.w2_weight,
+                    w1_scale_inv=(layer.w13_weight_scale_inv
+                        if self.block_quant else layer.w13_weight_scale),
+                    w2_scale_inv=(layer.w2_weight_scale_inv
+                        if self.block_quant else layer.w2_weight_scale),
+                    a1_scale_inv=layer.w13_input_scale,
+                    a2_scale_inv=layer.w2_input_scale,
+                    use_prepack=True,
+                )
+
+            return
+
         # If checkpoint is fp8, we need to handle that the
         # MoE kernels require single activation scale and single weight
         # scale for w13 per expert.
@@ -796,6 +831,26 @@ class Fp8MoEMethod(FusedMoEMethodBase):
         apply_router_weight_on_input: bool = False,
         activation: str = "silu",
     ) -> torch.Tensor:
+        if current_platform.is_xpu():
+            return self.forward_xpu(
+                x=x,
+                layer=layer,
+                router_logits=router_logits,
+                top_k=top_k,
+                renormalize=renormalize,
+                use_grouped_topk=use_grouped_topk,
+                topk_group=topk_group,
+                num_expert_group=num_expert_group,
+                global_num_experts=global_num_experts,
+                expert_map=expert_map,
+                custom_routing_function=custom_routing_function,
+                scoring_func=scoring_func,
+                e_score_correction_bias=e_score_correction_bias,
+                activation=activation,
+                apply_router_weight_on_input=apply_router_weight_on_input)
+
+        from vllm.model_executor.layers.fused_moe import fused_experts
+
         topk_weights, topk_ids = FusedMoE.select_experts(
             hidden_states=x,
             router_logits=router_logits,
@@ -866,6 +921,29 @@ class Fp8MoEMethod(FusedMoEMethodBase):
                 a2_scale=layer.w2_input_scale,
             )
 
+    def forward_xpu(
+            self,
+            layer: torch.nn.Module,
+            x: torch.Tensor,
+            use_grouped_topk: bool,
+            top_k: int,
+            router_logits: torch.Tensor,
+            renormalize: bool,
+            topk_group: Optional[int] = None,
+            num_expert_group: Optional[int] = None,
+            custom_routing_function: Optional[Callable] = None,
+            **kwargs,
+    ):
+        assert custom_routing_function is None
+        return layer.ipex_fusion(
+            x,
+            use_grouped_topk,
+            top_k,
+            router_logits,
+            renormalize,
+            topk_group,
+            num_expert_group,
+        )
 
 class Fp8KVCacheMethod(BaseKVCacheMethod):
     """
diff --git a/vllm/model_executor/layers/quantization/ipex_quant.py b/vllm/model_executor/layers/quantization/ipex_quant.py
index 8108c7976..f4b4dca93 100644
--- a/vllm/model_executor/layers/quantization/ipex_quant.py
+++ b/vllm/model_executor/layers/quantization/ipex_quant.py
@@ -3,6 +3,7 @@
 from typing import Any, Optional
 
 import torch
+import time
 
 from vllm.model_executor.layers.linear import (LinearBase, LinearMethodBase,
                                                UnquantizedLinearMethod)
@@ -12,10 +13,17 @@ from vllm.model_executor.layers.quantization.awq import (AWQLinearMethod,
 from vllm.model_executor.layers.quantization.base_config import (
     QuantizationConfig)
 from vllm.model_executor.layers.quantization.gptq import GPTQLinearMethod
+from vllm.model_executor.parameter import (ModelWeightParameter,
+                                           PerTensorScaleParameter)
+from vllm.model_executor.utils import set_weight_attrs
 from vllm.platforms import current_platform
 
-MIN_IPEX_VERSION = "2.7.0"
+from vllm.model_executor.layers.quantization import register_quantization_config
+from vllm.model_executor.layers.quantization import get_quantization_config
+
 
+MIN_IPEX_VERSION = "2.7.0"
+ACTIVATION_SCHEMES = ["static", "dynamic"]
 
 class IPEXConfig(QuantizationConfig):
     """INT8 quantization config class using IPEX for the CPU/XPU backend,
@@ -35,6 +43,7 @@ class IPEXConfig(QuantizationConfig):
         modules_to_not_convert: Optional[list[str]] = None,
         desc_act: Optional[bool] = None,
         lm_head_quantized: Optional[bool] = None,
+        is_checkpoint_fp8_serialized: bool = False,
     ) -> None:
         super().__init__()
         self.method = method
@@ -44,14 +53,15 @@ class IPEXConfig(QuantizationConfig):
         self.desc_act = desc_act
         self.lm_head_quantized = lm_head_quantized
         self.pack_factor = 32 // self.weight_bits
-
-        if self.weight_bits not in [4]:
-            raise ValueError(f"IPEX quantization supports weight bits [4], "
-                             f"but got {self.weight_bits}.")
-
-        if self.method not in ["awq", "gptq"]:
-            raise ValueError(f"IPEX quantization supports [awq, gptq], "
+        self.is_checkpoint_fp8_serialized = is_checkpoint_fp8_serialized
+        if self.method not in ["awq", "gptq", "auto-round", "fp8"]:
+            raise ValueError(f"IPEX quantization supports [awq, gptq, auto-round, fp8], "
                              f"but got {self.method}.")
+        if is_checkpoint_fp8_serialized:
+            self.quant_method = "fp8"
+            print("Detected fp8 checkpoint. Please note that the "
+                   "format is experimental and subject to change.")
+        self.activation_scheme = "dynamic"
 
     def __repr__(self) -> str:
         return (f"IPEXConfig(method={self.method},"
@@ -93,9 +103,13 @@ class IPEXConfig(QuantizationConfig):
         group_size = cls.get_from_keys(config, ["group_size"])
         lm_head_quantized = cls.get_from_keys_or(config, ["lm_head"],
                                                  default=False)
+        data_type = cls.get_from_keys_or(config, ["data_type"],
+                                      default="int4")
+        is_checkpoint_fp8_serialized = ("fp8" in data_type)
+
         desc_act = cls.get_from_keys_or(config, ["desc_act"], default=False)
         return cls(method, weight_bits, group_size, [], desc_act,
-                   lm_head_quantized)
+                   lm_head_quantized, is_checkpoint_fp8_serialized)
 
     @classmethod
     def override_quantization_method(
@@ -105,7 +119,7 @@ class IPEXConfig(QuantizationConfig):
 
         quant_method = hf_quant_cfg.get("quant_method", "").lower()
 
-        if quant_method in ["awq", "gptq"]:
+        if quant_method in ["awq", "gptq", "auto-round", "fp8"]:
             return cls.get_name()
 
         return None
@@ -119,8 +133,85 @@ class IPEXConfig(QuantizationConfig):
                 return IPEXAWQLinearMethod(self)
             if self.method == "gptq":
                 return IPEXGPTQLinearMethod(self)
+            if self.method == "auto-round" or self.method == "fp8":
+                return IPEXAutoRoundLinearMethod(self)
         return None
 
+class IPEXAutoRoundLinearMethod(LinearMethodBase):
+    def __init__(self, quant_config: IPEXConfig):
+        self.quant_config = quant_config
+        self.out_dtype = torch.get_default_dtype()
+
+    def create_weights(
+            self,
+            layer: torch.nn.Module,
+            input_size_per_partition: int,
+            output_partition_sizes: list[int],
+            input_size: int,
+            output_size: int,
+            params_dtype: torch.dtype,
+            **extra_weight_attrs,
+    ):
+        # maybe_create_device_identity()
+
+        output_size_per_partition = sum(output_partition_sizes)
+        weight_loader = extra_weight_attrs.get("weight_loader")
+        layer.logical_widths = output_partition_sizes
+
+        layer.input_size_per_partition = input_size_per_partition
+        layer.output_size_per_partition = output_size_per_partition
+        layer.orig_dtype = params_dtype
+        # WEIGHT
+        weight_dtype = (torch.float8_e5m2
+                        if self.quant_config.is_checkpoint_fp8_serialized else
+                        params_dtype)
+
+        weight = ModelWeightParameter(data=torch.empty(
+            output_size_per_partition,
+            input_size_per_partition,
+            dtype=weight_dtype),
+            input_dim=1,
+            output_dim=0,
+            weight_loader=weight_loader)
+        layer.register_parameter("weight", weight)
+
+        # If checkpoint is serialized fp8, load them.
+        # Otherwise, wait until process_weights_after_loading.
+        if self.quant_config.is_checkpoint_fp8_serialized:
+            # WEIGHT SCALE
+            scale = PerTensorScaleParameter(
+                data=torch.empty(len(output_partition_sizes),
+                                 dtype=torch.float32),
+                weight_loader=weight_loader,
+            )
+            scale[:] = torch.finfo(torch.float32).min
+            set_weight_attrs(scale, {"scale_type": "weight_scale"})
+            set_weight_attrs(scale, {"needs_scalar_to_array": True})
+            layer.register_parameter("weight_scale", scale)
+            # INPUT ACTIVATION SCALE
+            if self.quant_config.activation_scheme == "static":
+                scale = PerTensorScaleParameter(data=torch.empty(
+                    1, dtype=torch.float32),
+                    weight_loader=weight_loader)
+
+                scale[:] = torch.finfo(torch.float32).min
+                set_weight_attrs(scale, {"scale_type": "input_scale"})
+                layer.register_parameter("input_scale", scale)
+            else:
+                layer.register_parameter("input_scale", None)
+
+    def process_weights_after_loading(self, layer: torch.nn.Module) -> None:
+        pass
+
+    def apply(self,
+              layer: torch.nn.Module,
+              x: torch.Tensor,
+              bias: Optional[torch.Tensor] = None) -> torch.Tensor:
+        weight = layer.weight.data
+        scale = layer.weight_scale.data
+
+        output = torch.ops.vllm.fp8_gemm(x, False, weight, True, None, x.dtype, None, scale, bias, False)
+        return output
 
 class IPEXGPTQLinearMethod(GPTQLinearMethod):
     """GPTQ linear method using IPEX for the CPU/XPU backend.
diff --git a/vllm/model_executor/layers/quantization/sym_int4.py b/vllm/model_executor/layers/quantization/sym_int4.py
new file mode 100644
index 000000000..137f29a82
--- /dev/null
+++ b/vllm/model_executor/layers/quantization/sym_int4.py
@@ -0,0 +1,220 @@
+# SPDX-License-Identifier: Apache-2.0
+
+from typing import Any, Dict, List, Optional, Tuple
+# from vllm.model_executor.layers.quantization import register_quantization_config
+from vllm.model_executor.layers.quantization.base_config import (
+    QuantizationConfig, QuantizeMethodBase)
+from vllm.model_executor.layers.linear import (LinearBase, LinearMethodBase,
+                                               UnquantizedLinearMethod)
+from vllm.model_executor.parameter import (BlockQuantScaleParameter,
+                                           ModelWeightParameter,
+                                           PerTensorScaleParameter)
+
+import torch
+from torch.nn import Module
+from torch.nn.parameter import Parameter
+# from vllm.model_executor.layers.quantization.ipex_quant import MIN_IPEX_VERSION
+
+from vllm.envs import VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT, VLLM_QUANTIZE_Q40_LIB
+import ctypes
+
+MIN_IPEX_VERSION = "2.5.0"
+class SymInt4Config(QuantizationConfig):
+    """SYM_INT4 quantization config class which uses IPEX kernel behind the scene...
+    The weight will be quantized according to GPTQ setups...
+    """
+    def __init__(
+        self,
+    ) -> None:
+        super().__init__()
+
+
+    @classmethod
+    def get_name(cls) -> str:
+        return "sym_int4"
+
+
+    @classmethod
+    def get_supported_act_dtypes(cls) -> List[torch.dtype]:
+        return [torch.half]
+
+
+    @classmethod
+    def get_min_capability(cls) -> int:
+        # TODO: check if this will affect things...
+        # May need to check platform xpu
+        return -1
+
+
+    @classmethod
+    def get_config_filenames(cls) -> List[str]:
+        return []
+
+
+    @classmethod
+    def from_config(cls, config: Dict[str, Any]) -> "SymInt4Config":
+        return cls()
+
+    @classmethod
+    def get_quant_method(self, layer: torch.nn.Module,
+                         prefix: str) -> Optional["QuantizeMethodBase"]:
+        """Get the quantize method to use for the quantized layer.
+
+        Args:
+            layer: The layer for the quant method.
+            prefix: The full name of the layer in the state dict
+        Returns:
+            The quantize method. None if the given layer doesn't support quant
+            method.
+        """
+        if isinstance(layer, LinearBase):
+            return SymInt4LinearMethod(self)
+        else:
+            return None
+
+
+
+class SymInt4LinearMethod(LinearMethodBase):
+    def __init__(self, quant_config: SymInt4Config):
+        self.quant_config = quant_config
+        # Initialize the quant_config
+        try:
+            self.clib = ctypes.CDLL(VLLM_QUANTIZE_Q40_LIB)
+        except OSError as e:
+            raise RuntimeError(f"Failed to load required quantization lib at {VLLM_QUANTIZE_Q40_LIB}: {e}")
+        self.clib.quantize_q4_0_to_qweight_and_scale.argtypes = [
+            ctypes.POINTER(ctypes.c_float),
+            ctypes.POINTER(ctypes.c_int32),
+            ctypes.POINTER(ctypes.c_uint16),
+            ctypes.c_int,
+            ctypes.c_int,
+        ]
+        self.clib.quantize_q4_0_to_qweight_and_scale.restype = ctypes.c_size_t
+
+    def ggml_quantize_tensor(self, weight: torch.Tensor, out_qweight: torch.Tensor, out_scale:torch.Tensor, out_features: int, in_features: int):
+        # Convert src to float *
+        # Currently, only handles dimension = 2
+        assert(weight.dim()==2)
+        assert out_qweight.shape == (out_features, in_features // 8)
+        assert out_scale.shape == (out_features, in_features // 64)
+
+        assert weight.dtype == torch.float32
+        assert out_qweight.dtype == torch.int32
+        assert out_scale.dtype == torch.float16
+
+        assert(out_qweight.is_contiguous())
+        assert(out_scale.is_contiguous())
+        src = weight.data.data_ptr()
+        src = ctypes.cast(src, ctypes.POINTER(ctypes.c_float))
+
+        qweight = out_qweight.data.data_ptr()
+        qweight = ctypes.cast(qweight, ctypes.POINTER(ctypes.c_int32))
+
+        scale = out_scale.data.data_ptr()
+        scale = ctypes.cast(scale, ctypes.POINTER(ctypes.c_uint16))
+        self.clib.quantize_q4_0_to_qweight_and_scale(src, qweight, scale, out_features, in_features)
+        out_qweight = out_qweight.transpose(0,1).contiguous()
+        out_scale = out_scale.transpose(0,1).contiguous()
+        return out_qweight, out_scale
+
+
+    def create_weights(
+        self,
+        layer: torch.nn.Module,
+        input_size_per_partition: int,
+        output_partition_sizes: List[int],
+        input_size: int,
+        output_size: int,
+        params_dtype: torch.dtype,
+        **extra_weight_attrs,
+    ):
+        output_size_per_partition = sum(output_partition_sizes)
+        weight_loader = extra_weight_attrs.get("weight_loader")
+
+        layer.logical_widths = output_partition_sizes
+
+        layer.input_size_per_partition = input_size_per_partition
+        layer.output_size_per_partition = output_size_per_partition
+        layer.orig_dtype = params_dtype
+
+        weight_dtype = params_dtype
+        weight = ModelWeightParameter(data=torch.empty(
+            output_size_per_partition,
+            input_size_per_partition,
+            dtype=weight_dtype,
+            device="cpu" if VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT else None),
+                                      input_dim=1,
+                                      output_dim=0,
+                                      weight_loader=weight_loader)
+        layer.register_parameter("weight", weight)
+
+
+    def apply(self,
+              layer: torch.nn.Module,
+              x: torch.Tensor,
+              bias: Optional[torch.Tensor] = None) -> torch.Tensor:
+        # The same with the GPTQ's linear method by IPEX
+        reshaped_x = x.reshape(-1, x.shape[-1])
+        out = layer.ipex_qlinear(reshaped_x)
+        if bias is not None:
+            out.add_(bias)
+        return out.reshape(x.shape[:-1] + (layer.ipex_output_size, ))
+
+
+    def process_weights_after_loading(self, layer: Module) -> None:
+        weight = layer.weight.float()
+        out_features = layer.weight.shape[0]
+        in_features = layer.weight.shape[1]
+
+        qweight = torch.zeros((out_features, in_features // 8), dtype=torch.int32, device=layer.weight.device)
+        scale = torch.zeros((out_features, in_features // 64), dtype=torch.float16, device=layer.weight.device)
+        qweight, scale = self.ggml_quantize_tensor(weight, qweight, scale, out_features, in_features)
+        
+        qweight = qweight.to("xpu")
+        scale = scale.to("xpu")
+
+        # Use qweight to replace weight...
+        layer.weight = Parameter(qweight, requires_grad=False)
+        # qweight_scale
+        layer.weight_scale = Parameter(scale, requires_grad=False)
+        # layer.input_scale = None
+        try:
+            import intel_extension_for_pytorch as ipex
+            if ipex.__version__ < MIN_IPEX_VERSION:
+                raise ImportError(
+                    "intel_extension_for_pytorch version is "
+                    "wrong. Please install "
+                    f"intel_extension_for_pytorch>={MIN_IPEX_VERSION}.")
+        except ImportError as err:
+            raise ImportError(
+                "Please install "
+                f"intel_extension_for_pytorch>={MIN_IPEX_VERSION} via "
+                f"`pip install intel_extension_for_pytorch>={MIN_IPEX_VERSION}`"
+                " to use IPEX-AWQ linear method.") from err
+        lowp_mode = ipex.quantization.WoqLowpMode.INT8
+        # The weight will be de-packed from INT4 to INT8.
+        weight_dtype = ipex.quantization.WoqWeightDtype.INT4
+        # The float activation will be quantized (dynamic, per-token) to INT8.
+        act_quant_mode = ipex.quantization.WoqActQuantMode.PER_BATCH_IC_BLOCK
+        qconfig = ipex.quantization.get_weight_only_quant_qconfig_mapping(
+            weight_dtype=weight_dtype,
+            lowp_mode=lowp_mode,
+            act_quant_mode=act_quant_mode,
+            group_size=64,
+        )
+        layer.ipex_output_size = layer.weight.shape[-1]
+        g_idx = None
+        layer.ipex_qlinear = ipex.llm.quantization.woq_linear. \
+            IPEXWeightOnlyQuantizedLinear.from_weight(
+            layer.weight,     # weight should be on xpu...
+            layer.weight_scale,
+            torch.tensor([8], device=layer.weight.device, dtype=torch.int8),
+            layer.weight.size(0),
+            layer.ipex_output_size,
+            qconfig=qconfig,
+            g_idx=g_idx,
+            bias=None,
+            group_size=64,
+            # For GPTQ layout
+            quant_method=0
+        )
diff --git a/vllm/model_executor/layers/rotary_embedding.py b/vllm/model_executor/layers/rotary_embedding.py
index 70463ecd9..79a277d5c 100644
--- a/vllm/model_executor/layers/rotary_embedding.py
+++ b/vllm/model_executor/layers/rotary_embedding.py
@@ -22,6 +22,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Rotary Positional Embeddings."""
+import itertools
 import math
 from typing import Any, Optional, Union
 
@@ -1023,6 +1024,8 @@ class MRotaryEmbedding(RotaryEmbedding):
         assert positions.ndim == 1 or positions.ndim == 2
         assert key is not None
 
+        return self.forward_xpu(positions, query, key)
+        '''
         num_tokens = positions.shape[-1]
         cos_sin = self.cos_sin_cache[positions]
         cos, sin = cos_sin.chunk(2, dim=-1)
@@ -1054,6 +1057,7 @@ class MRotaryEmbedding(RotaryEmbedding):
         key_rot = _apply_rotary_emb(key_rot, cos, sin, self.is_neox_style)
         key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
         return query, key
+        '''
 
     @classmethod
     def get_input_positions(
@@ -1116,6 +1120,15 @@ class MRotaryEmbedding(RotaryEmbedding):
                 audio_feature_lengths=audio_feature_lengths,
                 use_audio_in_video=use_audio_in_video,
             )
+        elif "glm4v" in hf_config.model_type:
+            return cls._glm4v_get_input_positions_tensor(
+                input_tokens=input_tokens,
+                hf_config=hf_config,
+                image_grid_thw=image_grid_thw,
+                video_grid_thw=video_grid_thw,
+                context_len=context_len,
+                seq_len=seq_len,
+            )
         else:
             return cls._vl_get_input_positions_tensor(
                 input_tokens=input_tokens,
@@ -1127,6 +1140,115 @@ class MRotaryEmbedding(RotaryEmbedding):
                 seq_len=seq_len,
             )
 
+    @classmethod
+    def _glm4v_get_input_positions_tensor(
+        cls,
+        input_tokens: list[int],
+        hf_config: PretrainedConfig,
+        image_grid_thw: Union[list[list[int]], torch.Tensor],
+        video_grid_thw: Union[list[list[int]], torch.Tensor],
+        context_len: int = 0,
+        seq_len: Optional[int] = None,
+    ) -> tuple[torch.Tensor, int]:
+        """Get mrope input positions and delta value for GLM4V."""
+
+        image_token_id = hf_config.image_token_id
+        video_start_token_id = hf_config.video_start_token_id
+        video_end_token_id = hf_config.video_end_token_id
+        spatial_merge_size = hf_config.vision_config.spatial_merge_size
+        llm_pos_ids_list: list = []
+
+        if not (image_grid_thw is None and video_grid_thw is None):
+            if isinstance(image_grid_thw, torch.Tensor):
+                image_grid_thw = image_grid_thw.tolist()
+
+            input_token_type: list[str] = []
+            video_check_flg = False
+            for token in input_tokens:
+                if token == video_start_token_id:
+                    video_check_flg = True
+                elif token == video_end_token_id:
+                    video_check_flg = False
+
+                if (token == image_token_id) and (video_check_flg is False):
+                    input_token_type.append("image")
+                elif (token == image_token_id) and (video_check_flg is True):
+                    input_token_type.append("video")
+                else:
+                    input_token_type.append("text")
+
+            input_type_group: list[tuple[str, int, int]] = []
+            for key, group_iter in itertools.groupby(
+                    enumerate(input_token_type), lambda x: x[1]):
+                group_list = list(group_iter)
+                start_index = group_list[0][0]
+                end_index = group_list[-1][0] + 1
+                input_type_group.append((key, start_index, end_index))
+
+            video_frame_num = 1
+            mm_data_idx = 0
+            for modality_type, start_idx, end_idx in input_type_group:
+                st_idx = llm_pos_ids_list[-1].max() + 1 if len(
+                    llm_pos_ids_list) > 0 else 0
+                if modality_type == "image":
+                    t, h, w = (
+                        image_grid_thw[mm_data_idx][0],
+                        image_grid_thw[mm_data_idx][1],
+                        image_grid_thw[mm_data_idx][2],
+                    )
+                    llm_grid_t, llm_grid_h, llm_grid_w = \
+                        t, h // spatial_merge_size, w // spatial_merge_size
+
+                    t_index = torch.arange(llm_grid_t).view(-1, 1).expand(
+                        -1, llm_grid_h * llm_grid_w).flatten()
+                    h_index = torch.arange(llm_grid_h).view(1, -1, 1).expand(
+                        llm_grid_t, -1, llm_grid_w).flatten()
+                    w_index = torch.arange(llm_grid_w).view(1, 1, -1).expand(
+                        llm_grid_t, llm_grid_h, -1).flatten()
+                    llm_pos_ids_list.append(
+                        torch.stack([t_index, h_index, w_index]) + st_idx)
+                    mm_data_idx += 1
+
+                elif modality_type == "video":
+                    t, h, w = (
+                        video_frame_num,
+                        image_grid_thw[mm_data_idx][1],
+                        image_grid_thw[mm_data_idx][2],
+                    )
+                    llm_grid_t, llm_grid_h, llm_grid_w = \
+                        t, h // spatial_merge_size, w // spatial_merge_size
+
+                    for t_idx in range(llm_grid_t):
+                        t_index = torch.tensor(t_idx).view(-1, 1).expand(
+                            -1, llm_grid_h * llm_grid_w).flatten()
+                        h_index = torch.arange(llm_grid_h).view(
+                            1, -1, 1).expand(1, -1, llm_grid_w).flatten()
+                        w_index = torch.arange(llm_grid_w).view(
+                            1, 1, -1).expand(1, llm_grid_h, -1).flatten()
+                        llm_pos_ids_list.append(
+                            torch.stack([t_index, h_index, w_index]) + st_idx)
+
+                    mm_data_idx += 1
+                    video_frame_num += 1
+
+                else:
+                    text_len = end_idx - start_idx
+                    llm_pos_ids_list.append(
+                        torch.arange(text_len).view(1, -1).expand(3, -1) +
+                        st_idx)
+                    video_frame_num = 1
+
+        else:
+            text_len = len(input_tokens)
+            llm_pos_ids_list.append(
+                torch.arange(text_len).view(1, -1).expand(3, -1))
+
+        llm_positions = torch.cat(llm_pos_ids_list, dim=1).reshape(3, -1)
+        llm_positions = llm_positions[:, context_len:seq_len]
+        mrope_position_delta = (llm_positions.max() + 1 -
+                                len(input_tokens)).item()
+        return llm_positions, mrope_position_delta
+
     @classmethod
     def _vl_get_input_positions_tensor(
         cls,
diff --git a/vllm/model_executor/model_loader/utils.py b/vllm/model_executor/model_loader/utils.py
index 9c8d647a2..d75196874 100644
--- a/vllm/model_executor/model_loader/utils.py
+++ b/vllm/model_executor/model_loader/utils.py
@@ -15,6 +15,7 @@ from transformers.dynamic_module_utils import get_class_from_dynamic_module
 from vllm.attention import Attention
 from vllm.config import (ModelConfig, ModelImpl, VllmConfig,
                          set_current_vllm_config)
+from vllm.envs import VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT
 from vllm.logger import init_logger
 from vllm.model_executor.layers.linear import QKVCrossParallelLinear
 from vllm.model_executor.layers.quantization.base_config import (
@@ -24,6 +25,7 @@ from vllm.model_executor.models.adapters import (as_classification_model,
                                                  as_embedding_model,
                                                  as_reward_model)
 from vllm.utils import is_pin_memory_available
+from vllm.model_executor.layers.quantization.sym_int4 import SymInt4LinearMethod
 
 logger = init_logger(__name__)
 
@@ -91,7 +93,8 @@ def initialize_model(
 
 def process_weights_after_loading(model: nn.Module, model_config: ModelConfig,
                                   target_device: torch.device) -> None:
-    for _, module in model.named_modules():
+    modules_to_not_convert=["visual", "vision", "vpm", "resampler"]
+    for name, module in model.named_modules():
         if isinstance(module, QKVCrossParallelLinear):
             # NOTE(Isotr0py): special case for cross QKV layer because
             # q and kv proj aren't registered as submodules intentionally
@@ -99,12 +102,18 @@ def process_weights_after_loading(model: nn.Module, model_config: ModelConfig,
             continue
         quant_method = getattr(module, "quant_method", None)
         if isinstance(quant_method, QuantizeMethodBase):
-            # When quant methods need to process weights after loading
-            # (for repacking, quantizing, etc), they expect parameters
-            # to be on the global target device. This scope is for the
-            # case where cpu offloading is used, where we will move the
-            # parameters onto device for processing and back off after.
-            with device_loading_context(module, target_device):
+            # The quantization of SYM_INT4 happens on CPU instead of XPU.
+            # We uses the parameter quantization_on_cpu=isinstance(quant_method, SymInt4LinearMethod)
+            # to skip moving tensors to XPU
+            with device_loading_context(module, target_device, isinstance(quant_method, SymInt4LinearMethod)):
+                # When quant methods need to process weights after loading
+                # (for repacking, quantizing, etc), they expect parameters
+                # to be on the global target device. This scope is for the
+                # case where cpu offloading is used, where we will move the
+                # parameters onto device for processing and back off after.
+                if any(key in name for key in modules_to_not_convert):
+                    continue
+
                 quant_method.process_weights_after_loading(module)
 
     # Currently only used by MLA.
@@ -120,7 +129,8 @@ def process_weights_after_loading(model: nn.Module, model_config: ModelConfig,
 
 @contextmanager
 def device_loading_context(module: torch.nn.Module,
-                           target_device: torch.device):
+                           target_device: torch.device,
+                           quantization_on_cpu: False):
     if target_device.type == "cpu":
         # If target is CPU, no need to move anything
         yield module
@@ -129,36 +139,41 @@ def device_loading_context(module: torch.nn.Module,
     original_device_states: dict[str, torch.device] = {}
 
     # Store original device states and move parameters to GPU if they're on CPU
-    for name, p in module.named_parameters():
-        if p.device.type == "cpu":
-            original_device_states[name] = p.device
-            p.data = p.data.to(target_device)
-        # Parameters already on target device are not touched
+    if not quantization_on_cpu:
+        for name, p in module.named_parameters():
+            if p.device.type == "cpu":
+                original_device_states[name] = p.device
+                p.data = p.data.to(target_device)
+            # Parameters already on target device are not touched
 
     try:
         yield module
 
     finally:
-        # Restore parameters to their original devices, ignoring new parameters
-        pin_memory = is_pin_memory_available()
-        for name, p in module.named_parameters():
-            if name in original_device_states:
-                original_device: torch.device = original_device_states[name]
-                if original_device.type == "cpu":
-                    # `torch.empty_like` does not support `pin_memory` argument
-                    cpu_data = torch.empty_strided(
-                        size=p.data.size(),
-                        stride=p.data.stride(),
-                        dtype=p.data.dtype,
-                        layout=p.data.layout,
-                        device="cpu",
-                        pin_memory=pin_memory,
-                    )
-                    cpu_data.copy_(p.data)
-                    p.data = cpu_data
-                else:
-                    p.data = p.data.to(original_device)
-        # New parameters or parameters already on target device are untouched
+        # If weights were loaded onto the CPU for FP8 online quantization, there
+        # is no need to move them back to the original device.
+        if not VLLM_OFFLOAD_WEIGHTS_BEFORE_QUANT:
+            # Restore parameters to their original devices, ignoring new parameters # noqa: E501
+            pin_memory = is_pin_memory_available()
+            for name, p in module.named_parameters():
+                if name in original_device_states:
+                    original_device: torch.device = original_device_states[
+                        name]
+                    if original_device.type == "cpu":
+                        # `torch.empty_like` does not support `pin_memory` argument # noqa: E501
+                        cpu_data = torch.empty_strided(
+                            size=p.data.size(),
+                            stride=p.data.stride(),
+                            dtype=p.data.dtype,
+                            layout=p.data.layout,
+                            device="cpu",
+                            pin_memory=pin_memory,
+                        )
+                        cpu_data.copy_(p.data)
+                        p.data = cpu_data
+                    else:
+                        p.data = p.data.to(original_device)
+            # New parameters or parameters already on target device are untouched # noqa: E501
 
 
 def resolve_transformers_arch(model_config: ModelConfig,
diff --git a/vllm/model_executor/models/dots1.py b/vllm/model_executor/models/dots1.py
new file mode 100644
index 000000000..9b21a7944
--- /dev/null
+++ b/vllm/model_executor/models/dots1.py
@@ -0,0 +1,544 @@
+# SPDX-License-Identifier: Apache-2.0
+# SPDX-FileCopyrightText: Copyright contributors to the vLLM project
+
+# Adapted from
+# https://github.com/huggingface/transformers/blob/v4.28.0/src/transformers/models/llama/modeling_llama.py
+# Copyright 2025 The rednote-hilab team.
+# Copyright 2023 The vLLM team.
+# Copyright 2023 DeepSeek-AI and the HuggingFace Inc. team. All rights reserved.
+#
+# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX
+# and OPT implementations in this library. It has been modified from its
+# original forms to accommodate minor architectural differences compared
+# to GPT-NeoX and OPT used by the Meta AI team that trained the model.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+"""Inference-only dots1 model."""
+from collections.abc import Iterable
+from typing import Any, Optional, Union
+
+import torch
+from torch import nn
+from transformers import PretrainedConfig
+
+from vllm.attention import Attention
+from vllm.compilation.decorators import support_torch_compile
+from vllm.config import CacheConfig, ModelConfig, VllmConfig
+from vllm.distributed import (get_pp_group,
+                              get_tensor_model_parallel_world_size,
+                              tensor_model_parallel_all_reduce)
+from vllm.model_executor.layers.activation import SiluAndMul
+from vllm.model_executor.layers.fused_moe import FusedMoE
+from vllm.model_executor.layers.layernorm import RMSNorm
+from vllm.model_executor.layers.linear import (MergedColumnParallelLinear,
+                                               QKVParallelLinear,
+                                               ReplicatedLinear,
+                                               RowParallelLinear)
+from vllm.model_executor.layers.logits_processor import LogitsProcessor
+from vllm.model_executor.layers.quantization import QuantizationConfig
+from vllm.model_executor.layers.rotary_embedding import get_rope
+from vllm.model_executor.layers.vocab_parallel_embedding import (
+    ParallelLMHead, VocabParallelEmbedding)
+from vllm.model_executor.model_loader.weight_utils import (
+    default_weight_loader, maybe_remap_kv_scale_name)
+from vllm.model_executor.sampling_metadata import SamplingMetadata
+from vllm.sequence import IntermediateTensors
+
+from .interfaces import SupportsLoRA, SupportsPP
+from .utils import (AutoWeightsLoader, PPMissingLayer, is_pp_missing_parameter,
+                    make_empty_intermediate_tensors_factory, make_layers,
+                    maybe_prefix)
+
+
+class Dots1MLP(nn.Module):
+
+    def __init__(
+        self,
+        hidden_size: int,
+        intermediate_size: int,
+        hidden_act: str,
+        quant_config: Optional[QuantizationConfig] = None,
+        reduce_results: bool = True,
+        prefix: str = "",
+    ) -> None:
+        super().__init__()
+        self.gate_up_proj = MergedColumnParallelLinear(
+            hidden_size, [intermediate_size] * 2,
+            bias=False,
+            quant_config=quant_config,
+            prefix=f"{prefix}.gate_up_proj")
+        self.down_proj = RowParallelLinear(intermediate_size,
+                                           hidden_size,
+                                           bias=False,
+                                           quant_config=quant_config,
+                                           reduce_results=reduce_results,
+                                           prefix=f"{prefix}.down_proj")
+        if hidden_act != "silu":
+            raise ValueError(f"Unsupported activation: {hidden_act}. "
+                             "Only silu is supported for now.")
+        self.act_fn = SiluAndMul()
+
+    def forward(self, x):
+        gate_up, _ = self.gate_up_proj(x)
+        x = self.act_fn(gate_up)
+        x, _ = self.down_proj(x)
+        return x
+
+
+class Dots1MoE(nn.Module):
+
+    def __init__(
+        self,
+        config: PretrainedConfig,
+        quant_config: Optional[QuantizationConfig] = None,
+        prefix: str = "",
+    ):
+        super().__init__()
+        self.tp_size = get_tensor_model_parallel_world_size()
+        self.routed_scaling_factor = config.routed_scaling_factor
+        self.n_shared_experts = config.n_shared_experts
+
+        if config.hidden_act != "silu":
+            raise ValueError(f"Unsupported activation: {config.hidden_act}. "
+                             "Only silu is supported for now.")
+
+        self.gate = ReplicatedLinear(config.hidden_size,
+                                     config.n_routed_experts,
+                                     bias=False,
+                                     quant_config=None,
+                                     prefix=f"{prefix}.gate")
+        if config.topk_method == "noaux_tc":
+            self.gate.e_score_correction_bias = (nn.Parameter(
+                torch.empty(config.n_routed_experts)))
+        else:
+            self.gate.e_score_correction_bias = None
+
+        self.experts = FusedMoE(
+            num_experts=config.n_routed_experts,
+            top_k=config.num_experts_per_tok,
+            hidden_size=config.hidden_size,
+            intermediate_size=config.moe_intermediate_size,
+            reduce_results=False,
+            renormalize=config.norm_topk_prob,
+            quant_config=quant_config,
+            use_grouped_topk=True,
+            num_expert_group=config.n_group,
+            topk_group=config.topk_group,
+            prefix=f"{prefix}.experts",
+            scoring_func=config.scoring_func,
+            e_score_correction_bias=self.gate.e_score_correction_bias)
+
+        if config.n_shared_experts is not None:
+            intermediate_size = (config.moe_intermediate_size *
+                                 config.n_shared_experts)
+            self.shared_experts = Dots1MLP(
+                hidden_size=config.hidden_size,
+                intermediate_size=intermediate_size,
+                hidden_act=config.hidden_act,
+                quant_config=quant_config,
+                reduce_results=False,
+                prefix=f"{prefix}.shared_experts",
+            )
+
+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
+        num_tokens, hidden_dim = hidden_states.shape
+        hidden_states = hidden_states.view(-1, hidden_dim)
+        if self.n_shared_experts is not None:
+            shared_output = self.shared_experts(hidden_states)
+        router_logits, _ = self.gate(hidden_states)
+        final_hidden_states = self.experts(
+            hidden_states=hidden_states,
+            router_logits=router_logits) * self.routed_scaling_factor
+        if shared_output is not None:
+            final_hidden_states = final_hidden_states + shared_output
+        if self.tp_size > 1:
+            final_hidden_states = tensor_model_parallel_all_reduce(
+                final_hidden_states)
+        return final_hidden_states.view(num_tokens, hidden_dim)
+
+
+class Dots1Attention(nn.Module):
+
+    def __init__(
+        self,
+        hidden_size: int,
+        num_heads: int,
+        num_kv_heads: int,
+        config: PretrainedConfig,
+        rope_theta: float = 10000,
+        rope_scaling: Optional[dict[str, Any]] = None,
+        max_position_embeddings: int = 8192,
+        cache_config: Optional[CacheConfig] = None,
+        quant_config: Optional[QuantizationConfig] = None,
+        prefix: str = "",
+    ) -> None:
+        super().__init__()
+        self.hidden_size = hidden_size
+        tp_size = get_tensor_model_parallel_world_size()
+        self.total_num_heads = num_heads
+        assert self.total_num_heads % tp_size == 0
+        self.num_heads = self.total_num_heads // tp_size
+        self.total_num_kv_heads = num_kv_heads
+        if self.total_num_kv_heads >= tp_size:
+            # Number of KV heads is greater than TP size, so we partition
+            # the KV heads across multiple tensor parallel GPUs.
+            assert self.total_num_kv_heads % tp_size == 0
+        else:
+            # Number of KV heads is less than TP size, so we replicate
+            # the KV heads across multiple tensor parallel GPUs.
+            assert tp_size % self.total_num_kv_heads == 0
+        self.num_kv_heads = max(1, self.total_num_kv_heads // tp_size)
+        self.head_dim = getattr(config, "head_dim",
+                                hidden_size // self.total_num_heads)
+        self.q_size = self.num_heads * self.head_dim
+        self.kv_size = self.num_kv_heads * self.head_dim
+        self.scaling = self.head_dim**-0.5
+        self.rope_theta = rope_theta
+        self.max_position_embeddings = max_position_embeddings
+        attention_bias = config.attention_bias
+
+        self.qkv_proj = QKVParallelLinear(
+            hidden_size,
+            self.head_dim,
+            self.total_num_heads,
+            self.total_num_kv_heads,
+            bias=attention_bias,
+            quant_config=quant_config,
+        )
+
+        self.o_proj = RowParallelLinear(
+            self.total_num_heads * self.head_dim,
+            hidden_size,
+            bias=False,
+            quant_config=quant_config,
+        )
+
+        self.rotary_emb = get_rope(
+            self.head_dim,
+            rotary_dim=self.head_dim,
+            max_position=max_position_embeddings,
+            base=rope_theta,
+            rope_scaling=rope_scaling,
+        )
+        self.attn = Attention(
+            self.num_heads,
+            self.head_dim,
+            self.scaling,
+            num_kv_heads=self.num_kv_heads,
+            cache_config=cache_config,
+            quant_config=quant_config,
+            prefix=f"{prefix}.attn",
+        )
+        self.q_norm = RMSNorm(self.head_dim, eps=config.rms_norm_eps)
+        self.k_norm = RMSNorm(self.head_dim, eps=config.rms_norm_eps)
+
+    def forward(self, positions: torch.Tensor,
+                hidden_states: torch.Tensor) -> torch.Tensor:
+        qkv, _ = self.qkv_proj(hidden_states)
+        q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
+        q = self.q_norm(q.reshape(-1, self.num_heads,
+                                  self.head_dim)).reshape(q.shape)
+        k = self.k_norm(k.reshape(-1, self.num_kv_heads,
+                                  self.head_dim)).reshape(k.shape)
+        q, k = self.rotary_emb(positions, q, k)
+        attn_output = self.attn(q, k, v)
+        output, _ = self.o_proj(attn_output)
+        return output
+
+
+class Dots1DecoderLayer(nn.Module):
+
+    def __init__(
+        self,
+        config: PretrainedConfig,
+        prefix: str,
+        model_config: ModelConfig,
+        cache_config: Optional[CacheConfig] = None,
+        quant_config: Optional[QuantizationConfig] = None,
+    ) -> None:
+        super().__init__()
+        self.hidden_size = config.hidden_size
+        rope_theta = getattr(config, "rope_theta", 10000)
+        rope_scaling = getattr(config, "rope_scaling", None)
+        max_position_embeddings = getattr(config, "max_position_embeddings",
+                                          8192)
+        layer_idx = int(prefix.split(sep='.')[-1])
+        self.layer_idx = layer_idx
+
+        self.self_attn = Dots1Attention(
+            hidden_size=self.hidden_size,
+            num_heads=config.num_attention_heads,
+            num_kv_heads=config.num_key_value_heads,
+            config=config,
+            rope_theta=rope_theta,
+            rope_scaling=rope_scaling,
+            max_position_embeddings=max_position_embeddings,
+            cache_config=cache_config,
+            quant_config=quant_config,
+            prefix=f"{prefix}.self_attn",
+        )
+        if (config.n_routed_experts is not None
+                and layer_idx >= config.first_k_dense_replace
+                and layer_idx % config.moe_layer_freq == 0):
+            self.mlp = Dots1MoE(config=config,
+                                quant_config=quant_config,
+                                prefix=f"{prefix}.mlp")
+        else:
+            self.mlp = Dots1MLP(
+                hidden_size=config.hidden_size,
+                intermediate_size=config.intermediate_size,
+                hidden_act=config.hidden_act,
+                quant_config=quant_config,
+                prefix=f"{prefix}.mlp",
+            )
+        self.input_layernorm = RMSNorm(config.hidden_size,
+                                       eps=config.rms_norm_eps)
+        self.post_attention_layernorm = RMSNorm(config.hidden_size,
+                                                eps=config.rms_norm_eps)
+        self.routed_scaling_factor = config.routed_scaling_factor
+
+    def forward(
+        self,
+        positions: torch.Tensor,
+        hidden_states: torch.Tensor,
+        residual: Optional[torch.Tensor],
+    ) -> torch.Tensor:
+        if residual is None:
+            residual = hidden_states
+            hidden_states = self.input_layernorm(hidden_states)
+        else:
+            hidden_states, residual = self.input_layernorm(
+                hidden_states, residual)
+        hidden_states = self.self_attn(positions=positions,
+                                       hidden_states=hidden_states)
+        hidden_states, residual = self.post_attention_layernorm(
+            hidden_states, residual)
+        hidden_states = self.mlp(hidden_states)
+        return hidden_states, residual
+
+
+@support_torch_compile
+class Dots1Model(nn.Module):
+
+    fall_back_to_pt_during_load = False
+
+    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
+        super().__init__()
+
+        config = vllm_config.model_config.hf_config
+        model_config = vllm_config.model_config
+        cache_config = vllm_config.cache_config
+        quant_config = vllm_config.quant_config
+        self.config = config
+
+        self.vocab_size = config.vocab_size
+
+        if get_pp_group().is_first_rank:
+            self.embed_tokens = VocabParallelEmbedding(
+                config.vocab_size,
+                config.hidden_size,
+                quant_config=quant_config,
+                prefix=f"{prefix}.embed_tokens")
+        else:
+            self.embed_tokens = PPMissingLayer()
+
+        self.start_layer, self.end_layer, self.layers = make_layers(
+            config.num_hidden_layers,
+            lambda prefix: Dots1DecoderLayer(
+                config,
+                prefix,
+                model_config=model_config,
+                cache_config=cache_config,
+                quant_config=quant_config,
+            ),
+            prefix=f"{prefix}.layers")
+
+        if get_pp_group().is_last_rank:
+            self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
+        else:
+            self.norm = PPMissingLayer()
+        self.make_empty_intermediate_tensors = (
+            make_empty_intermediate_tensors_factory(
+                ["hidden_states", "residual"], config.hidden_size))
+
+    def get_input_embeddings(self, input_ids: torch.Tensor) -> torch.Tensor:
+        return self.embed_tokens(input_ids)
+
+    def forward(
+        self,
+        input_ids: torch.Tensor,
+        positions: torch.Tensor,
+        intermediate_tensors: Optional[IntermediateTensors],
+        inputs_embeds: Optional[torch.Tensor] = None,
+    ) -> Union[torch.Tensor, IntermediateTensors]:
+        if get_pp_group().is_first_rank:
+            if inputs_embeds is not None:
+                hidden_states = inputs_embeds
+            else:
+                hidden_states = self.get_input_embeddings(input_ids)
+            residual = None
+        else:
+            assert intermediate_tensors is not None
+            hidden_states = intermediate_tensors["hidden_states"]
+            residual = intermediate_tensors["residual"]
+        for layer in self.layers[self.start_layer:self.end_layer]:
+            hidden_states, residual = layer(
+                positions,
+                hidden_states,
+                residual,
+            )
+        if not get_pp_group().is_last_rank:
+            return IntermediateTensors({
+                "hidden_states": hidden_states,
+                "residual": residual
+            })
+        hidden_states, _ = self.norm(hidden_states, residual)
+        return hidden_states
+
+    def get_expert_mapping(self) -> list[tuple[str, str, int, str]]:
+        return FusedMoE.make_expert_params_mapping(
+            ckpt_gate_proj_name="gate_proj",
+            ckpt_down_proj_name="down_proj",
+            ckpt_up_proj_name="up_proj",
+            num_experts=self.config.n_routed_experts)
+
+    def load_weights(self, weights: Iterable[tuple[str,
+                                                   torch.Tensor]]) -> set[str]:
+        stacked_params_mapping = [
+            ("qkv_proj", "q_proj", "q"),
+            ("qkv_proj", "k_proj", "k"),
+            ("qkv_proj", "v_proj", "v"),
+            ("gate_up_proj", "gate_proj", 0),
+            ("gate_up_proj", "up_proj", 1),
+        ]
+
+        params_dict = dict(self.named_parameters())
+        loaded_params: set[str] = set()
+        expert_params_mapping = self.get_expert_mapping()
+        for name, loaded_weight in weights:
+            if "rotary_emb.inv_freq" in name:
+                continue
+            for (param_name, weight_name, shard_id) in stacked_params_mapping:
+                if weight_name not in name:
+                    continue
+                if (("mlp.experts." in name) and name not in params_dict):
+                    continue
+                name = name.replace(weight_name, param_name)
+                if name.endswith(".bias") and name not in params_dict:
+                    continue
+                if is_pp_missing_parameter(name, self):
+                    continue
+                param = params_dict[name]
+                weight_loader = param.weight_loader
+                weight_loader(param, loaded_weight, shard_id)
+                break
+            else:
+                for mapping in expert_params_mapping:
+                    param_name, weight_name, expert_id, shard_id = mapping
+                    if weight_name not in name:
+                        continue
+                    name = name.replace(weight_name, param_name)
+
+                    if is_pp_missing_parameter(name, self):
+                        continue
+
+                    param = params_dict[name]
+                    weight_loader = param.weight_loader
+                    weight_loader(param,
+                                  loaded_weight,
+                                  name,
+                                  shard_id=shard_id,
+                                  expert_id=expert_id)
+                    break
+                else:
+                    if name.endswith(".bias") and name not in params_dict:
+                        continue
+                    name = maybe_remap_kv_scale_name(name, params_dict)
+                    if name is None:
+                        continue
+                    if is_pp_missing_parameter(name, self):
+                        continue
+                    param = params_dict[name]
+                    weight_loader = getattr(param, "weight_loader",
+                                            default_weight_loader)
+                    weight_loader(param, loaded_weight)
+            loaded_params.add(name)
+        return loaded_params
+
+
+class Dots1ForCausalLM(nn.Module, SupportsPP, SupportsLoRA):
+
+    packed_modules_mapping = {
+        "qkv_proj": [
+            "q_proj",
+            "k_proj",
+            "v_proj",
+        ],
+        "gate_up_proj": [
+            "gate_proj",
+            "up_proj",
+        ],
+    }
+
+    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
+        super().__init__()
+        config = vllm_config.model_config.hf_config
+        quant_config = vllm_config.quant_config
+        self.config = config
+        self.quant_config = quant_config
+        self.model = Dots1Model(vllm_config=vllm_config,
+                                prefix=maybe_prefix(prefix, "model"))
+        if get_pp_group().is_last_rank:
+            self.lm_head = ParallelLMHead(config.vocab_size,
+                                          config.hidden_size,
+                                          quant_config=quant_config)
+        else:
+            self.lm_head = PPMissingLayer()
+        self.logits_processor = LogitsProcessor(config.vocab_size)
+        self.make_empty_intermediate_tensors = (
+            self.model.make_empty_intermediate_tensors)
+
+    def get_input_embeddings(self, input_ids: torch.Tensor) -> torch.Tensor:
+        return self.model.get_input_embeddings(input_ids)
+
+    def forward(
+        self,
+        input_ids: torch.Tensor,
+        positions: torch.Tensor,
+        intermediate_tensors: Optional[IntermediateTensors] = None,
+        inputs_embeds: Optional[torch.Tensor] = None,
+    ) -> Union[torch.Tensor, IntermediateTensors]:
+        hidden_states = self.model(
+            input_ids,
+            positions,
+            intermediate_tensors,
+            inputs_embeds,
+        )
+        return hidden_states
+
+    def compute_logits(
+        self,
+        hidden_states: torch.Tensor,
+        sampling_metadata: SamplingMetadata,
+    ) -> Optional[torch.Tensor]:
+        logits = self.logits_processor(self.lm_head, hidden_states,
+                                       sampling_metadata)
+        return logits
+
+    def load_weights(self, weights: Iterable[tuple[str,
+                                                   torch.Tensor]]) -> set[str]:
+        loader = AutoWeightsLoader(self)
+        return loader.load_weights(weights)
+
+    def get_expert_mapping(self) -> list[tuple[str, str, int, str]]:
+        return self.model.get_expert_mapping()
diff --git a/vllm/model_executor/models/gemma2.py b/vllm/model_executor/models/gemma2.py
index b46716213..9051ad5ea 100644
--- a/vllm/model_executor/models/gemma2.py
+++ b/vllm/model_executor/models/gemma2.py
@@ -146,10 +146,7 @@ class Gemma2Attention(nn.Module):
         # reference:
         # https://github.com/huggingface/transformers/blob/54be2d7ae87e873482b984cc956e165ca4dc0ba3/src/transformers/models/gemma2/modeling_gemma2.py#L312 # noqa
         layer_idx = extract_layer_index(prefix)
-        use_sliding_window = (layer_idx % 2 == 0 and getattr(
-            config, "interleaved_sliding_window", None) is not None)
-        sliding_window = config.interleaved_sliding_window if \
-            use_sliding_window else None
+        sliding_window = config.sliding_window if not bool(layer_idx % 2) else None
         self.attn = Attention(self.num_heads,
                               self.head_dim,
                               self.scaling,
diff --git a/vllm/model_executor/models/gemma3.py b/vllm/model_executor/models/gemma3.py
index 3a88adcce..3ec721e23 100644
--- a/vllm/model_executor/models/gemma3.py
+++ b/vllm/model_executor/models/gemma3.py
@@ -155,7 +155,10 @@ class Gemma3Attention(nn.Module):
             # Local attention. Override the values in config.json.
             self.rope_theta = config.rope_local_base_freq
             self.rope_scaling = {"rope_type": "default"}
-            self.sliding_window = config.interleaved_sliding_window
+            if hasattr(config, "sliding_window"):
+                self.sliding_window = config.sliding_window
+            else:
+                self.sliding_window = config.interleaved_sliding_window
         else:
             # Global attention. Use the values in config.json.
             self.rope_theta = config.rope_theta
diff --git a/vllm/model_executor/models/gemma3_mm.py b/vllm/model_executor/models/gemma3_mm.py
index 182cc86d3..ee68715f3 100644
--- a/vllm/model_executor/models/gemma3_mm.py
+++ b/vllm/model_executor/models/gemma3_mm.py
@@ -478,8 +478,13 @@ class Gemma3ForConditionalGeneration(nn.Module, SupportsMultiModal, SupportsPP,
         self.config = config
         self.quant_config = quant_config
         self.multimodal_config = multimodal_config
-        self.sliding_window = getattr(config.text_config,
-                                      "interleaved_sliding_window", None)
+        if hasattr(config, "sliding_window"):
+            self.sliding_window = getattr(config.text_config,
+                                    "sliding_window", None)
+        else:
+            self.sliding_window = getattr(config.text_config,
+                                    "interleaved_sliding_window", None)
+
 
         self.vision_tower = SiglipVisionModel(config.vision_config,
                                               quant_config,
diff --git a/vllm/model_executor/models/glm4_1v.py b/vllm/model_executor/models/glm4_1v.py
new file mode 100644
index 000000000..ac4939aee
--- /dev/null
+++ b/vllm/model_executor/models/glm4_1v.py
@@ -0,0 +1,1640 @@
+# SPDX-License-Identifier: Apache-2.0
+
+# Adapted from
+# https://github.com/huggingface/transformers/blob/main/src/transformers/models/Glm4v/modeling_Glm4v.py
+# Copyright 2025 The vLLM team.
+# Copyright 2025 The ZhipuAI Team.
+# Copyright 2025 The HuggingFace Inc. team.
+# All rights reserved.
+#
+# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX
+# and OPT implementations in this library. It has been modified from its
+# original forms to accommodate minor architectural differences compared
+# to GPT-NeoX and OPT used by the Meta AI team that trained the model.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+"""Inference-only GLM-4V model compatible with HuggingFace weights."""
+
+import math
+from collections.abc import Iterable, Mapping, Sequence
+from functools import partial
+from typing import Any, Callable, Literal, Optional, TypedDict, Union
+
+import numpy as np
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+from einops import rearrange
+from transformers import BatchFeature
+from transformers.models.glm4v.configuration_glm4v import (Glm4vConfig,
+                                                           Glm4vVisionConfig)
+from transformers.models.glm4v.image_processing_glm4v import (
+    Glm4vImageProcessor, smart_resize)
+from transformers.models.glm4v.video_processing_glm4v import (
+    Glm4vVideoProcessor)
+from transformers.video_utils import VideoMetadata
+
+from vllm.config import VllmConfig
+from vllm.distributed import parallel_state
+from vllm.distributed import utils as dist_utils
+from vllm.logger import init_logger
+from vllm.model_executor import SamplingMetadata
+from vllm.model_executor.layers.layernorm import RMSNorm
+from vllm.model_executor.layers.linear import (ColumnParallelLinear,
+                                               MergedColumnParallelLinear,
+                                               QKVParallelLinear,
+                                               RowParallelLinear)
+from vllm.model_executor.layers.quantization import QuantizationConfig
+from vllm.model_executor.layers.quantization.gptq import GPTQConfig
+from vllm.model_executor.layers.quantization.gptq_marlin import (
+    GPTQMarlinConfig)
+from vllm.model_executor.model_loader.weight_utils import default_weight_loader
+from vllm.model_executor.models.module_mapping import MultiModelKeys
+from vllm.multimodal import MULTIMODAL_REGISTRY
+from vllm.multimodal.inputs import (MultiModalDataDict, MultiModalFieldConfig,
+                                    MultiModalKwargs, VideoItem)
+from vllm.multimodal.parse import (ImageSize, MultiModalDataItems,
+                                   MultiModalDataParser)
+from vllm.multimodal.processing import (BaseMultiModalProcessor,
+                                        BaseProcessingInfo, PromptReplacement,
+                                        PromptUpdate)
+from vllm.multimodal.profiling import BaseDummyInputsBuilder
+from vllm.platforms import _Backend
+from vllm.sequence import IntermediateTensors
+from vllm.transformers_utils.config import uses_mrope
+
+from ..layers.activation import SiluAndMul
+from .interfaces import (MultiModalEmbeddings, SupportsLoRA,
+                         SupportsMultiModal, SupportsPP)
+from .qwen2_vl import _qwen2vl_field_config, apply_rotary_pos_emb_vision
+from .utils import (AutoWeightsLoader, WeightsMapper,
+                    init_vllm_registered_model, maybe_prefix,
+                    merge_multimodal_embeddings)
+from .vision import get_vit_attn_backend
+
+logger = init_logger(__name__)
+
+# For profile run
+_MAX_FRAMES_PER_VIDEO = 600
+
+# === Vision Inputs === #
+
+
+class Glm4vImagePixelInputs(TypedDict):
+    type: Literal["pixel_values"]
+    pixel_values: torch.Tensor
+    """Shape:
+    `(num_patches, num_channels * patch_size * patch_size)`
+    """
+
+    image_grid_thw: torch.Tensor
+    """Shape: `(num_images, 3)`
+    This should be in `(grid_t, grid_h, grid_w)` format.
+    """
+
+
+class Glm4vImageEmbeddingInputs(TypedDict):
+    type: Literal["image_embeds"]
+    image_embeds: torch.Tensor
+    """Supported types:
+    - List[`torch.Tensor`]: A list of tensors holding all images' features.
+        Each tensor holds an image's features.
+    - `torch.Tensor`: A tensor holding all images' features
+        (concatenation of all images' feature tensors).
+
+    Tensor shape: `(num_image_features, hidden_size)`
+    - `num_image_features` varies based on
+        the number and resolution of the images.
+    - `hidden_size` must match the hidden size of language model backbone.
+    """
+
+    image_grid_thw: torch.Tensor
+    """Shape: `(num_images, 3)`
+    This should be in `(grid_t, grid_h, grid_w)` format.
+    """
+
+
+Glm4vImageInputs = Union[Glm4vImagePixelInputs, Glm4vImageEmbeddingInputs]
+
+
+class Glm4vVideoPixelInputs(TypedDict):
+    type: Literal["pixel_values_videos"]
+    pixel_values_videos: torch.Tensor
+    """Shape:
+    `(num_patches,
+      num_channels * temporal_patch_size * patch_size * patch_size)`
+    """
+    # video_metadata: Union[list[VideoMetadata], list[dict]]
+    video_grid_thw: Union[list[torch.Tensor], torch.Tensor]
+    """Shape: `(num_videos, num_frames, 3)` or `(1, num_frames, 3)` 
+    for single video.
+    Each entry represents [grid_t, grid_h, grid_w] format where:
+    - grid_t: Temporal grid size (usually 1 for processed video)
+    - grid_h: Height grid size  
+    - grid_w: Width grid size
+    This describes the grid structure of the video patches.
+    """
+
+
+class Glm4vVideoEmbeddingInputs(TypedDict):
+    type: Literal["video_embeds"]
+
+    video_embeds: torch.Tensor
+    """
+    Tensor shape: `(num_video_patches, hidden_size)`
+    - `num_video_patches`: Total number of video patches across all frames
+    - `hidden_size`: Must match the hidden size of language model backbone
+    """
+
+    video_grid_thw: torch.Tensor
+    """Shape: `(num_videos, 1, 3)` or `(1, 1, 3)` for single video
+    Each entry represents [grid_t, grid_h, grid_w] format where:
+    - grid_t: Temporal grid size (usually 1 for processed video)
+    - grid_h: Height grid size  
+    - grid_w: Width grid size
+    This describes the grid structure of the video patches.
+    """
+
+
+Glm4vVideoInputs = Union[Glm4vVideoPixelInputs, Glm4vVideoEmbeddingInputs]
+
+# === Vision Encoder === #
+
+
+class Glm4vVisionMLP(nn.Module):
+
+    def __init__(
+        self,
+        in_features: int,
+        hidden_features: int,
+        bias: bool = False,
+        quant_config: Optional[QuantizationConfig] = None,
+    ):
+        super().__init__()
+        self.gate_up_proj = MergedColumnParallelLinear(
+            input_size=in_features,
+            output_sizes=[hidden_features] * 2,
+            bias=bias,
+            quant_config=quant_config,
+        )
+        self.down_proj = RowParallelLinear(
+            hidden_features,
+            in_features,
+            bias=bias,
+            quant_config=quant_config,
+        )
+        self.act_fn = SiluAndMul()
+
+    def forward(self, x: torch.Tensor):
+        x, _ = self.gate_up_proj(x)
+        x = self.act_fn(x)
+        x, _ = self.down_proj(x)
+        return x
+
+
+def all_gather_interleave(local_tensor, hidden_size: int, tp_size: int):
+    """All-gather the input tensor interleavely across model parallel group."""
+    import torch.distributed as dist
+
+    gathered_tensors = [torch.zeros_like(local_tensor) for _ in range(tp_size)]
+    dist.all_gather(
+        gathered_tensors,
+        local_tensor,
+        group=parallel_state.get_tp_group().device_group,
+    )
+
+    gathered_tensors_split = [
+        torch.split(tensor, hidden_size // tp_size, -1)
+        for tensor in gathered_tensors
+    ]
+    ordered_tensors = [
+        tensor for pair in zip(*gathered_tensors_split) for tensor in pair
+    ]
+    result_tensor = torch.cat(ordered_tensors, dim=-1)
+    return result_tensor
+
+
+class Glm4vVisionAttention(nn.Module):
+
+    def __init__(
+        self,
+        embed_dim: int,
+        num_heads: int,
+        projection_size: int,
+        quant_config: Optional[QuantizationConfig] = None,
+        prefix: str = "",
+    ) -> None:
+        super().__init__()
+        # Per attention head and per partition values.
+        self.tp_size = parallel_state.get_tensor_model_parallel_world_size()
+        self.tp_rank = parallel_state.get_tensor_model_parallel_rank()
+        self.hidden_size_per_attention_head = dist_utils.divide(
+            projection_size, num_heads)
+        self.num_attention_heads_per_partition = dist_utils.divide(
+            num_heads, self.tp_size)
+
+        self.qkv = QKVParallelLinear(
+            hidden_size=embed_dim,
+            head_size=self.hidden_size_per_attention_head,
+            total_num_heads=num_heads,
+            total_num_kv_heads=num_heads,
+            bias=False,
+            quant_config=quant_config,
+            prefix=f"{prefix}.qkv",
+        )
+        self.proj = RowParallelLinear(
+            input_size=projection_size,
+            output_size=embed_dim,
+            quant_config=quant_config,
+            prefix=f"{prefix}.proj",
+            bias=False,
+        )
+
+        # Detect attention implementation.
+        self.attn_backend: _Backend = get_vit_attn_backend(support_fa=True)
+        if self.attn_backend not in {
+                _Backend.FLASH_ATTN,
+                _Backend.TORCH_SDPA,
+                _Backend.XFORMERS,
+                _Backend.IPEX_V1,
+        }:
+            raise RuntimeError(
+                f"GLM-4V does not support {self.attn_backend} backend now.")
+
+    def split_qkv(self, qkv: torch.Tensor) -> tuple[torch.Tensor, ...]:
+        # [s, b, 3 * head * head_dim]
+        seq_len, bs, _ = qkv.shape
+        if self.tp_size > 1:
+            qkv = all_gather_interleave(qkv, self.qkv.hidden_size,
+                                        self.tp_size)
+
+        # [s, b, 3 * head * head_dim] -> 3 * [s, b, head * head_dim]
+        q, k, v = qkv.chunk(3, dim=2)
+
+        # 3 * [s, b, head * head_dim]
+        if self.tp_size > 1:
+            splitter = partial(
+                dist_utils.split_tensor_along_last_dim,
+                num_partitions=self.tp_size,
+            )
+            q = splitter(q)[self.tp_rank]
+            k = splitter(k)[self.tp_rank]
+            v = splitter(v)[self.tp_rank]
+
+        # 3 * [s, b, head * head_dim] -> 3 * [s, b, head, head_dim]
+        new_shape = (
+            seq_len,
+            bs,
+            self.num_attention_heads_per_partition,
+            self.hidden_size_per_attention_head,
+        )
+        q, k, v = (x.view(*new_shape) for x in (q, k, v))
+        return q, k, v
+
+    def forward(
+            self,
+            x: torch.Tensor,
+            cu_seqlens: torch.Tensor,
+            rotary_pos_emb: torch.Tensor,
+            max_seqlen: Optional[int] = None,  # Only used for Flash Attention
+            seqlens: Optional[list[int]] = None,  # Only used for xFormers
+    ) -> torch.Tensor:
+        # [s, b, c] --> [s, b, head * 3 * head_dim]
+        x, _ = self.qkv(x)
+
+        # [s, b, 3 * head * head_dim] -> 3 * [s, b, head, head_dim]
+        q, k, v = self.split_qkv(x)
+        batch_size = q.shape[1]
+
+        q, k, v = (rearrange(x, "s b ... -> b s ...").contiguous()
+                   for x in (q, k, v))
+        if rotary_pos_emb is not None:
+            q = apply_rotary_pos_emb_vision(q, rotary_pos_emb)
+            k = apply_rotary_pos_emb_vision(k, rotary_pos_emb)
+
+        if self.attn_backend == _Backend.FLASH_ATTN:
+            # from vllm_flash_attn.flash_attn_interface import (
+            #   flash_attn_varlen_func)
+            from flash_attn import flash_attn_varlen_func
+
+            q, k, v = (rearrange(x, "b s ... -> (b s) ...") for x in [q, k, v])
+
+            output = flash_attn_varlen_func(
+                q,
+                k,
+                v,
+                cu_seqlens_q=cu_seqlens,
+                cu_seqlens_k=cu_seqlens,
+                max_seqlen_q=max_seqlen,
+                max_seqlen_k=max_seqlen,
+                dropout_p=0,
+                causal=False,
+            )
+
+            context_layer = rearrange(output,
+                                      "(b s) ... -> b s ...",
+                                      b=batch_size)
+        elif self.attn_backend == _Backend.TORCH_SDPA:
+            # Execute attention entry by entry for speed & less VRAM.
+            outputs = []
+            for i in range(1, len(cu_seqlens)):
+                start_idx = cu_seqlens[i - 1]
+                end_idx = cu_seqlens[i]
+                q_i = q[:, start_idx:end_idx]
+                k_i = k[:, start_idx:end_idx]
+                v_i = v[:, start_idx:end_idx]
+                q_i, k_i, v_i = (rearrange(x, "b s h d -> b h s d")
+                                 for x in [q_i, k_i, v_i])
+                output_i = F.scaled_dot_product_attention(q_i,
+                                                          k_i,
+                                                          v_i,
+                                                          dropout_p=0.0)
+                output_i = rearrange(output_i, "b h s d -> b s h d ")
+                outputs.append(output_i)
+            context_layer = torch.cat(outputs, dim=1)
+        elif self.attn_backend == _Backend.IPEX_V1:
+            # Execute attention entry by entry for speed & less VRAM.
+            q, k, v = (rearrange(x, "b s ... -> (b s) ...") for x in [q, k, v])
+            from vllm._ipex_ops import ipex_ops
+            output = torch.empty(
+                        (q.shape[0], q.shape[1], q.shape[2]),
+                        dtype=q.dtype,
+                        device=q.device)
+            import math
+            head_dim = q.shape[-1]
+            scale = 1 / math.sqrt(head_dim)
+            print(f"cu_seqlens: {cu_seqlens}, max_seqlen: {max_seqlen}, scale:{scale}")
+            ipex_ops.varlen_attention(q, k, v, output,
+                                    cu_seqlens,
+                                    cu_seqlens,
+                                    None,
+                                    max_seqlen,
+                                    max_seqlen,
+                                    pdropout=0,
+                                    softmax_scale=scale,
+                                    zero_tensors=False,
+                                    is_causal=False,
+                                    return_softmax=False,
+                                    window_size_left=-1,
+                                    window_size_right=-1,
+                                    gen_=None,
+                                    logits_soft_cap=0
+                                    )
+
+            context_layer = rearrange(output,
+                                      "(b s) ... -> b s ...",
+                                      b=batch_size)
+            '''
+            outputs = []
+            for i in range(1, len(cu_seqlens)):
+                start_idx = cu_seqlens[i - 1]
+                end_idx = cu_seqlens[i]
+                q_i = q[:, start_idx:end_idx]
+                k_i = k[:, start_idx:end_idx]
+                v_i = v[:, start_idx:end_idx]
+                q_i, k_i, v_i = (rearrange(x, "b s h d -> b h s d")
+                                 for x in [q_i, k_i, v_i])
+                output_i = F.scaled_dot_product_attention(q_i,
+                                                          k_i,
+                                                          v_i,
+                                                          dropout_p=0.0)
+                output_i = rearrange(output_i, "b h s d -> b s h d ")
+                outputs.append(output_i)
+            context_layer = torch.cat(outputs, dim=1)
+            '''
+        elif self.attn_backend == _Backend.XFORMERS:
+            from xformers import ops as xops
+            from xformers.ops.fmha.attn_bias import BlockDiagonalMask
+
+            attn_bias = BlockDiagonalMask.from_seqlens(q_seqlen=seqlens,
+                                                       kv_seqlen=None,
+                                                       device=q.device)
+
+            context_layer = xops.memory_efficient_attention_forward(
+                q, k, v, attn_bias=attn_bias, p=0, scale=None)
+
+        context_layer = rearrange(context_layer,
+                                  "b s h d -> s b (h d)").contiguous()
+
+        output, _ = self.proj(context_layer)
+        return output
+
+
+class Glm4vVisionBlock(nn.Module):
+
+    def __init__(
+        self,
+        dim: int,
+        num_heads: int,
+        mlp_hidden_dim: int,
+        norm_layer: Optional[Callable[[int], nn.Module]] = None,
+        quant_config: Optional[QuantizationConfig] = None,
+        prefix: str = "",
+    ) -> None:
+        super().__init__()
+        if norm_layer is None:
+            norm_layer = partial(nn.LayerNorm, eps=1e-6)
+        self.norm1 = norm_layer(dim)
+        self.norm2 = norm_layer(dim)
+        self.attn = Glm4vVisionAttention(
+            embed_dim=dim,
+            num_heads=num_heads,
+            projection_size=dim,
+            quant_config=quant_config,
+            prefix=f"{prefix}.attn",
+        )
+        self.mlp = Glm4vVisionMLP(
+            dim,
+            mlp_hidden_dim,
+            bias=False,
+            quant_config=quant_config,
+        )
+
+    def forward(
+            self,
+            x: torch.Tensor,
+            cu_seqlens: torch.Tensor,
+            rotary_pos_emb: torch.Tensor,
+            max_seqlen: Optional[int] = None,  # Only used for Flash Attention
+            seqlens: Optional[list[int]] = None,  # Only used for xFormers
+    ) -> torch.Tensor:
+        x = x + self.attn(
+            self.norm1(x),
+            cu_seqlens=cu_seqlens,
+            rotary_pos_emb=rotary_pos_emb,
+            max_seqlen=max_seqlen,
+            seqlens=seqlens,
+        )
+
+        x = x + self.mlp(self.norm2(x))
+        return x
+
+
+class Glm4vVisionPatchEmbed(nn.Module):
+
+    def __init__(
+        self,
+        patch_size: int = 14,
+        temporal_patch_size: int = 1,
+        in_channels: int = 3,
+        hidden_size: int = 1536,
+    ) -> None:
+        super().__init__()
+        self.patch_size = patch_size
+        self.temporal_patch_size = temporal_patch_size
+        self.hidden_size = hidden_size
+
+        kernel_size = (temporal_patch_size, patch_size, patch_size)
+        self.proj = nn.Conv3d(
+            in_channels,
+            hidden_size,
+            kernel_size=kernel_size,
+            stride=kernel_size,
+            bias=True,
+        )
+
+    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        L, C = x.shape
+        x = x.view(L, -1, self.temporal_patch_size, self.patch_size,
+                   self.patch_size)
+        x = self.proj(x).view(L, self.hidden_size)
+        return x
+
+
+class Glm4vPatchMerger(nn.Module):
+
+    def __init__(
+        self,
+        d_model: int,
+        context_dim: int,
+        quant_config: Optional[QuantizationConfig] = None,
+        bias: bool = False,
+    ) -> None:
+        super().__init__()
+        self.hidden_size = d_model
+        self.proj = ColumnParallelLinear(self.hidden_size,
+                                         self.hidden_size,
+                                         bias=bias,
+                                         gather_output=True)
+        self.post_projection_norm = nn.LayerNorm(self.hidden_size)
+        self.gate_up_proj = MergedColumnParallelLinear(
+            input_size=self.hidden_size,
+            output_sizes=[context_dim] * 2,
+            bias=bias,
+            quant_config=quant_config,
+        )
+        self.down_proj = RowParallelLinear(
+            context_dim,
+            self.hidden_size,
+            bias=bias,
+            quant_config=quant_config,
+        )
+        self.act_fn = SiluAndMul()
+        self.extra_activation_func = nn.GELU()
+
+    def forward(self, x: torch.Tensor):
+        x, _ = self.proj(x)
+        x = self.extra_activation_func(self.post_projection_norm(x))
+        gate_up, _ = self.gate_up_proj(x)
+        x = self.act_fn(gate_up)
+        x, _ = self.down_proj(x)
+        return x
+
+
+class Glm4vVisionEmbeddings(nn.Module):
+
+    def __init__(self, config: Glm4vVisionConfig):
+        super().__init__()
+        self.config = config
+        self.embed_dim = config.hidden_size
+        self.image_size = config.image_size
+        self.patch_size = config.patch_size
+
+        self.num_patches = (self.image_size // self.patch_size)**2
+        self.num_positions = self.num_patches
+        self.position_embedding = nn.Embedding(self.num_positions,
+                                               self.embed_dim)
+        self.register_buffer(
+            "position_ids",
+            torch.arange(self.num_positions).expand((1, -1)),
+            persistent=False,
+        )
+
+    def forward(self, embeddings, lengths, image_shapes, h_coords,
+                w_coords) -> torch.Tensor:
+        pos_embed_weight = self.position_embedding.weight
+        hidden_size = pos_embed_weight.shape[1]
+        total_seq = h_coords.shape[0]
+        device = pos_embed_weight.device
+
+        # Move coordinates to correct device
+        h_coords, w_coords = h_coords.to(device), w_coords.to(device)
+
+        # Handle empty sequence case
+        if total_seq == 0:
+            adapted_pos_embed = torch.empty(0,
+                                            hidden_size,
+                                            device=device,
+                                            dtype=pos_embed_weight.dtype)
+        else:
+            # Convert inputs to tensors if needed
+            if isinstance(lengths, list):
+                lengths = torch.tensor(lengths,
+                                       device=device,
+                                       dtype=torch.long)
+            if not isinstance(image_shapes, torch.Tensor):
+                image_shapes = torch.tensor(image_shapes,
+                                            device=device,
+                                            dtype=torch.long)
+
+            # Prepare 2D position embedding
+            orig_size_sq = pos_embed_weight.shape[0]
+            orig_size = int(orig_size_sq**0.5)
+            pos_embed_2d = (pos_embed_weight.view(
+                orig_size, orig_size,
+                hidden_size).permute(2, 0,
+                                     1).unsqueeze(0).to(device=device,
+                                                        dtype=torch.float32))
+
+            # Calculate target dimensions for each patch
+            target_h = torch.cat([
+                image_shapes[i, 1].repeat(lengths[i])
+                for i in range(len(lengths))
+            ]).to(device=device, dtype=torch.float32)
+            target_w = torch.cat([
+                image_shapes[i, 2].repeat(lengths[i])
+                for i in range(len(lengths))
+            ]).to(device=device, dtype=torch.float32)
+
+            # Normalize coordinates to [-1, 1] range for grid_sample
+            h_coords = h_coords.to(device=device, dtype=torch.float32)
+            w_coords = w_coords.to(device=device, dtype=torch.float32)
+            norm_w = ((w_coords + 0.5) / target_w) * 2 - 1
+            norm_h = ((h_coords + 0.5) / target_h) * 2 - 1
+
+            # Create sampling grid
+            grid = (torch.stack((norm_w, norm_h),
+                                dim=-1).unsqueeze(0).unsqueeze(2))
+
+            # Perform bicubic interpolation
+            interpolated_embed_fp32 = F.grid_sample(
+                pos_embed_2d,
+                grid,
+                mode="bicubic",
+                align_corners=False,
+                padding_mode="border",
+            )
+
+            # Reshape and convert back to original dtype
+            adapted_pos_embed_fp32 = (
+                interpolated_embed_fp32.squeeze(0).squeeze(-1).permute(1, 0))
+            adapted_pos_embed = adapted_pos_embed_fp32.to(
+                pos_embed_weight.dtype).to(embeddings.device)
+
+        # Add adapted position encoding to embeddings
+        embeddings = embeddings + adapted_pos_embed
+        return embeddings
+
+
+class Glm4vVisionRotaryEmbedding(nn.Module):
+
+    def __init__(self, dim: int, theta: float = 10000.0) -> None:
+        super().__init__()
+        self.dim = dim
+        self.theta = theta
+        inv_freq = 1.0 / (theta
+                          **(torch.arange(0, dim, 2, dtype=torch.float) / dim))
+        self.register_buffer("inv_freq", inv_freq, persistent=False)
+        self._seq_len_cached = 0
+        self._freqs_cached = None
+
+    def update_freqs_cache(self, seqlen: int) -> None:
+        if seqlen > self._seq_len_cached:
+            seqlen *= 2
+            self._seq_len_cached = seqlen
+            self.inv_freq = 1.0 / (self.theta**(torch.arange(
+                0,
+                self.dim,
+                2,
+                dtype=torch.float,
+                device=self.inv_freq.device,
+            ) / self.dim))
+            seq = torch.arange(seqlen,
+                               device=self.inv_freq.device,
+                               dtype=self.inv_freq.dtype)
+            freqs = torch.outer(seq, self.inv_freq)
+            self._freqs_cached = freqs
+
+    def forward(self, seqlen: int) -> torch.Tensor:
+        self.update_freqs_cache(seqlen)
+        return self._freqs_cached[:seqlen]
+
+
+class Glm4vVisionTransformer(nn.Module):
+
+    def __init__(
+        self,
+        vision_config: Glm4vVisionConfig,
+        norm_eps: float = 1e-6,
+        quant_config: Optional[QuantizationConfig] = None,
+        prefix: str = "",
+    ) -> None:
+        super().__init__()
+
+        patch_size = vision_config.patch_size
+        temporal_patch_size = vision_config.temporal_patch_size
+        in_channels = vision_config.in_channels
+        depth = vision_config.depth
+        self.hidden_size = vision_config.hidden_size
+        self.num_heads = vision_config.num_heads
+
+        self.patch_size = vision_config.patch_size
+        self.spatial_merge_size = vision_config.spatial_merge_size
+        self.out_hidden_size = vision_config.out_hidden_size
+
+        self.patch_embed = Glm4vVisionPatchEmbed(
+            patch_size=patch_size,
+            temporal_patch_size=temporal_patch_size,
+            in_channels=in_channels,
+            hidden_size=self.hidden_size,
+        )
+
+        norm_layer = partial(RMSNorm, eps=norm_eps)
+        head_dim = self.hidden_size // self.num_heads
+        self.rotary_pos_emb = Glm4vVisionRotaryEmbedding(head_dim // 2)
+        self.blocks = nn.ModuleList([
+            Glm4vVisionBlock(
+                dim=self.hidden_size,
+                num_heads=self.num_heads,
+                mlp_hidden_dim=vision_config.out_hidden_size,
+                norm_layer=norm_layer,
+                quant_config=quant_config,
+                prefix=f"{prefix}.blocks.{layer_idx}",
+            ) for layer_idx in range(depth)
+        ])
+        self.merger = Glm4vPatchMerger(
+            d_model=vision_config.out_hidden_size,
+            context_dim=vision_config.intermediate_size,
+            quant_config=quant_config,
+            bias=False,
+        )
+        self.embeddings = Glm4vVisionEmbeddings(vision_config)
+
+        self.post_conv_layernorm = RMSNorm(vision_config.hidden_size,
+                                           eps=vision_config.rms_norm_eps)
+        self.downsample = nn.Conv2d(
+            in_channels=vision_config.hidden_size,
+            out_channels=vision_config.out_hidden_size,
+            kernel_size=vision_config.spatial_merge_size,
+            stride=vision_config.spatial_merge_size,
+        )
+        self.post_layernorm = RMSNorm(vision_config.hidden_size,
+                                      eps=vision_config.rms_norm_eps)
+
+        self.attn_backend: _Backend = get_vit_attn_backend(support_fa=True)
+
+    @property
+    def dtype(self) -> torch.dtype:
+        return self.patch_embed.proj.weight.dtype
+
+    @property
+    def device(self) -> torch.device:
+        return self.patch_embed.proj.weight.device
+
+    def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:
+        pos_ids = []
+        for t, h, w in grid_thw:
+            hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)
+            wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)
+            hpos_ids = (hpos_ids.reshape(
+                h // self.spatial_merge_size,
+                self.spatial_merge_size,
+                w // self.spatial_merge_size,
+                self.spatial_merge_size,
+            ).permute(0, 2, 1, 3).flatten())
+            wpos_ids = (wpos_ids.reshape(
+                h // self.spatial_merge_size,
+                self.spatial_merge_size,
+                w // self.spatial_merge_size,
+                self.spatial_merge_size,
+            ).permute(0, 2, 1, 3).flatten())
+            pos_ids.append(
+                torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))
+        pos_ids = torch.cat(pos_ids, dim=0)
+        max_grid_size = grid_thw[:, 1:].max()
+        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size)
+        rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)
+        return rotary_pos_emb, pos_ids
+
+    def compute_attn_mask_seqlen(
+        self,
+        cu_seqlens: torch.Tensor,
+    ) -> tuple[Optional[int], Optional[list[int]]]:
+        max_seqlen, seqlens = None, None
+        seqlens = (cu_seqlens[1:] - cu_seqlens[:-1]).tolist()
+        if self.attn_backend == _Backend.FLASH_ATTN or self.attn_backend == _Backend.IPEX_V1:
+            max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()
+        return max_seqlen, seqlens
+
+    def forward(
+        self,
+        x: torch.Tensor,
+        grid_thw: torch.Tensor,
+    ) -> torch.Tensor:
+        # patchify
+        x = x.to(device=self.device, dtype=self.dtype)
+        x = self.patch_embed(x)
+        x = self.post_conv_layernorm(x)
+
+        # compute position embedding
+        rotary_pos_emb, image_type_ids = self.rot_pos_emb(grid_thw)
+        # compute cu_seqlens
+        cu_seqlens = torch.repeat_interleave(grid_thw[:, 1] * grid_thw[:, 2],
+                                             grid_thw[:, 0]).cumsum(
+                                                 dim=0, dtype=torch.int32)
+        cu_seqlens = F.pad(cu_seqlens, (1, 0), "constant", 0)
+
+        # pre-compute seqlens for attn mask to reduce cuMemcpy operations
+        max_seqlen, seqlens = self.compute_attn_mask_seqlen(cu_seqlens)
+        x = self.embeddings(x, seqlens, grid_thw, image_type_ids[:, 0],
+                            image_type_ids[:, 1])
+
+        # transformers
+        x = x.unsqueeze(1)
+        for blk in self.blocks:
+            x = blk(
+                x,
+                cu_seqlens=cu_seqlens,
+                rotary_pos_emb=rotary_pos_emb,
+                max_seqlen=max_seqlen,
+                seqlens=seqlens,
+            )
+
+        # adapter
+        x = self.post_layernorm(x)
+
+        x = x.view(-1, self.spatial_merge_size, self.spatial_merge_size,
+                   x.shape[-1])
+        x = x.permute(0, 3, 1, 2)
+        x = self.downsample(x).view(-1, self.out_hidden_size)
+        x = self.merger(x)
+
+        return x
+
+    def load_weights(self, weights: Iterable[tuple[str,
+                                                   torch.Tensor]]) -> set[str]:
+        stacked_params_mapping = [
+            # (param_name, shard_name, shard_id)
+            ("attn.qkv.", "attn.q.", "q"),
+            ("attn.qkv.", "attn.k.", "k"),
+            ("attn.qkv.", "attn.v.", "v"),
+            ("gate_up_proj", "gate_proj", 0),
+            ("gate_up_proj", "up_proj", 1),
+        ]
+        params_dict = dict(self.named_parameters(remove_duplicate=False))
+        loaded_params: set[str] = set()
+
+        for name, loaded_weight in weights:
+            for param_name, weight_name, shard_id in stacked_params_mapping:
+                if weight_name not in name:
+                    continue
+                name = name.replace(weight_name, param_name)
+
+                param = params_dict[name]
+                weight_loader = param.weight_loader
+                weight_loader(param, loaded_weight, shard_id)
+                break
+            else:
+                param = params_dict[name]
+                weight_loader = getattr(param, "weight_loader",
+                                        default_weight_loader)
+                weight_loader(param, loaded_weight)
+            loaded_params.add(name)
+        return loaded_params
+
+
+class Glm4vProcessingInfo(BaseProcessingInfo):
+
+    def get_hf_config(self):
+        return self.ctx.get_hf_config(Glm4vConfig)
+
+    def get_tokenizer(self):
+        return self.ctx.tokenizer
+
+    def get_supported_mm_limits(self) -> Mapping[str, Optional[int]]:
+        return {"image": None, "video": 1}
+
+    def get_image_processor(self) -> Glm4vImageProcessor:
+        return self.get_hf_processor().image_processor
+
+    def get_video_processor(self) -> Glm4vVideoProcessor:
+        return self.get_hf_processor().video_processor
+
+    def _get_vision_info(
+        self,
+        *,
+        image_width: int,
+        image_height: int,
+        num_frames: int = 16,
+        do_resize: bool = True,
+        max_image_pixels: int = 28 * 28 * 2 * 30000,
+    ) -> tuple[ImageSize, int]:
+        hf_config = self.get_hf_config()
+        vision_config = hf_config.vision_config
+        patch_size = vision_config.patch_size
+        merge_size = vision_config.spatial_merge_size
+        temporal_patch_size = vision_config.temporal_patch_size
+        if do_resize:
+            resized_height, resized_width = smart_resize(
+                num_frames=num_frames
+                if num_frames > temporal_patch_size else temporal_patch_size,
+                height=image_height,
+                width=image_width,
+                factor=patch_size * merge_size,
+                max_pixels=max_image_pixels,
+            )
+            preprocessed_size = ImageSize(width=resized_width,
+                                          height=resized_height)
+        else:
+            preprocessed_size = ImageSize(width=image_width,
+                                          height=image_height)
+
+        # NOTE: Frames are padded to be divisible by `temporal_patch_size`
+        # https://github.com/huggingface/transformers/blob/v4.48.3/src/transformers/models/qwen2_vl/image_processing_qwen2_vl.py#L294
+        padded_num_frames = num_frames + num_frames % temporal_patch_size
+
+        grid_t = max(padded_num_frames // temporal_patch_size, 1)
+        grid_h = preprocessed_size.height // patch_size
+        grid_w = preprocessed_size.width // patch_size
+
+        num_patches = grid_t * grid_h * grid_w
+        num_vision_tokens = num_patches // (merge_size**2)
+
+        return preprocessed_size, num_vision_tokens
+
+    def get_image_size_with_most_features(self) -> ImageSize:
+        max_image_size, _ = self._get_vision_info(image_width=9999999,
+                                                  image_height=9999999)
+        return max_image_size
+
+    def get_num_image_tokens(
+        self,
+        *,
+        image_width: int,
+        image_height: int,
+    ) -> int:
+        _, num_image_tokens = self._get_vision_info(
+            image_width=image_width,
+            image_height=image_height,
+            max_image_pixels=28 * 28 * 2 * 6144,
+        )
+        return num_image_tokens
+
+    def get_max_image_tokens(self) -> int:
+        target_width, target_height = self.get_image_size_with_most_features()
+
+        return self.get_num_image_tokens(
+            image_width=target_width,
+            image_height=target_height,
+        )
+
+    def get_num_video_tokens(
+        self,
+        *,
+        image_width: int,
+        image_height: int,
+        num_frames: int,
+    ) -> int:
+        _, num_video_tokens = self._get_vision_info(
+            image_width=image_width,
+            image_height=image_height,
+            num_frames=num_frames,
+            max_image_pixels=28 * 28 * 2 * 30000,
+        )
+        return num_video_tokens
+
+    def _get_max_video_frames(self, max_tokens: int) -> int:
+        target_width, target_height = self.get_image_size_with_most_features()
+
+        num_frames = 0
+
+        while True:
+            next_num_frames = num_frames + 1
+            next_max_tokens = self.get_num_video_tokens(
+                image_width=target_width,
+                image_height=target_height,
+                num_frames=next_num_frames,
+            )
+            if next_max_tokens > max_tokens or next_max_tokens == 0:
+                break
+
+            num_frames = next_num_frames
+
+        return num_frames
+
+    def get_num_frames_with_most_features(
+        self,
+        seq_len: int,
+        mm_counts: Mapping[str, int],
+    ) -> int:
+        max_images = mm_counts.get("image", 0)
+        max_videos = mm_counts.get("video", 0)
+
+        max_image_tokens = self.get_max_image_tokens() * max_images
+        max_total_frames = self._get_max_video_frames(seq_len -
+                                                      max_image_tokens)
+        max_frames_per_video = min(max_total_frames // max(max_videos, 1),
+                                   _MAX_FRAMES_PER_VIDEO)
+
+        return max(max_frames_per_video, 1)
+
+    def _get_video_second_idx(self, metadata: dict[str, Any],
+                              total_frames: int) -> list[int]:
+        video_processor = self.get_video_processor()
+
+        video_fps = metadata.get("fps", 2.0)
+        meta_frames = metadata.get("total_num_frames", total_frames)
+        max_frame_idx = meta_frames - 1
+        duration = metadata.get("duration",
+                                round(max_frame_idx / video_fps) + 1)
+        if duration <= video_processor.max_duration:
+            n = int(math.floor(duration * video_processor.fps))
+            frame_indices = [
+                min(
+                    max_frame_idx,
+                    int(math.ceil(i * video_fps / video_processor.fps)),
+                ) for i in range(n)
+            ]
+        else:
+            num_samples = int(video_processor.max_duration *
+                              video_processor.fps)
+            if num_samples >= meta_frames:
+                frame_indices = list(range(meta_frames))
+            else:
+                target_seconds = np.linspace(0,
+                                             duration,
+                                             num_samples,
+                                             endpoint=True)
+                frame_indices = [
+                    min(max_frame_idx, int(math.ceil(t * video_fps)))
+                    for t in target_seconds
+                ]
+
+        seen, uniq = set(), []
+        for idx in frame_indices:
+            if idx not in seen:
+                seen.add(idx)
+                uniq.append(idx)
+        if len(uniq) & 1:
+            uniq.append(uniq[-1])
+        frame_indices = uniq
+
+        full_second_idxs = [int(idx / video_fps) for idx in frame_indices]
+        timestamps_list = full_second_idxs[::2]
+        selected_timestamps = []
+        for idx in range(0, len(timestamps_list)):
+            selected_timestamps.append(timestamps_list[idx])
+        return selected_timestamps
+
+
+class Glm4vDummyInputsBuilder(BaseDummyInputsBuilder[Glm4vProcessingInfo]):
+
+    def get_dummy_text(self, mm_counts: Mapping[str, int]) -> str:
+        num_images = mm_counts.get("image", 0)
+        num_videos = mm_counts.get("video", 0)
+
+        hf_config = self.info.get_hf_config()
+        hf_processor = self.info.get_hf_processor()
+        tokenizer = self.info.get_tokenizer()
+
+        image_token: str = hf_processor.image_token
+        video_token_ids = [
+            hf_config.video_start_token_id,
+            hf_processor.video_token_id,
+            hf_config.video_end_token_id,
+        ]
+        video_token = tokenizer.decode(video_token_ids)
+
+        return image_token * num_images + video_token * num_videos
+
+    def get_dummy_mm_data(
+        self,
+        seq_len: int,
+        mm_counts: Mapping[str, int],
+    ) -> MultiModalDataDict:
+        num_images = mm_counts.get("image", 0)
+        num_videos = mm_counts.get("video", 0)
+
+        target_width, target_height = (
+            self.info.get_image_size_with_most_features())
+        target_num_frames = self.info.get_num_frames_with_most_features(
+            seq_len, mm_counts)
+        return {
+            "image":
+            self._get_dummy_images(width=target_width,
+                                   height=target_height,
+                                   num_images=num_images),
+            "video":
+            self._get_dummy_videos(
+                width=target_width,
+                height=target_height,
+                num_frames=target_num_frames,
+                num_videos=num_videos,
+            ),
+        }
+
+    def _get_dummy_videos(
+        self,
+        *,
+        width: int,
+        height: int,
+        num_frames: int,
+        num_videos: int,
+    ) -> list[VideoItem]:
+        video = np.full((num_frames, width, height, 3), 255, dtype=np.uint8)
+        video_items = []
+        for i in range(num_videos):
+            video_metadata = {
+                "fps": 2.0,
+                "duration": num_frames / 2.0,
+                "total_num_frames": num_frames,
+                "video_backend": "opencv",
+            }
+            video_item = (video.copy(), video_metadata)
+            video_items.append(video_item)
+
+        return video_items
+
+
+class Glm4vMultiModalProcessor(BaseMultiModalProcessor[Glm4vProcessingInfo]):
+
+    def _get_data_parser(self) -> MultiModalDataParser:
+        return MultiModalDataParser(video_needs_metadata=True)
+
+    def _call_hf_processor(
+        self,
+        prompt: str,
+        mm_data: Mapping[str, object],
+        mm_kwargs: Mapping[str, object],
+        tok_kwargs: Mapping[str, object] = None,
+    ) -> BatchFeature:
+        mm_data = dict(mm_data)
+        processor = self.info.get_hf_processor(**mm_kwargs)
+
+        # GLM-4.1V use `image_token_id` as video placeholder, we need to
+        # replace it with `video_token_id` for video processing. So we
+        # separate video processing from image processing.
+        if ("videos" in mm_data and isinstance(mm_data["videos"], list)
+                and len(mm_data["videos"]) > 0):
+            video_grid_thw_lst = []
+            pixel_values_videos_lst = []
+            for item in mm_data.pop("videos", []):
+                video_array, metadata = item
+
+                # FIXME(Isotr0py): Activate the below logic after we can disable
+                # resampling from video loader backend.
+                # assert metadata["total_num_frames"] == len(video_array), (
+                #     f"Total frames {metadata['total_num_frames']} does not "
+                #     f"match the length of video array {len(video_array)}.")
+
+                # NOTE: Temporary workaround for resampled videos.
+                # this can cause a divergence with HF implementation if
+                # the input video is resampled in advance.
+
+                if metadata["total_num_frames"] != len(video_array):
+                    logger.warning(
+                        "Total frames in metadata "
+                        "(%s) does not match the length of "
+                        "video array %s. This can "
+                        "be because the video is resampled "
+                        "in advance. This may cause "
+                        "a divergence with HF implementation.",
+                        metadata["total_num_frames"],
+                        len(video_array),
+                    )
+                    metadata["total_num_frames"] = len(video_array)
+                metadata = VideoMetadata(**metadata)
+
+                video_mm_data = dict()
+                video_mm_data["videos"] = [[video_array]]
+                video_mm_data["video_metadata"] = [[metadata]]
+
+                video_outputs = super()._call_hf_processor(
+                    prompt="<|begin_of_video|><|video|><|end_of_video|>",
+                    mm_data=video_mm_data,
+                    mm_kwargs=mm_kwargs,
+                    #tok_kwargs=tok_kwargs,
+                )
+                input_ids = video_outputs.pop("input_ids")
+                input_ids[input_ids == processor.image_token_id] = (
+                    processor.video_token_id)
+                video_placeholder = processor.tokenizer.batch_decode(
+                    input_ids)[0]
+                prompt = prompt.replace(
+                    "<|begin_of_video|><|video|><|end_of_video|>",
+                    video_placeholder,
+                )
+
+                grid_t = len(video_outputs["video_grid_thw"])
+                _, grid_h, grid_w = video_outputs["video_grid_thw"][0]
+                grid_thw = torch.tensor([[grid_t, grid_h, grid_w]])
+
+                video_grid_thw_lst.append(grid_thw)
+                pixel_values_videos_lst.append(
+                    video_outputs["pixel_values_videos"])
+            video_outputs = dict(
+                pixel_values_videos=torch.cat(pixel_values_videos_lst),
+                video_grid_thw=torch.cat(video_grid_thw_lst),
+            )
+        else:
+            video_outputs = dict()
+
+        processed_outputs = super()._call_hf_processor(
+            prompt=prompt,
+            mm_data=mm_data,
+            mm_kwargs=mm_kwargs,
+            #tok_kwargs=tok_kwargs,
+        )
+        combined_outputs = dict(
+            processed_outputs,
+            **video_outputs,
+        )
+        return BatchFeature(combined_outputs)
+
+    def _get_mm_fields_config(
+        self,
+        hf_inputs: BatchFeature,
+        hf_processor_mm_kwargs: Mapping[str, object],
+    ) -> Mapping[str, MultiModalFieldConfig]:
+        return _qwen2vl_field_config(hf_inputs)
+
+    def _get_prompt_updates(
+        self,
+        mm_items: MultiModalDataItems,
+        hf_processor_mm_kwargs: Mapping[str, Any],
+        out_mm_kwargs: MultiModalKwargs,
+    ) -> Sequence[PromptUpdate]:
+        hf_processor = self.info.get_hf_processor(**hf_processor_mm_kwargs)
+        image_processor = self.info.get_image_processor(
+            **hf_processor_mm_kwargs)
+        tokenizer = self.info.get_tokenizer()
+        hf_config = self.info.get_hf_config()
+
+        boi_token_id = hf_config.image_start_token_id
+        eoi_token_id = hf_config.image_end_token_id
+
+        bov_token_id = hf_config.video_start_token_id
+        eov_token_id = hf_config.video_end_token_id
+
+        merge_length = image_processor.merge_size**2
+
+        def get_image_replacement_glm4v(item_idx: int):
+            grid_thw = out_mm_kwargs["image_grid_thw"][item_idx]
+            assert isinstance(grid_thw, torch.Tensor)
+
+            num_tokens = int(grid_thw.prod()) // merge_length
+            return [hf_processor.image_token_id] * num_tokens
+
+        def get_video_replacement_glm4v(item_idx: int):
+            grid_thw = out_mm_kwargs["video_grid_thw"][item_idx]
+            assert isinstance(grid_thw, torch.Tensor)
+
+            video, metadata = mm_items["video"][item_idx]
+            timestamps = self.info._get_video_second_idx(metadata, len(video))
+            frames_idx_token = [
+                tokenizer.encode(str(i), add_special_tokens=False)
+                for i in timestamps
+            ]
+            num_tokens_per_frame = int(grid_thw[1:].prod()) // merge_length
+            placeholder = []
+            placeholder.append(bov_token_id)
+            for frame_idx in frames_idx_token:
+                placeholder.append(boi_token_id)
+                placeholder.extend([hf_processor.video_token_id] *
+                                   num_tokens_per_frame)
+                placeholder.append(eoi_token_id)
+                placeholder.extend(frame_idx)
+            placeholder.append(eov_token_id)
+            return placeholder
+
+        return [
+            PromptReplacement(
+                modality="image",
+                target=hf_processor.image_token,
+                replacement=get_image_replacement_glm4v,
+            ),
+            PromptReplacement(
+                modality="video",
+                target="<|begin_of_video|><|video|><|end_of_video|>",
+                replacement=get_video_replacement_glm4v,
+            ),
+        ]
+
+
+@MULTIMODAL_REGISTRY.register_processor(
+    Glm4vMultiModalProcessor,
+    info=Glm4vProcessingInfo,
+    dummy_inputs=Glm4vDummyInputsBuilder,
+)
+class Glm4vForConditionalGeneration(nn.Module, SupportsMultiModal,
+                                    SupportsLoRA, SupportsPP):
+    packed_modules_mapping = {
+        "qkv_proj": [
+            "q_proj",
+            "k_proj",
+            "v_proj",
+        ],
+        "gate_up_proj": [
+            "gate_proj",
+            "up_proj",
+        ],
+    }
+
+    # To ensure correct weight loading and mapping.
+    hf_to_vllm_mapper = WeightsMapper(
+        orig_to_new_prefix={
+            "lm_head.": "language_model.lm_head.",
+            "model.language_model.": "language_model.model.",
+            "model.visual.": "visual.",
+        })
+
+    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
+        super().__init__()
+        config: Glm4vConfig = vllm_config.model_config.hf_config
+        quant_config = vllm_config.quant_config
+        multimodal_config = vllm_config.model_config.multimodal_config
+
+        self.config = config
+        self.multimodal_config = multimodal_config
+
+        self.visual = Glm4vVisionTransformer(
+            config.vision_config,
+            norm_eps=getattr(config, "rms_norm_eps", 1e-5),
+            quant_config=self._maybe_ignore_quant_config(quant_config),
+            prefix=maybe_prefix(prefix, "visual"),
+        )
+
+        self.language_model = init_vllm_registered_model(
+            vllm_config=vllm_config,
+            prefix=maybe_prefix(prefix, ""),
+            architectures=["Glm4ForCausalLM"],
+        )
+
+        self.make_empty_intermediate_tensors = (
+            self.language_model.make_empty_intermediate_tensors)
+
+    def _maybe_ignore_quant_config(self, quant_config: QuantizationConfig):
+        # GPTQ configs do not have a list of ignored modules, however AutoGPTQ
+        # seems to avoid vision encoder sections for some models.
+        if isinstance(quant_config, (GPTQConfig, GPTQMarlinConfig)):
+            return None
+        return quant_config
+
+    def _validate_and_reshape_mm_tensor(self, mm_input: object,
+                                        name: str) -> torch.Tensor:
+        if not isinstance(mm_input, (torch.Tensor, list)):
+            raise ValueError(
+                f"Incorrect type of {name}. Got type: {type(mm_input)}")
+        if isinstance(mm_input, torch.Tensor):
+            if mm_input.ndim == 2:
+                return mm_input
+            if mm_input.ndim != 3:
+                raise ValueError(f"{name} should be 2D or batched 3D tensor. "
+                                 f"Got ndim: {mm_input.ndim} "
+                                 f"(shape={mm_input.shape})")
+            return torch.concat(list(mm_input))
+        else:
+            return torch.concat(mm_input)
+
+    def _parse_and_validate_image_input(
+            self, **kwargs: object) -> Optional[Glm4vImageInputs]:
+        pixel_values = kwargs.pop("pixel_values", None)
+        image_embeds = kwargs.pop("image_embeds", None)
+        image_grid_thw = kwargs.pop("image_grid_thw", None)
+
+        if pixel_values is None and image_embeds is None:
+            return None
+
+        if pixel_values is not None:
+            pixel_values = self._validate_and_reshape_mm_tensor(
+                pixel_values, "image pixel values")
+            image_grid_thw = self._validate_and_reshape_mm_tensor(
+                image_grid_thw, "image grid_thw")
+
+            if not isinstance(pixel_values, (torch.Tensor, list)):
+                raise ValueError("Incorrect type of image pixel values. "
+                                 f"Got type: {type(pixel_values)}")
+
+            return Glm4vImagePixelInputs(
+                type="pixel_values",
+                pixel_values=pixel_values,
+                image_grid_thw=image_grid_thw,
+            )
+
+        if image_embeds is not None:
+            image_embeds = self._validate_and_reshape_mm_tensor(
+                image_embeds, "image embeds")
+            image_grid_thw = self._validate_and_reshape_mm_tensor(
+                image_grid_thw, "image grid_thw")
+
+            if not isinstance(image_embeds, torch.Tensor):
+                raise ValueError("Incorrect type of image embeddings. "
+                                 f"Got type: {type(image_embeds)}")
+            return Glm4vImageEmbeddingInputs(
+                type="image_embeds",
+                image_embeds=image_embeds,
+                image_grid_thw=image_grid_thw,
+            )
+
+    def _parse_and_validate_video_input(
+            self, **kwargs: object) -> Optional[Glm4vVideoInputs]:
+        pixel_values_videos = kwargs.pop("pixel_values_videos", None)
+        video_embeds = kwargs.pop("video_embeds", None)
+        video_grid_thw = kwargs.pop("video_grid_thw", None)
+        if pixel_values_videos is None and video_embeds is None:
+            return None
+        if pixel_values_videos is not None:
+            pixel_values_videos = self._validate_and_reshape_mm_tensor(
+                pixel_values_videos, "video pixel values")
+            video_grid_thw = self._validate_and_reshape_mm_tensor(
+                video_grid_thw, "video grid_thw")
+
+            return Glm4vVideoPixelInputs(
+                type="pixel_values_videos",
+                # video_metadata=video_metadata,
+                pixel_values_videos=pixel_values_videos,
+                video_grid_thw=video_grid_thw,
+            )
+
+        if video_embeds is not None:
+            video_embeds = self._validate_and_reshape_mm_tensor(
+                video_embeds, "video embeds")
+            video_grid_thw = self._validate_and_reshape_mm_tensor(
+                video_grid_thw, "video grid_thw")
+
+            if not isinstance(video_embeds, torch.Tensor):
+                raise ValueError("Incorrect type of video embeddings. "
+                                 f"Got type: {type(video_embeds)}")
+            return Glm4vVideoEmbeddingInputs(
+                type="video_embeds",
+                video_embeds=video_embeds,
+                video_grid_thw=video_grid_thw,
+            )
+
+    def _process_image_input(
+            self, image_input: Glm4vImageInputs) -> tuple[torch.Tensor, ...]:
+        grid_thw = image_input["image_grid_thw"]
+        assert grid_thw.ndim == 2
+
+        if image_input["type"] == "image_embeds":
+            image_embeds = image_input["image_embeds"].type(self.visual.dtype)
+        else:
+            pixel_values = image_input["pixel_values"].type(self.visual.dtype)
+            image_embeds = self.visual(pixel_values, grid_thw=grid_thw)
+
+        merge_size = self.visual.spatial_merge_size
+        sizes = grid_thw.prod(-1) // merge_size // merge_size
+        return image_embeds.split(sizes.tolist())
+
+    def _process_video_input(
+            self, video_input: Glm4vVideoInputs) -> tuple[torch.Tensor, ...]:
+        grid_thw = video_input["video_grid_thw"]
+        assert grid_thw.ndim == 2
+
+        device = self.visual.device
+        flat_grid_thw = torch.cat([
+            torch.tensor([[1, h, w]] * t, device=device)
+            for t, h, w in grid_thw
+        ])
+        if video_input["type"] == "video_embeds":
+            video_embeds = video_input["video_embeds"].type(self.visual.dtype)
+        else:
+            pixel_values_videos = video_input["pixel_values_videos"].type(
+                self.visual.dtype)
+            video_embeds = self.visual(pixel_values_videos,
+                                       grid_thw=flat_grid_thw)
+
+        # Split concatenated embeddings for each video item.
+        merge_size = self.visual.spatial_merge_size
+        sizes = grid_thw.prod(-1) // merge_size // merge_size
+
+        return video_embeds.split(sizes.tolist())
+
+    def _parse_and_validate_multimodal_inputs(self, **kwargs: object) -> dict:
+        mm_input_by_modality = {}
+
+        # Preserve the order of modalities if there are multiple of them
+        # from the order of kwargs.
+        for input_key in kwargs:
+            if (input_key in ("pixel_values", "image_embeds")
+                    and "image" not in mm_input_by_modality):
+                mm_input_by_modality["image"] = (
+                    self._parse_and_validate_image_input(**kwargs))
+            if (input_key in ("pixel_values_videos", "video_embeds")
+                    and "video" not in mm_input_by_modality):
+                mm_input_by_modality["video"] = (
+                    self._parse_and_validate_video_input(**kwargs))
+        return mm_input_by_modality
+
+    def get_language_model(self) -> torch.nn.Module:
+        return self.language_model
+
+    def get_multimodal_embeddings(
+            self, **kwargs: object) -> Optional[MultiModalEmbeddings]:
+        mm_input_by_modality = self._parse_and_validate_multimodal_inputs(
+            **kwargs)
+        if not mm_input_by_modality:
+            return None
+
+        # The result multimodal_embeddings is tuple of tensors, with each
+        # tensor correspoending to a multimodal data item (image or video).
+        multimodal_embeddings: tuple[torch.Tensor, ...] = ()
+
+        # NOTE: It is important to iterate over the keys in this dictionary
+        # to preserve the order of the modalities.
+        for modality in mm_input_by_modality:
+            multimodal_input = mm_input_by_modality[modality]
+            if modality == "image":
+                vision_embeddings = self._process_image_input(multimodal_input)
+                multimodal_embeddings += vision_embeddings
+            if modality == "video":
+                video_embeddings = self._process_video_input(multimodal_input)
+                multimodal_embeddings += video_embeddings
+        return multimodal_embeddings
+
+    def get_input_embeddings(
+        self,
+        input_ids: torch.Tensor,
+        multimodal_embeddings: Optional[MultiModalEmbeddings] = None,
+    ) -> torch.Tensor:
+        inputs_embeds = self.language_model.get_input_embeddings(input_ids)
+        if (multimodal_embeddings is not None
+                and len(multimodal_embeddings) != 0
+                and all(embed.numel() > 0 for embed in multimodal_embeddings)):
+            inputs_embeds = merge_multimodal_embeddings(
+                input_ids,
+                inputs_embeds,
+                multimodal_embeddings,
+                [self.config.image_token_id, self.config.video_token_id],
+            )
+        return inputs_embeds
+
+    def get_input_embeddings_v0(
+        self,
+        input_ids: torch.Tensor,
+        image_input: Optional[Glm4vImageInputs] = None,
+        video_input: Optional[Glm4vVideoInputs] = None,
+    ) -> torch.Tensor:
+        inputs_embeds = self.get_input_embeddings(input_ids)
+        if image_input is not None:
+            image_embeds = self._process_image_input(image_input)
+            inputs_embeds = merge_multimodal_embeddings(
+                input_ids,
+                inputs_embeds,
+                image_embeds,
+                placeholder_token_id=self.config.image_token_id,
+            )
+
+        if video_input is not None:
+            video_embeds = self._process_video_input(video_input)
+            inputs_embeds = merge_multimodal_embeddings(
+                input_ids,
+                inputs_embeds,
+                video_embeds,
+                placeholder_token_id=self.config.video_token_id,
+            )
+        return inputs_embeds
+
+    def forward(
+        self,
+        input_ids: torch.Tensor,
+        positions: torch.Tensor,
+        intermediate_tensors: Optional[IntermediateTensors] = None,
+        inputs_embeds: Optional[torch.Tensor] = None,
+        **kwargs: object,
+    ) -> Union[torch.Tensor, IntermediateTensors]:
+        """Run forward pass for GLM-4V.
+
+        Args:
+            input_ids: Flattened (concatenated) input_ids corresponding to a
+                batch.
+            positions: Flattened (concatenated) position ids corresponding to a
+                batch.
+                **NOTE**: If mrope is enabled (default setting for GLM-4V
+                opensource models), the shape will be `(3, seq_len)`,
+                otherwise it will be `(seq_len,).
+            pixel_values: Pixel values to be fed to a model.
+                `None` if no images are passed.
+            image_grid_thw: Tensor `(n_images, 3)` of image 3D grid in LLM.
+                `None` if no images are passed.
+            pixel_values_videos: Pixel values of videos to be fed to a model.
+                `None` if no videos are passed.
+            video_grid_thw: Tensor `(n_videos, 3)` of video 3D grid in LLM.
+                `None` if no videos are passed.
+            second_per_grid_ts: Tensor `(num_videos)` of video time interval (
+                in seconds) for each grid along the temporal dimension in the
+                3D position IDs. `None` if no videos are passed.
+        """
+        if intermediate_tensors is not None:
+            inputs_embeds = None
+
+        # NOTE: In v1, inputs_embeds is always generated at model runner from
+        # `get_multimodal_embeddings` and `get_input_embeddings`, this
+        # condition is only for v0 compatibility.
+        elif inputs_embeds is None:
+            image_input = self._parse_and_validate_image_input(**kwargs)
+            video_input = self._parse_and_validate_video_input(**kwargs)
+
+            if image_input is None and video_input is None:
+                inputs_embeds = None
+            else:
+                if uses_mrope(self.config):
+                    assert positions.ndim == 2 and positions.size(0) == 3, (
+                        "multimodal section rotary embedding requires "
+                        f"(3, seq_len) positions, but got {positions.size()}")
+                inputs_embeds = self.get_input_embeddings_v0(
+                    input_ids,
+                    image_input=image_input,
+                    video_input=video_input)
+                input_ids = None
+
+        hidden_states = self.language_model.model(
+            input_ids=input_ids,
+            positions=positions,
+            intermediate_tensors=intermediate_tensors,
+            inputs_embeds=inputs_embeds,
+        )
+        return hidden_states
+
+    def compute_logits(
+        self,
+        hidden_states: torch.Tensor,
+        sampling_metadata: SamplingMetadata,
+    ) -> Optional[torch.Tensor]:
+        return self.language_model.compute_logits(hidden_states,
+                                                  sampling_metadata)
+
+    def load_weights(self, weights: Iterable[tuple[str,
+                                                   torch.Tensor]]) -> set[str]:
+        loader = AutoWeightsLoader(self)
+        return loader.load_weights(weights, mapper=self.hf_to_vllm_mapper)
+
+    def get_mm_mapping(self) -> MultiModelKeys:
+        """
+        Get the module prefix in multimodal models
+        """
+        return MultiModelKeys.from_string_field(
+            language_model="language_model",
+            connector="visual.merger.",
+            tower_model="visual.",
+        )
diff --git a/vllm/model_executor/models/glm4_moe.py b/vllm/model_executor/models/glm4_moe.py
new file mode 100644
index 000000000..2ca32f556
--- /dev/null
+++ b/vllm/model_executor/models/glm4_moe.py
@@ -0,0 +1,701 @@
+# SPDX-License-Identifier: Apache-2.0
+# SPDX-FileCopyrightText: Copyright contributors to the vLLM project
+
+# Copyright 2025 The ZhipuAI Team.
+# Copyright 2023 The vLLM team.
+# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.
+#
+# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX
+# and OPT implementations in this library. It has been modified from its
+# original forms to accommodate minor architectural differences compared
+# to GPT-NeoX and OPT used by the Meta AI team that trained the model.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+"""Inference-only GLM-4.5 model compatible with HuggingFace weights."""
+import typing
+from collections.abc import Callable, Iterable
+from typing import Any, Optional, Union
+
+import torch
+from torch import nn
+from transformers import PretrainedConfig
+
+from vllm.attention import Attention
+from vllm.compilation.decorators import support_torch_compile
+from vllm.config import CacheConfig, VllmConfig, get_current_vllm_config
+from vllm.distributed import (get_ep_group, get_pp_group,
+                              get_tensor_model_parallel_world_size)
+from vllm.logger import init_logger
+from vllm.model_executor.layers.activation import SiluAndMul
+from vllm.model_executor.layers.fused_moe import FusedMoE
+from vllm.model_executor.layers.layernorm import RMSNorm
+from vllm.model_executor.layers.linear import (MergedColumnParallelLinear,
+                                               QKVParallelLinear,
+                                               RowParallelLinear)
+from vllm.model_executor.layers.logits_processor import LogitsProcessor
+from vllm.model_executor.layers.quantization import QuantizationConfig
+from vllm.model_executor.layers.rotary_embedding import get_rope
+from vllm.model_executor.layers.vocab_parallel_embedding import (
+    ParallelLMHead, VocabParallelEmbedding)
+from vllm.model_executor.model_loader.weight_utils import (
+    default_weight_loader, maybe_remap_kv_scale_name)
+from vllm.model_executor.sampling_metadata import SamplingMetadata
+from vllm.sequence import IntermediateTensors
+
+from .interfaces import SupportsLoRA, SupportsPP
+from .utils import (AutoWeightsLoader, PPMissingLayer, is_pp_missing_parameter,
+                    make_empty_intermediate_tensors_factory, make_layers,
+                    maybe_prefix)
+
+logger = init_logger(__name__)
+
+
+class Glm4MoeMLP(nn.Module):
+
+    def __init__(
+        self,
+        hidden_size: int,
+        intermediate_size: int,
+        hidden_act: str,
+        quant_config: Optional[QuantizationConfig] = None,
+        reduce_results: bool = True,
+        prefix: str = "",
+    ) -> None:
+        super().__init__()
+        self.gate_up_proj = MergedColumnParallelLinear(
+            hidden_size, [intermediate_size] * 2,
+            bias=False,
+            quant_config=quant_config,
+            prefix=f"{prefix}.gate_up_proj")
+        self.down_proj = RowParallelLinear(intermediate_size,
+                                           hidden_size,
+                                           bias=False,
+                                           quant_config=quant_config,
+                                           reduce_results=reduce_results,
+                                           prefix=f"{prefix}.down_proj")
+        if hidden_act != "silu":
+            raise ValueError(f"Unsupported activation: {hidden_act}. "
+                             "Only silu is supported for now.")
+        self.act_fn = SiluAndMul()
+
+    def forward(self, x):
+        gate_up, _ = self.gate_up_proj(x)
+        x = self.act_fn(gate_up)
+        x, _ = self.down_proj(x)
+        return x
+
+
+class Glm4MoE(nn.Module):
+
+    def __init__(
+        self,
+        config: PretrainedConfig,
+        quant_config: Optional[QuantizationConfig] = None,
+        prefix: str = "",
+        enable_eplb: bool = False,
+    ):
+        super().__init__()
+        self.tp_size = get_tensor_model_parallel_world_size()
+        self.routed_scaling_factor = config.routed_scaling_factor
+
+        self.ep_group = get_ep_group().device_group
+        self.ep_rank = self.ep_group.rank()
+        self.ep_size = self.ep_group.size()
+        self.n_routed_experts: int = config.n_routed_experts
+        self.n_shared_experts: int = config.n_shared_experts
+
+        if config.hidden_act != "silu":
+            raise ValueError(f"Unsupported activation: {config.hidden_act}. "
+                             "Only silu is supported for now.")
+        # NOTE In the transformers implementation, the gate isn't an nn.Linear,
+        # so we cannot use ReplicatedLinear here.
+        # See: https://github.com/huggingface/transformers/blob/v4.55.1/src/transformers/models/glm4_moe/modeling_glm4_moe.py#L260
+        self.gate = nn.Linear(
+            config.hidden_size,
+            config.n_routed_experts,
+            bias=False,
+            dtype=torch.float32,
+        )
+        self.gate.e_score_correction_bias = nn.Parameter(
+            torch.empty(config.n_routed_experts, dtype=torch.float32))
+
+        # Load balancing settings.
+        vllm_config = get_current_vllm_config()
+        parallel_config = vllm_config.parallel_config
+        self.enable_eplb = enable_eplb
+
+        #self.n_redundant_experts = parallel_config.num_redundant_experts
+        self.n_redundant_experts = 0
+        self.n_logical_experts = self.n_routed_experts
+        self.n_physical_experts = (self.n_logical_experts +
+                                   self.n_redundant_experts)
+        self.n_local_physical_experts = self.n_physical_experts // self.ep_size
+
+        self.physical_expert_start = (self.ep_rank *
+                                      self.n_local_physical_experts)
+        self.physical_expert_end = (self.physical_expert_start +
+                                    self.n_local_physical_experts)
+
+        self.experts = FusedMoE(
+            num_experts=config.n_routed_experts,
+            top_k=config.num_experts_per_tok,
+            hidden_size=config.hidden_size,
+            intermediate_size=config.moe_intermediate_size,
+            reduce_results=False,
+            renormalize=config.norm_topk_prob,
+            quant_config=quant_config,
+            use_grouped_topk=True,
+            num_expert_group=config.n_group,
+            topk_group=config.topk_group,
+            prefix=f"{prefix}.experts",
+            scoring_func="sigmoid",
+            e_score_correction_bias=self.gate.e_score_correction_bias,
+            )
+            # enable_eplb=self.enable_eplb,
+            # num_redundant_experts=self.n_redundant_experts)
+
+        if config.n_shared_experts is not None:
+            intermediate_size = (config.moe_intermediate_size *
+                                 config.n_shared_experts)
+            self.shared_experts = Glm4MoeMLP(
+                hidden_size=config.hidden_size,
+                intermediate_size=intermediate_size,
+                hidden_act=config.hidden_act,
+                quant_config=quant_config,
+                reduce_results=self.experts.must_reduce_shared_expert_outputs(
+                ),
+                prefix=f"{prefix}.shared_experts",
+            )
+
+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
+        num_tokens, hidden_dim = hidden_states.shape
+        hidden_states = hidden_states.view(-1, hidden_dim)
+
+        if self.n_shared_experts is not None:
+            shared_output = self.shared_experts(hidden_states)
+        router_logits = self.gate(hidden_states.to(dtype=torch.float32))
+        final_hidden_states = self.experts(
+            hidden_states=hidden_states,
+            router_logits=router_logits) * self.routed_scaling_factor
+        if shared_output is not None:
+            final_hidden_states = final_hidden_states + shared_output
+        if self.tp_size > 1:
+            final_hidden_states = (
+                self.experts.maybe_all_reduce_tensor_model_parallel(
+                    final_hidden_states))
+        return final_hidden_states.view(num_tokens, hidden_dim)
+
+
+class Glm4MoeAttention(nn.Module):
+
+    def __init__(
+        self,
+        config: PretrainedConfig,
+        hidden_size: int,
+        num_heads: int,
+        num_kv_heads: int,
+        rope_theta: float = 10000,
+        rope_scaling: Optional[dict[str, Any]] = None,
+        max_position_embeddings: int = 131072,
+        head_dim: Optional[int] = None,
+        rms_norm_eps: float = 1e-05,
+        qkv_bias: bool = False,
+        use_qk_norm: bool = False,
+        cache_config: Optional[CacheConfig] = None,
+        quant_config: Optional[QuantizationConfig] = None,
+        prefix: str = "",
+    ) -> None:
+        super().__init__()
+        self.hidden_size = hidden_size
+        tp_size = get_tensor_model_parallel_world_size()
+        self.total_num_heads = num_heads
+        assert self.total_num_heads % tp_size == 0
+        self.num_heads = self.total_num_heads // tp_size
+        self.total_num_kv_heads = num_kv_heads
+        if self.total_num_kv_heads >= tp_size:
+            # Number of KV heads is greater than TP size, so we partition
+            # the KV heads across multiple tensor parallel GPUs.
+            assert self.total_num_kv_heads % tp_size == 0
+        else:
+            # Number of KV heads is less than TP size, so we replicate
+            # the KV heads across multiple tensor parallel GPUs.
+            assert tp_size % self.total_num_kv_heads == 0
+        self.num_kv_heads = max(1, self.total_num_kv_heads // tp_size)
+        self.head_dim = head_dim or (hidden_size // self.total_num_heads)
+        self.q_size = self.num_heads * self.head_dim
+        self.kv_size = self.num_kv_heads * self.head_dim
+        self.scaling = self.head_dim**-0.5
+        self.rope_theta = rope_theta
+        self.max_position_embeddings = max_position_embeddings
+        self.use_qk_norm = use_qk_norm
+
+        self.qkv_proj = QKVParallelLinear(hidden_size,
+                                          self.head_dim,
+                                          self.total_num_heads,
+                                          self.total_num_kv_heads,
+                                          bias=qkv_bias,
+                                          quant_config=quant_config,
+                                          prefix=f"{prefix}.qkv_proj")
+
+        self.o_proj = RowParallelLinear(self.total_num_heads * self.head_dim,
+                                        hidden_size,
+                                        bias=False,
+                                        quant_config=quant_config,
+                                        prefix=f"{prefix}.o_proj")
+
+        partial_rotary_factor = getattr(config, "partial_rotary_factor", 0.5)
+        self.rotary_emb = get_rope(
+            self.head_dim,
+            rotary_dim=self.head_dim,
+            max_position=max_position_embeddings,
+            base=rope_theta,
+            rope_scaling=rope_scaling,
+            partial_rotary_factor=partial_rotary_factor,
+        )
+        self.attn = Attention(
+            self.num_heads,
+            self.head_dim,
+            self.scaling,
+            num_kv_heads=self.num_kv_heads,
+            cache_config=cache_config,
+            quant_config=quant_config,
+            prefix=f"{prefix}.attn",
+        )
+
+        if self.use_qk_norm:
+            self.q_norm = RMSNorm(self.head_dim, eps=rms_norm_eps)
+            self.k_norm = RMSNorm(self.head_dim, eps=rms_norm_eps)
+
+    def forward(
+        self,
+        positions: torch.Tensor,
+        hidden_states: torch.Tensor,
+    ) -> torch.Tensor:
+        qkv, _ = self.qkv_proj(hidden_states)
+        q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
+        if self.use_qk_norm:
+            q = self.q_norm(q.reshape(-1, self.num_heads,
+                                      self.head_dim)).reshape(q.shape)
+            k = self.k_norm(k.reshape(-1, self.num_kv_heads,
+                                      self.head_dim)).reshape(k.shape)
+
+        q, k = self.rotary_emb(positions, q, k)
+        attn_output = self.attn(q, k, v)
+        output, _ = self.o_proj(attn_output)
+        return output
+
+
+class Glm4MoeDecoderLayer(nn.Module):
+
+    def __init__(
+        self,
+        config: PretrainedConfig,
+        cache_config: Optional[CacheConfig] = None,
+        quant_config: Optional[QuantizationConfig] = None,
+        prefix: str = "",
+        enable_eplb: bool = False,
+    ) -> None:
+        super().__init__()
+        self.hidden_size = config.hidden_size
+        rope_theta = getattr(config, "rope_theta", 10000)
+        rope_scaling = getattr(config, "rope_scaling", None)
+        max_position_embeddings = getattr(config, "max_position_embeddings",
+                                          131072)
+        # DecoderLayers are created with `make_layers` which passes the prefix
+        # with the layer's index.
+        layer_idx = int(prefix.split(sep='.')[-1])
+        self.layer_idx = layer_idx
+
+        self.self_attn = Glm4MoeAttention(
+            config=config,
+            hidden_size=self.hidden_size,
+            num_heads=config.num_attention_heads,
+            num_kv_heads=config.num_key_value_heads,
+            rope_theta=rope_theta,
+            rope_scaling=rope_scaling,
+            max_position_embeddings=max_position_embeddings,
+            head_dim=config.head_dim,
+            rms_norm_eps=config.rms_norm_eps,
+            qkv_bias=config.attention_bias,
+            cache_config=cache_config,
+            quant_config=quant_config,
+            prefix=f"{prefix}.self_attn",
+            use_qk_norm=config.use_qk_norm,
+        )
+
+        if (config.n_routed_experts is not None
+                and layer_idx >= config.first_k_dense_replace):
+            self.mlp = Glm4MoE(
+                config=config,
+                quant_config=quant_config,
+                prefix=f"{prefix}.mlp",
+                enable_eplb=enable_eplb,
+            )
+        else:
+            self.mlp = Glm4MoeMLP(hidden_size=config.hidden_size,
+                                  intermediate_size=config.intermediate_size,
+                                  hidden_act=config.hidden_act,
+                                  quant_config=quant_config,
+                                  prefix=f"{prefix}.mlp")
+
+        self.input_layernorm = RMSNorm(config.hidden_size,
+                                       eps=config.rms_norm_eps)
+        self.post_attention_layernorm = RMSNorm(config.hidden_size,
+                                                eps=config.rms_norm_eps)
+        self.routed_scaling_factor = config.routed_scaling_factor
+
+    def forward(
+        self,
+        positions: torch.Tensor,
+        hidden_states: torch.Tensor,
+        residual: Optional[torch.Tensor],
+    ) -> tuple[torch.Tensor, torch.Tensor]:
+        if residual is None:
+            residual = hidden_states
+            hidden_states = self.input_layernorm(hidden_states)
+        else:
+            hidden_states, residual = self.input_layernorm(
+                hidden_states, residual)
+        hidden_states = self.self_attn(positions=positions,
+                                       hidden_states=hidden_states)
+        hidden_states, residual = self.post_attention_layernorm(
+            hidden_states, residual)
+        hidden_states = self.mlp(hidden_states)
+        return hidden_states, residual
+
+
+@support_torch_compile(
+    dynamic_arg_dims={
+        "input_ids": 0,
+        "positions": -1,
+        "intermediate_tensors": 0,
+        "inputs_embeds": 0,
+    })
+class Glm4MoeModel(nn.Module):
+
+    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
+        super().__init__()
+
+        config = vllm_config.model_config.hf_config
+        cache_config = vllm_config.cache_config
+        quant_config = vllm_config.quant_config
+        #quant_config = None
+        #enable_eplb = vllm_config.parallel_config.enable_eplb
+        enable_eplb = False
+        self.config = config
+
+        self.vocab_size = config.vocab_size
+
+        if get_pp_group().is_first_rank:
+            self.embed_tokens = VocabParallelEmbedding(
+                config.vocab_size,
+                config.hidden_size,
+                prefix=f"{prefix}.embed_tokens")
+        else:
+            self.embed_tokens = PPMissingLayer()
+
+        self.start_layer, self.end_layer, self.layers = make_layers(
+            config.num_hidden_layers,
+            lambda prefix: Glm4MoeDecoderLayer(
+                config=config,
+                cache_config=cache_config,
+                quant_config=quant_config,
+                prefix=prefix,
+                enable_eplb=enable_eplb,
+            ),
+            prefix=f"{prefix}.layers")
+
+        if get_pp_group().is_last_rank:
+            self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
+        else:
+            self.norm = PPMissingLayer()
+        self.make_empty_intermediate_tensors = (
+            make_empty_intermediate_tensors_factory(
+                ["hidden_states", "residual"], config.hidden_size))
+
+    def get_input_embeddings(self, input_ids: torch.Tensor) -> torch.Tensor:
+        return self.embed_tokens(input_ids)
+
+    def forward(
+        self,
+        input_ids: torch.Tensor,
+        positions: torch.Tensor,
+        intermediate_tensors: Optional[IntermediateTensors] = None,
+        inputs_embeds: Optional[torch.Tensor] = None,
+    ) -> Union[torch.Tensor, IntermediateTensors]:
+        if get_pp_group().is_first_rank:
+            if inputs_embeds is not None:
+                hidden_states = inputs_embeds
+            else:
+                hidden_states = self.get_input_embeddings(input_ids)
+            residual = None
+        else:
+            assert intermediate_tensors is not None
+            hidden_states = intermediate_tensors["hidden_states"]
+            residual = intermediate_tensors["residual"]
+
+        for i in range(self.start_layer, self.end_layer):
+            layer = self.layers[i]
+            hidden_states, residual = layer(positions, hidden_states, residual)
+
+        if not get_pp_group().is_last_rank:
+            return IntermediateTensors({
+                "hidden_states": hidden_states,
+                "residual": residual
+            })
+
+        hidden_states, _ = self.norm(hidden_states, residual)
+        return hidden_states
+
+    def make_empty_intermediate_tensors(
+            self, batch_size: int, dtype: torch.dtype,
+            device: torch.device) -> IntermediateTensors:
+        return IntermediateTensors({
+            "hidden_states":
+            torch.zeros((batch_size, self.config.hidden_size),
+                        dtype=dtype,
+                        device=device),
+            "residual":
+            torch.zeros((batch_size, self.config.hidden_size),
+                        dtype=dtype,
+                        device=device),
+        })
+
+    def get_expert_mapping(self) -> list[tuple[str, str, int, str]]:
+        # Params for weights, fp8 weight scales, fp8 activation scales
+        # (param_name, weight_name, expert_id, shard_id)
+        return FusedMoE.make_expert_params_mapping(
+            ckpt_gate_proj_name="gate_proj",
+            ckpt_down_proj_name="down_proj",
+            ckpt_up_proj_name="up_proj",
+            num_experts=self.config.n_routed_experts)
+
+    def load_weights(self, weights: Iterable[tuple[str,
+                                                   torch.Tensor]]) -> set[str]:
+        stacked_params_mapping = [
+            # (param_name, shard_name, shard_id)
+            ("qkv_proj", "q_proj", "q"),
+            ("qkv_proj", "k_proj", "k"),
+            ("qkv_proj", "v_proj", "v"),
+            ("gate_up_proj", "gate_proj", 0),
+            ("gate_up_proj", "up_proj", 1),
+        ]
+
+        params_dict = dict(self.named_parameters())
+        loaded_params: set[str] = set()
+        expert_params_mapping = self.get_expert_mapping()
+        for name, loaded_weight in weights:
+            spec_layer = get_spec_layer_idx_from_weight_name(self.config, name)
+            if spec_layer is not None:
+                continue
+            for (param_name, weight_name, shard_id) in stacked_params_mapping:
+                # Skip non-stacked layers and experts (experts handled below).
+                if weight_name not in name:
+                    continue
+                # We have mlp.experts[0].gate_proj in the checkpoint.
+                # Since we handle the experts below in expert_params_mapping,
+                # we need to skip here BEFORE we update the name, otherwise
+                # name will be updated to mlp.experts[0].gate_up_proj, which
+                # will then be updated below in expert_params_mapping
+                # for mlp.experts[0].gate_gate_up_proj, which breaks load.
+                if (("mlp.experts." in name) and name not in params_dict):
+                    continue
+                name = name.replace(weight_name, param_name)
+                # Skip loading extra bias for GPTQ models.
+                if name.endswith(".bias") and name not in params_dict:
+                    continue
+                if is_pp_missing_parameter(name, self):
+                    continue
+
+                param = params_dict[name]
+                weight_loader = param.weight_loader
+                weight_loader(param, loaded_weight, shard_id)
+                break
+            else:
+                is_expert_weight = False
+                for mapping in expert_params_mapping:
+                    param_name, weight_name, expert_id, shard_id = mapping
+                    if weight_name not in name:
+                        continue
+
+                    # Anyway, this is an expert weight and should not be
+                    # attempted to load as other weights later
+                    is_expert_weight = True
+
+                    # Do not modify `name` since the loop may continue here
+                    # Instead, create a new variable
+                    name_mapped = name.replace(weight_name, param_name)
+
+                    if is_pp_missing_parameter(name_mapped, self):
+                        continue
+
+                    param = params_dict[name_mapped]
+                    # We should ask the weight loader to return success or not
+                    # here since otherwise we may skip experts with other
+                    # available replicas.
+                    weight_loader = typing.cast(Callable[..., bool],
+                                                param.weight_loader)
+                    weight_loader(param,
+                                  loaded_weight,
+                                  name_mapped,
+                                  shard_id=shard_id,
+                                  expert_id=expert_id)
+                    name = name_mapped
+                    break
+                else:
+                    if is_expert_weight:
+                        # We've checked that this is an expert weight
+                        # However it's not mapped locally to this rank
+                        # So we simply skip it
+                        continue
+
+                    # Skip loading extra bias for GPTQ models.
+                    if name.endswith(".bias") and name not in params_dict:
+                        continue
+
+                    # Remapping the name of FP8 kv-scale.
+                    name = maybe_remap_kv_scale_name(name, params_dict)
+                    if name is None:
+                        continue
+
+                    if is_pp_missing_parameter(name, self):
+                        continue
+
+                    param = params_dict[name]
+                    weight_loader = getattr(param, "weight_loader",
+                                            default_weight_loader)
+                    weight_loader(param, loaded_weight)
+            loaded_params.add(name)
+
+        return loaded_params
+
+
+class Glm4MoeForCausalLM(nn.Module, SupportsPP, SupportsLoRA):
+    packed_modules_mapping = {
+        "qkv_proj": [
+            "q_proj",
+            "k_proj",
+            "v_proj",
+        ],
+        "gate_up_proj": [
+            "gate_proj",
+            "up_proj",
+        ],
+    }
+
+    fall_back_to_pt_during_load = False
+
+    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
+        super().__init__()
+        config = vllm_config.model_config.hf_config
+        quant_config = vllm_config.quant_config
+        self.config = config
+        self.quant_config = quant_config
+        self.model = Glm4MoeModel(vllm_config=vllm_config,
+                                  prefix=maybe_prefix(prefix, "model"))
+        if get_pp_group().is_last_rank:
+            self.lm_head = ParallelLMHead(config.vocab_size,
+                                          config.hidden_size,
+                                          quant_config=quant_config)
+        else:
+            self.lm_head = PPMissingLayer()
+        self.logits_processor = LogitsProcessor(config.vocab_size)
+        self.make_empty_intermediate_tensors = (
+            self.model.make_empty_intermediate_tensors)
+        self.expert_weights = []
+
+        # Set MoE hyperparameters
+        self.num_moe_layers = (config.num_hidden_layers -
+                               config.first_k_dense_replace)
+        self.num_expert_groups = config.n_group
+
+        self.moe_layers: list[FusedMoE] = []
+        example_moe = None
+        for layer in self.model.layers:
+            if isinstance(layer, PPMissingLayer):
+                continue
+
+            assert isinstance(layer, Glm4MoeDecoderLayer)
+            if isinstance(layer.mlp, Glm4MoE):
+                # Pick last one layer since the first ones may be dense layers.
+                example_moe = layer.mlp
+                self.moe_layers.append(layer.mlp.experts)
+
+        if example_moe is None:
+            raise RuntimeError("No Glm4MoE layer found in model.layers.")
+
+        self.num_logical_experts = example_moe.n_logical_experts
+        self.num_physical_experts = example_moe.n_physical_experts
+        self.num_local_physical_experts = example_moe.n_local_physical_experts
+        self.num_routed_experts = example_moe.n_routed_experts
+        self.num_shared_experts = example_moe.n_shared_experts
+        self.num_redundant_experts = example_moe.n_redundant_experts
+
+    def set_eplb_state(
+        self,
+        expert_load_view: torch.Tensor,
+        logical_to_physical_map: torch.Tensor,
+        logical_replica_count: torch.Tensor,
+    ) -> None:
+        for layer_idx, layer in enumerate(self.moe_layers):
+            # Register the expert weights.
+            self.expert_weights.append(layer.get_expert_weights())
+            layer.set_eplb_state(
+                moe_layer_idx=layer_idx,
+                expert_load_view=expert_load_view,
+                logical_to_physical_map=logical_to_physical_map,
+                logical_replica_count=logical_replica_count,
+            )
+
+    def get_input_embeddings(self, input_ids: torch.Tensor) -> torch.Tensor:
+        return self.model.get_input_embeddings(input_ids)
+
+    def forward(
+        self,
+        input_ids: torch.Tensor,
+        positions: torch.Tensor,
+        intermediate_tensors: Optional[IntermediateTensors] = None,
+        inputs_embeds: Optional[torch.Tensor] = None,
+    ) -> Union[torch.Tensor, IntermediateTensors]:
+        hidden_states = self.model(input_ids, positions, intermediate_tensors,
+                                   inputs_embeds)
+        return hidden_states
+
+    def compute_logits(
+        self,
+        hidden_states: torch.Tensor,
+        sampling_metadata: SamplingMetadata,
+    ) -> Optional[torch.Tensor]:
+        logits = self.logits_processor(self.lm_head, hidden_states,
+                                       sampling_metadata)
+        return logits
+
+    def load_weights(self, weights: Iterable[tuple[str,
+                                                   torch.Tensor]]) -> set[str]:
+        loader = AutoWeightsLoader(self)
+        return loader.load_weights(weights)
+
+    def get_expert_mapping(self) -> list[tuple[str, str, int, str]]:
+        return self.model.get_expert_mapping()
+
+
+def get_spec_layer_idx_from_weight_name(config: PretrainedConfig,
+                                        weight_name: str) -> Optional[int]:
+    if hasattr(config,
+               "num_nextn_predict_layers") and (config.num_nextn_predict_layers
+                                                > 0):
+        layer_idx = config.num_hidden_layers
+        for i in range(config.num_nextn_predict_layers):
+            if f"layers.{layer_idx+i}." in weight_name:
+                return layer_idx + i
+    return None
\ No newline at end of file
diff --git a/vllm/model_executor/models/glm4_moe_mtp.py b/vllm/model_executor/models/glm4_moe_mtp.py
new file mode 100644
index 000000000..5d1b23ab3
--- /dev/null
+++ b/vllm/model_executor/models/glm4_moe_mtp.py
@@ -0,0 +1,307 @@
+# SPDX-License-Identifier: Apache-2.0
+# SPDX-FileCopyrightText: Copyright contributors to the vLLM project
+
+# Copyright 2025 The ZhipuAI Team.
+# Copyright 2023 The vLLM team.
+# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.
+#
+# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX
+# and OPT implementations in this library. It has been modified from its
+# original forms to accommodate minor architectural differences compared
+# to GPT-NeoX and OPT used by the Meta AI team that trained the model.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+"""Inference-only GLM-4.5 MTP model compatible with HuggingFace weights."""
+
+from collections.abc import Iterable
+from typing import Optional
+
+import torch
+import torch.nn as nn
+from transformers import PretrainedConfig
+
+from vllm.config import CacheConfig, VllmConfig
+from vllm.model_executor.layers.fused_moe import FusedMoE
+from vllm.model_executor.layers.layernorm import RMSNorm
+from vllm.model_executor.layers.logits_processor import LogitsProcessor
+from vllm.model_executor.layers.quantization import QuantizationConfig
+from vllm.model_executor.layers.vocab_parallel_embedding import (
+    ParallelLMHead, VocabParallelEmbedding)
+from vllm.model_executor.model_loader.weight_utils import default_weight_loader
+from vllm.model_executor.sampling_metadata import SamplingMetadata
+from vllm.sequence import IntermediateTensors
+
+from .glm4_moe import Glm4MoeDecoderLayer, get_spec_layer_idx_from_weight_name
+from .interfaces import SupportsPP
+from .utils import maybe_prefix
+
+
+class SharedHead(nn.Module):
+
+    def __init__(
+        self,
+        config: PretrainedConfig,
+        quant_config: Optional[QuantizationConfig] = None,
+    ) -> None:
+        super().__init__()
+        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
+        self.head = ParallelLMHead(config.vocab_size,
+                                   config.hidden_size,
+                                   quant_config=quant_config)
+
+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
+        return self.norm(hidden_states)
+
+
+class Glm4MoeMultiTokenPredictorLayer(nn.Module):
+
+    def __init__(
+        self,
+        config: PretrainedConfig,
+        prefix: str,
+        cache_config: Optional[CacheConfig] = None,
+        quant_config: Optional[QuantizationConfig] = None,
+    ) -> None:
+        super().__init__()
+        self.enorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
+        self.hnorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
+        self.eh_proj = nn.Linear(config.hidden_size * 2,
+                                 config.hidden_size,
+                                 bias=False)
+        self.shared_head = SharedHead(config=config, quant_config=quant_config)
+        self.mtp_block = Glm4MoeDecoderLayer(config=config,
+                                             cache_config=cache_config,
+                                             quant_config=quant_config,
+                                             prefix=prefix)
+
+    def forward(
+        self,
+        input_ids: torch.Tensor,
+        positions: torch.Tensor,
+        previous_hidden_states: torch.Tensor,
+        inputs_embeds: Optional[torch.Tensor] = None,
+        spec_step_index: int = 0,
+    ) -> torch.Tensor:
+        assert inputs_embeds is not None
+        # masking inputs at position 0, as not needed by MTP
+        inputs_embeds[positions == 0] = 0
+        inputs_embeds = self.enorm(inputs_embeds)
+        previous_hidden_states = self.hnorm(previous_hidden_states)
+
+        hidden_states = self.eh_proj(
+            torch.cat([inputs_embeds, previous_hidden_states], dim=-1))
+
+        hidden_states, residual = self.mtp_block(positions=positions,
+                                                 hidden_states=hidden_states,
+                                                 residual=None)
+        hidden_states = residual + hidden_states
+        return hidden_states
+
+
+class Glm4MoeMultiTokenPredictor(nn.Module):
+
+    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
+        super().__init__()
+        config = vllm_config.model_config.hf_config
+        self.mtp_start_layer_idx = config.num_hidden_layers
+        self.num_mtp_layers = config.num_nextn_predict_layers
+        # to map the exact layer index from weights
+        self.layers = torch.nn.ModuleDict({
+            str(idx):
+            Glm4MoeMultiTokenPredictorLayer(
+                config,
+                f"{prefix}.layers.{idx}",
+                cache_config=vllm_config.cache_config,
+                quant_config=vllm_config.quant_config,
+            )
+            for idx in range(self.mtp_start_layer_idx,
+                             self.mtp_start_layer_idx + self.num_mtp_layers)
+        })
+        self.embed_tokens = VocabParallelEmbedding(
+            config.vocab_size,
+            config.hidden_size,
+        )
+        self.logits_processor = LogitsProcessor(config.vocab_size)
+
+    def forward(
+        self,
+        input_ids: torch.Tensor,
+        positions: torch.Tensor,
+        previous_hidden_states: torch.Tensor,
+        inputs_embeds: Optional[torch.Tensor] = None,
+        spec_step_idx: int = 0,
+    ) -> torch.Tensor:
+        if inputs_embeds is None:
+            inputs_embeds = self.embed_tokens(input_ids)
+        current_step_idx = (spec_step_idx % self.num_mtp_layers)
+        return self.layers[str(self.mtp_start_layer_idx + current_step_idx)](
+            input_ids,
+            positions,
+            previous_hidden_states,
+            inputs_embeds,
+            current_step_idx,
+        )
+
+    def compute_logits(
+        self,
+        hidden_states: torch.Tensor,
+        sampling_metadata: SamplingMetadata,
+        spec_step_idx: int = 0,
+    ) -> torch.Tensor:
+        current_step_idx = (spec_step_idx % self.num_mtp_layers)
+        mtp_layer = self.layers[str(self.mtp_start_layer_idx +
+                                    current_step_idx)]
+        logits = self.logits_processor(mtp_layer.shared_head.head,
+                                       mtp_layer.shared_head(hidden_states),
+                                       sampling_metadata)
+        return logits
+
+
+class Glm4MoeMTP(nn.Module, SupportsPP):
+
+    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
+        super().__init__()
+        self.config = vllm_config.model_config.hf_config
+        self.model = Glm4MoeMultiTokenPredictor(vllm_config=vllm_config,
+                                                prefix=maybe_prefix(
+                                                    prefix, "model"))
+
+    def forward(
+        self,
+        input_ids: torch.Tensor,
+        positions: torch.Tensor,
+        previous_hidden_states: torch.Tensor,
+        intermediate_tensors: Optional[IntermediateTensors] = None,
+        inputs_embeds: Optional[torch.Tensor] = None,
+        spec_step_idx: int = 0,
+    ) -> torch.Tensor:
+        hidden_states = self.model(input_ids, positions,
+                                   previous_hidden_states, inputs_embeds,
+                                   spec_step_idx)
+        return hidden_states
+
+    def compute_logits(
+        self,
+        hidden_states: torch.Tensor,
+        sampling_metadata: SamplingMetadata,
+        spec_step_idx: int = 0,
+    ) -> Optional[torch.Tensor]:
+        return self.model.compute_logits(hidden_states, sampling_metadata,
+                                         spec_step_idx)
+
+    def load_weights(self, weights: Iterable[tuple[str,
+                                                   torch.Tensor]]) -> set[str]:
+        stacked_params_mapping = [
+            # (param_name, shard_name, shard_id)
+            ("qkv_proj", "q_proj", "q"),
+            ("qkv_proj", "k_proj", "k"),
+            ("qkv_proj", "v_proj", "v"),
+            ("gate_up_proj", "gate_proj", 0),
+            ("gate_up_proj", "up_proj", 1),
+        ]
+
+        # Params for weights, fp8 weight scales, fp8 activation scales
+        # (param_name, weight_name, expert_id, shard_id)
+        expert_params_mapping = FusedMoE.make_expert_params_mapping(
+            ckpt_gate_proj_name="gate_proj",
+            ckpt_down_proj_name="down_proj",
+            ckpt_up_proj_name="up_proj",
+            num_experts=self.config.n_routed_experts)
+
+        params_dict = dict(self.named_parameters())
+        loaded_params: set[str] = set()
+        for name, loaded_weight in weights:
+            spec_layer = get_spec_layer_idx_from_weight_name(self.config, name)
+            if spec_layer is None:
+                continue
+            name = self._rewrite_spec_layer_name(spec_layer, name)
+            for (param_name, weight_name, shard_id) in stacked_params_mapping:
+                # Skip non-stacked layers and experts (experts handled below).
+                if weight_name not in name:
+                    continue
+                # We have mlp.experts[0].gate_proj in the checkpoint.
+                # Since we handle the experts below in expert_params_mapping,
+                # we need to skip here BEFORE we update the name, otherwise
+                # name will be updated to mlp.experts[0].gate_up_proj, which
+                # will then be updated below in expert_params_mapping
+                # for mlp.experts[0].gate_gate_up_proj, which breaks load.
+                if (("mlp.experts." in name) and name not in params_dict):
+                    continue
+                name = name.replace(weight_name, param_name)
+                # Skip loading extra bias for GPTQ models.
+                if name.endswith(".bias") and name not in params_dict:
+                    continue
+
+                param = params_dict[name]
+                weight_loader = param.weight_loader
+                weight_loader(param, loaded_weight, shard_id)
+                break
+            else:
+                for mapping in expert_params_mapping:
+                    param_name, weight_name, expert_id, shard_id = mapping
+                    if weight_name not in name:
+                        continue
+                    name = name.replace(weight_name, param_name)
+
+                    param = params_dict[name]
+                    weight_loader = param.weight_loader
+                    weight_loader(param,
+                                  loaded_weight,
+                                  name,
+                                  shard_id=shard_id,
+                                  expert_id=expert_id)
+                    break
+                else:
+                    # Skip loading extra bias for GPTQ models.
+                    if name.endswith(".bias") and name not in params_dict:
+                        continue
+
+                    # According to DeepSeek-V3 Technical Report, MTP modules
+                    # shares embedding layer. We only load the first weights.
+                    if (spec_layer != self.model.mtp_start_layer_idx
+                            and ".layers" not in name):
+                        continue
+
+                    param = params_dict[name]
+                    weight_loader = getattr(param, "weight_loader",
+                                            default_weight_loader)
+                    weight_loader(param, loaded_weight)
+            loaded_params.add(name)
+        return loaded_params
+
+    def _rewrite_spec_layer_name(self, spec_layer: int, name: str) -> str:
+        """
+        Rewrite the weight name to match the format of the original model.
+        Add .mtp_block for modules in transformer layer block for spec layer
+        and rename shared layer weights to be top level.
+        """
+        spec_layer_weight_names = [
+            "embed_tokens", "enorm", "hnorm", "eh_proj", "shared_head"
+        ]
+        shared_weight_names = ["embed_tokens"]
+        spec_layer_weight = False
+        shared_weight = False
+        for weight_name in spec_layer_weight_names:
+            if weight_name in name:
+                spec_layer_weight = True
+                if weight_name in shared_weight_names:
+                    shared_weight = True
+                break
+        if not spec_layer_weight:
+            # treat rest weights as weights for transformer layer block
+            name = name.replace(f"model.layers.{spec_layer}.",
+                                f"model.layers.{spec_layer}.mtp_block.")
+        elif shared_weight:
+            # treat shared weights as top level weights
+            name = name.replace(f"model.layers.{spec_layer}.", "model.")
+        return name
\ No newline at end of file
diff --git a/vllm/model_executor/models/glm4v.py b/vllm/model_executor/models/glm4v.py
index 4e1371671..ffe45d214 100644
--- a/vllm/model_executor/models/glm4v.py
+++ b/vllm/model_executor/models/glm4v.py
@@ -17,6 +17,7 @@ from transformers.image_utils import ImageInput
 from transformers.tokenization_utils_base import TextInput
 
 from vllm.attention.layer import MultiHeadAttention
+from vllm.attention.layer import SelfMultiHeadAttention
 from vllm.config import VllmConfig
 from vllm.distributed import get_tensor_model_parallel_world_size
 from vllm.model_executor.layers.activation import SiluAndMul, get_act_fn
@@ -111,7 +112,9 @@ class EVA2CLIPAttention(nn.Module):
             prefix=f"{prefix}.dense",
         )
 
-        self.attn = MultiHeadAttention(self.num_heads_per_rank, self.head_dim,
+        # self.attn = MultiHeadAttention(self.num_heads_per_rank, self.head_dim,
+        #                                self.scale)
+        self.attn = SelfMultiHeadAttention(self.num_heads_per_rank, self.head_dim,
                                        self.scale)
         self.output_dropout = torch.nn.Dropout(config.dropout_prob)
 
diff --git a/vllm/model_executor/models/idefics2_vision_model.py b/vllm/model_executor/models/idefics2_vision_model.py
index b8bdc7aa3..1c33fd799 100644
--- a/vllm/model_executor/models/idefics2_vision_model.py
+++ b/vllm/model_executor/models/idefics2_vision_model.py
@@ -26,6 +26,7 @@ from transformers.models.idefics2.configuration_idefics2 import (
     Idefics2Config, Idefics2VisionConfig)
 
 from vllm.attention.layer import MultiHeadAttention
+from vllm.attention.layer import SelfMultiHeadAttention
 from vllm.distributed import divide, get_tensor_model_parallel_world_size
 from vllm.model_executor.layers.activation import get_act_fn
 from vllm.model_executor.layers.linear import (ColumnParallelLinear,
@@ -145,8 +146,10 @@ class Idefics2VisionAttention(nn.Module):
         )
         self.tp_size = get_tensor_model_parallel_world_size()
         self.num_heads_per_partition = divide(self.num_heads, self.tp_size)
-        self.attn = MultiHeadAttention(self.num_heads_per_partition,
-                                       self.head_dim, self.scale)
+        # self.attn = MultiHeadAttention(self.num_heads_per_partition,
+        #                                self.head_dim, self.scale)
+        self.attn = SelfMultiHeadAttention(self.num_heads_per_partition, self.head_dim,
+                                       self.scale)
 
     def forward(
         self,
diff --git a/vllm/model_executor/models/llama.py b/vllm/model_executor/models/llama.py
index d36b6466c..1de985f6d 100644
--- a/vllm/model_executor/models/llama.py
+++ b/vllm/model_executor/models/llama.py
@@ -436,6 +436,9 @@ class LlamaModel(nn.Module):
             if "scale" in name:
                 # Remapping the name of FP8 kv-scale.
                 name = maybe_remap_kv_scale_name(name, params_dict)
+                # temp fix for unit scale INC model, will can be removed
+                if "proj.scale" in name and not "scales" in name:
+                    name = name.replace("scale", "weight_scale")
                 if name is None:
                     continue
             for param_name, weight_name, shard_id in stacked_params_mapping:
diff --git a/vllm/model_executor/models/qwen2_5_vl.py b/vllm/model_executor/models/qwen2_5_vl.py
index 68dd07820..ad6d764aa 100644
--- a/vllm/model_executor/models/qwen2_5_vl.py
+++ b/vllm/model_executor/models/qwen2_5_vl.py
@@ -248,8 +248,9 @@ class Qwen2_5_VisionAttention(nn.Module):
 
         # Detect attention implementation.
         self.attn_backend: _Backend = get_vit_attn_backend(support_fa=True)
+        self.attn_backend = _Backend.TORCH_SDPA
         if self.attn_backend not in {
-                _Backend.FLASH_ATTN, _Backend.TORCH_SDPA, _Backend.XFORMERS
+                _Backend.FLASH_ATTN, _Backend.TORCH_SDPA, _Backend.XFORMERS, _Backend.IPEX_V1
         }:
             raise RuntimeError(
                 f"Qwen2.5-VL does not support {self.attn_backend} backend now."
@@ -320,24 +321,86 @@ class Qwen2_5_VisionAttention(nn.Module):
             context_layer = rearrange(output,
                                       "(b s) ... -> b s ...",
                                       b=batch_size)
+        elif self.attn_backend == _Backend.IPEX_V1:
+            from vllm._ipex_ops import ipex_ops
+
+            q, k, v = (rearrange(x, "b s ... -> (b s) ...") for x in [q, k, v])
+
+            output = torch.empty(
+                q.shape,
+                dtype=q.dtype,
+                device=q.device)
+            ipex_ops.varlen_attention(
+                    q,
+                    k,
+                    v,
+                    output,
+                    cu_seqlens,
+                    cu_seqlens,
+                    None,
+                    max_seqlen,
+                    max_seqlen,
+                    pdropout=0.0,
+                    softmax_scale=1.0/(q.shape[-1] ** 0.5),
+                    zero_tensors=False,
+                    is_causal=True,
+                    return_softmax=False,
+                    gen_=None,
+                    window_size_left=-1,
+                    window_size_right=-1,
+                    logits_soft_cap=-1,
+            )
+            context_layer = rearrange(output,
+                            "(b s) ... -> b s ...",
+                            b=batch_size)
         elif self.attn_backend == _Backend.TORCH_SDPA:
             # Execute attention entry by entry for speed & less VRAM.
-            outputs = []
-            for i in range(1, len(cu_seqlens)):
-                start_idx = cu_seqlens[i - 1]
-                end_idx = cu_seqlens[i]
-                q_i = q[:, start_idx:end_idx]
-                k_i = k[:, start_idx:end_idx]
-                v_i = v[:, start_idx:end_idx]
-                q_i, k_i, v_i = (rearrange(x, "b s h d -> b h s d")
-                                 for x in [q_i, k_i, v_i])
-                output_i = F.scaled_dot_product_attention(q_i,
-                                                          k_i,
-                                                          v_i,
-                                                          dropout_p=0.0)
-                output_i = rearrange(output_i, "b h s d -> b s h d ")
-                outputs.append(output_i)
-            context_layer = torch.cat(outputs, dim=1)
+            # TODO(xiangyu): Maybe add attn_backend xpu?
+            q, k, v = (rearrange(x, "b s ... -> (b s) ...") for x in [q, k, v])
+            from vllm._ipex_ops import ipex_ops
+            output = torch.empty(
+                        (q.shape[0], q.shape[1], q.shape[2]),
+                        dtype=q.dtype,
+                        device=q.device)
+            import math
+            head_dim = q.shape[-1]
+            scale = 1 / math.sqrt(head_dim)
+            ipex_ops.varlen_attention(q, k, v, output,
+                                    cu_seqlens,
+                                    cu_seqlens,
+                                    None,
+                                    max_seqlen,
+                                    max_seqlen,
+                                    pdropout=0,
+                                    softmax_scale=scale,
+                                    zero_tensors=False,
+                                    is_causal=False,
+                                    return_softmax=False,
+                                    window_size_left=-1,
+                                    window_size_right=-1,
+                                    gen_=None,
+                                    logits_soft_cap=0
+                                    )
+
+            context_layer = rearrange(output,
+                                      "(b s) ... -> b s ...",
+                                      b=batch_size)
+            # outputs = []
+            # for i in range(1, len(cu_seqlens)):
+            #     start_idx = cu_seqlens[i - 1]
+            #     end_idx = cu_seqlens[i]
+            #     q_i = q[:, start_idx:end_idx]
+            #     k_i = k[:, start_idx:end_idx]
+            #     v_i = v[:, start_idx:end_idx]
+            #     q_i, k_i, v_i = (rearrange(x, "b s h d -> b h s d")
+            #                      for x in [q_i, k_i, v_i])
+            #     output_i = F.scaled_dot_product_attention(q_i,
+            #                                               k_i,
+            #                                               v_i,
+            #                                               dropout_p=0.0)
+            #     output_i = rearrange(output_i, "b h s d -> b s h d ")
+            #     outputs.append(output_i)
+            # context_layer = torch.cat(outputs, dim=1)
         elif self.attn_backend == _Backend.XFORMERS:
             from xformers import ops as xops
             from xformers.ops.fmha.attn_bias import BlockDiagonalMask
@@ -642,6 +705,8 @@ class Qwen2_5_VisionTransformer(nn.Module):
             max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()
         elif self.attn_backend == _Backend.XFORMERS:
             seqlens = (cu_seqlens[1:] - cu_seqlens[:-1]).tolist()
+        elif self.attn_backend == _Backend.IPEX_V1:
+            max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()
         return max_seqlen, seqlens
 
     def forward(
@@ -1128,10 +1193,10 @@ class Qwen2_5_VLForConditionalGeneration(nn.Module, SupportsMultiModal,
             if image_input is None and video_input is None:
                 inputs_embeds = None
             else:
-                if uses_mrope(self.config):
-                    assert positions.ndim == 2 and positions.size(0) == 3, (
-                        "multimodal section rotary embedding requires "
-                        f"(3, seq_len) positions, but got {positions.size()}")
+                # if uses_mrope(self.config):
+                #     assert positions.ndim == 2 and positions.size(0) == 3, (
+                #         "multimodal section rotary embedding requires "
+                #         f"(3, seq_len) positions, but got {positions.size()}")
                 inputs_embeds = self.get_input_embeddings_v0(
                     input_ids,
                     image_input=image_input,
diff --git a/vllm/model_executor/models/qwen2_vl.py b/vllm/model_executor/models/qwen2_vl.py
index 0ff0836b0..542b10370 100644
--- a/vllm/model_executor/models/qwen2_vl.py
+++ b/vllm/model_executor/models/qwen2_vl.py
@@ -271,8 +271,9 @@ class Qwen2VisionAttention(nn.Module):
 
         # Detect attention implementation.
         self.attn_backend: _Backend = get_vit_attn_backend(support_fa=True)
+        self.attn_backend = _Backend.TORCH_SDPA
         if self.attn_backend not in {
-                _Backend.FLASH_ATTN, _Backend.TORCH_SDPA, _Backend.XFORMERS
+                _Backend.FLASH_ATTN, _Backend.TORCH_SDPA, _Backend.XFORMERS, _Backend.IPEX_V1
         }:
             raise RuntimeError(
                 f"Qwen2-VL does not support {self.attn_backend} backend now.")
@@ -342,24 +343,69 @@ class Qwen2VisionAttention(nn.Module):
             context_layer = rearrange(output,
                                       "(b s) ... -> b s ...",
                                       b=batch_size)
+        elif self.attn_backend == _Backend.IPEX_V1:
+            from vllm._ipex_ops import ipex_ops
+
+            q, k, v = (rearrange(x, "b s ... -> (b s) ...") for x in [q, k, v])
+
+            output = torch.empty(
+                q.shape,
+                dtype=q.dtype,
+                device=q.device)
+            ipex_ops.varlen_attention(
+                    q,
+                    k,
+                    v,
+                    output,
+                    cu_seqlens,
+                    cu_seqlens,
+                    None,
+                    max_seqlen,
+                    max_seqlen,
+                    pdropout=0.0,
+                    softmax_scale=1.0/(q.shape[-1] ** 0.5),
+                    zero_tensors=False,
+                    is_causal=True,
+                    return_softmax=False,
+                    gen_=None,
+                    window_size_left=-1,
+                    window_size_right=-1,
+                    logits_soft_cap=-1,
+            )
+            context_layer = rearrange(output,
+                            "(b s) ... -> b s ...",
+                            b=batch_size)
         elif self.attn_backend == _Backend.TORCH_SDPA:
             # Execute attention entry by entry for speed & less VRAM.
-            outputs = []
-            for i in range(1, len(cu_seqlens)):
-                start_idx = cu_seqlens[i - 1]
-                end_idx = cu_seqlens[i]
-                q_i = q[:, start_idx:end_idx]
-                k_i = k[:, start_idx:end_idx]
-                v_i = v[:, start_idx:end_idx]
-                q_i, k_i, v_i = (rearrange(x, "b s h d -> b h s d")
-                                 for x in [q_i, k_i, v_i])
-                output_i = F.scaled_dot_product_attention(q_i,
-                                                          k_i,
-                                                          v_i,
-                                                          dropout_p=0.0)
-                output_i = rearrange(output_i, "b h s d -> b s h d ")
-                outputs.append(output_i)
-            context_layer = torch.cat(outputs, dim=1)
+            q, k, v = (rearrange(x, "b s ... -> (b s) ...") for x in [q, k, v])
+            from vllm._ipex_ops import ipex_ops
+            output = torch.empty(
+                        (q.shape[0], q.shape[1], q.shape[2]),
+                        dtype=q.dtype,
+                        device=q.device)
+            import math
+            head_dim = q.shape[-1]
+            scale = 1 / math.sqrt(head_dim)
+            ipex_ops.varlen_attention(q, k, v, output,
+                                    cu_seqlens,
+                                    cu_seqlens,
+                                    None,
+                                    max_seqlen,
+                                    max_seqlen,
+                                    pdropout=0,
+                                    softmax_scale=scale,
+                                    zero_tensors=False,
+                                    is_causal=False,
+                                    return_softmax=False,
+                                    window_size_left=-1,
+                                    window_size_right=-1,
+                                    gen_=None,
+                                    logits_soft_cap=0
+                                    )
+
+            context_layer = rearrange(output,
+                                    "(b s) ... -> b s ...",
+                                    b=batch_size)
         elif self.attn_backend == _Backend.XFORMERS:
             from xformers import ops as xops
             from xformers.ops.fmha.attn_bias import BlockDiagonalMask
@@ -622,6 +668,8 @@ class Qwen2VisionTransformer(nn.Module):
             max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()
         elif self.attn_backend == _Backend.XFORMERS:
             seqlens = (cu_seqlens[1:] - cu_seqlens[:-1]).tolist()
+        elif self.attn_backend == _Backend.IPEX_V1:
+            max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()
         return max_seqlen, seqlens
 
     def forward(
diff --git a/vllm/model_executor/models/qwen3.py b/vllm/model_executor/models/qwen3.py
index dbe2be8a7..5d5c2025d 100644
--- a/vllm/model_executor/models/qwen3.py
+++ b/vllm/model_executor/models/qwen3.py
@@ -134,11 +134,11 @@ class Qwen3Attention(nn.Module):
         # Add qk-norm
         q_by_head = q.view(*q.shape[:-1], q.shape[-1] // self.head_dim,
                            self.head_dim)
-        q_by_head = self.q_norm(q_by_head)
+        q_by_head = self.q_norm.forward(q_by_head.contiguous())
         q = q_by_head.view(q.shape)
         k_by_head = k.view(*k.shape[:-1], k.shape[-1] // self.head_dim,
                            self.head_dim)
-        k_by_head = self.k_norm(k_by_head)
+        k_by_head = self.k_norm.forward(k_by_head.contiguous())
         k = k_by_head.view(k.shape)
         q, k = self.rotary_emb(positions, q, k)
         attn_output = self.attn(q, k, v)
diff --git a/vllm/model_executor/models/qwen3_moe.py b/vllm/model_executor/models/qwen3_moe.py
index 8a4c2850d..e433df0f0 100644
--- a/vllm/model_executor/models/qwen3_moe.py
+++ b/vllm/model_executor/models/qwen3_moe.py
@@ -224,12 +224,12 @@ class Qwen3MoeAttention(nn.Module):
         # Add qk-norm
         q_by_head = q.view(*q.shape[:-1], q.shape[-1] // self.head_dim,
                            self.head_dim)
-        q_by_head = self.q_norm(q_by_head)
+        q_by_head = self.q_norm.forward(q_by_head.contiguous())
         q = q_by_head.view(q.shape)
 
         k_by_head = k.view(*k.shape[:-1], k.shape[-1] // self.head_dim,
                            self.head_dim)
-        k_by_head = self.k_norm(k_by_head)
+        k_by_head = self.k_norm.forward(k_by_head.contiguous())
         k = k_by_head.view(k.shape)
         q, k = self.rotary_emb(positions, q, k)
         attn_output = self.attn(q, k, v)
diff --git a/vllm/model_executor/models/registry.py b/vllm/model_executor/models/registry.py
index 97ea12de6..c860cde8c 100644
--- a/vllm/model_executor/models/registry.py
+++ b/vllm/model_executor/models/registry.py
@@ -58,6 +58,7 @@ _TEXT_GENERATION_MODELS = {
     "Gemma3ForCausalLM": ("gemma3", "Gemma3ForCausalLM"),
     "GlmForCausalLM": ("glm", "GlmForCausalLM"),
     "Glm4ForCausalLM": ("glm4", "Glm4ForCausalLM"),
+    "Glm4MoeForCausalLM": ("glm4_moe", "Glm4MoeForCausalLM"),
     "GPT2LMHeadModel": ("gpt2", "GPT2LMHeadModel"),
     "GPTBigCodeForCausalLM": ("gpt_bigcode", "GPTBigCodeForCausalLM"),
     "GPTJForCausalLM": ("gpt_j", "GPTJForCausalLM"),
@@ -113,6 +114,7 @@ _TEXT_GENERATION_MODELS = {
     "Starcoder2ForCausalLM": ("starcoder2", "Starcoder2ForCausalLM"),
     "SolarForCausalLM": ("solar", "SolarForCausalLM"),
     "TeleChat2ForCausalLM": ("telechat2", "TeleChat2ForCausalLM"),
+    "TeleChatForCausalLM": ("telechat2", "TeleChat2ForCausalLM"),
     "TeleFLMForCausalLM": ("teleflm", "TeleFLMForCausalLM"),
     "XverseForCausalLM": ("llama", "LlamaForCausalLM"),
     "Zamba2ForCausalLM": ("zamba2", "Zamba2ForCausalLM"),
@@ -182,6 +184,7 @@ _MULTIMODAL_MODELS = {
     "FuyuForCausalLM": ("fuyu", "FuyuForCausalLM"),
     "Gemma3ForConditionalGeneration": ("gemma3_mm", "Gemma3ForConditionalGeneration"),  # noqa: E501
     "GLM4VForCausalLM": ("glm4v", "GLM4VForCausalLM"),
+    "Glm4vMoeForConditionalGeneration": ("glm4_1v", "Glm4vForConditionalGeneration"),  # noqa: E501
     "GraniteSpeechForConditionalGeneration": ("granite_speech", "GraniteSpeechForConditionalGeneration"),  # noqa: E501
     "H2OVLChatModel": ("h2ovl", "H2OVLChatModel"),
     "InternVLChatModel": ("internvl", "InternVLChatModel"),
@@ -226,6 +229,7 @@ _SPECULATIVE_DECODING_MODELS = {
     "Eagle3LlamaForCausalLM": ("llama_eagle3", "Eagle3LlamaForCausalLM"),
     "DeepSeekMTPModel": ("deepseek_mtp", "DeepSeekMTP"),
     "MedusaModel": ("medusa", "Medusa"),
+    "Glm4MoeMTPModel": ("glm4_moe_mtp", "Glm4MoeMTP"),
     "MLPSpeculatorPreTrainedModel": ("mlp_speculator", "MLPSpeculator"),
 }
 
diff --git a/vllm/model_executor/models/siglip.py b/vllm/model_executor/models/siglip.py
index 3b5334afa..9f684e988 100644
--- a/vllm/model_executor/models/siglip.py
+++ b/vllm/model_executor/models/siglip.py
@@ -11,6 +11,7 @@ from torch import nn
 from transformers import SiglipVisionConfig
 
 from vllm.attention.layer import MultiHeadAttention
+from vllm.attention.layer import SelfMultiHeadAttention
 from vllm.distributed import divide, get_tensor_model_parallel_world_size
 from vllm.model_executor.layers.activation import get_act_fn
 from vllm.model_executor.layers.linear import (ColumnParallelLinear,
@@ -177,7 +178,9 @@ class SiglipAttention(nn.Module):
         self.tp_size = get_tensor_model_parallel_world_size()
         self.num_heads_per_partition = divide(self.num_heads, self.tp_size)
 
-        self.attn = MultiHeadAttention(self.num_heads_per_partition,
+        # self.attn = MultiHeadAttention(self.num_heads_per_partition,
+        #                                self.head_dim, self.scale)
+        self.attn = SelfMultiHeadAttention(self.num_heads_per_partition,
                                        self.head_dim, self.scale)
 
     def forward(
diff --git a/vllm/model_executor/models/vision.py b/vllm/model_executor/models/vision.py
index 901d83ec5..3a8f7c579 100644
--- a/vllm/model_executor/models/vision.py
+++ b/vllm/model_executor/models/vision.py
@@ -96,6 +96,8 @@ def get_vit_attn_backend(support_fa: bool = False) -> _Backend:
             else:
                 # For Volta and Turing GPUs, use xformers instead.
                 selected_backend = _Backend.XFORMERS
+        elif current_platform.is_xpu:
+            selected_backend = _Backend.IPEX_V1
         else:
             # Default to torch SDPA for other non-GPU platforms.
             selected_backend = _Backend.TORCH_SDPA
diff --git a/vllm/multimodal/inputs.py b/vllm/multimodal/inputs.py
index 600a34d39..0e3eecb50 100644
--- a/vllm/multimodal/inputs.py
+++ b/vllm/multimodal/inputs.py
@@ -56,10 +56,12 @@ which are treated as image embeddings;
 these are directly passed to the model without HF processing.
 """
 
-VideoItem: TypeAlias = Union[HfVideoItem, "torch.Tensor"]
+VideoItem: TypeAlias = Union[HfVideoItem, "torch.Tensor",
+                             tuple[HfVideoItem, dict[str, Any]]]
 """
-A `transformers.image_utils.VideoInput` representing a single video
-item, which can be passed to a HuggingFace `VideoProcessor`.
+A `transformers.video_utils.VideoInput` representing a single video item. 
+This can be passed to a HuggingFace `VideoProcessor` 
+with `transformers.video_utils.VideoMetadata`.
 
 Alternatively, a 3-D tensor or batch of 2-D tensors,
 which are treated as video embeddings;
diff --git a/vllm/multimodal/parse.py b/vllm/multimodal/parse.py
index 63af84274..779041c58 100644
--- a/vllm/multimodal/parse.py
+++ b/vllm/multimodal/parse.py
@@ -223,8 +223,14 @@ class ImageEmbeddingItems(EmbeddingItems):
 
 class VideoProcessorItems(ProcessorBatchItems[HfVideoItem]):
 
-    def __init__(self, data: Sequence[HfVideoItem]) -> None:
+    def __init__(
+        self,
+        data: Sequence[HfVideoItem],
+        metadata: Optional[Union[dict[str, Any],
+                                 list[Optional[dict[str, Any]]]]] = None,
+    ) -> None:
         super().__init__(data, "video")
+        self.metadata = metadata
 
     def get_num_frames(self, item_idx: int) -> int:
         return len(self.get(item_idx))
@@ -319,6 +325,7 @@ class MultiModalDataParser:
         *,
         target_sr: Optional[float] = None,
         audio_resample_method: Literal["librosa", "scipy"] = "librosa",
+        video_needs_metadata: bool = False,
     ) -> None:
         super().__init__()
 
@@ -326,6 +333,7 @@ class MultiModalDataParser:
             target_sr=target_sr,
             method=audio_resample_method,
         )
+        self.video_needs_metadata = video_needs_metadata
 
     def _is_embeddings(
             self, data: object
@@ -360,6 +368,21 @@ class MultiModalDataParser:
 
         assert_never(audio)
 
+    def _get_video_with_metadata(
+        self,
+        video: VideoItem,
+    ) -> tuple[np.ndarray, Optional[dict[str, Any]]]:
+        if isinstance(video, tuple):
+            return video
+        if isinstance(video, list):
+            return np.array(video), None
+        if isinstance(video, np.ndarray):
+            return video, None
+        if isinstance(video, torch.Tensor):
+            return video.numpy(), None
+
+        assert_never(video)
+
     def _parse_audio_data(
         self,
         data: ModalityData[AudioItem],
@@ -432,10 +455,25 @@ class MultiModalDataParser:
             data_items = [data]
         elif isinstance(data, (np.ndarray, torch.Tensor)):
             data_items = [elem for elem in data]
+        elif isinstance(data, tuple) and len(data) == 2:
+            data_items = [data]
         else:
             data_items = data
 
-        return VideoProcessorItems(data_items)
+        new_videos = list[tuple[np.ndarray, Optional[dict[str, Any]]]]()
+        metadata_lst: list[Optional[dict[str, Any]]] = []
+        for data_item in data_items:
+            video, metadata = self._get_video_with_metadata(data_item)
+            if self.video_needs_metadata:
+                new_videos.append((video, metadata))
+                metadata_lst.append(metadata)
+            else:
+                new_videos.append(video)
+
+        if not self.video_needs_metadata:
+            metadata = None
+
+        return VideoProcessorItems(new_videos, metadata=metadata_lst)
 
     def _get_subparsers(self) -> Mapping[str, ModalityDataParser]:
         return {
diff --git a/vllm/multimodal/video.py b/vllm/multimodal/video.py
index 261d56aba..c8aad92b7 100644
--- a/vllm/multimodal/video.py
+++ b/vllm/multimodal/video.py
@@ -23,6 +23,7 @@ def resize_video(frames: npt.NDArray, size: tuple[int, int]) -> npt.NDArray:
                               dtype=frames.dtype)
     # lazy import cv2 to avoid bothering users who only use text models
     import cv2
+
     for i, frame in enumerate(frames):
         resized_frame = cv2.resize(frame, (new_width, new_height))
         resized_frames[i] = resized_frame
@@ -91,14 +92,16 @@ class OpenCVVideoBackend(VideoLoader):
                 continue
             if not vr.isBackendBuiltIn(backend):
                 _, abi, api = vr.getStreamBufferedBackendPluginVersion(backend)
-                if (abi < 1 or (abi == 1 and api < 2)):
+                if abi < 1 or (abi == 1 and api < 2):
                     continue
             api_pref = backend
             break
         return api_pref
 
     @classmethod
-    def load_bytes(cls, data: bytes, num_frames: int = -1) -> npt.NDArray:
+    def load_bytes(cls,
+                   data: bytes,
+                   num_frames: int = -1) -> tuple[npt.NDArray, dict]:
         import cv2
 
         backend = cls().get_cv2_video_api()
@@ -107,6 +110,9 @@ class OpenCVVideoBackend(VideoLoader):
             raise ValueError("Could not open video stream")
 
         total_frames_num = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
+        original_fps = cap.get(cv2.CAP_PROP_FPS)
+        duration = total_frames_num / original_fps if original_fps > 0 else 0
+
         full_read = num_frames == -1 or total_frames_num < num_frames
         if full_read:
             num_frames = total_frames_num
@@ -124,18 +130,27 @@ class OpenCVVideoBackend(VideoLoader):
 
         i = 0
         for idx in range(total_frames_num):
-            ok = cap.grab()  # next img
+            ok = cap.grab()
             if not ok:
                 break
-            if idx in frame_idx:  # only decompress needed
+            if idx in frame_idx:
                 ret, frame = cap.retrieve()
                 if ret:
                     frames[i] = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                     i += 1
-        # we expect all frames loaded
+
         assert i == num_frames, (f"Expected reading {num_frames} frames, "
                                  f"but only loaded {i} frames from video.")
-        return frames
+
+        # Use transformers transformers.video_utils.VideoMetadata format
+        metadata = {
+            "total_num_frames": total_frames_num,
+            "fps": original_fps,
+            "duration": duration,
+            "video_backend": "opencv"
+        }
+
+        return frames, metadata
 
 
 class VideoMediaIO(MediaIO[npt.NDArray]):
diff --git a/vllm/platforms/__init__.py b/vllm/platforms/__init__.py
index 00d00d05f..801115bbc 100644
--- a/vllm/platforms/__init__.py
+++ b/vllm/platforms/__init__.py
@@ -6,7 +6,7 @@ from itertools import chain
 from typing import TYPE_CHECKING, Optional
 
 from vllm.plugins import load_plugins_by_group
-from vllm.utils import resolve_obj_by_qualname
+from vllm.utils import resolve_obj_by_qualname, supports_xccl
 
 from .interface import _Backend  # noqa: F401
 from .interface import CpuArchEnum, Platform, PlatformEnum
@@ -138,10 +138,21 @@ def xpu_platform_plugin() -> Optional[str]:
     try:
         # installed IPEX if the machine has XPUs.
         import intel_extension_for_pytorch  # noqa: F401
-        import oneccl_bindings_for_pytorch  # noqa: F401
         import torch
+        import os
+        default_backend = "xccl" if supports_xccl() else "ccl"
+        XPU_CCL_BACKEND = os.getenv("XPU_CCL_BACKEND", default_backend)
+
+        if XPU_CCL_BACKEND not in ["xccl", "ccl"]:
+            raise ValueError(f"Unknown {XPU_CCL_BACKEND} backend for XPU platform")
+
+        if XPU_CCL_BACKEND == "ccl":
+            logger.debug("xccl is not available in current torch, checking ccl")
+            import oneccl_bindings_for_pytorch  # noqa: F401
+
         if hasattr(torch, 'xpu') and torch.xpu.is_available():
             is_xpu = True
+            logger.debug(f"Confirmed {XPU_CCL_BACKEND} backend is available.")
             logger.debug("Confirmed XPU platform is available.")
     except Exception as e:
         logger.debug("XPU platform is not available because: %s", str(e))
diff --git a/vllm/platforms/interface.py b/vllm/platforms/interface.py
index 504c3b42a..7e85eed3b 100644
--- a/vllm/platforms/interface.py
+++ b/vllm/platforms/interface.py
@@ -50,6 +50,7 @@ class _Backend(enum.Enum):
     PALLAS = enum.auto()
     PALLAS_VLLM_V1 = enum.auto()
     IPEX = enum.auto()
+    IPEX_V1 = enum.auto()
     BLOCK_SPARSE_FLASH_ATTN = enum.auto()
     DUAL_CHUNK_FLASH_ATTN = enum.auto()
     NO_ATTENTION = enum.auto()
diff --git a/vllm/platforms/xpu.py b/vllm/platforms/xpu.py
index b2a6ad5d7..3542ee542 100644
--- a/vllm/platforms/xpu.py
+++ b/vllm/platforms/xpu.py
@@ -2,20 +2,40 @@
 
 from typing import TYPE_CHECKING, Optional
 
+import os
 import torch
 
+import vllm.envs as envs
 from vllm.logger import init_logger
 from vllm.utils import DEFAULT_MAX_NUM_BATCHED_TOKENS
 
 from .interface import DeviceCapability, Platform, PlatformEnum, _Backend
 
 if TYPE_CHECKING:
-    from vllm.config import VllmConfig
+    from vllm.config import ModelConfig, VllmConfig
 else:
+    ModelConfig = None
     VllmConfig = None
 
 logger = init_logger(__name__)
 
+def device_id_to_physical_device_id(device_id: int) -> int:
+    if "ZE_AFFINITY_MASK" in os.environ:
+        device_ids = os.environ["ZE_AFFINITY_MASK"].split(",")
+        if device_ids == [""]:
+            msg = (
+                "ZE_AFFINITY_MASK is set to empty string, which means"
+                " GPU support is disabled. If you are using ray, please unset"
+                " the environment variable `ZE_AFFINITY_MASK` inside the"
+                " worker/actor. "
+                "Check https://github.com/vllm-project/vllm/issues/8402 for"
+                " more information.")
+            raise RuntimeError(msg)
+        physical_device_id = device_ids[device_id]
+        return int(physical_device_id)
+    else:
+        return device_id
+
 
 class XPUPlatform(Platform):
     _enum = PlatformEnum.XPU
@@ -25,17 +45,29 @@ class XPUPlatform(Platform):
     # Intel XPU's device key is "GPU" for Ray.
     # see https://github.com/ray-project/ray/blob/6a5eb5865eeb9ccf058a79b44f107e327e360673/python/ray/_private/accelerators/intel_gpu.py#L20 # noqa: E501
     ray_device_key: str = "GPU"
-    device_control_env_var: str = "ONEAPI_DEVICE_SELECTOR"
+    device_control_env_var: str = "ZE_AFFINITY_MASK"
 
     @classmethod
     def get_attn_backend_cls(cls, selected_backend: _Backend, head_size: int,
                              dtype: torch.dtype, kv_cache_dtype: Optional[str],
                              block_size: int, use_v1: bool,
                              use_mla: bool) -> str:
-        if selected_backend != _Backend.IPEX:
-            logger.info("Cannot use %s backend on XPU.", selected_backend)
-        logger.info("Using IPEX attention backend.")
-        return "vllm.attention.backends.ipex_attn.IpexAttnBackend"
+        if selected_backend not in [_Backend.IPEX, _Backend.IPEX_V1]:
+            logger.warning_once(
+                f"Cannot use {selected_backend} backend on XPU.")
+        use_v1 = envs.VLLM_USE_V1
+        if use_v1:
+            if selected_backend == _Backend.IPEX:
+                logger.warning_once("For v1 on XPU, should use "
+                                    "IPEX_V1 attention backend.")
+            logger.info_once("Using IPEX_V1 attention backend.")
+            return "vllm.v1.attention.backends.ipex_attn.IPEXAttentionBackend"
+        else:
+            if selected_backend == _Backend.IPEX:
+                logger.warning_once("For v0 on XPU, should use "
+                                    "IPEX attention backend.")
+            logger.info_once("Using IPEX attention backend.")
+            return "vllm.attention.backends.ipex_attn.IpexAttnBackend"
 
     @classmethod
     def get_device_capability(
@@ -50,6 +82,10 @@ class XPUPlatform(Platform):
     def get_device_name(cls, device_id: int = 0) -> str:
         return torch.xpu.get_device_name(device_id)
 
+    @classmethod
+    def get_punica_wrapper(cls) -> str:
+        return "vllm.lora.punica_wrapper.punica_gpu.PunicaWrapperGPU"
+
     @classmethod
     def get_device_total_memory(cls, device_id: int = 0) -> int:
         device_props = torch.xpu.get_device_properties(device_id)
@@ -59,6 +95,10 @@ class XPUPlatform(Platform):
     def is_async_output_supported(cls, enforce_eager: Optional[bool]) -> bool:
         return True
 
+    @classmethod
+    def get_piecewise_backend_cls(cls) -> str:
+        return "vllm.compilation.cuda_piecewise_backend.CUDAPiecewiseBackend"  # noqa
+
     @classmethod
     def inference_mode(cls):
         return torch.no_grad()
@@ -67,49 +107,51 @@ class XPUPlatform(Platform):
     def check_and_update_config(cls, vllm_config: VllmConfig) -> None:
         cache_config = vllm_config.cache_config
         if cache_config and cache_config.block_size is None:
-            cache_config.block_size = 16
+            if envs.VLLM_USE_V1:
+                cache_config.block_size = 64
+            else:
+                cache_config.block_size = 16
 
         # check and update model config
         model_config = vllm_config.model_config
-        if model_config.dtype == torch.bfloat16:
-            bf16_supported = cls.device_support_bf16()
-            if not bf16_supported:
-                logger.warning(
-                    "bfloat16 is only supported on Intel Data Center GPU, "
-                    "Intel Arc GPU is not supported yet. Your device is %s,"
-                    " which is not supported. will fallback to float16",
-                    cls.get_device_name())
-                model_config.dtype = torch.float16
         if not model_config.enforce_eager:
             logger.warning(
                 "CUDA graph is not supported on XPU, fallback to the eager "
                 "mode.")
             model_config.enforce_eager = True
 
-        if vllm_config.speculative_config is not None:
-            raise NotImplementedError(
-                "XPU does not support speculative decoding")
-
         if vllm_config.device_config is not None:
             assert vllm_config.device_config.device_type == "xpu"
 
         # check and update parallel config
         parallel_config = vllm_config.parallel_config
-        if parallel_config.worker_cls == "auto":
-            parallel_config.worker_cls = "vllm.worker.xpu_worker.XPUWorker"
+        if vllm_config.speculative_config:
+            if envs.VLLM_USE_V1:
+                parallel_config.worker_cls = \
+                    "vllm.v1.worker.xpu_worker.XPUWorker"
+            else:
+                raise NotImplementedError(
+                    "XPU v0 does not support speculative decoding")
+        else:
+            if envs.VLLM_USE_V1:
+                parallel_config.worker_cls =\
+                    "vllm.v1.worker.xpu_worker.XPUWorker"
+            else:
+                parallel_config.worker_cls = "vllm.worker.xpu_worker.XPUWorker"
 
         if parallel_config.distributed_executor_backend is None:
-            parallel_config.distributed_executor_backend = "ray"
+            if parallel_config.world_size > 1:
+                parallel_config.distributed_executor_backend = "ray"
+            else:
+                parallel_config.distributed_executor_backend = "uni"
         elif parallel_config.distributed_executor_backend == "mp":
             # FIXME(kunshang):
             # spawn needs calling `if __name__ == '__main__':``
             # fork is not supported for xpu start new process.
-            logger.error(
-                "Both start methods (spawn and fork) have issue "
-                "on XPU if you use mp backend, setting it to ray instead.")
-            parallel_config.distributed_executor_backend = "ray"
-
-        elif parallel_config.distributed_executor_backend != "ray":
+            logger.warning(
+                "Please use spawn as start method if you want to use mp.")
+        elif parallel_config.distributed_executor_backend != "ray" and \
+                parallel_config.distributed_executor_backend != "uni":
             logger.warning(
                 "%s is not supported on XPU, fallback to ray distributed"
                 " executor backend.",
@@ -128,8 +170,7 @@ class XPUPlatform(Platform):
 
     @classmethod
     def is_pin_memory_available(cls):
-        logger.warning("Pin memory is not supported on XPU.")
-        return False
+        return True
 
     @classmethod
     def get_current_memory_usage(cls,
@@ -139,17 +180,25 @@ class XPUPlatform(Platform):
         return torch.xpu.max_memory_allocated(device)
 
     @classmethod
-    def device_support_bf16(cls) -> bool:
-        device_name = cls.get_device_name().lower()
-        if device_name.count("arc") > 0:
-            return False
-        elif device_name.count("data center gpu") > 0:
-            return True
+    def fp8_dtype(cls) -> torch.dtype:
+        if envs.VLLM_XPU_FP8_DTYPE == "e4m3":
+            return torch.float8_e4m3fn
         else:
-            logger.warning("Unknown device name %s, always use float16",
-                           device_name)
-            return False
+            return torch.float8_e5m2
+
+    @classmethod
+    def is_data_center_gpu(cls) -> bool:
+        device_name = cls.get_device_name().lower()
+        return device_name.count("data center gpu") > 0
 
     @classmethod
     def get_device_communicator_cls(cls) -> str:
         return "vllm.distributed.device_communicators.xpu_communicator.XpuCommunicator"  # noqa
+
+    @classmethod
+    def supports_v1(cls, model_config: ModelConfig) -> bool:
+        return True
+
+    @classmethod
+    def device_count(cls) -> int:
+        return torch.xpu.device_count()
diff --git a/vllm/plugins/__init__.py b/vllm/plugins/__init__.py
index 2884cb46f..238597862 100644
--- a/vllm/plugins/__init__.py
+++ b/vllm/plugins/__init__.py
@@ -64,10 +64,7 @@ def load_general_plugins():
     # some platform-specific configurations
     from vllm.platforms import current_platform
 
-    if current_platform.is_xpu():
-        # see https://github.com/pytorch/pytorch/blob/43c5f59/torch/_dynamo/config.py#L158
-        torch._dynamo.config.disable = True
-    elif current_platform.is_hpu():
+    if current_platform.is_hpu():
         # NOTE(kzawora): PT HPU lazy backend (PT_HPU_LAZY_MODE = 1)
         # does not support torch.compile
         # Eager backend (PT_HPU_LAZY_MODE = 0) must be selected for
diff --git a/vllm/reasoning/__init__.py b/vllm/reasoning/__init__.py
index 65606ce55..5a9777297 100644
--- a/vllm/reasoning/__init__.py
+++ b/vllm/reasoning/__init__.py
@@ -2,6 +2,7 @@
 
 from .abs_reasoning_parsers import ReasoningParser, ReasoningParserManager
 from .deepseek_r1_reasoning_parser import DeepSeekR1ReasoningParser
+from .glm4_moe_reasoning_parser import Glm4MoeModelReasoningParser
 from .granite_reasoning_parser import GraniteReasoningParser
 from .qwen3_reasoning_parser import Qwen3ReasoningParser
 
@@ -11,4 +12,5 @@ __all__ = [
     "DeepSeekR1ReasoningParser",
     "GraniteReasoningParser",
     "Qwen3ReasoningParser",
+    "Glm4MoeModelReasoningParser",
 ]
diff --git a/vllm/reasoning/glm4_moe_reasoning_parser.py b/vllm/reasoning/glm4_moe_reasoning_parser.py
new file mode 100644
index 000000000..03ab6e01c
--- /dev/null
+++ b/vllm/reasoning/glm4_moe_reasoning_parser.py
@@ -0,0 +1,148 @@
+# SPDX-License-Identifier: Apache-2.0
+# SPDX-FileCopyrightText: Copyright contributors to the vLLM project
+
+from collections.abc import Sequence
+from typing import Optional, Union
+
+from transformers import PreTrainedTokenizerBase
+
+from vllm.entrypoints.openai.protocol import (ChatCompletionRequest,
+                                              DeltaMessage)
+from vllm.logger import init_logger
+from vllm.reasoning import ReasoningParser, ReasoningParserManager
+
+logger = init_logger(__name__)
+
+
+@ReasoningParserManager.register_module("glm45")
+class Glm4MoeModelReasoningParser(ReasoningParser):
+    """
+    Reasoning parser for the Glm4MoeModel model.
+    The Glm4MoeModel model uses <think>...</think> tokens to denote reasoning
+    text within its output. The model provides a strict switch to disable
+    reasoning output via the 'enable_thinking=False' parameter. This parser
+    extracts the reasoning content enclosed by <think> and </think> tokens
+    from the model's output.
+    """
+
+    def __init__(self, tokenizer: PreTrainedTokenizerBase):
+        super().__init__(tokenizer)
+        self.think_start_token = "<think>"
+        self.think_end_token = "</think>"
+
+        if not self.model_tokenizer:
+            raise ValueError(
+                "The model tokenizer must be passed to the ReasoningParser "
+                "constructor during construction.")
+
+        self.think_start_token_id = self.vocab.get(self.think_start_token)
+        self.think_end_token_id = self.vocab.get(self.think_end_token)
+        if (self.think_start_token_id is None
+                or self.think_end_token_id is None):
+            raise RuntimeError(
+                "Glm4MoeModel reasoning parser could not locate "
+                "think start/end tokens in the tokenizer!")
+
+    def is_reasoning_end(self, input_ids: list[int]) -> bool:
+        return self.think_end_token_id in input_ids
+
+    def extract_content_ids(self, input_ids: list[int]) -> list[int]:
+        """
+        Extract the content after the end tokens
+        """
+        if self.think_end_token_id not in input_ids[:-1]:
+            return []
+        else:
+            return input_ids[input_ids.index(self.think_end_token_id) + 1:]
+
+    def extract_reasoning_content_streaming(
+        self,
+        previous_text: str,
+        current_text: str,
+        delta_text: str,
+        previous_token_ids: Sequence[int],
+        current_token_ids: Sequence[int],
+        delta_token_ids: Sequence[int],
+    ) -> Union[DeltaMessage, None]:
+        """
+        Extract reasoning content from a delta message.
+        Handles streaming output where previous + delta = current.
+        Uses token IDs for faster processing.
+        For text <think>abc</think>xyz:
+        - 'abc' goes to reasoning_content
+        - 'xyz' goes to content
+        """
+        # Skip single special tokens
+        if len(delta_token_ids) == 1 and (delta_token_ids[0] in [
+                self.think_start_token_id, self.think_end_token_id
+        ]):
+            return None
+
+        if self.think_start_token_id in previous_token_ids:
+            if self.think_end_token_id in delta_token_ids:
+                # <think> in previous, </think> in delta,
+                # extract reasoning content
+                end_index = delta_text.find(self.think_end_token)
+                reasoning_content = delta_text[:end_index]
+                content = delta_text[end_index + len(self.think_end_token):]
+                return DeltaMessage(reasoning_content=reasoning_content,
+                                    content=content if content else None)
+            elif self.think_end_token_id in previous_token_ids:
+                # <think> in previous, </think> in previous,
+                # reasoning content continues
+                return DeltaMessage(content=delta_text)
+            else:
+                # <think> in previous, no </think> in previous or delta,
+                # reasoning content continues
+                return DeltaMessage(reasoning_content=delta_text)
+        elif self.think_start_token_id in delta_token_ids:
+            if self.think_end_token_id in delta_token_ids:
+                # <think> in delta, </think> in delta, extract reasoning content
+                start_index = delta_text.find(self.think_start_token)
+                end_index = delta_text.find(self.think_end_token)
+                reasoning_content = delta_text[start_index +
+                                               len(self.think_start_token
+                                                   ):end_index]
+                content = delta_text[end_index + len(self.think_end_token):]
+                return DeltaMessage(reasoning_content=reasoning_content,
+                                    content=content if content else None)
+            else:
+                # <think> in delta, no </think> in delta,
+                # reasoning content continues
+                return DeltaMessage(reasoning_content=delta_text)
+        else:
+            # thinking is disabled, just content
+            return DeltaMessage(content=delta_text)
+
+    def extract_reasoning_content(
+            self, model_output: str, request: ChatCompletionRequest
+    ) -> tuple[Optional[str], Optional[str]]:
+        """
+        Extract reasoning content from the model output.
+        For text <think>abc</think>xyz:
+        - 'abc' goes to reasoning_content
+        - 'xyz' goes to content
+        Returns:
+            tuple[Optional[str], Optional[str]]: reasoning content and content
+        """
+
+        # Check if the model output contains the <think> and </think> tokens.
+        if (self.think_start_token not in model_output
+                or self.think_end_token not in model_output):
+            return None, model_output
+        # Check if the <think> is present in the model output, remove it
+        # if it is present.
+        model_output_parts = model_output.partition(self.think_start_token)
+        model_output = model_output_parts[2] if model_output_parts[
+            1] else model_output_parts[0]
+        # Check if the model output contains the </think> tokens.
+        # If the end token is not found, return the model output as is.
+        if self.think_end_token not in model_output:
+            return None, model_output
+
+        # Extract reasoning content from the model output.
+        reasoning_content, _, content = model_output.partition(
+            self.think_end_token)
+
+        final_content = content or None
+        return reasoning_content, final_content
\ No newline at end of file
diff --git a/vllm/transformers_utils/config.py b/vllm/transformers_utils/config.py
index 69e7207cc..c930520d5 100644
--- a/vllm/transformers_utils/config.py
+++ b/vllm/transformers_utils/config.py
@@ -234,7 +234,8 @@ def _uses_mrope(config: PretrainedConfig) -> bool:
 
 def uses_mrope(config: PretrainedConfig) -> bool:
     """Detect if the model with this config uses M-ROPE."""
-    return _uses_mrope(config) or thinker_uses_mrope(config)
+    return _uses_mrope(config) or _uses_mrope(
+        config.get_text_config()) or thinker_uses_mrope(config)
 
 
 def thinker_uses_mrope(config: PretrainedConfig) -> bool:
diff --git a/vllm/utils.py b/vllm/utils.py
index 846df7743..7fa8be5ff 100644
--- a/vllm/utils.py
+++ b/vllm/utils.py
@@ -174,9 +174,9 @@ STR_DTYPE_TO_TORCH_DTYPE = {
     "half": torch.half,
     "bfloat16": torch.bfloat16,
     "float": torch.float,
-    "fp8": torch.uint8,
-    "fp8_e4m3": torch.uint8,
-    "fp8_e5m2": torch.uint8,
+    "fp8": torch.float8_e4m3fn,
+    "fp8_e4m3": torch.float8_e4m3fn,
+    "fp8_e5m2": torch.float8_e5m2,
     "int8": torch.int8,
 }
 
@@ -1765,6 +1765,12 @@ def supports_dynamo() -> bool:
     return base_torch_version >= Version("2.4.0")
 
 
+# Supports xccl with PyTorch versions >= 2.8.0 for XPU platform
+def supports_xccl() -> bool:
+    base_torch_version = Version(Version(torch.__version__).base_version)
+    return base_torch_version >= Version("2.8.0") and hasattr(torch, 'xpu') and torch.distributed.is_xccl_available()
+
+
 # Some backends use pytorch version < 2.4.0 which doesn't
 # support `torch.library.custom_op`.
 def supports_custom_op() -> bool:
@@ -2665,7 +2671,8 @@ def warn_for_unimplemented_methods(cls: type[T]) -> type[T]:
             src = inspect.getsource(attr_func)
             if "NotImplementedError" in src:
                 unimplemented_methods.append(attr_name)
-        if unimplemented_methods:
+        from vllm.platforms import current_platform
+        if unimplemented_methods and not current_platform.is_xpu():
             method_names = ','.join(unimplemented_methods)
             msg = (f"Methods {method_names} not implemented in {self}")
             logger.warning(msg)
diff --git a/vllm/v1/attention/backends/ipex_attn.py b/vllm/v1/attention/backends/ipex_attn.py
new file mode 100644
index 000000000..913784b8d
--- /dev/null
+++ b/vllm/v1/attention/backends/ipex_attn.py
@@ -0,0 +1,254 @@
+# SPDX-License-Identifier: Apache-2.0
+from dataclasses import dataclass
+from typing import TYPE_CHECKING, Any, Optional
+
+import torch
+
+from vllm._ipex_ops import ipex_ops
+from vllm.attention.backends.abstract import (AttentionBackend, AttentionImpl,
+                                              AttentionLayer,
+                                              AttentionMetadata, AttentionType)
+from vllm.attention.utils.fa_utils import get_flash_attn_version
+from vllm.v1.attention.backends.flash_attn import (
+    FlashAttentionMetadata, FlashAttentionMetadataBuilder)
+from vllm.v1.attention.backends.utils import CommonAttentionMetadata
+from vllm.v1.kv_cache_interface import AttentionSpec
+from vllm.v1.worker.block_table import BlockTable
+
+if TYPE_CHECKING:
+    from vllm.v1.core.sched.output import SchedulerOutput
+    from vllm.v1.worker.gpu_input_batch import InputBatch
+    from vllm.v1.worker.xpu_model_runner import XPUModelRunner
+
+
+@dataclass
+class IPEXAttentionMetadata(FlashAttentionMetadata):
+    seq_start_loc: torch.Tensor = None
+    def __init__(self,
+                 flash_attn_metadata: FlashAttentionMetadata,
+                 seq_start_loc: torch.Tensor = None,
+                 **kwargs) -> None:
+        super().__init__(**flash_attn_metadata.__dict__, **kwargs)
+        if seq_start_loc is not None:
+            self.seq_start_loc = seq_start_loc
+        else:
+            self.seq_start_loc = torch.tensor([0],
+                                              dtype=torch.int32,
+                                              device=self.block_table.device)
+
+
+class IPEXAttentionMetadataBuilder(FlashAttentionMetadataBuilder):
+
+    def __init__(self, runner: "XPUModelRunner", kv_cache_spec: AttentionSpec,
+                 block_table: BlockTable):
+        super().__init__(runner, kv_cache_spec, block_table)
+        # avoid “GPUModelerunner” has no attribute
+        self.runner: XPUModelRunner = runner
+        self.aot_schedule = (get_flash_attn_version() == 3)
+
+    def reorder_batch(self, input_batch: "InputBatch",
+                      scheduler_output: "SchedulerOutput") -> bool:
+        return False
+
+    def build(self, num_reqs: int, num_actual_tokens: int, max_query_len: int,
+              common_prefix_len: int,
+              common_attn_metadata: CommonAttentionMetadata):
+        attn_metadata = super().build(num_reqs, num_actual_tokens,
+                                      max_query_len, common_prefix_len,
+                                      common_attn_metadata)
+        seq_start_loc_cpu = self.runner.seq_start_loc_cpu[:num_reqs + 1]
+        seq_start_loc = seq_start_loc_cpu.to(self.runner.device,
+                                             non_blocking=True)
+        return IPEXAttentionMetadata(attn_metadata,
+                                     seq_start_loc=seq_start_loc)
+
+
+class IPEXAttentionBackend(AttentionBackend):
+
+    accept_output_buffer: bool = True
+
+    @staticmethod
+    def get_supported_head_sizes() -> list[int]:
+        return [32, 64, 80, 96, 128, 160, 192, 224, 256]
+
+    @staticmethod
+    def get_name() -> str:
+        return "IPEX_V1"
+
+    @staticmethod
+    def get_impl_cls() -> type["IPEXAttentionImpl"]:
+        return IPEXAttentionImpl
+
+    @staticmethod
+    def get_metadata_cls() -> type["AttentionMetadata"]:
+        return IPEXAttentionMetadata
+
+    @staticmethod
+    def get_kv_cache_shape(
+        num_blocks: int,
+        block_size: int,
+        num_kv_heads: int,
+        head_size: int,
+    ) -> tuple[int, ...]:
+        if block_size % 16 != 0:
+            raise ValueError("Block size must be a multiple of 16.")
+        return (2, num_blocks, block_size, num_kv_heads, head_size)
+
+    @staticmethod
+    def get_builder_cls() -> type["IPEXAttentionMetadataBuilder"]:
+        return IPEXAttentionMetadataBuilder
+
+    def use_cascade_attention(*args, **kwargs) -> bool:
+        # TODO: support cascade attention
+        return False
+
+
+class IPEXAttentionImpl(AttentionImpl):
+
+    def __init__(
+        self,
+        num_heads: int,
+        head_size: int,
+        scale: float,
+        num_kv_heads: int,
+        alibi_slopes: Optional[list[float]],
+        sliding_window: Optional[int],
+        kv_cache_dtype: str,
+        blocksparse_params: Optional[dict[str, Any]] = None,
+        logits_soft_cap: Optional[float] = None,
+        attn_type: str = AttentionType.DECODER,
+        use_irope: bool = False,
+    ) -> None:
+        if blocksparse_params is not None:
+            raise ValueError(
+                "FlashAttention does not support block-sparse attention.")
+        self.num_heads = num_heads
+        self.head_size = head_size
+        self.scale = float(scale)
+        self.num_kv_heads = num_kv_heads
+        if alibi_slopes is not None:
+            alibi_slopes = torch.tensor(alibi_slopes, dtype=torch.float32)
+        self.alibi_slopes = alibi_slopes
+        if sliding_window is None:
+            self.sliding_window = (-1, -1)
+        else:
+            self.sliding_window = (sliding_window - 1, 0)
+        self.kv_cache_dtype = kv_cache_dtype
+        self.use_irope = use_irope
+        if logits_soft_cap is None:
+            # In flash-attn, setting logits_soft_cap as 0 means no soft cap.
+            logits_soft_cap = 0
+        self.logits_soft_cap = logits_soft_cap
+
+        assert self.num_heads % self.num_kv_heads == 0
+        self.num_queries_per_kv = self.num_heads // self.num_kv_heads
+
+        support_head_sizes = IPEXAttentionBackend.get_supported_head_sizes()
+        if head_size not in support_head_sizes:
+            raise ValueError(
+                f"Head size {head_size} is not supported by FlashAttention. "
+                f"Supported head sizes are: {support_head_sizes}.")
+        if attn_type != AttentionType.DECODER:
+            raise NotImplementedError("Encoder self-attention and "
+                                      "encoder/decoder cross-attention "
+                                      "are not implemented for "
+                                      "IpexAttnBackendImpl")
+
+    def forward(
+        self,
+        layer: AttentionLayer,
+        query: torch.Tensor,
+        key: torch.Tensor,
+        value: torch.Tensor,
+        kv_cache: torch.Tensor,
+        attn_metadata: IPEXAttentionMetadata,
+        output: Optional[torch.Tensor] = None,
+    ) -> torch.Tensor:
+        """Forward pass with IPEXAttention.
+        Args:
+            query: shape = [num_tokens, num_heads * head_size]
+            key: shape = [num_tokens, num_kv_heads * head_size]
+            value: shape = [num_tokens, num_kv_heads * head_size]
+            kv_cache = [2, num_blocks, block_size, num_kv_heads, head_size]
+            attn_metadata: Metadata for attention.
+        Returns:
+            shape = [num_tokens, num_heads * head_size]
+        """
+        assert output is not None, "Output tensor must be provided."
+        if attn_metadata is None:
+            # Profiling run.
+            return output.random_(0, 10)
+
+        # NOTE(woosuk): IPEXAttention does not support FP8 KV cache.
+        assert layer._k_scale_float == 1.0 and layer._v_scale_float == 1.0, (
+            "key/v_scale is not supported in IPEXAttention.")
+
+        num_actual_tokens = attn_metadata.num_actual_tokens
+        num_heads = self.num_heads
+        head_size = self.head_size
+        num_kv_heads = self.num_kv_heads
+        query = query.view(-1, num_heads, head_size)
+        key = key.view(-1, num_kv_heads, head_size)
+        value = value.view(-1, num_kv_heads, head_size)
+
+        # Reshape the input keys and values and store them in the cache.
+        key_cache, value_cache = kv_cache.unbind(0)
+
+        ipex_ops.reshape_and_cache_flash(
+            key[:num_actual_tokens],
+            value[:num_actual_tokens],
+            key_cache,
+            value_cache,
+            attn_metadata.slot_mapping,
+            self.kv_cache_dtype,
+            layer._k_scale_float,
+            layer._v_scale_float,
+        )
+
+        use_local_attn = \
+            (self.use_irope and attn_metadata.local_attn_metadata is not None)
+
+        if use_local_attn:
+            assert attn_metadata.local_attn_metadata is not None
+            local_metadata = attn_metadata.local_attn_metadata
+            cu_seqlens_q = local_metadata.local_query_start_loc
+            sequesd_k = local_metadata.local_seqused_k
+            max_seqlen_q = local_metadata.local_max_query_len
+            max_seqlen_k = local_metadata.local_max_seq_len
+            block_table = local_metadata.local_block_table
+        else:
+            cu_seqlens_q = attn_metadata.query_start_loc
+            sequesd_k = attn_metadata.seq_lens
+            max_seqlen_q = attn_metadata.max_query_len
+            max_seqlen_k = attn_metadata.max_seq_len
+            block_table = attn_metadata.block_table
+        if not hasattr(attn_metadata, "seq_start_loc"):
+            cumsum = torch.cumsum(sequesd_k, dim=0)
+            seq_start_loc = torch.cat([torch.tensor([0], device=sequesd_k.device, dtype=torch.int32), cumsum]).to(torch.int32)
+        else:
+            seq_start_loc = attn_metadata.seq_start_loc
+
+        ipex_ops.chunked_prefill(
+            query[:num_actual_tokens],
+            key_cache,
+            value_cache,
+            output[:num_actual_tokens],
+            cu_seqlens_q,
+            seq_start_loc,
+            None,
+            block_table,
+            self.alibi_slopes,
+            max_seqlen_q,
+            max_seqlen_k,
+            0.0,
+            self.scale,
+            False,
+            self.sliding_window[0],
+            self.sliding_window[1],
+            True,
+            False,
+            None,
+            self.kv_cache_dtype,
+        )
+        return output
+
diff --git a/vllm/v1/core/kv_cache_utils.py b/vllm/v1/core/kv_cache_utils.py
index 403b5401b..3adf22fc4 100644
--- a/vllm/v1/core/kv_cache_utils.py
+++ b/vllm/v1/core/kv_cache_utils.py
@@ -2,7 +2,7 @@
 """KV-Cache Utilities."""
 import os
 from collections import deque
-from collections.abc import Sequence
+from collections.abc import Iterable, Sequence
 from dataclasses import dataclass
 from typing import Any, Callable, NamedTuple, Optional
 
@@ -464,6 +464,15 @@ def hash_request_tokens(hash_function: Any, block_size: int,
     return ret
 
 
+def max_memory_usage_bytes(vllm_config: VllmConfig,
+                           kv_cache_specs: Iterable[KVCacheSpec]) -> int:
+    """
+    Get the maximum memory usage in bytes for the given KV cache specs.
+    """
+    return sum(
+        spec.max_memory_usage_bytes(vllm_config) for spec in kv_cache_specs)
+
+
 def estimate_max_model_len(vllm_config: VllmConfig,
                            kv_cache_spec: dict[str, KVCacheSpec],
                            available_memory: int) -> int:
@@ -485,11 +494,8 @@ def estimate_max_model_len(vllm_config: VllmConfig,
         # Modify the max_model_len for this calculation
         vllm_config.model_config.max_model_len = model_len
         # Calculate memory needed for the given model length
-        memory_needed = sum(
-            (layer_spec.max_memory_usage_bytes(vllm_config)
-             for layer_spec in kv_cache_spec.values()),
-            start=0,
-        )
+        memory_needed = max_memory_usage_bytes(vllm_config,
+                                               kv_cache_spec.values())
         return memory_needed <= available_memory
 
     # Binary search for the maximum model length
@@ -544,8 +550,9 @@ def check_enough_kv_cache_memory(vllm_config: VllmConfig,
                                                    available_memory)
         estimated_msg = ""
         if estimated_max_len > 0:
-            estimated_msg = " Based on the available memory,"
-            f" the estimated maximum model length is {estimated_max_len}."
+            estimated_msg = (
+                "Based on the available memory, "
+                f"the estimated maximum model length is {estimated_max_len}.")
 
         raise ValueError(
             f"To serve at least one request with the models's max seq len "
diff --git a/vllm/v1/engine/core.py b/vllm/v1/engine/core.py
index 740ba60fe..967907fd5 100644
--- a/vllm/v1/engine/core.py
+++ b/vllm/v1/engine/core.py
@@ -217,7 +217,7 @@ class EngineCore:
 
         # Check for any requests remaining in the scheduler - unfinished,
         # or finished and not yet removed from the batch.
-        if not self.scheduler.has_requests():
+        if not self.scheduler.has_unfinished_requests():
             return EngineCoreOutputs(
                 outputs=[],
                 scheduler_stats=self.scheduler.make_stats(),
@@ -523,7 +523,7 @@ class EngineCoreProc(EngineCore):
         """Exits when an engine step needs to be performed."""
 
         waited = False
-        while not self.engines_running and not (self.scheduler.has_requests()):
+        while not self.engines_running and not (self.scheduler.has_unfinished_requests()):
             if logger.isEnabledFor(DEBUG) and self.input_queue.empty():
                 logger.debug("EngineCore waiting for work.")
                 waited = True
diff --git a/vllm/v1/engine/detokenizer.py b/vllm/v1/engine/detokenizer.py
index dca327cc5..f55db3fef 100644
--- a/vllm/v1/engine/detokenizer.py
+++ b/vllm/v1/engine/detokenizer.py
@@ -16,6 +16,14 @@ from vllm.v1.engine import EngineCoreRequest
 
 logger = init_logger(__name__)
 
+# Only tokenizers >= 0.21.1 supports DecodeStream used for
+# FastIncrementalDetokenizer.
+USE_FAST_DETOKENIZER = version.parse(
+    tokenizers.__version__) >= version.parse("0.21.1")
+
+# Error string from https://github.com/huggingface/tokenizers/blob/909fdde2a4ffedd9295206f705eb612be2a91b12/tokenizers/src/tokenizer/mod.rs#L1042
+INVALID_PREFIX_ERR_MSG = "Invalid prefix encountered"
+
 
 class IncrementalDetokenizer:
 
@@ -45,10 +53,9 @@ class IncrementalDetokenizer:
             # No tokenizer => skipping detokenization.
             return IncrementalDetokenizer()
 
-        if (isinstance(tokenizer, PreTrainedTokenizerFast) and version.parse(
-                tokenizers.__version__) >= version.parse("0.21.1")):
+        if USE_FAST_DETOKENIZER and isinstance(tokenizer,
+                                               PreTrainedTokenizerFast):
             # Fast tokenizer => use tokenizers library DecodeStream.
-            # And only tokenizers >= 0.21.1 supports Fast Detokenizer.
             return FastIncrementalDetokenizer(tokenizer, request)
 
         # Fall back to slow python-based incremental detokenization.
@@ -156,8 +163,11 @@ class FastIncrementalDetokenizer(BaseIncrementalDetokenizer):
         super().__init__(request)
 
         sampling_params = request.sampling_params
+
+        self.request_id = request.request_id
+        self.skip_special_tokens = sampling_params.skip_special_tokens
         self.stream = DecodeStream(
-            skip_special_tokens=sampling_params.skip_special_tokens)
+            skip_special_tokens=self.skip_special_tokens)
 
         self.tokenizer: Tokenizer = tokenizer._tokenizer
 
@@ -173,7 +183,7 @@ class FastIncrementalDetokenizer(BaseIncrementalDetokenizer):
 
         # Prime the stream.
         for tid in prompt_suffix:
-            self.stream.step(self.tokenizer, tid)
+            self._protected_step(tid)
 
         self.spaces_between_special_tokens = (
             sampling_params.skip_special_tokens
@@ -198,7 +208,7 @@ class FastIncrementalDetokenizer(BaseIncrementalDetokenizer):
                 self.spaces_between_special_tokens = True
 
     def decode_next(self, next_token_id: int) -> str:
-        token = self.stream.step(self.tokenizer, next_token_id)
+        token = self._protected_step(next_token_id)
 
         if not self.spaces_between_special_tokens:
             special_token = self.added_token_ids.get(next_token_id)
@@ -210,6 +220,23 @@ class FastIncrementalDetokenizer(BaseIncrementalDetokenizer):
 
         return token or ""
 
+    def _protected_step(self, next_token_id: int) -> Optional[str]:
+        try:
+            token = self.stream.step(self.tokenizer, next_token_id)
+        except Exception as e:
+            if str(e) != INVALID_PREFIX_ERR_MSG:
+                raise e
+            # Recover from edge case where tokenizer can produce non-monotonic,
+            # invalid UTF-8 output, which breaks the internal state of
+            # tokenizers' DecodeStream.
+            # See https://github.com/vllm-project/vllm/issues/17448.
+            logger.warning(
+                "Encountered invalid prefix detokenization error"
+                " for request %s, resetting decode stream.", self.request_id)
+            self.stream = DecodeStream(self.skip_special_tokens)
+            token = self.stream.step(self.tokenizer, next_token_id)
+        return token
+
 
 class SlowIncrementalDetokenizer(BaseIncrementalDetokenizer):
 
diff --git a/vllm/v1/executor/abstract.py b/vllm/v1/executor/abstract.py
index 3b9feb0d3..240453168 100644
--- a/vllm/v1/executor/abstract.py
+++ b/vllm/v1/executor/abstract.py
@@ -29,6 +29,7 @@ class Executor(ExecutorBase):
         parallel_config = vllm_config.parallel_config
         distributed_executor_backend = (
             parallel_config.distributed_executor_backend)
+        data_parallel_size = parallel_config.data_parallel_size
         # distributed_executor_backend must be set in VllmConfig.__post_init__
         if isinstance(distributed_executor_backend, type):
             if not issubclass(distributed_executor_backend, ExecutorBase):
@@ -37,10 +38,16 @@ class Executor(ExecutorBase):
                     f"ExecutorBase. Got {distributed_executor_backend}.")
             executor_class = distributed_executor_backend
         elif distributed_executor_backend == "ray":
-            from vllm.v1.executor.ray_distributed_executor import (  # noqa
-                RayDistributedExecutor)
-            executor_class = RayDistributedExecutor
-        elif distributed_executor_backend == "mp":
+            from vllm.platforms import current_platform
+            if current_platform.is_xpu():
+                from vllm.v1.executor.ray_distributed_executor import (  # noqa
+                    XPURayDistributedExecutor)
+                executor_class = XPURayDistributedExecutor
+            else:
+                from vllm.v1.executor.ray_distributed_executor import (  # noqa
+                    RayDistributedExecutor)
+                executor_class = RayDistributedExecutor
+        elif distributed_executor_backend == "mp" or data_parallel_size > 1:
             from vllm.v1.executor.multiproc_executor import MultiprocExecutor
             executor_class = MultiprocExecutor
         elif distributed_executor_backend == "uni":
diff --git a/vllm/v1/executor/ray_distributed_executor.py b/vllm/v1/executor/ray_distributed_executor.py
index 320ebfd37..e93316cdd 100644
--- a/vllm/v1/executor/ray_distributed_executor.py
+++ b/vllm/v1/executor/ray_distributed_executor.py
@@ -1,12 +1,14 @@
 # SPDX-License-Identifier: Apache-2.0
 
 from concurrent.futures import Future
-from typing import Union
+from typing import Union, Optional, Callable, Any
 
 from vllm.executor.ray_distributed_executor import (  # noqa
     RayDistributedExecutor as RayDistributedExecutorV0)
 from vllm.v1.executor.abstract import Executor
 from vllm.v1.outputs import ModelRunnerOutput
+import ray
+import cloudpickle
 
 
 class FutureWrapper(Future):
@@ -21,7 +23,8 @@ class FutureWrapper(Future):
     def result(self, timeout=None):
         if timeout is not None:
             raise NotImplementedError("timeout is not supported")
-        return self.ref.get()
+        # After ray 2.x, need to use ray.get
+        return ray.get(self.ref)
 
 
 class RayDistributedExecutor(RayDistributedExecutorV0, Executor):
@@ -59,3 +62,68 @@ class RayDistributedExecutor(RayDistributedExecutorV0, Executor):
         # When PP is used, we return a FutureWrapper immediately so that
         # the scheduler can yield to the next batch.
         return FutureWrapper(refs[0])
+
+
+class XPURayDistributedExecutor(RayDistributedExecutor, Executor):
+    """XPU Ray distributed executor without Compiled Graphs.
+    SPMD worker is enabled
+    """
+
+    @property
+    def max_concurrent_batches(self) -> int:
+        """Ray distributed executor supports pipeline parallelism,
+        meaning that it allows PP size batches to be executed concurrently.
+        """
+        return self.parallel_config.pipeline_parallel_size
+
+    def _run_workers(
+        self,
+        method: Union[str, Callable],
+        *args,
+        async_run_tensor_parallel_workers_only: bool = False,
+        max_concurrent_workers: Optional[int] = None,
+        **kwargs,
+    ) -> Any:
+        """Runs the given method on all workers. Can be used in the following
+        ways:
+
+        Args:
+        - async_run_tensor_parallel_workers_only: If True the method will be
+          run only in the remote TP workers, not the driver worker.
+          It will also be run asynchronously and return a list of futures
+          rather than blocking on the results.
+        - args/kwargs: All workers share the same args/kwargs
+        """
+        if isinstance(method, str):
+            sent_method = method
+        else:
+            sent_method = cloudpickle.dumps(method)
+        del method
+
+        if max_concurrent_workers:
+            raise NotImplementedError(
+                "max_concurrent_workers is not supported yet.")
+
+        ray_workers = self.workers
+
+        ray_worker_outputs = [
+            worker.execute_method.remote(sent_method, *args, **kwargs)
+            for worker in ray_workers
+        ]
+
+        if async_run_tensor_parallel_workers_only:
+            return ray_worker_outputs
+
+        ray_worker_outputs = ray.get(ray_worker_outputs)
+        return ray_worker_outputs
+
+
+    def execute_model(
+        self,
+        scheduler_output,
+    ) -> Union[ModelRunnerOutput, Future[ModelRunnerOutput]]:
+        output = self.collective_rpc("execute_model",
+                                     args=(scheduler_output, ),
+                                     kwargs={"async_run_tensor_parallel_workers_only": True}
+                                     )
+        return FutureWrapper(output[-1])
diff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py
index 910c0e80b..92cf85780 100644
--- a/vllm/v1/worker/gpu_model_runner.py
+++ b/vllm/v1/worker/gpu_model_runner.py
@@ -31,6 +31,7 @@ from vllm.model_executor.model_loader import TensorizerLoader, get_model
 from vllm.multimodal import MULTIMODAL_REGISTRY
 from vllm.multimodal.inputs import MultiModalKwargs, PlaceholderRange
 from vllm.multimodal.utils import group_mm_inputs_by_modality
+from vllm.platforms import current_platform
 from vllm.sampling_params import SamplingType
 from vllm.sequence import IntermediateTensors
 from vllm.utils import (STR_DTYPE_TO_TORCH_DTYPE, DeviceMemoryProfiler,
@@ -68,6 +69,9 @@ if TYPE_CHECKING:
 else:
     xgr = LazyLoader("xgr", globals(), "xgrammar")
 
+if current_platform.is_xpu():
+    import intel_extension_for_pytorch
+
 logger = init_logger(__name__)
 
 
@@ -1432,6 +1436,11 @@ class GPUModelRunner(LoRAModelRunnerMixin):
         if has_kv_transfer_group():
             get_kv_transfer_group().clear_connector_metadata()
 
+        if current_platform.is_xpu():
+            reserved_mem = torch.xpu.memory_reserved()
+            if reserved_mem >= self.vllm_config.cache_config.threshold_mem:
+                torch.xpu.empty_cache()
+
         return ModelRunnerOutput(
             req_ids=self.input_batch.req_ids,
             req_id_to_index=self.input_batch.req_id_to_index,
diff --git a/vllm/v1/worker/gpu_worker.py b/vllm/v1/worker/gpu_worker.py
index bce5cbb5f..5230cc4bf 100644
--- a/vllm/v1/worker/gpu_worker.py
+++ b/vllm/v1/worker/gpu_worker.py
@@ -248,12 +248,16 @@ class Worker(WorkerBase):
         # fragmentation issue.
         # NOTE: This is called after `capture_model` on purpose to prevent
         # memory buffers from being cleared by `torch.cuda.empty_cache`.
-        if get_pp_group().is_last_rank:
+        if get_pp_group().is_last_rank and get_pp_group().world_size > 1:
             max_num_reqs = min(self.scheduler_config.max_num_seqs,
                                self.scheduler_config.max_num_batched_tokens)
-            self.model_runner._dummy_sampler_run(
-                hidden_states=self.model_runner._dummy_run(
-                    num_tokens=max_num_reqs))
+            if self.model_runner.use_spec_decode:
+                idden_states=self.model_runner._dummy_run(
+                        num_tokens=max_num_reqs)
+            else:
+                self.model_runner._dummy_sampler_run(
+                        hidden_states=self.model_runner._dummy_run(
+                            num_tokens=max_num_reqs))
 
         # Reset the seed to ensure that the random state is not affected by
         # the model initialization and profiling.
diff --git a/vllm/v1/worker/xpu_model_runner.py b/vllm/v1/worker/xpu_model_runner.py
new file mode 100644
index 000000000..d5dbf7be5
--- /dev/null
+++ b/vllm/v1/worker/xpu_model_runner.py
@@ -0,0 +1,389 @@
+# SPDX-License-Identifier: Apache-2.0
+import gc
+from typing import TYPE_CHECKING, Optional
+
+import numpy as np
+import torch
+
+from vllm.attention.backends.abstract import (AttentionBackend,
+                                              AttentionMetadataBuilder)
+from vllm.config import CompilationLevel, VllmConfig
+from vllm.distributed.parallel_state import get_pp_group
+from vllm.logger import init_logger
+from vllm.multimodal import MULTIMODAL_REGISTRY
+from vllm.sequence import IntermediateTensors
+from vllm.utils import (STR_DTYPE_TO_TORCH_DTYPE, LayerBlockType, cdiv,
+                        check_use_alibi, is_pin_memory_available)
+from vllm.v1.core.encoder_cache_manager import compute_encoder_budget
+from vllm.v1.outputs import LogprobsTensors
+from vllm.v1.sample.rejection_sampler import RejectionSampler
+from vllm.v1.sample.sampler import Sampler
+from vllm.v1.spec_decode.eagle import EagleProposer
+from vllm.v1.spec_decode.ngram_proposer import NgramProposer
+from vllm.v1.worker.gpu_input_batch import CachedRequestState, InputBatch
+from vllm.v1.worker.gpu_model_runner import GPUModelRunner
+
+if TYPE_CHECKING:
+    from vllm.v1.core.scheduler import SchedulerOutput
+
+logger = init_logger(__name__)
+
+
+class XPUModelRunner(GPUModelRunner):
+    """A model runner for XPU devices."""
+
+    def __init__(
+        self,
+        vllm_config: VllmConfig,
+        device: torch.device,
+    ):
+        self.vllm_config = vllm_config
+        self.model_config = vllm_config.model_config
+        self.cache_config = vllm_config.cache_config
+        self.lora_config = vllm_config.lora_config
+        self.load_config = vllm_config.load_config
+        self.parallel_config = vllm_config.parallel_config
+        self.scheduler_config = vllm_config.scheduler_config
+        self.speculative_config = vllm_config.speculative_config
+        self.prompt_adapter_config = vllm_config.prompt_adapter_config
+        self.observability_config = vllm_config.observability_config
+
+        from vllm.model_executor.models.utils import set_cpu_offload_max_bytes
+        set_cpu_offload_max_bytes(
+            int(self.cache_config.cpu_offload_gb * 1024**3))
+
+        model_config = self.model_config
+        cache_config = self.cache_config
+        scheduler_config = self.scheduler_config
+        parallel_config = self.parallel_config
+        self.device = device
+        self.pin_memory = is_pin_memory_available()
+        self.dtype = self.model_config.dtype
+        if cache_config.cache_dtype == "auto":
+            self.kv_cache_dtype = self.dtype
+        else:
+            self.kv_cache_dtype = STR_DTYPE_TO_TORCH_DTYPE[
+                cache_config.cache_dtype]
+
+        self.sliding_window = model_config.get_sliding_window()
+        self.interleaved_sliding_window = getattr(
+            model_config.hf_text_config, "interleaved_sliding_window", None)
+        self.window_size = (self.sliding_window
+                            or self.interleaved_sliding_window)
+
+        self.is_multimodal_model = model_config.is_multimodal_model
+        self.block_size = cache_config.block_size
+        self.max_model_len = model_config.max_model_len
+        self.max_num_blocks_per_req = cdiv(self.max_model_len, self.block_size)
+        self.max_num_tokens = scheduler_config.max_num_batched_tokens
+        self.max_num_reqs = scheduler_config.max_num_seqs
+
+        # Model-related.
+        self.num_attn_layers = model_config.get_num_layers_by_block_type(
+            parallel_config, LayerBlockType.attention)
+        self.num_query_heads = model_config.get_num_attention_heads(
+            parallel_config)
+        self.num_kv_heads = model_config.get_num_kv_heads(parallel_config)
+        self.head_size = model_config.get_head_size()
+        self.hidden_size = model_config.get_hidden_size()
+        self.attention_chunk_size = model_config.attention_chunk_size
+
+        self.cascade_attn_enabled = False
+
+        # Multi-modal data support
+        self.mm_registry = MULTIMODAL_REGISTRY
+        self.uses_mrope = model_config.uses_mrope
+
+        encoder_compute_budget, encoder_cache_size = compute_encoder_budget(
+            model_config=model_config,
+            scheduler_config=scheduler_config,
+            mm_registry=self.mm_registry,
+        )
+        self.max_num_encoder_input_tokens = encoder_compute_budget
+        self.encoder_cache_size = encoder_cache_size
+
+        # Sampler
+        self.sampler = Sampler()
+
+        # Lazy initializations
+        # self.model: nn.Module  # Set after load_model
+        # Initialize in initialize_kv_cache
+        self.kv_caches: list[torch.Tensor] = []
+        self.attn_metadata_builders: list[AttentionMetadataBuilder] = []
+        self.attn_backends: list[type[AttentionBackend]] = []
+        # self.kv_cache_config: KVCacheConfig
+        # self.attn_metadata_builder: type[AttentionMetadataBuilder]
+
+        # req_id -> (input_id -> encoder_output)
+        self.encoder_cache: dict[str, dict[int, torch.Tensor]] = {}
+
+        # Set up speculative decoding.
+        self.use_spec_decode = False
+        self.use_aux_hidden_state_outputs = False
+        if self.speculative_config:
+            self.use_spec_decode = True
+            if get_pp_group().is_last_rank:
+                if self.speculative_config.method == "ngram":
+                    self.drafter = NgramProposer(self.vllm_config)
+                elif self.speculative_config.use_eagle():
+                    self.drafter = EagleProposer(self.vllm_config,
+                                                 self.device)  # type: ignore
+                    if self.speculative_config.method == "eagle3":
+                        self.use_aux_hidden_state_outputs = True
+                else:
+                    raise ValueError("Unknown speculative decoding method: "
+                                     f"{self.speculative_config.method}")
+                self.rejection_sampler = RejectionSampler()
+
+        # Request states.
+        self.requests: dict[str, CachedRequestState] = {}
+        self.input_batch = InputBatch(
+            max_num_reqs=self.max_num_reqs,
+            max_model_len=self.max_model_len,
+            max_num_batched_tokens=self.max_num_tokens,
+            device=self.device,
+            pin_memory=self.pin_memory,
+            vocab_size=self.model_config.get_vocab_size(),
+            block_size=self.cache_config.block_size,
+        )
+
+        self.use_cuda_graph = (self.vllm_config.compilation_config.level
+                               == CompilationLevel.PIECEWISE
+                               and not self.model_config.enforce_eager)
+        # TODO(woosuk): Provide an option to tune the max cudagraph batch size.
+        # The convention is different.
+        # self.cudagraph_batch_sizes sorts in ascending order.
+        # The batch sizes in the config are in descending order.
+        self.cudagraph_batch_sizes = list(
+            reversed(
+                self.vllm_config.compilation_config.cudagraph_capture_sizes))
+
+        # Persistent buffers for CUDA graphs.
+        self.input_ids = torch.zeros(self.max_num_tokens,
+                                     dtype=torch.int32,
+                                     device=self.device)
+        self.positions = torch.zeros(self.max_num_tokens,
+                                     dtype=torch.int64,
+                                     device=self.device)
+        self.query_start_loc = torch.zeros(self.max_num_reqs + 1,
+                                           dtype=torch.int32,
+                                           device=self.device)
+        self.seq_lens = torch.zeros(self.max_num_reqs,
+                                    dtype=torch.int32,
+                                    device=self.device)
+        self.slot_mapping = torch.zeros(self.max_num_tokens,
+                                        dtype=torch.int64,
+                                        device=self.device)
+
+        # None in the first PP rank. The rest are set after load_model.
+        self.intermediate_tensors: Optional[IntermediateTensors] = None
+
+        # Only relevant for models using M-RoPE (e.g, Qwen2-VL)
+        if self.uses_mrope:
+            # NOTE: `mrope_positions` is implemented with one additional dummy
+            # position on purpose to make it non-contiguous so that it can work
+            # with torch compile.
+            # See detailed explanation in https://github.com/vllm-project/vllm/pull/12128#discussion_r1926431923
+
+            # NOTE: When M-RoPE is enabled, position ids are 3D regardless of
+            # the modality of inputs. For text-only inputs, each dimension has
+            # identical position IDs, making M-RoPE functionally equivalent to
+            # 1D-RoPE.
+            # See page 5 of https://arxiv.org/abs/2409.12191
+            self.mrope_positions = torch.zeros((3, self.max_num_tokens + 1),
+                                               dtype=torch.int64,
+                                               device=self.device)
+            self.mrope_positions_cpu = torch.zeros(
+                (3, self.max_num_tokens + 1),
+                dtype=torch.int64,
+                device="cpu",
+                pin_memory=self.pin_memory)
+
+        # Only relevant for models using ALiBi (e.g, MPT)
+        self.use_alibi = check_use_alibi(model_config)
+
+        self.inputs_embeds = torch.zeros(
+            (self.max_num_tokens, self.hidden_size),
+            dtype=self.dtype,
+            device=self.device)
+
+        # OPTIMIZATION: Cache the tensors rather than creating them every step.
+        self.arange_np = np.arange(max(self.max_num_reqs + 1,
+                                       self.max_model_len,
+                                       self.max_num_tokens),
+                                   dtype=np.int32)
+        # NOTE(woosuk): These tensors are "stateless", i.e., they are literally
+        # a faster version of creating a new tensor every time. Thus, we should
+        # not make any assumptions about the values in these tensors.
+        self.input_ids_cpu = torch.zeros(self.max_num_tokens,
+                                         dtype=torch.int32,
+                                         device="cpu",
+                                         pin_memory=self.pin_memory)
+        self.input_ids_np = self.input_ids_cpu.numpy()
+        self.positions_cpu = torch.zeros(self.max_num_tokens,
+                                         dtype=torch.int64,
+                                         device="cpu",
+                                         pin_memory=self.pin_memory)
+        self.positions_np = self.positions_cpu.numpy()
+        self.slot_mapping_cpu = torch.zeros(self.max_num_tokens,
+                                            dtype=torch.int32,
+                                            device="cpu",
+                                            pin_memory=self.pin_memory)
+        self.slot_mapping_np = self.slot_mapping_cpu.numpy()
+        self.query_start_loc_cpu = torch.zeros(self.max_num_reqs + 1,
+                                               dtype=torch.int32,
+                                               device="cpu",
+                                               pin_memory=self.pin_memory)
+        self.query_start_loc_np = self.query_start_loc_cpu.numpy()
+        # this is XPU specific
+        self.seq_start_loc_cpu = torch.zeros(self.max_num_reqs + 1,
+                                             dtype=torch.int32,
+                                             device="cpu",
+                                             pin_memory=self.pin_memory)
+        self.seq_start_loc_np = self.seq_start_loc_cpu.numpy()
+        self.seq_lens_cpu = torch.zeros(self.max_num_reqs,
+                                        dtype=torch.int32,
+                                        device="cpu",
+                                        pin_memory=self.pin_memory)
+        self.seq_lens_np = self.seq_lens_cpu.numpy()
+
+    # we can enable this if GPUModelRunner or parent class don't have
+    # torch.cuda in the future
+    # def __init__(self,
+    #     vllm_config: VllmConfig,
+    #     device: torch.device,):
+    #     super().__init__(vllm_config, device)
+    #     self.cascade_attn_enabled = False
+    #     # FIXME: support mrope
+    #     self.uses_mrope = False
+    #     # this is XPU specific
+    #     self.seq_start_loc_cpu = torch.zeros(self.max_num_reqs + 1,
+    #                                          dtype=torch.int32,
+    #                                          device="cpu",
+    #                                          pin_memory=self.pin_memory)
+    #     self.seq_start_loc_np = self.seq_start_loc_cpu.numpy()
+
+    def _prepare_inputs(self, scheduler_output: "SchedulerOutput"):
+        total_num_scheduled_tokens = scheduler_output.total_num_scheduled_tokens
+        assert total_num_scheduled_tokens > 0
+        num_reqs = self.input_batch.num_reqs
+        assert num_reqs > 0
+        # Get the number of scheduled tokens for each request.
+        req_ids = self.input_batch.req_ids
+        tokens = [scheduler_output.num_scheduled_tokens[i] for i in req_ids]
+        num_scheduled_tokens = np.array(tokens, dtype=np.int32)
+        # ======== XPU  start =========
+        seq_lens = (self.input_batch.num_computed_tokens_cpu[:num_reqs] +
+                    num_scheduled_tokens)
+        self.seq_start_loc_np[0] = 0
+        np.cumsum(seq_lens, out=self.seq_start_loc_np[1:num_reqs + 1])
+        # ======== XPU end =========
+        return super()._prepare_inputs(scheduler_output)
+
+    def _get_prompt_logprobs_dict(
+        self,
+        hidden_states: torch.Tensor,
+        scheduler_output: "SchedulerOutput",
+    ) -> dict[str, Optional[LogprobsTensors]]:
+        num_prompt_logprobs_dict = self.input_batch.num_prompt_logprobs
+        if not num_prompt_logprobs_dict:
+            return {}
+
+        in_progress_dict = self.input_batch.in_progress_prompt_logprobs_cpu
+        prompt_logprobs_dict: dict[str, Optional[LogprobsTensors]] = {}
+
+        # Since prompt logprobs are a rare feature, prioritize simple,
+        # maintainable loop over optimal performance.
+        completed_prefill_reqs = []
+        for req_id, num_prompt_logprobs in num_prompt_logprobs_dict.items():
+
+            num_tokens = scheduler_output.num_scheduled_tokens[req_id]
+
+            # Get metadata for this request.
+            request = self.requests[req_id]
+            num_prompt_tokens = len(request.prompt_token_ids)
+            prompt_token_ids = torch.tensor(request.prompt_token_ids).to(
+                self.device, non_blocking=True)
+
+            # Set up target LogprobsTensors object.
+            logprobs_tensors = in_progress_dict.get(req_id)
+            if not logprobs_tensors:
+                # Create empty logprobs CPU tensors for the entire prompt.
+                # If chunked, we'll copy in slice by slice.
+                logprobs_tensors = LogprobsTensors.empty_cpu(
+                    num_prompt_tokens - 1, num_prompt_logprobs + 1)
+                in_progress_dict[req_id] = logprobs_tensors
+
+            # Determine number of logits to retrieve.
+            start_idx = request.num_computed_tokens
+            start_tok = start_idx + 1
+            num_remaining_tokens = num_prompt_tokens - start_tok
+            if num_tokens <= num_remaining_tokens:
+                # This is a chunk, more tokens remain.
+                # In the == case, there are no more prompt logprobs to produce
+                # but we want to defer returning them to the next step where we
+                # have new generated tokens to return.
+                num_logits = num_tokens
+            else:
+                # This is the last chunk of prompt tokens to return.
+                num_logits = num_remaining_tokens
+                completed_prefill_reqs.append(req_id)
+                prompt_logprobs_dict[req_id] = logprobs_tensors
+
+            if num_logits <= 0:
+                # This can happen for the final chunk if we prefilled exactly
+                # (num_prompt_tokens - 1) tokens for this request in the prior
+                # step. There are no more prompt logprobs to produce.
+                continue
+
+            # Get the logits corresponding to this req's prompt tokens.
+            # If this is a partial request (i.e. chunked prefill),
+            # then there is prompt logprob generated for each index.
+            req_idx = self.input_batch.req_id_to_index[req_id]
+            offset = self.query_start_loc_np[req_idx].item()
+            prompt_hidden_states = hidden_states[offset:offset + num_logits]
+            logits = self.model.compute_logits(prompt_hidden_states, None)
+
+            # Get the "target" tokens for each index. For prompt at index i,
+            # the token at prompt index i+1 is the "sampled" token we want
+            # to gather the logprob for.
+            tgt_token_ids = prompt_token_ids[start_tok:start_tok + num_logits]
+
+            # Compute prompt logprobs.
+            logprobs = self.sampler.compute_logprobs(logits)
+            token_ids, logprobs, ranks = self.sampler.gather_logprobs(
+                logprobs, num_prompt_logprobs, tgt_token_ids)
+
+            # Transfer GPU->CPU async.
+            chunk_slice = slice(start_idx, start_idx + num_logits)
+            logprobs_tensors.logprob_token_ids[chunk_slice].copy_(
+                token_ids, non_blocking=True)
+            logprobs_tensors.logprobs[chunk_slice].copy_(logprobs,
+                                                         non_blocking=True)
+            logprobs_tensors.selected_token_ranks[chunk_slice].copy_(
+                ranks, non_blocking=True)
+
+        # Remove requests that have completed prefill from the batch
+        # num_prompt_logprobs_dict.
+        for req_id in completed_prefill_reqs:
+            del num_prompt_logprobs_dict[req_id]
+            del in_progress_dict[req_id]
+
+        # Must synchronize the non-blocking GPU->CPU transfers.
+        if prompt_logprobs_dict:
+            torch.xpu.synchronize()
+
+        return prompt_logprobs_dict
+
+
+    def profile_run(self) -> None:
+        # Trigger compilation for general shape.
+        hidden_states = self._dummy_run(self.max_num_tokens)
+        if not self.use_spec_decode and get_pp_group().is_last_rank and get_pp_group().world_size > 1:
+            sampler_output = self._dummy_sampler_run(hidden_states)
+        else:
+            sampler_output = None
+        torch.xpu.synchronize()
+        del hidden_states, sampler_output
+        self.encoder_cache.clear()
+        gc.collect()
diff --git a/vllm/v1/worker/xpu_worker.py b/vllm/v1/worker/xpu_worker.py
new file mode 100644
index 000000000..746f8b4bc
--- /dev/null
+++ b/vllm/v1/worker/xpu_worker.py
@@ -0,0 +1,180 @@
+# SPDX-License-Identifier: Apache-2.0
+import os
+from typing import Optional
+
+import torch
+import torch.distributed
+
+import vllm.envs as envs
+from vllm.config import ParallelConfig, VllmConfig
+from vllm.distributed import (ensure_model_parallel_initialized,
+                              init_distributed_environment)
+from vllm.logger import init_logger
+from vllm.model_executor import set_random_seed
+from vllm.platforms import current_platform
+from vllm.utils import supports_xccl
+from vllm.v1.worker.gpu_worker import Worker
+from vllm.v1.worker.xpu_model_runner import XPUModelRunner
+
+logger = init_logger(__name__)
+
+
+class XPUWorker(Worker):
+    """A XPU worker class."""
+
+    def __init__(
+        self,
+        vllm_config: VllmConfig,
+        local_rank: int,
+        rank: int,
+        distributed_init_method: str,
+        is_driver_worker: bool = False,
+    ):
+        super().__init__(vllm_config, local_rank, rank,
+                         distributed_init_method, is_driver_worker)
+        device_config = self.device_config
+        assert device_config.device_type == "xpu"
+        assert current_platform.is_xpu()
+
+        # Torch profiler. Enabled and configured through env vars:
+        # VLLM_TORCH_PROFILER_DIR=/path/to/save/trace
+        if envs.VLLM_TORCH_PROFILER_DIR:
+            torch_profiler_trace_dir = envs.VLLM_TORCH_PROFILER_DIR
+            logger.info("Profiling enabled. Traces will be saved to: %s",
+                        torch_profiler_trace_dir)
+            self.profiler = torch.profiler.profile(
+                activities=[
+                    torch.profiler.ProfilerActivity.CPU,
+                    torch.profiler.ProfilerActivity.XPU,
+                ],
+                with_stack=True,
+                on_trace_ready=torch.profiler.tensorboard_trace_handler(
+                    torch_profiler_trace_dir, use_gzip=True))
+        else:
+            self.profiler = None
+
+    # we provide this function due to `torch.xpu.mem_get_info()` doesn't
+    # return correct free_gpu_memory on intel client GPU. We need to
+    # calculate/estiamte it.
+    def xpu_get_mem_info(self):
+        if current_platform.is_data_center_gpu():
+            return torch.xpu.mem_get_info()
+        else:
+            _, total_gpu_memory = torch.xpu.mem_get_info()
+            # FIXME: memory_allocated() doesn't count non-torch allocations,
+            # and we don't have any API to get it. so we mark it as 128MB.
+            used_memory = torch.xpu.memory_allocated()
+            non_torch_allocations = 128 * 1024 * 1024
+            free_gpu_memory = total_gpu_memory - (used_memory +
+                                                  non_torch_allocations)
+            return free_gpu_memory, total_gpu_memory
+
+    @torch.inference_mode()
+    def determine_available_memory(self) -> int:
+        """Profiles the peak memory usage of the model to determine how many
+        KV blocks may be allocated without OOMs.
+
+        The engine will first conduct a profiling of the existing memory usage.
+        Then, it calculate the maximum possible number of GPU and CPU blocks
+        that can be allocated with the remaining free memory.
+
+        .. tip::
+            You may limit the usage of GPU memory
+            by adjusting the `gpu_memory_utilization` parameter.
+        """
+        # Profile the memory usage of the model and get the maximum number of
+        # cache blocks that can be allocated with the remaining free memory.
+        torch.xpu.empty_cache()
+
+        # Execute a forward pass with dummy inputs to profile the memory usage
+        # of the model.
+        self.model_runner.profile_run()
+
+        # Calculate the number of blocks that can be allocated with the
+        # profiled peak memory.
+        torch.xpu.synchronize()
+        used_memory = torch.xpu.memory_allocated()
+        used_memory = torch.xpu.memory_reserved()
+        reserved_memory = torch.xpu.memory_reserved()
+        total_gpu_memory = torch.xpu.get_device_properties(
+            self.local_rank).total_memory
+        free_gpu_memory = total_gpu_memory - used_memory
+
+        # NOTE(woosuk): Here we assume that the other processes using the same
+        # GPU did not change their memory usage during the profiling.
+        peak_memory = self.init_gpu_memory - free_gpu_memory
+        assert peak_memory > 0, (
+            "Error in memory profiling. "
+            f"Initial free memory {self.init_gpu_memory}, current free memory"
+            f" {free_gpu_memory}. This happens when the GPU memory was "
+            "not properly cleaned up before initializing the vLLM instance.")
+
+        torch.xpu.empty_cache()
+
+        available_kv_cache_memory = (
+            total_gpu_memory * self.cache_config.gpu_memory_utilization -
+            peak_memory)
+
+        self.cache_config.threshold_mem = reserved_memory + available_kv_cache_memory
+        self.cache_config.threshold_mem = total_gpu_memory * 0.97
+        return int(available_kv_cache_memory)
+
+    def init_device(self):
+        if self.device_config.device.type == "xpu" and current_platform.is_xpu(
+        ):
+            self.device = torch.device(f"xpu:{self.local_rank}")
+            torch.xpu.set_device(self.device)
+            torch.xpu.empty_cache()
+            self.init_gpu_memory = torch.xpu.get_device_properties(
+                self.local_rank).total_memory
+        else:
+            raise RuntimeError(
+                f"Not support device type: {self.device_config.device}")
+        init_worker_distributed_environment(self.parallel_config, self.rank,
+                                            self.distributed_init_method,
+                                            self.local_rank)
+        # Set random seed.
+        set_random_seed(self.model_config.seed)
+        self.model_runner = XPUModelRunner(  # type: ignore
+            self.vllm_config, self.device)
+
+
+def init_worker_distributed_environment(
+    parallel_config: ParallelConfig,
+    rank: int,
+    distributed_init_method: Optional[str] = None,
+    local_rank: int = -1,
+) -> None:
+    """Initialize the distributed environment."""
+
+    if torch.distributed.is_initialized():
+        torch_world_size = torch.distributed.get_world_size()
+        if torch_world_size != parallel_config.world_size:
+            raise RuntimeError(
+                "torch.distributed is already initialized but the torch "
+                "world size does not match parallel_config.world_size "
+                f"({torch_world_size} vs. {parallel_config.world_size}).")
+    elif not distributed_init_method:
+        raise ValueError(
+            "distributed_init_method must be set if torch.distributed "
+            "is not already initialized")
+    else:
+        default_backend = "xccl" if supports_xccl() else "ccl"
+        XPU_CCL_BACKEND = os.getenv("XPU_CCL_BACKEND", default_backend)
+        ENV_CCL_ATL_TRANSPORT = os.getenv("CCL_ATL_TRANSPORT", "ofi")
+        ENV_LOCAL_WORLD_SIZE = os.getenv("LOCAL_WORLD_SIZE",
+                                         str(parallel_config.world_size))
+        os.environ["CCL_ATL_TRANSPORT"] = ENV_CCL_ATL_TRANSPORT
+        os.environ["LOCAL_WORLD_SIZE"] = ENV_LOCAL_WORLD_SIZE
+        os.environ["LOCAL_RANK"] = str(local_rank)
+        init_distributed_environment(
+            world_size=parallel_config.world_size,
+            rank=rank,
+            distributed_init_method=distributed_init_method,
+            local_rank=local_rank,
+            backend=XPU_CCL_BACKEND)
+
+    ensure_model_parallel_initialized(parallel_config.tensor_parallel_size,
+                                      parallel_config.pipeline_parallel_size)
+    # global all_reduce needed for overall oneccl warm up
+    torch.distributed.all_reduce(torch.zeros(1).xpu())
diff --git a/vllm/worker/worker.py b/vllm/worker/worker.py
index 6e45b8423..934bd92da 100644
--- a/vllm/worker/worker.py
+++ b/vllm/worker/worker.py
@@ -75,6 +75,7 @@ class Worker(LocalOrDistributedWorkerBase):
                         "mlp_speculator",
                         "eagle",
                         "deepseek_mtp",
+                        "glm4_moe_mtp",
                          "mimo_mtp")) \
                     else {"return_hidden_states": True}
 
diff --git a/vllm/worker/xpu_enc_dec_model_runner.py b/vllm/worker/xpu_enc_dec_model_runner.py
new file mode 100644
index 000000000..82c16e348
--- /dev/null
+++ b/vllm/worker/xpu_enc_dec_model_runner.py
@@ -0,0 +1,527 @@
+# SPDX-License-Identifier: Apache-2.0
+
+import dataclasses
+import itertools
+from typing import Any, Dict, List, Optional, Tuple, Type, cast
+
+import torch
+import torch.distributed
+
+from vllm.attention.backends.abstract import (AttentionBackend,
+                                              AttentionMetadata)
+from vllm.attention.backends.utils import PAD_SLOT_ID
+from vllm.attention.selector import (get_env_variable_attn_backend,
+                                     get_global_forced_attn_backend)
+from vllm.config import VllmConfig
+from vllm.forward_context import set_forward_context
+from vllm.inputs import INPUT_REGISTRY, InputRegistry
+from vllm.logger import init_logger
+from vllm.model_executor import SamplingMetadata
+from vllm.model_executor.layers.sampler import SamplerOutput, Sampler
+from vllm.multimodal import (MULTIMODAL_REGISTRY, MultiModalKwargs,
+                             MultiModalRegistry)
+from vllm.platforms import _Backend
+from vllm.sampling_params import SamplingParams
+from vllm.sequence import (IntermediateTensors, PoolerOutput,
+                           SequenceGroupMetadata)
+from vllm.utils import STR_NOT_IMPL_ENC_DEC_BACKEND, make_tensor_with_pad
+from vllm.worker.model_runner import (GPUModelRunnerBase,
+                                      ModelInputForGPUBuilder)
+from vllm.worker.xpu_model_runner import (XPUModelRunnerBase,
+                                      ModelInputForXPUBuilder,
+                                      ModelInputForXPUWithSamplingMetadata)
+from vllm.worker.model_runner_base import (
+    _add_attn_metadata_broadcastable_dict,
+    _add_sampling_metadata_broadcastable_dict)
+from vllm.worker.utils import assert_enc_dec_mr_supported_scenario
+
+logger = init_logger(__name__)
+
+
+@dataclasses.dataclass(frozen=True)
+class XPUEncoderDecoderModelInput(ModelInputForXPUWithSamplingMetadata):
+    """
+    Used by the EncoderDecoderModelRunner.
+    """
+    encoder_input_tokens: Optional[torch.Tensor] = None
+    encoder_input_positions: Optional[torch.Tensor] = None
+
+    def as_broadcastable_tensor_dict(self) -> Dict[str, Any]:
+        tensor_dict = {
+            "input_tokens": self.input_tokens,
+            "input_positions": self.input_positions,
+            "encoder_input_tokens": self.encoder_input_tokens,
+            "encoder_input_positions": self.encoder_input_positions,
+            "virtual_engine": self.virtual_engine,
+            "request_ids_to_seq_ids": self.request_ids_to_seq_ids,
+            "finished_requests_ids": self.finished_requests_ids,
+            "multi_modal_kwargs": self.multi_modal_kwargs,
+        }
+        _add_attn_metadata_broadcastable_dict(tensor_dict, self.attn_metadata)
+        _add_sampling_metadata_broadcastable_dict(tensor_dict,
+                                                  self.sampling_metadata)
+        return tensor_dict
+
+    @classmethod
+    def from_broadcasted_tensor_dict(
+        cls,
+        tensor_dict: Dict[str, Any],
+        attn_backend: Optional["AttentionBackend"] = None,
+    ) -> "XPUEncoderDecoderModelInput":
+        return cast(
+            XPUEncoderDecoderModelInput,
+            super().from_broadcasted_tensor_dict(tensor_dict, attn_backend))
+
+
+class XPUEncoderDecoderModelRunner(XPUModelRunnerBase[XPUEncoderDecoderModelInput]):
+    _model_input_cls: Type[XPUEncoderDecoderModelInput] = (
+        XPUEncoderDecoderModelInput)
+    _builder_cls: Type[ModelInputForXPUBuilder] = (ModelInputForXPUBuilder)
+
+    def __init__(
+        self,
+        vllm_config: VllmConfig,
+        kv_cache_dtype: Optional[str] = "auto",
+        is_driver_worker: bool = False,
+        input_registry: InputRegistry = INPUT_REGISTRY,
+        mm_registry: MultiModalRegistry = MULTIMODAL_REGISTRY,
+    ):
+        '''
+        EncoderDecoderModelRunner constructor.
+
+        `lora_config` and `prompt_adapter_config` are
+        unused (since these features are not yet supported for encoder/decoder
+        models) but these arguments are present here for compatibility with
+        the base-class constructor.
+        '''
+        # self._maybe_force_supported_attention_backend()
+
+        super().__init__(
+            vllm_config=vllm_config,
+            kv_cache_dtype=kv_cache_dtype,
+            is_driver_worker=is_driver_worker,
+        )
+        self.sampler = Sampler()
+
+        # Crash for unsupported encoder/scenarios
+        assert_enc_dec_mr_supported_scenario(self)
+
+    def _maybe_force_supported_attention_backend(self):
+        '''
+        Force vLLM to use the XFormers attention backend,
+        which is currently the only supported option.
+        '''
+
+        def raise_backend_err():
+            # The user has specified an attention backend override
+            # which is invalid for encoder/decoder models
+            raise NotImplementedError(STR_NOT_IMPL_ENC_DEC_BACKEND)
+
+        maybe_env_var_forced_backend = get_env_variable_attn_backend()
+        maybe_global_forced_backend = get_global_forced_attn_backend()
+        is_forced_by_global = maybe_global_forced_backend is not None
+        is_forced_by_env_var = maybe_env_var_forced_backend is not None
+        if is_forced_by_global:  # noqa: SIM102
+            # Backend override enforced by global variable takes
+            # precedence over vLLM backend environment variable.
+            if maybe_global_forced_backend not in\
+                 [_Backend.XFORMERS, _Backend.FLASH_ATTN]:
+                raise_backend_err()
+        elif is_forced_by_env_var:  # noqa: SIM102
+            # Backend override enforced by vLLM backend
+            # environment variable
+            if maybe_env_var_forced_backend not in\
+                 [_Backend.XFORMERS, _Backend.FLASH_ATTN]:
+                raise_backend_err()
+
+    def _list_to_int32_tensor(
+        self,
+        _list: List[int],
+    ) -> torch.Tensor:
+        return torch.tensor(_list, dtype=torch.int32, device=self.device)
+
+    def _list_to_long_tensor(
+        self,
+        _list: List[int],
+    ) -> torch.Tensor:
+        return torch.tensor(_list, dtype=torch.long, device=self.device)
+
+    def _empty_int32_tensor(self) -> torch.Tensor:
+        return self._list_to_int32_tensor([])
+
+    def _empty_long_tensor(self) -> torch.Tensor:
+        return self._list_to_long_tensor([])
+
+    @torch.inference_mode()
+    def execute_model(
+        self,
+        model_input: XPUEncoderDecoderModelInput,
+        kv_caches: List[torch.Tensor],
+        intermediate_tensors: Optional[IntermediateTensors] = None,
+        num_steps: int = 1,
+    ) -> Optional[List[PoolerOutput]]:
+        if num_steps > 1:
+            raise ValueError("num_steps > 1 is not supported in "
+                             "EncoderDecoderModelRunner")
+
+        model_executable = self.model
+
+        # seqlen_agnostic_kwargs = {
+        #     "finished_requests_ids": model_input.finished_requests_ids,
+        #     "request_ids_to_seq_ids": model_input.request_ids_to_seq_ids,
+        # } if self.has_inner_state else {}
+
+        multi_modal_kwargs = model_input.multi_modal_kwargs or {}
+        with set_forward_context(model_input.attn_metadata, self.vllm_config,
+                                 model_input.virtual_engine):
+            hidden_or_intermediate_states = model_executable(
+                input_ids=model_input.input_tokens,
+                positions=model_input.input_positions,
+                encoder_input_ids=model_input.encoder_input_tokens,
+                encoder_positions=model_input.encoder_input_positions,
+                intermediate_tensors=intermediate_tensors,
+                **MultiModalKwargs.as_kwargs(multi_modal_kwargs,
+                                             device=self.device),
+                )
+                # **seqlen_agnostic_kwargs)
+
+        logits = self.model.compute_logits(hidden_or_intermediate_states,
+                                           model_input.sampling_metadata)
+
+        if not self.is_driver_worker:
+            return []
+
+        if model_input.async_callback is not None:
+            model_input.async_callback()
+
+        # Sample the next token.
+        output: SamplerOutput = self.sampler(
+            logits=logits,
+            sampling_metadata=model_input.sampling_metadata,
+        )
+
+        return [output]
+
+    def make_model_input_from_broadcasted_tensor_dict(
+            self, tensor_dict: Dict[str, Any]) -> XPUEncoderDecoderModelInput:
+        return XPUEncoderDecoderModelInput.from_broadcasted_tensor_dict(
+            tensor_dict,
+            attn_backend=self.attn_backend,
+        )
+
+    def prepare_model_input(
+        self,
+        seq_group_metadata_list: List[SequenceGroupMetadata],
+        virtual_engine: int = 0,
+        finished_requests_ids: Optional[List[str]] = None
+    ) -> XPUEncoderDecoderModelInput:
+        """Prepare the model input based on a given sequence group, including
+        metadata for the sampling step.
+
+        Since chunked prefill is not supported for encoder/decoder models,
+        `input_tokens` is assumed to be either entirely prefill tokens or
+        entirely decode tokens.
+
+        """
+        model_input = self._prepare_model_input_tensors(
+            seq_group_metadata_list, finished_requests_ids)
+        (
+            attn_metadata,
+            encoder_input_tokens_tensor,
+            encoder_input_positions_tensor,
+        ) = (self._prepare_encoder_model_input_tensors(seq_group_metadata_list,
+                                                       model_input))
+        # Inject attn_metadata encoder/cross-attention fields &
+        # encoder input tokens/positions into model_input.
+        # Frozen dataclass fields cannot be modified, so use
+        # dataclasses.replace to construct a new model input
+        # instance.
+        model_input = dataclasses.replace(
+            model_input,
+            attn_metadata=attn_metadata,
+            encoder_input_tokens=encoder_input_tokens_tensor,
+            encoder_input_positions=encoder_input_positions_tensor,
+        )
+
+        generators = self.get_generators(finished_requests_ids)
+        sampling_metadata = SamplingMetadata.prepare(seq_group_metadata_list,
+                                                     model_input.seq_lens,
+                                                     model_input.query_lens,
+                                                     self.device,
+                                                     pin_memory=False,
+                                                     generators=generators,
+                                                     cache=self.sampling_metadata_cache)
+        is_prompt = (seq_group_metadata_list[0].is_prompt
+                     if seq_group_metadata_list else None)
+        return dataclasses.replace(model_input,
+                                   sampling_metadata=sampling_metadata,
+                                   is_prompt=is_prompt,
+                                   virtual_engine=virtual_engine)
+
+    @torch.inference_mode()
+    def profile_run(self) -> None:
+        # Enable top-k sampling to reflect the accurate memory usage.
+        sampling_params = SamplingParams(top_p=0.99, top_k=self.vocab_size - 1)
+        max_num_batched_tokens = self.scheduler_config.max_num_batched_tokens
+        max_num_seqs = self.scheduler_config.max_num_seqs
+
+        # Profile memory usage with max_num_sequences sequences and the total
+        # number of tokens equal to max_num_batched_tokens.
+        seqs: List[SequenceGroupMetadata] = []
+
+        max_mm_tokens = self.mm_registry.get_max_multimodal_tokens(
+            self.model_config)
+        if max_mm_tokens > 0:
+            logger.info("Starting profile run for multi-modal models.")
+
+        batch_size = 0
+        import os
+        self_max_num_batched_tokens = os.getenv("IPEX_LLM_SELF_MAX_NUM_BATCHED_TOKENS", None)
+        if self_max_num_batched_tokens is not None:
+            max_num_batched_tokens = int(self_max_num_batched_tokens)
+            self_max_num_seqs = os.getenv("IPEX_LLM_SELF_MAX_NUM_SEQS", None)
+            if self_max_num_seqs is not None:
+                max_num_seqs = int(self_max_num_seqs)
+            else:
+                max_num_seqs = 1
+        for group_id in range(max_num_seqs):
+            seq_len = (max_num_batched_tokens // max_num_seqs +
+                       (group_id < max_num_batched_tokens % max_num_seqs))
+            batch_size += seq_len
+
+            decoder_dummy_data = self.input_registry \
+                .dummy_data_for_profiling(self.model_config,
+                                          seq_len,
+                                          self.mm_registry,
+                                          is_encoder_data=False)
+            encoder_dummy_data = self.input_registry \
+                .dummy_data_for_profiling(self.model_config,
+                                          seq_len,
+                                          self.mm_registry,
+                                          is_encoder_data=True)
+
+            # Having more tokens is over-conservative but otherwise fine
+            assert len(
+                decoder_dummy_data.seq_data.prompt_token_ids
+            ) >= seq_len, (
+                f"Expected at least {seq_len} dummy tokens for profiling, "
+                f"but got: {len(decoder_dummy_data.seq_data.prompt_token_ids)}"
+            )
+
+            assert decoder_dummy_data.multi_modal_data is None or \
+            encoder_dummy_data.multi_modal_data is None, (
+                "Multi-modal data can't be provided in both encoder and decoder"
+            )
+
+            seq = SequenceGroupMetadata(
+                request_id=str(group_id),
+                is_prompt=True,
+                seq_data={group_id: decoder_dummy_data.seq_data},
+                sampling_params=sampling_params,
+                block_tables=None,
+                encoder_seq_data=encoder_dummy_data.seq_data,
+                cross_block_table=None,
+                multi_modal_data=decoder_dummy_data.multi_modal_data
+                or encoder_dummy_data.multi_modal_data,
+                multi_modal_placeholders=decoder_dummy_data.
+                multi_modal_placeholders
+                or encoder_dummy_data.multi_modal_placeholders)
+            seqs.append(seq)
+
+        finished_requests_ids = [seq.request_id for seq in seqs]
+        model_input = self.prepare_model_input(
+            seqs, finished_requests_ids=finished_requests_ids)
+        intermediate_tensors = None
+
+        num_layers = self.model_config.get_num_layers(self.parallel_config)
+        kv_caches = [None] * num_layers
+
+        self.execute_model(model_input, kv_caches, intermediate_tensors)
+        torch.xpu.synchronize()
+        return
+
+    def _prepare_encoder_model_input_tensors(
+        self,
+        seq_group_metadata_list: List[SequenceGroupMetadata],
+        model_input: XPUEncoderDecoderModelInput,
+    ) -> Tuple[AttentionMetadata, Optional[torch.Tensor],
+               Optional[torch.Tensor]]:
+        """Helper method to prepare the encoder- and cross-attn-related
+        model inputs based on a given sequence group. These additional inputs
+        are used to augment an already-computed `XPUEncoderDecoderModelInput`
+        data structure which already has decoder-related model inputs
+        populated.
+
+        Sets the following attn_metadata fields:
+        * `num_encoder_tokens`
+        * `encoder_seq_lens`
+        * `encoder_seq_lens_tensor`
+        * `max_encoder_seq_len`
+        * `cross_slot_mapping`
+        * `cross_block_tables`
+
+        Constructs a new model inputs data structure, based on
+        (1) the existing fields in the `model_inputs` argument,
+        and (2) the following additional fields which are
+        computed (or in the case of `attn_metadata`, updated)
+        by this function:
+        * attn_metadata
+        * encoder_input_tokens
+        * encoder_input_positions
+
+        Arguments:
+
+        * seq_group_metadata_list: list of sequence groups for which to
+                                   compute inputs
+        * model_inputs: model inputs data structure with decoder-oriented
+                        fields already computed.
+
+        Return:
+
+        * Updated model inputs data structure
+        """
+
+        if len(seq_group_metadata_list) == 0:
+            return (model_input.attn_metadata, None, None)
+
+        # Since we are not supporting chunked prefill either the entire
+        # batch is prefill or it is decode
+        is_prompt = seq_group_metadata_list[0].is_prompt
+
+        # Build encoder inputs
+        encoder_seq_lens: List[int] = []
+        if is_prompt:
+            # Prefill phase.
+            cross_block_tables = self._empty_int32_tensor().view(
+                len(seq_group_metadata_list), -1)
+
+            # Extract input tokens/positions, cross-attention slot-mapping,
+            # & seq len from each sequence group metadata
+            (
+                encoder_input_tokens,
+                encoder_input_positions,
+                cross_slot_mapping,
+            ) = (
+                [],
+                [],
+                [],
+            )
+            for seq_group_metadata in seq_group_metadata_list:
+                # Build seq lens
+                seq_len = seq_group_metadata.encoder_seq_data.get_len()
+                token_ids = seq_group_metadata.encoder_seq_data.get_token_ids()
+                encoder_seq_lens.append(seq_len)
+
+                # Build slot mapping
+                is_profile_run = (seq_group_metadata.block_tables is None)
+                if is_profile_run:
+                    # During memory profiling, the block tables are not
+                    # initialized yet. In this case, we just use a dummy
+                    # slot mapping.
+                    # In embeddings, the block tables are {seq_id: None}.
+                    cross_slot_mapping.extend([PAD_SLOT_ID] * seq_len)
+                else:
+                    for i in range(0, seq_len):
+                        block_number = seq_group_metadata.cross_block_table[
+                            i // self.block_size]
+                        block_offset = i % self.block_size
+                        slot = block_number * self.block_size + block_offset
+                        cross_slot_mapping.append(slot)
+
+                # Build encoder input tokens
+                encoder_input_tokens.extend(token_ids)
+                encoder_input_positions.extend(list(range(0, seq_len)))
+
+            # Convert tokens/positions & cross-attention
+            # slot-mapping to encoder input tensors
+            encoder_input_tokens_tensor = self._list_to_long_tensor(
+                encoder_input_tokens)
+            encoder_input_positions_tensor = self._list_to_long_tensor(
+                encoder_input_positions)
+            cross_slot_mapping_tensor = self._list_to_long_tensor(
+                cross_slot_mapping)
+
+        else:
+            # Decode phase.
+            encoder_input_tokens_tensor = self._empty_long_tensor()
+            encoder_input_positions_tensor = self._empty_long_tensor()
+            cross_slot_mapping_tensor = self._empty_long_tensor()
+            # Extract cross-attention block tables &
+            # seq len from each sequence group metadata.
+            # Cross-attention block tables are empty
+            # during vLLM memory profiling.
+            cross_block_tables = []
+            for seq_group_metadata in seq_group_metadata_list:
+                for _ in range(len(seq_group_metadata.seq_data)):
+                    encoder_seq_lens.append(
+                        seq_group_metadata.encoder_seq_data.get_len())
+                    cross_block_table = seq_group_metadata.cross_block_table
+                    cross_block_tables.append([] if (
+                        cross_block_table is None) else cross_block_table)
+
+            # if (model_input.attn_metadata is not None
+            #         and model_input.attn_metadata.use_cuda_graph and False):
+            if False:
+                # We will be using CUDA graph replay for this decode.
+                max_len_of_block_table = self.get_max_block_per_batch()
+                batch_size = len(encoder_seq_lens)
+                graph_batch_size = self.vllm_config.pad_for_cudagraph(
+                    batch_size)
+                assert graph_batch_size >= batch_size
+                cuda_graph_pad_size = graph_batch_size - batch_size
+                # extend the cross_block_tables and encoder_seq_lens to match
+                # the graph_batch_size.
+                cross_block_tables.extend([[]
+                                           for _ in range(cuda_graph_pad_size)
+                                           ])
+                encoder_seq_lens.extend(
+                    itertools.repeat(1, cuda_graph_pad_size))
+
+            else:
+                max_len_of_block_table = max(
+                    len(block_table) for block_table in cross_block_tables)
+
+            cross_block_tables = make_tensor_with_pad(
+                cross_block_tables,
+                max_len=max_len_of_block_table,
+                pad=0,
+                dtype=torch.int32,
+                device=self.device,
+            )
+
+        # Compute encoder sequence lengths & encoder
+        # sequence starting offset tensors
+        max_encoder_seq_len = max(encoder_seq_lens, default=0)
+        encoder_seq_lens_tensor = self._list_to_int32_tensor(encoder_seq_lens)
+        encoder_seq_start_loc = torch.zeros(encoder_seq_lens_tensor.shape[0] +
+                                            1,
+                                            dtype=torch.int32,
+                                            device=self.device)
+        torch.cumsum(encoder_seq_lens_tensor,
+                     dim=0,
+                     dtype=encoder_seq_start_loc.dtype,
+                     out=encoder_seq_start_loc[1:])
+
+        # Update attention metadata with encoder-oriented attributes
+        attn_metadata = model_input.attn_metadata
+        assert attn_metadata is not None
+        (
+            attn_metadata.num_encoder_tokens,
+            attn_metadata.encoder_seq_lens,
+            attn_metadata.encoder_seq_lens_tensor,
+            attn_metadata.max_encoder_seq_len,
+            attn_metadata.encoder_seq_start_loc,
+            attn_metadata.cross_slot_mapping,
+            attn_metadata.cross_block_tables,
+        ) = (
+            sum(encoder_seq_lens),
+            encoder_seq_lens,
+            encoder_seq_lens_tensor,
+            max_encoder_seq_len,
+            encoder_seq_start_loc,
+            cross_slot_mapping_tensor,
+            cross_block_tables,
+        )
+
+        return (attn_metadata, encoder_input_tokens_tensor,
+                encoder_input_positions_tensor)
diff --git a/vllm/worker/xpu_model_runner.py b/vllm/worker/xpu_model_runner.py
index 79fa7d2c7..2749cbf3a 100644
--- a/vllm/worker/xpu_model_runner.py
+++ b/vllm/worker/xpu_model_runner.py
@@ -5,7 +5,7 @@ import time
 import weakref
 from collections import defaultdict
 from dataclasses import dataclass
-from typing import (TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple,
+from typing import (TYPE_CHECKING, Any, Callable, Dict, List, Optional, Set, Tuple,
                     Type, TypeVar)
 
 import torch
@@ -17,15 +17,23 @@ from vllm.distributed import get_pp_group
 from vllm.forward_context import set_forward_context
 from vllm.inputs import INPUT_REGISTRY, InputRegistry
 from vllm.logger import init_logger
+from vllm.lora.layers import LoRAMapping
+from vllm.lora.request import LoRARequest
+from vllm.lora.worker_manager import LRUCacheWorkerLoRAManager
 from vllm.model_executor import SamplingMetadataCache
+from vllm.model_executor.layers.rotary_embedding import MRotaryEmbedding
 from vllm.model_executor.layers.sampler import SamplerOutput, get_sampler
+from vllm.model_executor.models import supports_lora, supports_multimodal
 from vllm.model_executor.model_loader import get_model
 from vllm.multimodal import (MULTIMODAL_REGISTRY, BatchedTensorInputs,
                              MultiModalKwargs, MultiModalPlaceholderMap,
                              MultiModalRegistry)
+from vllm.prompt_adapter.request import PromptAdapterRequest
 from vllm.sampling_params import SamplingParams
 from vllm.sequence import IntermediateTensors, SequenceGroupMetadata
-from vllm.utils import DeviceMemoryProfiler, GiB_bytes, make_tensor_with_pad
+from vllm.utils import (
+    DeviceMemoryProfiler, GiB_bytes, make_tensor_with_pad, PyObjectCache,
+    is_pin_memory_available, flatten_2d_lists)
 from vllm.worker.model_runner import AttentionMetadata, SamplingMetadata
 from vllm.worker.model_runner_base import (
     ModelRunnerBase, ModelRunnerInputBase, ModelRunnerInputBuilderBase,
@@ -37,6 +45,8 @@ from vllm.worker.model_runner_base import (
 if TYPE_CHECKING:
     from vllm.attention.backends.abstract import AttentionBackend
 
+LORA_WARMUP_RANK = 8
+
 logger = init_logger(__name__)
 
 _PAD_SLOT_ID = -1
@@ -53,15 +63,22 @@ class ModelInputForXPU(ModelRunnerInputBase):
     input_positions: Optional[torch.Tensor] = None
     attn_metadata: Optional["AttentionMetadata"] = None
     multi_modal_kwargs: Optional[BatchedTensorInputs] = None
+    request_ids_to_seq_ids: Optional[Dict[str, List[int]]] = None
+    finished_requests_ids: Optional[List[str]] = None
     virtual_engine: Optional[int] = None
     seq_lens: Optional[List[int]] = None
     query_lens: Optional[List[int]] = None
+    lora_mapping: Optional["LoRAMapping"] = None
+    lora_requests: Optional[Set[LoRARequest]] = None
     async_callback: Optional[Callable] = None
+    is_prompt: Optional[bool] = None
 
     def as_broadcastable_tensor_dict(self) -> Dict[str, Any]:
         tensor_dict = {
             "input_tokens": self.input_tokens,
             "input_positions": self.input_positions,
+            "lora_requests": self.lora_requests,
+            "lora_mapping": self.lora_mapping,
         }
         _add_attn_metadata_broadcastable_dict(tensor_dict, self.attn_metadata)
 
@@ -85,11 +102,14 @@ class ModelInputForXPUWithSamplingMetadata(ModelInputForXPU):
     Used by the ModelRunner.
     """
     sampling_metadata: Optional["SamplingMetadata"] = None
+    is_prompt: Optional[bool] = None
 
     def as_broadcastable_tensor_dict(self) -> Dict[str, Any]:
         tensor_dict = {
             "input_tokens": self.input_tokens,
             "input_positions": self.input_positions,
+            "lora_requests": self.lora_requests,
+            "lora_mapping": self.lora_mapping,
         }
         _add_attn_metadata_broadcastable_dict(tensor_dict, self.attn_metadata)
         _add_sampling_metadata_broadcastable_dict(tensor_dict,
@@ -110,6 +130,245 @@ class ModelInputForXPUWithSamplingMetadata(ModelInputForXPU):
 
 
 class ModelInputForXPUBuilder(ModelRunnerInputBuilderBase[ModelInputForXPU]):
+    class InterDataForSeqGroup:
+        """Intermediate data for the current sequence group."""
+
+        def simple_reinit(self):
+            self.input_tokens[0].clear()  # type: ignore
+            self.input_positions[0].clear()  # type: ignore
+            self.token_types[0].clear()  # type: ignore
+            self.mrope_input_positions = None  # type: ignore
+            self.seq_lens[0] = 0  # type: ignore
+            self.orig_seq_lens[0] = 0  # type: ignore
+            self.query_lens[0] = 0  # type: ignore
+            self.context_lens[0] = 0  # type: ignore
+            self.curr_sliding_window_blocks[0] = 0  # type: ignore
+            self.lora_index_mapping.clear()  # type: ignore
+            self.lora_prompt_mapping.clear()  # type: ignore
+            self.lora_requests.clear()  # type: ignore
+            self.prompt_adapter_index_mapping.clear()  # type: ignore
+            self.prompt_adapter_prompt_mapping.clear()  # type: ignore
+
+        def __init__(
+            self,
+            *,
+            # From sequence group metadata.
+            request_id: str,
+            seq_ids: List[int],
+            is_prompt: bool,
+            block_tables: Optional[Dict[int, List[int]]],
+            computed_block_nums: List[int],
+            n_seqs: int = 0,
+
+            # Input tokens and positions.
+            input_tokens: Optional[List[List[int]]] = None,
+            input_positions: Optional[List[List[int]]] = None,
+            token_types: Optional[List[List[int]]] = None,
+            mrope_input_positions: Optional[List[List[List[int]]]] = None,
+
+            # The sequence length (may be capped to the sliding window).
+            seq_lens: Optional[List[int]] = None,
+            # The original sequence length (before applying sliding window).
+            # This is used to compute slot mapping.
+            orig_seq_lens: Optional[List[int]] = None,
+            # The query length.
+            query_lens: Optional[List[int]] = None,
+            # The number of tokens that are already computed.
+            context_lens: Optional[List[int]] = None,
+            # The current sliding window block.
+            curr_sliding_window_blocks: Optional[List[int]] = None,
+
+            # LoRA inputs.
+            lora_index_mapping: Optional[List[List[int]]] = None,
+            lora_prompt_mapping: Optional[List[List[int]]] = None,
+            lora_requests: Optional[Set[LoRARequest]] = None,
+
+            # Prompt adapter inputs.
+            prompt_adapter_index_mapping: Optional[List[int]] = None,
+            prompt_adapter_prompt_mapping: Optional[List[int]] = None,
+            prompt_adapter_request: Optional[PromptAdapterRequest] = None,
+
+            # Multi-modal inputs.
+            multi_modal_kwargs: Optional[MultiModalKwargs] = None,
+            multi_modal_placeholder_maps: Optional[Dict[
+                str, MultiModalPlaceholderMap]] = None,
+
+            # Whether the prefix cache is hit (prefill only).
+            prefix_cache_hit: bool = False,
+            reinit: bool = False,
+            reinit_use_defaults: bool = False,
+            encoder_seq_len: int = 0,
+        ):
+            if reinit:
+                assert len(self.seq_ids) == len(seq_ids)  # type: ignore
+                for i, seq_id in enumerate(seq_ids):
+                    self.seq_ids[i] = seq_id  # type: ignore
+            else:
+                self.seq_ids = seq_ids
+
+            self.request_id = request_id
+            self.is_prompt = is_prompt
+            self.block_tables = block_tables
+            self.computed_block_nums = computed_block_nums
+            self.n_seqs = n_seqs
+            self.encoder_seq_len = encoder_seq_len
+
+            if reinit:
+                if len(self.seq_ids) == 1 and reinit_use_defaults:
+                    self.simple_reinit()
+                else:
+                    if input_tokens:
+                        self.input_tokens = input_tokens
+                    else:
+                        for seq_id in range(len(self.seq_ids)):
+                            self.input_tokens[seq_id].clear()
+
+                    if input_positions:
+                        self.input_positions = input_positions
+                    else:
+                        for seq_id in range(len(self.seq_ids)):
+                            self.input_positions[seq_id].clear()
+
+                    if token_types:
+                        self.token_types = token_types
+                    else:
+                        for seq_id in range(len(self.seq_ids)):
+                            self.token_types[seq_id].clear()
+
+                    self.mrope_input_positions = None
+
+                    if seq_lens:
+                        self.seq_lens = seq_lens
+                    else:
+                        for seq_id in range(len(self.seq_ids)):
+                            self.seq_lens[seq_id] = 0
+
+                    if orig_seq_lens:
+                        self.orig_seq_lens = orig_seq_lens
+                    else:
+                        for seq_id in range(len(self.seq_ids)):
+                            self.orig_seq_lens[seq_id] = 0
+
+                    if query_lens:
+                        self.query_lens = query_lens
+                    else:
+                        for seq_id in range(len(self.seq_ids)):
+                            self.query_lens[seq_id] = 0
+
+                    if context_lens:
+                        self.context_lens = context_lens
+                    else:
+                        for seq_id in range(len(self.seq_ids)):
+                            self.context_lens[seq_id] = 0
+
+                    if curr_sliding_window_blocks:
+                        self.curr_sliding_window_blocks = \
+                            curr_sliding_window_blocks
+                    else:
+                        for seq_id in range(len(self.seq_ids)):
+                            self.curr_sliding_window_blocks[seq_id] = 0
+
+                    if lora_index_mapping:
+                        self.lora_index_mapping = lora_index_mapping
+                    else:
+                        self.lora_index_mapping.clear()
+
+                    if lora_prompt_mapping:
+                        self.lora_prompt_mapping = lora_prompt_mapping
+                    else:
+                        self.lora_prompt_mapping.clear()
+
+                    if lora_requests:
+                        self.lora_requests = lora_requests
+                    else:
+                        self.lora_requests.clear()
+
+                    if prompt_adapter_index_mapping:
+                        self.prompt_adapter_index_mapping = \
+                            prompt_adapter_index_mapping
+                    else:
+                        self.prompt_adapter_index_mapping.clear()
+
+                    if prompt_adapter_prompt_mapping:
+                        self.prompt_adapter_prompt_mapping = \
+                            prompt_adapter_prompt_mapping
+                    else:
+                        self.prompt_adapter_prompt_mapping.clear()
+
+            else:
+                self.input_tokens = input_tokens or []
+                self.input_positions = input_positions or []
+                self.token_types = token_types or []
+                self.mrope_input_positions = mrope_input_positions or None
+                self.seq_lens = seq_lens or []
+                self.orig_seq_lens = orig_seq_lens or []
+                self.query_lens = query_lens or []
+                self.context_lens = context_lens or []
+                self.curr_sliding_window_blocks = \
+                    curr_sliding_window_blocks or []
+
+                self.lora_index_mapping = lora_index_mapping or []
+                self.lora_prompt_mapping = lora_prompt_mapping or []
+                self.lora_requests = lora_requests or set()
+
+                self.prompt_adapter_index_mapping = (
+                    prompt_adapter_index_mapping or [])
+                self.prompt_adapter_prompt_mapping = (
+                    prompt_adapter_prompt_mapping or [])
+
+            self.prompt_adapter_request = prompt_adapter_request
+            self.multi_modal_kwargs = multi_modal_kwargs
+            self.multi_modal_placeholder_maps = multi_modal_placeholder_maps
+            self.prefix_cache_hit = prefix_cache_hit
+
+            self.n_seqs = len(self.seq_ids)
+
+            if not reinit:
+                self.__post_init__()
+
+        def __post_init__(self):
+            self.n_seqs = len(self.seq_ids)
+
+            self.input_tokens = [[] for _ in range(self.n_seqs)]
+            self.input_positions = [[] for _ in range(self.n_seqs)]
+            self.token_types = [[] for _ in range(self.n_seqs)]
+            self.mrope_input_positions = None
+            self.seq_lens = [0] * self.n_seqs
+            self.orig_seq_lens = [0] * self.n_seqs
+            self.query_lens = [0] * self.n_seqs
+            self.context_lens = [0] * self.n_seqs
+            self.curr_sliding_window_blocks = [0] * self.n_seqs
+
+            self.lora_index_mapping = []
+            self.lora_prompt_mapping = []
+
+    def gen_inter_data_builder(self, num_seqs: int):
+        return lambda: ModelInputForXPUBuilder.InterDataForSeqGroup(
+            request_id="",
+            seq_ids=[0] * num_seqs,
+            is_prompt=True,
+            block_tables=None,
+            computed_block_nums=[])
+
+    def init_cached_inter_data(self, *args, **kwargs):
+        assert len(args) == 0
+        assert "seq_ids" in kwargs
+        seq_ids = kwargs["seq_ids"]
+        num_seqs = len(seq_ids)
+
+        # The inter-data cache is per model_runner
+        inter_data_cache = self.runner.inter_data_cache
+        if num_seqs not in inter_data_cache:
+            inter_data_cache[num_seqs] = PyObjectCache(
+                self.gen_inter_data_builder(num_seqs))
+
+        obj = inter_data_cache[num_seqs].get_object()
+        obj.__init__(*args, **kwargs)
+        return obj
+
+    def reset_cached_inter_data(self):
+        for cache in self.runner.inter_data_cache.values():
+            cache.reset()
 
     def __init__(self,
                  runner: "XPUModelRunner",
@@ -121,15 +380,139 @@ class ModelInputForXPUBuilder(ModelRunnerInputBuilderBase[ModelInputForXPU]):
         self.sliding_window = self.runner.sliding_window
         self.block_size = self.runner.block_size
         self.device = self.runner.device
+        self.enable_lora = self.runner.lora_config is not None
+        self.scheduler_config = self.runner.scheduler_config
+        self.chunked_prefill_enabled = (
+            self.scheduler_config is not None
+            and self.scheduler_config.chunked_prefill_enabled)
 
     def prepare(self,
                 finished_requests_ids: Optional[List[str]] = None) -> None:
         self.seq_group_metadata_list: List[SequenceGroupMetadata] = []
+        self.decode_only = True
+        # Intermediate data (data in CPU before going to GPU) for
+        # the current sequence group.
+        self.inter_data_list: List[
+            ModelInputForXPUBuilder.InterDataForSeqGroup] = []
+
+    def _compute_lens(self, inter_data: InterDataForSeqGroup, seq_idx: int,
+                      seq_group_metadata: SequenceGroupMetadata):
+        """Compute context length, sequence length and tokens
+        for the given sequence data.
+        """
+        seq_data = seq_group_metadata.seq_data[inter_data.seq_ids[seq_idx]]
+        token_chunk_size = seq_group_metadata.token_chunk_size
+
+        # Compute context length (the number of tokens that are
+        # already computed) and sequence length (total number of tokens).
+
+        seq_len = seq_data.get_len()
+        if inter_data.is_prompt:
+            context_len = seq_data.get_num_computed_tokens()
+            seq_len = min(seq_len, context_len + token_chunk_size)
+        elif self.runner.scheduler_config.is_multi_step or \
+            self.runner.model_config.is_encoder_decoder:
+            context_len = seq_len - 1
+        else:
+            context_len = seq_data.get_num_computed_tokens()
+
+        # Compute tokens.
+        tokens = seq_data.get_token_ids()[context_len:seq_len]
+        token_types = seq_group_metadata.token_type_ids
+
+        inter_data.seq_lens[seq_idx] = seq_len
+        inter_data.orig_seq_lens[seq_idx] = seq_len
+        inter_data.context_lens[seq_idx] = context_len
+        inter_data.input_tokens[seq_idx].extend(tokens)
+        inter_data.input_positions[seq_idx].extend(range(context_len, seq_len))
+        inter_data.token_types[seq_idx].extend(
+            token_types if token_types else [])
+        inter_data.query_lens[seq_idx] = seq_len - context_len
+
+        if seq_data.mrope_position_delta is not None:
+            if inter_data.mrope_input_positions is None:
+                inter_data.mrope_input_positions = [None] * inter_data.n_seqs
+
+            inter_data.mrope_input_positions[
+                seq_idx] = MRotaryEmbedding.get_next_input_positions(
+                    seq_data.mrope_position_delta,
+                    context_len,
+                    seq_len,
+                )
+
+    def _compute_lora_input(self, inter_data: InterDataForSeqGroup,
+                            seq_idx: int,
+                            seq_group_metadata: SequenceGroupMetadata):
+        """If LoRA is enabled, compute LoRA index and prompt mapping."""
+        if not self.enable_lora:
+            return
+
+        lora_id = seq_group_metadata.lora_int_id
+        if lora_id > 0:
+            inter_data.lora_requests.add(seq_group_metadata.lora_request)
+        query_len = inter_data.query_lens[seq_idx]
+        inter_data.lora_index_mapping.append([lora_id] * query_len)
+        sampling_params = seq_group_metadata.sampling_params
+        if sampling_params and sampling_params.prompt_logprobs is not None:
+            inter_data.lora_prompt_mapping.append([lora_id] * query_len)
+        elif not self.chunked_prefill_enabled or seq_group_metadata.do_sample:
+            inter_data.lora_prompt_mapping.append([lora_id])
+        else:
+            inter_data.lora_prompt_mapping.append([])
 
     def add_seq_group(self, seq_group_metadata: SequenceGroupMetadata):
+        seq_ids = seq_group_metadata.seq_data.keys()
+        n_seqs = len(seq_ids)
+        is_prompt = seq_group_metadata.is_prompt
+
+        if is_prompt:
+            assert n_seqs == 1
+            self.decode_only = False
+
+        encoder_seq_len = 0
+
+        if self.runner.model_config.is_encoder_decoder:
+            encoder_seq_len = seq_group_metadata.encoder_seq_data.get_len()
+
+        inter_data = self.init_cached_inter_data(
+            request_id=seq_group_metadata.request_id,
+            seq_ids=seq_ids,
+            is_prompt=is_prompt,
+            block_tables=seq_group_metadata.block_tables,
+            computed_block_nums=seq_group_metadata.computed_block_nums,
+            reinit=True,
+            reinit_use_defaults=True,
+            encoder_seq_len=encoder_seq_len)
+
+        self.inter_data_list.append(inter_data)
+
+        for seq_idx in range(n_seqs):
+            self._compute_lens(inter_data, seq_idx, seq_group_metadata)
+            self._compute_lora_input(inter_data, seq_idx, seq_group_metadata)
+
         self.seq_group_metadata_list.append(seq_group_metadata)
 
     def build(self) -> ModelInputForXPU:
+        # LoRA data.
+        lora_requests = set()
+        lora_mapping = None
+        if self.enable_lora:
+            lora_requests = set(r for data in self.inter_data_list
+                                for r in data.lora_requests)
+            lora_index_mapping = flatten_2d_lists([
+                flatten_2d_lists(inter_data.lora_index_mapping)
+                for inter_data in self.inter_data_list
+            ])
+            lora_prompt_mapping = flatten_2d_lists([
+                flatten_2d_lists(inter_data.lora_prompt_mapping)
+                for inter_data in self.inter_data_list
+            ])
+
+            lora_mapping = LoRAMapping(
+                **dict(index_mapping=lora_index_mapping,
+                       prompt_mapping=lora_prompt_mapping,
+                       is_prefill=not self.decode_only))
+
         is_prompt = self.seq_group_metadata_list[0].is_prompt
         # Prepare input tensors.
         if is_prompt:
@@ -150,6 +533,8 @@ class ModelInputForXPUBuilder(ModelRunnerInputBuilderBase[ModelInputForXPU]):
             multi_modal_kwargs=multi_modal_kwargs,
             seq_lens=seq_lens,
             query_lens=seq_lens,
+            lora_mapping=lora_mapping,
+            lora_requests=lora_requests,
         )
 
     def _prepare_prompt(
@@ -199,7 +584,7 @@ class ModelInputForXPUBuilder(ModelRunnerInputBuilderBase[ModelInputForXPU]):
                     multi_modal_placeholder_maps[modality].extend(
                         placeholder_map)
 
-            if seq_group_metadata.block_tables is None:
+            if seq_group_metadata.block_tables is None or seq_group_metadata.block_tables[seq_id] is None:
                 # During memory profiling, the block tables are not initialized
                 # yet. In this case, we just use a dummy slot mapping.
                 slot_mapping.extend([_PAD_SLOT_ID] * seq_len)
@@ -249,6 +634,15 @@ class ModelInputForXPUBuilder(ModelRunnerInputBuilderBase[ModelInputForXPU]):
         tmp.extend(seq_lens)
         seqlen = torch.tensor(tmp)
         seqlen_q = torch.cumsum(seqlen, dim=0).to(device=self.device)
+        seq_lens_tensor = torch.tensor(seq_lens,
+                                       dtype=torch.int,
+                                       device=self.device)
+        # if "reranker" in self.runner.model_config.model.lower():
+        #     seq_lens_tensor = torch.tensor(seq_lens,
+        #                                    dtype=torch.int,
+        #                                    device=self.device)
+        # else:
+        #     seq_lens_tensor = torch.tensor([])
 
         attn_metadata = self.attn_backend.make_metadata(
             is_prompt=True,
@@ -258,7 +652,7 @@ class ModelInputForXPUBuilder(ModelRunnerInputBuilderBase[ModelInputForXPU]):
             seq_lens=seq_lens,
             seqlen_q=seqlen_q,
             max_seqlen=max_seqlen,
-            seq_lens_tensor=torch.tensor([]),
+            seq_lens_tensor=seq_lens_tensor,
             max_decode_seq_len=0,
             num_prefills=len(seq_lens),
             num_prefill_tokens=num_prompt_tokens,
@@ -356,11 +750,11 @@ class ModelInputForXPUBuilder(ModelRunnerInputBuilderBase[ModelInputForXPU]):
             attn_metadata,
         )
 
+# Add XPUModelRUnnerBase to support embedding models
+class XPUModelRunnerBase(ModelRunnerBase[TModelInputForXPU]):
 
-class XPUModelRunner(ModelRunnerBase[ModelInputForXPUWithSamplingMetadata]):
-    _model_input_cls: Type[ModelInputForXPUWithSamplingMetadata] = (
-        ModelInputForXPUWithSamplingMetadata)
-    _builder_cls: Type[ModelInputForXPUBuilder] = ModelInputForXPUBuilder
+    _model_input_cls: Type[TModelInputForXPU]
+    _builder_cls: Type[ModelInputForXPUBuilder]
 
     def __init__(
         self,
@@ -379,6 +773,7 @@ class XPUModelRunner(ModelRunnerBase[ModelInputForXPUWithSamplingMetadata]):
         self.return_hidden_states = return_hidden_states
 
         self.device = self.device_config.device
+        self.pin_memory = is_pin_memory_available()
 
         self.kv_cache_dtype = kv_cache_dtype
         self.sliding_window = model_config.get_sliding_window()
@@ -399,6 +794,11 @@ class XPUModelRunner(ModelRunnerBase[ModelInputForXPUWithSamplingMetadata]):
         # Lazy initialization.
         self.model: nn.Module  # Set after init_Model
         self.sampler = get_sampler()
+        # Set after load_model.
+        self.lora_manager: Optional[LRUCacheWorkerLoRAManager] = None
+
+        # Used to cache python objects
+        self.inter_data_cache: Dict[int, PyObjectCache] = {}
 
         self.sampling_metadata_cache: SamplingMetadataCache = \
               SamplingMetadataCache() \
@@ -414,6 +814,25 @@ class XPUModelRunner(ModelRunnerBase[ModelInputForXPUWithSamplingMetadata]):
         logger.info("Loading model weights took %.4f GiB",
                     self.model_memory_usage / GiB_bytes)
 
+        if self.lora_config:
+            assert supports_lora(self.model), "Model does not support LoRA"
+            assert not supports_multimodal(
+                self.model
+            ), "To be tested: Multi-modal model with LoRA settings."
+
+            self.lora_manager = LRUCacheWorkerLoRAManager(
+                self.scheduler_config.max_num_seqs,
+                self.scheduler_config.max_num_batched_tokens,
+                self.vocab_size,
+                self.lora_config,
+                self.device,
+                self.model.embedding_modules,
+                self.model.embedding_padding_modules,
+                max_position_embeddings=self.model.config.
+                max_position_embeddings,
+            )
+            self.model = self.lora_manager.create_lora_manager(self.model)
+
     def get_model(self) -> nn.Module:
         return self.model
 
@@ -428,6 +847,30 @@ class XPUModelRunner(ModelRunnerBase[ModelInputForXPUWithSamplingMetadata]):
         max_num_batched_tokens = self.scheduler_config.max_num_batched_tokens
         max_num_seqs = self.scheduler_config.max_num_seqs
 
+        # This represents the maximum number of different requests
+        # that will have unique loras, an therefore the max amount of memory
+        # consumption create dummy lora request copies from the lora request
+        # passed in, which contains a lora from the lora warmup path.
+        dummy_lora_requests: List[LoRARequest] = []
+        dummy_lora_requests_per_seq: List[LoRARequest] = []
+        if self.lora_config:
+            assert self.lora_manager is not None
+            with self.lora_manager.dummy_lora_cache():
+                for idx in range(self.lora_config.max_loras):
+                    lora_id = idx + 1
+                    dummy_lora_request = LoRARequest(
+                        lora_name=f"warmup_{lora_id}",
+                        lora_int_id=lora_id,
+                        lora_path="/not/a/real/path",
+                    )
+                    self.lora_manager.add_dummy_lora(dummy_lora_request,
+                                                     rank=LORA_WARMUP_RANK)
+                    dummy_lora_requests.append(dummy_lora_request)
+                dummy_lora_requests_per_seq = [
+                    dummy_lora_requests[idx % len(dummy_lora_requests)]
+                    for idx in range(max_num_seqs)
+                ]
+
         # Profile memory usage with max_num_sequences sequences and the total
         # number of tokens equal to max_num_batched_tokens.
         seqs: List[SequenceGroupMetadata] = []
@@ -468,7 +911,8 @@ class XPUModelRunner(ModelRunnerBase[ModelInputForXPUWithSamplingMetadata]):
                 seq_data={group_id: dummy_data.seq_data},
                 sampling_params=sampling_params,
                 block_tables=None,
-                lora_request=None,
+                lora_request=dummy_lora_requests_per_seq[group_id]
+                if dummy_lora_requests_per_seq else None,
                 multi_modal_data=dummy_data.multi_modal_data,
                 multi_modal_placeholders=dummy_data.multi_modal_placeholders)
             seqs.append(seq)
@@ -486,16 +930,6 @@ class XPUModelRunner(ModelRunnerBase[ModelInputForXPUWithSamplingMetadata]):
         torch.xpu.synchronize()
         return
 
-    def make_model_input_from_broadcasted_tensor_dict(
-            self,
-            tensor_dict: Dict[str,
-                              Any]) -> ModelInputForXPUWithSamplingMetadata:
-        return (
-            ModelInputForXPUWithSamplingMetadata.from_broadcasted_tensor_dict(
-                tensor_dict,
-                attn_backend=self.attn_backend,
-            ))
-
     def _prepare_model_input_tensors(
         self,
         seq_group_metadata_list: List[SequenceGroupMetadata],
@@ -511,8 +945,27 @@ class XPUModelRunner(ModelRunnerBase[ModelInputForXPUWithSamplingMetadata]):
         for seq_group_metadata in seq_group_metadata_list:
             builder.add_seq_group(seq_group_metadata)
 
+        self.builder.reset_cached_inter_data()
+
         return builder.build()  # type: ignore
 
+
+class XPUModelRunner(XPUModelRunnerBase[ModelInputForXPUWithSamplingMetadata]):
+
+    _model_input_cls: Type[ModelInputForXPUWithSamplingMetadata] = (
+        ModelInputForXPUWithSamplingMetadata)
+    _builder_cls: Type[ModelInputForXPUBuilder] = ModelInputForXPUBuilder
+
+    def make_model_input_from_broadcasted_tensor_dict(
+            self,
+            tensor_dict: Dict[str,
+                              Any]) -> ModelInputForXPUWithSamplingMetadata:
+        return (
+            ModelInputForXPUWithSamplingMetadata.from_broadcasted_tensor_dict(
+                tensor_dict,
+                attn_backend=self.attn_backend,
+            ))
+
     def prepare_model_input(
         self,
         seq_group_metadata_list: List[SequenceGroupMetadata],
@@ -527,17 +980,20 @@ class XPUModelRunner(ModelRunnerBase[ModelInputForXPUWithSamplingMetadata]):
             seq_group_metadata_list, finished_requests_ids)
         # Sampling metadata is only required for the final pp group
         generators = self.get_generators(finished_requests_ids)
-        sampling_metadata = SamplingMetadata.prepare(
-            seq_group_metadata_list,
-            model_input.seq_lens,
-            model_input.query_lens,
-            self.device,
-            pin_memory=False,
-            generators=generators,
-            cache=self.sampling_metadata_cache)
-
+        if get_pp_group().is_last_rank:
+            # Sampling metadata is only required for the final pp group
+            generators = self.get_generators(finished_requests_ids)
+            sampling_metadata = SamplingMetadata.prepare(
+                seq_group_metadata_list, model_input.seq_lens,
+                model_input.query_lens, self.device, self.pin_memory,
+                generators, self.sampling_metadata_cache)
+        else:
+            sampling_metadata = None
+        is_prompt = (seq_group_metadata_list[0].is_prompt
+             if seq_group_metadata_list else None)
         return dataclasses.replace(model_input,
                                    sampling_metadata=sampling_metadata,
+                                   is_prompt=is_prompt,
                                    virtual_engine=virtual_engine)
 
     @torch.inference_mode()
@@ -552,6 +1008,12 @@ class XPUModelRunner(ModelRunnerBase[ModelInputForXPUWithSamplingMetadata]):
             raise ValueError(
                 "XPUModelRunner does not support multi-step execution.")
 
+        if self.lora_config:
+            assert model_input.lora_requests is not None
+            assert model_input.lora_mapping is not None
+            self.set_active_loras(model_input.lora_requests,
+                                  model_input.lora_mapping)
+
         model_executable = self.model
         if (self.observability_config is not None
                 and self.observability_config.collect_model_forward_time):
@@ -604,3 +1066,14 @@ class XPUModelRunner(ModelRunnerBase[ModelInputForXPUWithSamplingMetadata]):
             output.model_forward_time = model_forward_time
 
         return [output]
+
+    def set_active_loras(self, lora_requests: Set[LoRARequest],
+                         lora_mapping: LoRAMapping) -> None:
+        if not self.lora_manager:
+            raise RuntimeError("LoRA is not enabled.")
+        self.lora_manager.set_active_adapters(lora_requests, lora_mapping)
+
+    def add_lora(self, lora_request: LoRARequest) -> bool:
+        if not self.lora_manager:
+            raise RuntimeError("LoRA is not enabled.")
+        return self.lora_manager.add_adapter(lora_request)
diff --git a/vllm/worker/xpu_pooling_model_runner.py b/vllm/worker/xpu_pooling_model_runner.py
new file mode 100644
index 000000000..be3887d0b
--- /dev/null
+++ b/vllm/worker/xpu_pooling_model_runner.py
@@ -0,0 +1,124 @@
+import dataclasses
+from typing import Any, Dict, List, Optional, Tuple, Type, Union
+
+import torch
+
+from vllm.forward_context import set_forward_context
+from vllm.model_executor.pooling_metadata import PoolingMetadata
+from vllm.multimodal import MultiModalKwargs
+from vllm.pooling_params import PoolingParams
+from vllm.sequence import (IntermediateTensors, PoolerOutput, SequenceData,
+                           SequenceGroupMetadata)
+# from vllm.worker.cpu_model_runner import (CPUModelRunnerBase, ModelInputForCPU,
+                                        #   ModelInputForCPUBuilder)
+from vllm.worker.xpu_model_runner import ModelInputForXPU, XPUModelRunnerBase, ModelInputForXPUBuilder
+
+
+@dataclasses.dataclass(frozen=True)
+class ModelInputForXPUWithPoolingMetadata(ModelInputForXPU):
+    """
+    Used by the XPUPoolingModelRunner.
+    """
+    pooling_metadata: Optional["PoolingMetadata"] = None
+
+
+class XPUPoolingModelRunner(
+        XPUModelRunnerBase[ModelInputForXPUWithPoolingMetadata]):
+    _model_input_cls: Type[ModelInputForXPUWithPoolingMetadata] = (
+        ModelInputForXPUWithPoolingMetadata)
+    _builder_cls: Type[ModelInputForXPUBuilder] = ModelInputForXPUBuilder
+
+    @torch.inference_mode()
+    def execute_model(
+        self,
+        model_input: ModelInputForXPUWithPoolingMetadata,
+        kv_caches: List[torch.Tensor],
+        intermediate_tensors: Optional[IntermediateTensors] = None,
+        num_steps: int = 1,
+    ) -> Optional[Union[List[PoolerOutput], IntermediateTensors]]:
+        if num_steps > 1:
+            raise ValueError(
+                "Currently multi-step worker does not support multi-steps...")
+
+        model_executable = self.model
+        cross_enc_kwargs = {}
+        # if model_input.token_type_ids is not None:
+        #     cross_enc_kwargs["token_type_ids"] = model_input.token_type_ids
+        execute_model_kwargs = {
+            "input_ids":
+            model_input.input_tokens,
+            "positions":
+            model_input.input_positions,
+            # "kv_caches":
+            # kv_caches,
+            # "attn_metadata":
+            # model_input.attn_metadata,
+            **MultiModalKwargs.as_kwargs(model_input.multi_modal_kwargs or {},
+                                         device=self.device),
+            **cross_enc_kwargs,
+            "intermediate_tensors":
+            intermediate_tensors,
+        }
+
+        with set_forward_context(model_input.attn_metadata, self.vllm_config):
+            hidden_states = model_executable(**execute_model_kwargs)
+
+        # Only perform pooling in the driver worker.
+        if not self.is_driver_worker:
+            return []
+
+        return [
+            self.model.pooler(hidden_states=hidden_states,
+                              pooling_metadata=model_input.pooling_metadata)
+        ]
+
+    def make_model_input_from_broadcasted_tensor_dict(
+            self,
+            tensor_dict: Dict[str,
+                              Any]) -> ModelInputForXPUWithPoolingMetadata:
+        return ModelInputForXPUWithPoolingMetadata.from_broadcasted_tensor_dict(
+            tensor_dict,
+            attn_backend=self.attn_backend,
+        )
+
+    def prepare_model_input(
+        self,
+        seq_group_metadata_list: Optional[List[SequenceGroupMetadata]],
+        virtual_engine: int = 0,
+        finished_requests_ids: Optional[List[str]] = None
+    ) -> ModelInputForXPUWithPoolingMetadata:
+        assert seq_group_metadata_list is not None
+        model_input = self._prepare_model_input_tensors(
+            seq_group_metadata_list, finished_requests_ids)
+        # Prepare PoolingMetadata.
+        assert model_input.seq_lens is not None
+        pooling_metadata = self._prepare_pooling(seq_group_metadata_list,
+                                                 model_input.seq_lens)
+
+        return dataclasses.replace(model_input,
+                                   virtual_engine=virtual_engine,
+                                   pooling_metadata=pooling_metadata)
+
+    def _prepare_pooling(
+        self,
+        seq_group_metadata_list: List[SequenceGroupMetadata],
+        prompt_lens: List[int],
+    ) -> PoolingMetadata:
+        """Prepare PoolingMetadata for the sequence group metadata list."""
+        seq_groups: List[Tuple[List[int], PoolingParams]] = []
+        for i, seq_group_metadata in enumerate(seq_group_metadata_list):
+            seq_ids = list(seq_group_metadata.seq_data.keys())
+            pooling_params = seq_group_metadata.pooling_params
+            seq_groups.append((seq_ids, pooling_params))
+
+        seq_data: Dict[int, SequenceData] = {}
+        for seq_group_metadata in seq_group_metadata_list:
+            seq_data.update(seq_group_metadata.seq_data)
+
+        pooling_metadata = PoolingMetadata(
+            seq_groups=seq_groups,
+            seq_data=seq_data,
+            prompt_lens=prompt_lens,
+        )
+
+        return pooling_metadata
\ No newline at end of file
diff --git a/vllm/worker/xpu_worker.py b/vllm/worker/xpu_worker.py
index a5109a982..f7ed6c324 100644
--- a/vllm/worker/xpu_worker.py
+++ b/vllm/worker/xpu_worker.py
@@ -2,10 +2,8 @@
 """A XPU worker class."""
 import gc
 import os
-from typing import List, Optional, Tuple
+from typing import List, Optional, Tuple, Type
 
-import intel_extension_for_pytorch  # noqa: F401
-import oneccl_bindings_for_pytorch  # noqa: F401
 import torch
 import torch.distributed
 
@@ -16,15 +14,18 @@ from vllm.distributed.parallel_state import get_pp_group
 from vllm.logger import init_logger
 from vllm.model_executor import set_random_seed
 from vllm.platforms import current_platform
+from vllm.utils import supports_xccl
 from vllm.worker.cache_engine import CacheEngine
 from vllm.worker.worker import Worker
-from vllm.worker.worker_base import LoRANotSupportedWorkerBase, WorkerBase
-from vllm.worker.xpu_model_runner import XPUModelRunner
+from vllm.worker.worker_base import WorkerBase
+from vllm.worker.xpu_model_runner import XPUModelRunner, XPUModelRunnerBase
+from vllm.worker.xpu_pooling_model_runner import XPUPoolingModelRunner
+from vllm.worker.xpu_enc_dec_model_runner import XPUEncoderDecoderModelRunner
 
 logger = init_logger(__name__)
 
 
-class XPUWorker(LoRANotSupportedWorkerBase, Worker):
+class XPUWorker(Worker):
     """A worker class that executes (a partition of) the model on a GPU.
 
     Each worker is associated with a single XPU device. The worker is 
@@ -57,7 +58,16 @@ class XPUWorker(LoRANotSupportedWorkerBase, Worker):
             assert rank % parallel_config.tensor_parallel_size == 0, \
                    "Driver worker should be rank 0 of tensor parallel group."
 
-        self.model_runner = XPUModelRunner(  # type: ignore
+        ModelRunnerClass: Type[XPUModelRunnerBase] = XPUModelRunner
+
+        model_config = self.model_config
+        print(model_config.task)
+        if model_config.task == "embed":
+            ModelRunnerClass = XPUPoolingModelRunner
+        elif model_config.is_encoder_decoder:
+            ModelRunnerClass = XPUEncoderDecoderModelRunner
+
+        self.model_runner = ModelRunnerClass(  # type: ignore
             vllm_config=vllm_config,
             kv_cache_dtype=self.cache_config.cache_dtype,
             is_driver_worker=is_driver_worker,
@@ -65,7 +75,7 @@ class XPUWorker(LoRANotSupportedWorkerBase, Worker):
         # Uninitialized cache engine. Will be initialized by
         # initialize_cache.
         self.cache_engine: List[CacheEngine]
-        self.gpu_cache: Optional[List[List[torch.Tensor]]]
+        self.gpu_cache: Optional[List[List[torch.Tensor]]] = None
 
     def init_device(self) -> None:
         if self.device_config.device.type == "xpu" and current_platform.is_xpu(
@@ -109,6 +119,7 @@ class XPUWorker(LoRANotSupportedWorkerBase, Worker):
         # profiled peak memory.
         torch.xpu.synchronize()
         used_memory = torch.xpu.memory_allocated()
+        used_memory = torch.xpu.memory_reserved()
         total_gpu_memory = torch.xpu.get_device_properties(
             self.local_rank).total_memory
         free_gpu_memory = total_gpu_memory - used_memory
@@ -160,6 +171,8 @@ class XPUWorker(LoRANotSupportedWorkerBase, Worker):
             # use sockets as default Level zero IPC exchange backend. By
             # default oneccl will use `drmfd` as mechanism which need extra
             # dependency (libdrm and drm headers) on your system.
+            default_backend = "xccl" if supports_xccl() else "ccl"
+            XPU_CCL_BACKEND = os.getenv("XPU_CCL_BACKEND", default_backend)
             ENV_CCL_ATL_TRANSPORT = os.getenv("CCL_ATL_TRANSPORT", "ofi")
             ENV_LOCAL_WORLD_SIZE = os.getenv("LOCAL_WORLD_SIZE",
                                              str(parallel_config.world_size))
@@ -171,7 +184,7 @@ class XPUWorker(LoRANotSupportedWorkerBase, Worker):
                 rank=rank,
                 distributed_init_method=distributed_init_method,
                 local_rank=self.local_rank,
-                backend="ccl")
+                backend=XPU_CCL_BACKEND)
 
         ensure_model_parallel_initialized(
             parallel_config.tensor_parallel_size,
